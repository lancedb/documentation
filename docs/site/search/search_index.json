{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LanceDB Documentation Portal","text":"<p>LanceDB is an open-source multimodal database designed for efficient storage, retrieval, and management of vectors and multimodal data. Use our documentation to build a production-grade service with a simple API for inserting, querying, and filtering vectors. </p> <p>LanceDB is ideal for semantic search, recommendation systems, and Generative AI applications. </p>"},{"location":"#product-release-announcing-lancedb-cloud","title":"Product Release: Announcing LanceDB Cloud!","text":"<ol> <li>LanceDB Cloud: Create a LanceDB Cloud cluster that's completely serverless. Your application simply connects over HTTP/HTTPS, and LanceDB handles scaling, storage, and indexing for you. </li> <li>LanceDB OSS: If self-hosting, embed LanceDB directly into your backend, whether that\u2019s a Django, Flask, FastAPI, or Node.js service. Check out our available client libraries.</li> <li>LanceDB Enterprise: Build the best private environment that suits your needs. Manage your own clusters via our Dashboard UI, but continue to run them within your own private infrastructure for complete security and sovereignty.</li> </ol>"},{"location":"#first-time-users","title":"First-Time Users:","text":"Tutorial Description Tutorial Description Connect to your database Configure local or cloud storage Build a RAG app Create a question-answering system Ingest blob storage data Load data from S3, GCS, Azure and more Build search queries Perform vector search and filtering"},{"location":"api/","title":"LanceDB API Reference","text":"<p>LanceDB provides multiple ways to interact with your vector database:</p> <ol> <li>LanceDB Cloud REST API - A RESTful API service for cloud-hosted LanceDB instances</li> <li>Client SDKs - Native language bindings for direct integration</li> </ol>"},{"location":"api/#lancedb-cloud-rest-api","title":"LanceDB Cloud REST API","text":"<p>The REST API allows you to interact with LanceDB Cloud instances using standard HTTP requests. This is ideal for building web applications, cross-platform integrations, serverless architectures and anguage-agnostic implementations. The complete API specification is available here:</p> LanceDB Cloud Documentation Link REST API REST API Documentation"},{"location":"api/#client-sdks","title":"Client SDKs","text":"<p>The SDKs provide a type-safe interfaces, native data structure integrations, advanced querying capabilities and better performance through optimized protocols.</p> <p>For tighter integration with your application code, LanceDB provides native SDK libraries in multiple languages:</p> LanceDB SDKs Documentation Link Description Python Python SDK Documentation Full-featured Python client with pandas &amp; numpy integration JavaScript (@lancedb/lancedb package) Current JS Documentation Modern JavaScript/TypeScript SDK for Node.js and browsers JavaScript (legacy vectordb package) Legacy JS Documentation Legacy JavaScript package (deprecated) Rust Rust Documentation Native Rust implementation for high performance"},{"location":"api/api-changes/","title":"Rust-backed Client Migration Guide","text":"<p>In an effort to ensure all clients have the same set of capabilities we have migrated the Python and Node clients onto a common Rust base library. In Python, both the synchronous and asynchronous clients are based on this implementation. In Node, the new client is available as <code>@lancedb/lancedb</code>, which replaces the existing <code>vectordb</code> package.</p> <p>This guide describes the differences between the two Node APIs and will hopefully assist users that would like to migrate to the new API.</p>"},{"location":"api/api-changes/#typescriptjavascript","title":"TypeScript/JavaScript","text":"<p>For JS/TS users, we offer a brand new SDK @lancedb/lancedb</p> <p>We tried to keep the API as similar as possible to the previous version, but there are a few small changes. Here are the most important ones:</p>"},{"location":"api/api-changes/#creating-tables","title":"Creating Tables","text":"<p>CreateTableOptions.writeOptions.writeMode has been replaced with CreateTableOptions.mode</p> vectordb (deprecated)@lancedb/lancedb <pre><code>db.createTable(tableName, data, { writeMode: lancedb.WriteMode.Overwrite });\n</code></pre> <pre><code>db.createTable(tableName, data, { mode: \"overwrite\" })\n</code></pre>"},{"location":"api/api-changes/#changes-to-table-apis","title":"Changes to Table APIs","text":"<p>Previously <code>Table.schema</code> was a property. Now it is an async method.</p>"},{"location":"api/api-changes/#creating-indices","title":"Creating Indices","text":"<p>The <code>Table.createIndex</code> method is now used for creating both vector indices and scalar indices. It currently requires a column name to be specified (the column to index). Vector index defaults are now smarter and scale better with the size of the data.</p> vectordb (deprecated)@lancedb/lancedb <pre><code>await tbl.createIndex({\n  column: \"vector\", // default\n  type: \"ivf_pq\",\n  num_partitions: 2,\n  num_sub_vectors: 2,\n});\n</code></pre> <pre><code>await table.createIndex(\"vector\", {\n  config: lancedb.Index.ivfPq({\n    numPartitions: 2,\n    numSubVectors: 2,\n  }),\n});\n</code></pre>"},{"location":"api/api-changes/#embedding-functions","title":"Embedding Functions","text":"<p>The embedding API has been completely reworked, and it now more closely resembles the Python API, including the new embedding registry:</p> vectordb (deprecated)@lancedb/lancedb <pre><code>const embeddingFunction = new lancedb.OpenAIEmbeddingFunction('text', API_KEY)\nconst data = [\n    { id: 1, text: 'Black T-Shirt', price: 10 },\n    { id: 2, text: 'Leather Jacket', price: 50 }\n]\nconst table = await db.createTable('vectors', data, embeddingFunction)\n</code></pre> <pre><code>import * as lancedb from \"@lancedb/lancedb\";\nimport * as arrow from \"apache-arrow\";\nimport { LanceSchema, getRegistry } from \"@lancedb/lancedb/embedding\";\n\nconst func = getRegistry().get(\"openai\").create({apiKey: API_KEY});\n\nconst data = [\n    { id: 1, text: 'Black T-Shirt', price: 10 },\n    { id: 2, text: 'Leather Jacket', price: 50 }\n]\n\nconst table = await db.createTable('vectors', data, {\n    embeddingFunction: {\n        sourceColumn: \"text\",\n        function: func,\n    }\n})\n</code></pre> <p>You can also use a schema driven approach, which parallels the Pydantic integration in our Python SDK:</p> <pre><code>const func = getRegistry().get(\"openai\").create({apiKey: API_KEY});\n\nconst data = [\n    { id: 1, text: 'Black T-Shirt', price: 10 },\n    { id: 2, text: 'Leather Jacket', price: 50 }\n]\nconst schema = LanceSchema({\n    id: new arrow.Int32(),\n    text: func.sourceField(new arrow.Utf8()),\n    price: new arrow.Float64(),\n    vector: func.vectorField()\n})\n\nconst table = await db.createTable('vectors', data, {schema})\n</code></pre>"},{"location":"api/cloud/","title":"LanceDB Cloud REST API Specification","text":"<p>Return to the previous page.</p>"},{"location":"api/cloud/#lancedb-cloud-api-100","title":"LanceDB Cloud API 1.0.0","text":"<p>LanceDB Cloud API is a RESTful API that allows users to access and modify data stored in LanceDB Cloud. Table actions are considered temporary resource creations and all use POST method.</p> Contact: LanceDB support contact@lancedb.com"},{"location":"api/cloud/#servers","title":"Servers","text":"Description URL LanceDB Cloud REST endpoint. https://{db}.{region}.api.lancedb.com"},{"location":"api/cloud/#tables","title":"Tables","text":""},{"location":"api/cloud/#get-v1table","title":"GET /v1/table/","text":"<p>List Tables</p> Description <p>List tables, optionally, with pagination.</p> <p>Input parameters</p> Parameter In Type Default Nullable Description <code>key_auth</code> header string N/A No API key <code>limit</code> query integer No Limits the number of items to return. <code>page_token</code> query string No Specifies the starting position of the next query <p> Response 200 OK </p> application/json <p><pre><code>{\n    \"tables\": [\n        \"string\"\n    ],\n    \"page_token\": \"string\"\n}\n</code></pre> \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information.</p> Schema of the response body <pre><code>{\n    \"type\": \"object\",\n    \"properties\": {\n        \"tables\": {\n            \"type\": \"array\",\n            \"items\": {\n                \"type\": \"string\"\n            }\n        },\n        \"page_token\": {\n            \"type\": \"string\"\n        }\n    }\n}\n</code></pre> <p> Response 400 Bad Request </p> <p>Refer to the common response description: invalid_request.</p> <p> Response 401 Unauthorized </p> <p>Refer to the common response description: unauthorized.</p> <p> Response 404 Not Found </p> <p>Refer to the common response description: not_found.</p>"},{"location":"api/cloud/#post-v1tablenamecreate","title":"POST /v1/table/{name}/create/","text":"<p>Create a new table</p> <p>Input parameters</p> Parameter In Type Default Nullable Description <code>key_auth</code> header string N/A No API key <code>name</code> path string No name of the table <p>Request body</p> application/vnd.apache.arrow.stream <p><pre><code>\"TG9yZW0gaXBzdW0gZG9sb3Igc2l0IGFtZXQ=\"\n</code></pre> \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information.</p> Schema of the request body <pre><code>{\n    \"type\": \"string\",\n    \"format\": \"binary\"\n}\n</code></pre> <p> Response 200 OK </p> <p> Response 400 Bad Request </p> <p>Refer to the common response description: invalid_request.</p> <p> Response 401 Unauthorized </p> <p>Refer to the common response description: unauthorized.</p> <p> Response 404 Not Found </p> <p>Refer to the common response description: not_found.</p>"},{"location":"api/cloud/#post-v1tablenamedrop","title":"POST /v1/table/{name}/drop/","text":"<p>Drop a table</p> <p>Input parameters</p> Parameter In Type Default Nullable Description <code>key_auth</code> header string N/A No API key <code>name</code> path string No name of the table <p>Request body</p> application/vnd.apache.arrow.stream <p><pre><code>\"TG9yZW0gaXBzdW0gZG9sb3Igc2l0IGFtZXQ=\"\n</code></pre> \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information.</p> Schema of the request body <pre><code>{\n    \"type\": \"string\",\n    \"format\": \"binary\"\n}\n</code></pre> <p> Response 200 OK </p> <p> Response 401 Unauthorized </p> <p>Refer to the common response description: unauthorized.</p>"},{"location":"api/cloud/#post-v1tablenamedescribe","title":"POST /v1/table/{name}/describe/","text":"<p>Describe a table</p> Description <p>Describe a table and return Table Information.</p> <p>Input parameters</p> Parameter In Type Default Nullable Description <code>key_auth</code> header string N/A No API key <code>name</code> path string No name of the table <p> Response 200 OK </p> application/json <p><pre><code>{\n    \"table\": \"string\",\n    \"version\": 0,\n    \"schema\": \"string\",\n    \"stats\": {}\n}\n</code></pre> \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information.</p> Schema of the response body <pre><code>{\n    \"type\": \"object\",\n    \"properties\": {\n        \"table\": {\n            \"type\": \"string\"\n        },\n        \"version\": {\n            \"type\": \"integer\"\n        },\n        \"schema\": {\n            \"type\": \"string\"\n        },\n        \"stats\": {\n            \"type\": \"object\"\n        }\n    }\n}\n</code></pre> <p> Response 401 Unauthorized </p> <p>Refer to the common response description: unauthorized.</p> <p> Response 404 Not Found </p> <p>Refer to the common response description: not_found.</p>"},{"location":"api/cloud/#post-v1tablenameindexlist","title":"POST /v1/table/{name}/index/list/","text":"<p>List indexes of a table</p> <p>Input parameters</p> Parameter In Type Default Nullable Description <code>key_auth</code> header string N/A No API key <code>name</code> path string No name of the table <p> Response 200 OK </p> application/json <p><pre><code>{\n    \"indexes\": [\n        {\n            \"columns\": [\n                \"string\"\n            ],\n            \"index_name\": \"string\",\n            \"index_uuid\": \"string\"\n        }\n    ]\n}\n</code></pre> \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information.</p> Schema of the response body <pre><code>{\n    \"type\": \"object\",\n    \"properties\": {\n        \"indexes\": {\n            \"type\": \"array\",\n            \"items\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"columns\": {\n                        \"type\": \"array\",\n                        \"items\": {\n                            \"type\": \"string\"\n                        }\n                    },\n                    \"index_name\": {\n                        \"type\": \"string\"\n                    },\n                    \"index_uuid\": {\n                        \"type\": \"string\"\n                    }\n                }\n            }\n        }\n    }\n}\n</code></pre> <p> Response 401 Unauthorized </p> <p>Refer to the common response description: unauthorized.</p> <p> Response 404 Not Found </p> <p>Refer to the common response description: not_found.</p>"},{"location":"api/cloud/#post-v1tablenamecreate_index","title":"POST /v1/table/{name}/create_index/","text":"<p>Create vector index on a Table</p> <p>Input parameters</p> Parameter In Type Default Nullable Description <code>key_auth</code> header string N/A No API key <code>name</code> path string No name of the table <p>Request body</p> application/json <p><pre><code>{\n    \"column\": \"string\",\n    \"metric_type\": \"string\",\n    \"index_type\": \"string\"\n}\n</code></pre> \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information.</p> Schema of the request body <pre><code>{\n    \"type\": \"object\",\n    \"properties\": {\n        \"column\": {\n            \"type\": \"string\"\n        },\n        \"metric_type\": {\n            \"type\": \"string\",\n            \"nullable\": false,\n            \"description\": \"The metric type to use for the index. l2, Cosine, Dot are supported.\\n\"\n        },\n        \"index_type\": {\n            \"type\": \"string\"\n        }\n    }\n}\n</code></pre> <p> Response 200 OK </p> <p> Response 400 Bad Request </p> <p>Refer to the common response description: invalid_request.</p> <p> Response 401 Unauthorized </p> <p>Refer to the common response description: unauthorized.</p> <p> Response 404 Not Found </p> <p>Refer to the common response description: not_found.</p>"},{"location":"api/cloud/#post-v1tablenamecreate_scalar_index","title":"POST /v1/table/{name}/create_scalar_index/","text":"<p>Create a scalar index on a table</p> <p>Input parameters</p> Parameter In Type Default Nullable Description <code>key_auth</code> header string N/A No API key <code>name</code> path string No name of the table <p>Request body</p> application/json <p><pre><code>{\n    \"column\": \"string\",\n    \"index_type\": \"string\"\n}\n</code></pre> \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information.</p> Schema of the request body <pre><code>{\n    \"type\": \"object\",\n    \"properties\": {\n        \"column\": {\n            \"type\": \"string\"\n        },\n        \"index_type\": {\n            \"type\": \"string\",\n            \"required\": false\n        }\n    }\n}\n</code></pre> <p> Response 200 OK </p> <p> Response 400 Bad Request </p> <p>Refer to the common response description: invalid_request.</p> <p> Response 401 Unauthorized </p> <p>Refer to the common response description: unauthorized.</p> <p> Response 404 Not Found </p> <p>Refer to the common response description: not_found.</p>"},{"location":"api/cloud/#post-v1tablenameindexindex_namedrop","title":"POST /v1/table/{name}/index/{index_name}/drop/","text":"<p>Drop an index from the table</p> <p>Input parameters</p> Parameter In Type Default Nullable Description <code>key_auth</code> header string N/A No API key <code>index_name</code> path string No name of the index <code>name</code> path string No name of the table <p> Response 200 OK </p> <p> Response 400 Bad Request </p> <p>Refer to the common response description: invalid_request.</p> <p> Response 401 Unauthorized </p> <p>Refer to the common response description: unauthorized.</p> <p> Response 404 Not Found </p> <p>Refer to the common response description: not_found.</p>"},{"location":"api/cloud/#data","title":"Data","text":""},{"location":"api/cloud/#post-v1tablenamequery","title":"POST /v1/table/{name}/query/","text":"<p>Vector Query</p> <p>Input parameters</p> Parameter In Type Default Nullable Description <code>key_auth</code> header string N/A No API key <code>name</code> path string No name of the table <p>Request body</p> application/json <p><pre><code>{\n    \"vector\": {\n        \"type\": \"FixedSizeList\",\n        \"description\": \"The targetted vector to search for. Required.\\n\"\n    },\n    \"vector_column\": \"string\",\n    \"prefilter\": true,\n    \"k\": 0,\n    \"distance_type\": \"string\",\n    \"bypass_vector_index\": true,\n    \"filter\": \"string\",\n    \"columns\": [\n        \"string\"\n    ],\n    \"nprobe\": 0,\n    \"refine_factor\": 0,\n    \"fast_search\": true\n}\n</code></pre> \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information.</p> Schema of the request body <pre><code>{\n    \"type\": \"object\",\n    \"properties\": {\n        \"vector\": {\n            \"type\": \"FixedSizeList\",\n            \"description\": \"The targetted vector to search for. Required.\\n\"\n        },\n        \"vector_column\": {\n            \"type\": \"string\",\n            \"description\": \"The column to query, it can be inferred from the schema if there is only one vector column.\\n\"\n        },\n        \"prefilter\": {\n            \"type\": \"boolean\",\n            \"description\": \"Whether to prefilter the data. Optional.\\n\"\n        },\n        \"k\": {\n            \"type\": \"integer\",\n            \"description\": \"The number of search results to return. Default is 10.\\n\"\n        },\n        \"distance_type\": {\n            \"type\": \"string\",\n            \"description\": \"The distance metric to use for search. l2, Cosine, Dot and Hamming are supported. Default is l2.\\n\"\n        },\n        \"bypass_vector_index\": {\n            \"type\": \"boolean\",\n            \"description\": \"Whether to bypass vector index. Optional.\\n\"\n        },\n        \"filter\": {\n            \"type\": \"string\",\n            \"description\": \"A filter expression that specifies the rows to query. Optional.\\n\"\n        },\n        \"columns\": {\n            \"type\": \"array\",\n            \"items\": {\n                \"type\": \"string\"\n            },\n            \"description\": \"The columns to return. Optional.\\n\"\n        },\n        \"nprobe\": {\n            \"type\": \"integer\",\n            \"description\": \"The number of probes to use for search. Optional.\\n\"\n        },\n        \"refine_factor\": {\n            \"type\": \"integer\",\n            \"description\": \"The refine factor to use for search. Optional.\\n\",\n            \"default\": null\n        },\n        \"fast_search\": {\n            \"type\": \"boolean\",\n            \"description\": \"Whether to use fast search. Optional.\\n\",\n            \"default\": false\n        }\n    },\n    \"required\": [\n        \"vector\"\n    ]\n}\n</code></pre> <p> Response 200 OK </p> application/json <p><pre><code>{\n    \"results\": [\n        {\n            \"id\": 0,\n            \"selected_col_1_to_return\": {\n                \"type\": \"col_1_type\"\n            },\n            \"selected_col_n_to_return\": {\n                \"type\": \"col_n_type\"\n            },\n            \"_distance\": {\n                \"type\": \"float\"\n            }\n        }\n    ]\n}\n</code></pre> \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information.</p> Schema of the response body <pre><code>{\n    \"type\": \"object\",\n    \"properties\": {\n        \"results\": {\n            \"type\": \"array\",\n            \"items\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"id\": {\n                        \"type\": \"integer\"\n                    },\n                    \"selected_col_1_to_return\": {\n                        \"type\": \"col_1_type\"\n                    },\n                    \"selected_col_n_to_return\": {\n                        \"type\": \"col_n_type\"\n                    },\n                    \"_distance\": {\n                        \"type\": \"float\"\n                    }\n                }\n            }\n        }\n    }\n}\n</code></pre> <p> Response 400 Bad Request </p> <p>Refer to the common response description: invalid_request.</p> <p> Response 401 Unauthorized </p> <p>Refer to the common response description: unauthorized.</p> <p> Response 404 Not Found </p> <p>Refer to the common response description: not_found.</p>"},{"location":"api/cloud/#post-v1tablenameinsert","title":"POST /v1/table/{name}/insert/","text":"<p>Insert new data.</p> Description <p>Insert new data to the Table.</p> <p>Input parameters</p> Parameter In Type Default Nullable Description <code>key_auth</code> header string N/A No API key <code>name</code> path string No name of the table <p>Request body</p> application/vnd.apache.arrow.stream <p><pre><code>\"TG9yZW0gaXBzdW0gZG9sb3Igc2l0IGFtZXQ=\"\n</code></pre> \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information.</p> Schema of the request body <pre><code>{\n    \"type\": \"string\",\n    \"format\": \"binary\"\n}\n</code></pre> <p> Response 200 OK </p> <p> Response 400 Bad Request </p> <p>Refer to the common response description: invalid_request.</p> <p> Response 401 Unauthorized </p> <p>Refer to the common response description: unauthorized.</p> <p> Response 404 Not Found </p> <p>Refer to the common response description: not_found.</p>"},{"location":"api/cloud/#post-v1tablenamemerge_insert","title":"POST /v1/table/{name}/merge_insert/","text":"<p>Merge Insert</p> Description <p>Create a \"merge insert\" operation This operation can add rows, update rows, and remove rows all in a single transaction. See python method <code>lancedb.table.Table.merge_insert</code> for examples.</p> <p>Input parameters</p> Parameter In Type Default Nullable Description <code>key_auth</code> header string N/A No API key <code>True</code> query string No The column to use as the primary key for the merge operation.  <code>name</code> path string No name of the table <code>when_matched_update_all</code> query boolean No Rows that exist in both the source table (new data) and the target table (old data) will be updated, replacing the old row with the corresponding matching row.  <code>when_matched_update_all_filt</code> query string No If present then only rows that satisfy the filter expression will be updated  <code>when_not_matched_by_source_delete</code> query boolean No Rows that exist only in the target table (old data) will be deleted. An optional condition (`when_not_matched_by_source_delete_filt`) can be provided to limit what data is deleted.  <code>when_not_matched_by_source_delete_filt</code> query string No The filter expression that specifies the rows to delete.  <code>when_not_matched_insert_all</code> query boolean No Rows that exist only in the source table (new data) will be inserted into the target table (old data).  <p>Request body</p> application/vnd.apache.arrow.stream <p><pre><code>\"TG9yZW0gaXBzdW0gZG9sb3Igc2l0IGFtZXQ=\"\n</code></pre> \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information.</p> Schema of the request body <pre><code>{\n    \"type\": \"string\",\n    \"format\": \"binary\"\n}\n</code></pre> <p> Response 200 OK </p> <p> Response 400 Bad Request </p> <p>Refer to the common response description: invalid_request.</p> <p> Response 401 Unauthorized </p> <p>Refer to the common response description: unauthorized.</p> <p> Response 404 Not Found </p> <p>Refer to the common response description: not_found.</p>"},{"location":"api/cloud/#post-v1tablenamedelete","title":"POST /v1/table/{name}/delete/","text":"<p>Delete rows from a table</p> Description <p>Delete rows from a table.</p> <p>Input parameters</p> Parameter In Type Default Nullable Description <code>key_auth</code> header string N/A No API key <code>name</code> path string No name of the table <p>Request body</p> application/json <p><pre><code>{\n    \"predicate\": \"string\"\n}\n</code></pre> \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information.</p> Schema of the request body <pre><code>{\n    \"type\": \"object\",\n    \"properties\": {\n        \"predicate\": {\n            \"type\": \"string\",\n            \"description\": \"A filter expression that specifies the rows to delete.\\n\"\n        }\n    }\n}\n</code></pre> <p> Response 200 OK </p> <p> Response 401 Unauthorized </p> <p>Refer to the common response description: unauthorized.</p>"},{"location":"api/cloud/#common-responses","title":"Common responses","text":"<p>This section describes common responses that are reused across operations.</p>"},{"location":"api/cloud/#invalid_request","title":"invalid_request","text":"<p>Invalid request</p> <p></p> text/plain <p><pre><code>\"string\"\n</code></pre> \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information.</p> Schema of the response body <pre><code>{\n    \"type\": \"string\"\n}\n</code></pre>"},{"location":"api/cloud/#not_found","title":"not_found","text":"<p>Not found</p> <p></p> text/plain <p><pre><code>\"string\"\n</code></pre> \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information.</p> Schema of the response body <pre><code>{\n    \"type\": \"string\"\n}\n</code></pre>"},{"location":"api/cloud/#unauthorized","title":"unauthorized","text":"<p>Unauthorized</p> <p></p> text/plain <p><pre><code>\"string\"\n</code></pre> \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information.</p> Schema of the response body <pre><code>{\n    \"type\": \"string\"\n}\n</code></pre>"},{"location":"api/cloud/#common-parameters","title":"Common parameters","text":"<p>This section describes common parameters that are reused across operations.</p>"},{"location":"api/cloud/#table_name","title":"table_name","text":"Name In Type Default Nullable Description <code>name</code> path string No"},{"location":"api/cloud/#index_name","title":"index_name","text":"Name In Type Default Nullable Description <code>index_name</code> path string No"},{"location":"api/cloud/#security-schemes","title":"Security schemes","text":"Name Type Scheme Description key_auth apiKey"},{"location":"cloud/","title":"About LanceDB Cloud","text":"<p>LanceDB Cloud is a SaaS (software-as-a-service) solution that runs serverless in the cloud, clearly separating storage from compute. It's designed to be highly scalable without breaking the bank. LanceDB Cloud is currently in private beta with general availability coming soon, but you can apply for early access with the private beta release by signing up below.</p> <p>Try out LanceDB Cloud (Public Beta)</p>"},{"location":"cloud/#architecture","title":"Architecture","text":"<p>LanceDB Cloud provides the same underlying fast vector store that powers the OSS version, but without the need to maintain your own infrastructure. Because it's serverless, you only pay for the storage you use, and you can scale compute up and down as needed depending on the size of your data and its associated index.</p> <p></p>"},{"location":"cloud/#transitioning-from-the-oss-to-the-cloud-version","title":"Transitioning from the OSS to the Cloud version","text":"<p>The OSS version of LanceDB is designed to be embedded in your application, and it runs in-process. This makes it incredibly simple to self-host your own AI retrieval workflows for RAG and more and build and test out your concepts on your own infrastructure. The OSS version is forever free, and you can continue to build and integrate LanceDB into your existing backend applications without any added costs.</p> <p>Should you decide that you need a managed deployment in production, it's possible to seamlessly transition from the OSS to the cloud version by changing the connection string to point to a remote database instead of a local one. With LanceDB Cloud, you can take your AI application from development to production without major code changes or infrastructure burden.</p>"},{"location":"cloud/cloud-faq/","title":"LanceDB Cloud FAQ","text":"<p>This section provides answers to the most common questions asked about LanceDB Cloud. By following these guidelines, you can ensure a smooth, performant experience with LanceDB Cloud.</p>"},{"location":"cloud/cloud-faq/#should-i-reuse-the-database-connection","title":"Should I reuse the database connection?","text":"<p>Yes! It is recommended to establish a single database connection and maintain it throughout your interaction with the tables within. </p> <p>LanceDB uses HTTP connections to communicate with the servers. By re-using the Connection object, you avoid the overhead of repeatedly establishing HTTP connections, significantly improving efficiency.</p>"},{"location":"cloud/cloud-faq/#should-i-re-use-the-table-object","title":"Should I re-use the <code>Table</code> object?","text":"<p><code>table = db.open_table()</code> should be called once and used for all subsequent table operations. If there are changes to the opened table, <code>table</code> always reflect the latest version of the data. </p>"},{"location":"cloud/cloud-faq/#what-should-i-do-if-i-need-to-search-for-rows-by-id","title":"What should I do if I need to search for rows by <code>id</code>?","text":"<p>LanceDB Cloud currently does not support an ID or primary key column. You are recommended to add a  user-defined ID column. To significantly improve the query performance with SQL causes, a scalar BITMAP/BTREE index should be created on this column. </p>"},{"location":"cloud/cloud-faq/#what-are-the-vector-indexing-types-supported-by-lancedb-cloud","title":"What are the vector indexing types supported by LanceDB Cloud?","text":"<p>We support <code>IVF_PQ</code> and <code>IVF_HNSW_SQ</code> as the <code>index_type</code> which is passed to <code>create_index</code>. LanceDB Cloud tunes the indexing parameters automatically to achieve the best tradeoff between query latency and query quality.</p>"},{"location":"cloud/cloud-faq/#when-i-add-new-rows-to-a-table-do-i-need-to-manually-update-the-index","title":"When I add new rows to a table, do I need to manually update the index?","text":"<p>No! LanceDB Cloud triggers an asynchronous background job to index the new vectors.</p> <p>Even though indexing is asynchronous, your vectors will still be immediately searchable. LanceDB uses brute-force search to search over unindexed rows. This makes you new data is immediately available, but does increase latency temporarily. To disable the brute-force part of search, set the <code>fast_search</code> flag in your query to <code>true</code>.</p>"},{"location":"cloud/cloud-faq/#do-i-need-to-reindex-the-whole-dataset-if-only-a-small-portion-of-the-data-is-deleted-or-updated","title":"Do I need to reindex the whole dataset if only a small portion of the data is deleted or updated?","text":"<p>No! Similar to adding data to the table, LanceDB Cloud triggers an asynchronous background job to update the existing indices. Therefore, no action is needed from users and there is absolutely no  downtime expected.</p>"},{"location":"cloud/cloud-faq/#how-do-i-know-whether-an-index-has-been-created","title":"How do I know whether an index has been created?","text":"<p>While index creation in LanceDB Cloud is generally fast, querying immediately after a <code>create_index</code> call may result in errors. It's recommended to use <code>list_indices</code> to verify index creation before querying.</p>"},{"location":"cloud/cloud-faq/#why-is-my-query-latency-higher-than-expected","title":"Why is my query latency higher than expected?","text":"<p>Multiple factors can impact query latency. To reduce query latency, consider the following: - Send pre-warm queries: send a few queries to warm up the cache before an actual user query. - Check network latency: LanceDB Cloud is hosted in AWS <code>us-east-1</code> region. It is recommended to run queries from an EC2 instance that is in the same region. - Create scalar indices: If you are filtering on metadata, it is recommended to create scalar indices on those columns. This will speedup searches with metadata filtering. See here for more details on creating a scalar index.</p>"},{"location":"cloud/logging/","title":"Enabling logging","text":"<p>To provide more information, especially for LanceDB Cloud related issues, enable debug logging. You can set the <code>LANCEDB_LOG</code> environment variable:</p> <pre><code>export LANCEDB_LOG=debug\n</code></pre> <p>You can turn off colors and formatting in the logs by setting</p> <pre><code>export LANCEDB_LOG_STYLE=never\n</code></pre>"},{"location":"cloud/quickstart/","title":"Quickstart: LanceDB Cloud and Enterprise","text":"<p>Explore the full implementation in this Quickstart guide:</p> <p>\u2192 Python Notebook  | TypeScript Example</p>"},{"location":"cloud/quickstart/#prerequisite","title":"Prerequisite:","text":"<p>Install the LanceDB SDK with your preferred language.</p> PythonTypeScript <pre><code># Install lancedb and the optional datasets package used in the quickstart example\npip install lancedb datasets\n</code></pre> <pre><code>npm install @lancedb/lancedb\n</code></pre>"},{"location":"cloud/quickstart/#1-connect-to-lancedb-cloudenterprise","title":"1. Connect to LanceDB Cloud/Enterprise","text":"<ul> <li> <p>For LanceDB Cloud users, the database URI (which starts with <code>db://</code>) and API key can both be retrieved from the LanceDB Cloud UI. For step-by-step instructions,refer to our onboarding tutorial.</p> </li> <li> <p>For LanceDB Enterprise user, please contact our team to obtain your database URI, API key and host_override URL.</p> </li> </ul> PythonTypeScript <pre><code>import lancedb\nimport numpy as np\nimport pyarrow as pa\nimport os\n\n# Connect to LanceDB Cloud/Enterprise\nuri = \"db://your-database-uri\"\napi_key = \"your-api-key\"\nregion = \"us-east-1\"\n\n# (Optional) For LanceDB Enterprise, set the host override to your enterprise endpoint\nhost_override = os.environ.get(\"LANCEDB_HOST_OVERRIDE\")\n\ndb = lancedb.connect(\n  uri=uri,\n  api_key=api_key,\n  region=region,\n  host_override=host_override\n)\n</code></pre> <pre><code>import { connect, Index, Table } from '@lancedb/lancedb';\nimport { FixedSizeList, Field, Float32, Schema, Utf8 } from 'apache-arrow';\n\n// Connect to LanceDB Cloud/Enterprise\nconst dbUri = process.env.LANCEDB_URI || 'db://your-database-uri';\nconst apiKey = process.env.LANCEDB_API_KEY;\nconst region = process.env.LANCEDB_REGION;\n\n// (Optional) For LanceDB Enterprise, set the host override to your enterprise endpoint\nconst hostOverride = process.env.LANCEDB_HOST_OVERRIDE;\n\nconst db = await connect(dbUri, { \n  apiKey,\n  region,\n  hostOverride\n});\n</code></pre>"},{"location":"cloud/quickstart/#2-load-dataset","title":"2. Load Dataset","text":"PythonTypeScript <pre><code>from datasets import load_dataset\n\n# Load a sample dataset from HuggingFace with pre-computed embeddings\nsample_dataset = load_dataset(\"sunhaozhepy/ag_news_sbert_keywords_embeddings\", split=\"test[:1000]\")\nprint(f\"Loaded {len(sample_dataset)} samples\")\nprint(f\"Sample features: {sample_dataset.features}\")\nprint(f\"Column names: {sample_dataset.column_names}\")\n\n# Preview the first sample\nprint(sample_dataset[0])\n\n# Get embedding dimension\nvector_dim = len(sample_dataset[0][\"keywords_embeddings\"])\nprint(f\"Embedding dimension: {vector_dim}\")\n</code></pre> <pre><code>const BATCH_SIZE = 100; // HF API default limit\nconst POLL_INTERVAL = 10000; // 10 seconds\nconst MAX_RETRIES = 5;\nconst INITIAL_RETRY_DELAY = 1000; // 1 second\n\ninterface Document {\n    text: string;\n    label: number;\n    keywords: string[];\n    embeddings?: number[];\n    [key: string]: unknown;\n}\n\ninterface HfDatasetResponse {\n    rows: {\n        row: {\n            text: string;\n            label: number;\n            keywords: string[];\n            keywords_embeddings?: number[];\n        };\n    }[];\n}\n\n// This loads documents from the Hugging Face dataset API in batches\nasync function loadDataset(datasetName: string, split: string = 'train', targetSize: number = 1000, offset: number = 0): Promise&lt;Document[]&gt; {    \n    try {\n        console.log('Fetching dataset...');\n        const batches = Math.ceil(targetSize / BATCH_SIZE);\n        let allDocuments: Document[] = [];\n        const hfToken = process.env.HF_TOKEN; // Optional Hugging Face token\n\n        for (let i = 0; i &lt; batches; i++) {\n            const offset = i * BATCH_SIZE;\n            const url = `https://datasets-server.huggingface.co/rows?dataset=${datasetName}&amp;config=default&amp;split=${split}&amp;offset=${offset}&amp;limit=${BATCH_SIZE}`;\n            console.log(`Fetching batch ${i + 1}/${batches} from offset ${offset}...`);\n\n            // Add retry logic with exponential backoff\n            let retries = 0;\n            let success = false;\n            let data: HfDatasetResponse | null = null;\n\n            while (!success &amp;&amp; retries &lt; MAX_RETRIES) {\n                try {\n                    const headers: HeadersInit = {\n                        'Content-Type': 'application/json',\n                    };\n\n                    // Add authorization header if token is available\n                    if (hfToken) {\n                        headers['Authorization'] = `Bearer ${hfToken}`;\n                    }\n\n                    const fetchOptions = {\n                        method: 'GET',\n                        headers,\n                        timeout: 30000, // 30 second timeout\n                    };\n\n                    const response = await fetch(url, fetchOptions);\n                    if (!response.ok) {\n                        const errorText = await response.text();\n                        console.error(`Error response (attempt ${retries + 1}):`, errorText);\n                        throw new Error(`HTTP error! status: ${response.status}, body: ${errorText}`);\n                    }\n\n                    data = JSON.parse(await response.text()) as HfDatasetResponse;\n                    if (!data.rows) {\n                        throw new Error('No rows found in response');\n                    }\n\n                    success = true;\n                } catch (error) {\n                    retries++;\n                    if (retries &gt;= MAX_RETRIES) {\n                        console.error(`Failed after ${MAX_RETRIES} retries:`, error);\n                        throw error;\n                    }\n\n                    const delay = INITIAL_RETRY_DELAY * Math.pow(2, retries - 1);\n                    console.log(`Retry ${retries}/${MAX_RETRIES} after ${delay}ms...`);\n                    await new Promise(resolve =&gt; setTimeout(resolve, delay));\n                }\n            }\n\n            // Ensure data is defined before using it\n            if (!data || !data.rows) {\n                throw new Error('No data received after retries');\n            }\n\n            console.log(`Received ${data.rows.length} rows in batch ${i + 1}`);\n            const documents = data.rows.map(({ row }) =&gt; ({\n                text: row.text,\n                label: row.label,\n                keywords: row.keywords,\n                embeddings: row.keywords_embeddings\n            }));\n            allDocuments = allDocuments.concat(documents);\n\n            if (data.rows.length &lt; BATCH_SIZE) {\n                console.log('Reached end of dataset');\n                break;\n            }\n        }\n\n        console.log(`Total documents loaded: ${allDocuments.length}`);\n        return allDocuments;\n    } catch (error) {\n        console.error(\"Failed to load dataset:\", error);\n        throw error;\n    }\n}\n\n// Load dataset\nconsole.log('Loading AG News dataset...');\nconst datasetName = \"sunhaozhepy/ag_news_sbert_keywords_embeddings\";\nconst split = \"test\";\nconst targetSize = 1000;\nconst sampleData = await loadDataset(datasetName, split, targetSize);\nconsole.log(`Loaded ${sampleData.length} examples from AG News dataset`);\n</code></pre>"},{"location":"cloud/quickstart/#3-create-a-table-and-ingest-data","title":"3. Create a table and ingest data","text":"PythonTypeScript <pre><code>import pyarrow as pa\n\n# Create a table with the dataset\ntable_name = \"lancedb-cloud-quickstart\"\ntable = db.create_table(table_name, data=sample_dataset, mode=\"overwrite\")\n\n# Convert list to fixedsizelist on the vector column\ntable.alter_columns(dict(path=\"keywords_embeddings\", data_type=pa.list_(pa.float32(), vector_dim)))\nprint(f\"Table '{table_name}' created successfully\")\n</code></pre> <pre><code>const tableName = \"lancedb-cloud-quickstart\";\n\nconst dataWithEmbeddings: Document[] = sampleData;\nconst firstDocWithEmbedding = dataWithEmbeddings.find((doc: Document) =&gt; \n    (doc.embeddings &amp;&amp; Array.isArray(doc.embeddings) &amp;&amp; doc.embeddings.length &gt; 0));\n\nif (!firstDocWithEmbedding || !firstDocWithEmbedding.embeddings || !Array.isArray(firstDocWithEmbedding.embeddings)) {\n    throw new Error('No document with valid embeddings found in the dataset. Please check if keywords_embeddings field exists.');\n}\nconst embeddingDimension = firstDocWithEmbedding.embeddings.length;\n\n// Create schema\nconst schema = new Schema([\n    new Field('text', new Utf8(), true),\n    new Field('label', new Float32(), true),\n    new Field('keywords', new Utf8(), true),\n    new Field('embeddings', new FixedSizeList(embeddingDimension, new Field('item', new Float32(), true)), true)\n]);\n\n// Create table with data\nconst table = await db.createTable(tableName, dataWithEmbeddings, { \n    schema,\n    mode: \"overwrite\" \n});\nconsole.log('Successfully created table');\n</code></pre>"},{"location":"cloud/quickstart/#4-create-a-vector-index","title":"4. Create a vector index","text":"PythonTypeScript <pre><code>from datetime import timedelta\n\n# Create a vector index and wait for it to complete\ntable.create_index(\"cosine\", vector_column_name=\"keywords_embeddings\", wait_timeout=timedelta(seconds=120))\nprint(table.index_stats(\"keywords_embeddings_idx\"))\n</code></pre> <pre><code>// Create a vector index\nawait table.createIndex(\"embeddings\", {\n  config: Index.ivfPq({\n    distanceType: \"cosine\",\n  }),\n});\n\n// Wait for the index to be ready\nconst indexName = \"embeddings_idx\";\nawait table.waitForIndex([indexName], 120);\nconsole.log(await table.indexStats(indexName));\n</code></pre> <p>Note: The <code>create_index</code>/<code>createIndex</code> operation executes asynchronously in LanceDB Cloud/Enterprise. To ensure the index is fully built, you can use the <code>wait_timeout</code> parameter or call <code>wait_for_index</code> on the table.</p>"},{"location":"cloud/quickstart/#5-perform-a-vector-search","title":"5. Perform a vector search","text":"PythonTypeScript <pre><code>query_dataset = load_dataset(\"sunhaozhepy/ag_news_sbert_keywords_embeddings\", split=\"test[5000:5001]\")\nprint(f\"Query keywords: {query_dataset[0]['keywords']}\")\nquery_embed = query_dataset[\"keywords_embeddings\"][0]\n\n# A vector search\nresult = (\n    table.search(query_embed)\n    .select([\"text\", \"keywords\", \"label\"])\n    .limit(5)\n    .to_pandas()\n)\nprint(\"Search results:\")\nprint(result)\n\n# A vector search with a filter\nfiltered_result = (\n    table.search(query_embed)\n    .where(\"label &gt; 2\")\n    .select([\"text\", \"keywords\", \"label\"])\n    .limit(5)\n    .to_pandas()\n)\nprint(\"Filtered search results (label &gt; 2):\")\nprint(filtered_result)\n</code></pre> <pre><code>// Perform semantic search with a new query\nconst queryDocs = await loadDataset(datasetName, split, 1, targetSize);\nif (queryDocs.length === 0) {\n    throw new Error(\"Failed to load a query document\");\n}\nconst queryDoc = queryDocs[0];\nif (!queryDoc.embeddings || !Array.isArray(queryDoc.embeddings)) {\n    throw new Error(\"Query document doesn't have a valid embedding after processing\");\n}\nconst results = await table.search(queryDoc.embeddings)\n    .limit(5)\n    .select(['text','keywords','label'])\n    .toArray();\n\nconsole.log('Search Results:');\nconsole.log(results);\n\n// perform semantic search with a filter applied\nconst filteredResultsesults = await table.search(queryDoc.embeddings)\n    .where(\"label &gt; 2\")\n    .limit(5)\n    .select(['text', 'keywords','label'])\n    .toArray();\n\nconsole.log('Search Results with filter:');\nconsole.log(filteredResultsesults);\n</code></pre>"},{"location":"cloud/quickstart/#6-drop-the-table","title":"6. Drop the table","text":"PythonTypeScript <pre><code>db.drop_table(table_name)\n</code></pre> <pre><code>await db.dropTable(tableName);\n</code></pre>"},{"location":"cloud/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Dive into the <code>Work with Data</code> section to unlock LanceDB's full potential.</li> <li>Explore our python notebooks and typescript examples for real-world use cases.</li> </ul>"},{"location":"concepts/data/","title":"Data Management in LanceDB","text":"<p>This section covers essential concepts for managing your data effectively in LanceDB.</p>"},{"location":"concepts/data/#understanding-lance-data-format","title":"Understanding Lance Data Format","text":"<p>Because LanceDB is built on top of the Lance data format, it helps to understand some of its core ideas. Just like Apache Arrow, Lance is a fast columnar data format, but it has the added benefit of being versionable, query and train ML models on. Lance is designed to be used with simple and complex data types, like tabular data, images, videos audio, 3D point clouds (which are deeply nested) and more.</p> <p>The following concepts are important to keep in mind:</p> <ul> <li>Data storage is columnar and is interoperable with other columnar formats (such as Parquet) via Arrow</li> <li>Data is divided into fragments that represent a subset of the data</li> <li>Data is versioned, with each insert operation creating a new version of the dataset and an update to the manifest that tracks versions via metadata</li> </ul> <p>Note</p> <ol> <li>First, each version contains metadata and just the new/updated data in your transaction. So if you have 100 versions, they aren't 100 duplicates of the same data. However, they do have 100x the metadata overhead of a single version, which can result in slower queries.  </li> <li>Second, these versions exist to keep LanceDB scalable and consistent. We do not immediately blow away old versions when creating new ones because other clients might be in the middle of querying the old version. It's important to retain older versions for as long as they might be queried.</li> </ol>"},{"location":"concepts/data/#understanding-data-fragments","title":"Understanding Data Fragments","text":"<p>Fragments are chunks of data in a Lance dataset. Each fragment includes multiple files that contain several columns in the chunk of data that it represents.</p>"},{"location":"concepts/data/#data-compaction-and-performance","title":"Data Compaction and Performance","text":"<p>As you insert more data, your dataset will grow and you'll need to perform compaction to maintain query throughput (i.e., keep latencies down to a minimum). Compaction is the process of merging fragments together to reduce the amount of metadata that needs to be managed, and to reduce the number of files that need to be opened while scanning the dataset.</p>"},{"location":"concepts/data/#performance-optimization-through-compaction","title":"Performance Optimization Through Compaction","text":"<p>Compaction performs the following tasks in the background:</p> <ul> <li>Removes deleted rows from fragments</li> <li>Removes dropped columns from fragments</li> <li>Merges small fragments into larger ones</li> </ul> <p>Depending on the use case and dataset, optimal compaction will have different requirements. As a rule of thumb:</p> <ul> <li>It's always better to use batch inserts rather than adding 1 row at a time (to avoid too small fragments). If single-row inserts are unavoidable, run compaction on a regular basis to merge them into larger fragments.</li> <li>Keep the number of fragments under 100, which is suitable for most use cases (for really large datasets of &gt;500M rows, more fragments might be needed)</li> </ul>"},{"location":"concepts/data/#data-deletion-and-recovery","title":"Data Deletion and Recovery","text":"<p>Although Lance allows you to delete rows from a dataset, it does not actually delete the data immediately. It simply marks the row as deleted in the <code>DataFile</code> that represents a fragment. For a given version of the dataset, each fragment can have up to one deletion file (if no rows were ever deleted from that fragment, it will not have a deletion file). This is important to keep in mind because it means that the data is still there, and can be recovered if needed, as long as that version still exists based on your backup policy.</p>"},{"location":"concepts/indexing/","title":"Vector Indexing in LanceDB","text":""},{"location":"concepts/indexing/#understanding-ivf-pq-index","title":"Understanding IVF-PQ Index","text":"<p>An ANN (Approximate Nearest Neighbors) index is a data structure that represents data in a way that makes it more efficient to search and retrieve. Using an ANN index is faster, but less accurate than kNN or brute force search because, in essence, the index is a lossy representation of the data.</p> <p>LanceDB is fundamentally different from other vector databases in that it is built on top of Lance, an open-source columnar data format designed for performant ML workloads and fast random access. Due to the design of Lance, LanceDB's indexing philosophy adopts a primarily disk-based indexing philosophy.</p>"},{"location":"concepts/indexing/#ivf-pq","title":"IVF-PQ","text":"<p>IVF-PQ is a composite index that combines inverted file index (IVF) and product quantization (PQ). The implementation in LanceDB provides several parameters to fine-tune the index's size, query throughput, latency and recall, which are described later in this section.</p>"},{"location":"concepts/indexing/#product-quantization","title":"Product Quantization","text":"<p>Quantization is a compression technique used to reduce the dimensionality of an embedding to speed up search.</p> <p>Product quantization (PQ) works by dividing a large, high-dimensional vector of size into equally sized subvectors. Each subvector is assigned a \"reproduction value\" that maps to the nearest centroid of points for that subvector. The reproduction values are then assigned to a codebook using unique IDs, which can be used to reconstruct the original vector.</p> <p></p> <p>It's important to remember that quantization is a lossy process, i.e., the reconstructed vector is not identical to the original vector. This results in a trade-off between the size of the index and the accuracy of the search results.</p> <p>As an example, consider starting with 128-dimensional vector consisting of 32-bit floats. Quantizing it to an 8-bit integer vector with 4 dimensions as in the image above, we can significantly reduce memory requirements.</p> <p>Effect of quantization</p> <p>Original: <code>128 \u00d7 32 = 4096</code> bits Quantized: <code>4 \u00d7 8 = 32</code> bits</p> <p>Quantization results in a 128x reduction in memory requirements for each vector in the index, which is substantial.</p>"},{"location":"concepts/indexing/#inverted-file-index-ivf-implementation","title":"Inverted File Index (IVF) Implementation","text":"<p>While PQ helps with reducing the size of the index, IVF primarily addresses search performance. The primary purpose of an inverted file index is to facilitate rapid and effective nearest neighbor search by narrowing down the search space.</p> <p>In IVF, the PQ vector space is divided into Voronoi cells, which are essentially partitions that consist of all the points in the space that are within a threshold distance of the given region's seed point. These seed points are initialized by running K-means over the stored vectors. The centroids of K-means turn into the seed points which then each define a region. These regions are then are used to create an inverted index that correlates each centroid with a list of vectors in the space, allowing a search to be restricted to just a subset of vectors in the index.</p> <p></p> <p>During query time, depending on where the query lands in vector space, it may be close to the border of multiple Voronoi cells, which could make the top-k results ambiguous and span across multiple cells. To address this, the IVF-PQ introduces the <code>nprobe</code> parameter, which controls the number of Voronoi cells to search during a query. The higher the <code>nprobe</code>, the more accurate the results, but the slower the query.</p> <p></p>"},{"location":"concepts/indexing/#hnsw-index-implementation","title":"HNSW Index Implementation","text":"<p>Approximate Nearest Neighbor (ANN) search is a method for finding data points near a given point in a dataset, though not always the exact nearest one. HNSW is one of the most accurate and fastest Approximate Nearest Neighbour search algorithms, It's beneficial in high-dimensional spaces where finding the same nearest neighbor would be too slow and costly.</p>"},{"location":"concepts/indexing/#types-of-ann-search-algorithms","title":"Types of ANN Search Algorithms","text":"<p>Approximate Nearest Neighbor (ANN) search is a method for finding data points near a given point in a dataset, though not always the exact nearest one. HNSW is one of the most accurate and fastest Approximate Nearest Neighbour search algorithms, It's beneficial in high-dimensional spaces where finding the same nearest neighbor would be too slow and costly</p> <p>There are three main types of ANN search algorithms:</p> <ul> <li>Tree-based search algorithms: Use a tree structure to organize and store data points.</li> <li>Hash-based search algorithms: Use a specialized geometric hash table to store and manage data points. These algorithms typically focus on theoretical guarantees, and don't usually perform as well as the other approaches in practice.</li> <li>Graph-based search algorithms: Use a graph structure to store data points, which can be a bit complex. </li> </ul> <p>HNSW is a graph-based algorithm. All graph-based search algorithms rely on the idea of a k-nearest neighbor (or k-approximate nearest neighbor) graph, which we outline below. HNSW also combines this with the ideas behind a classic 1-dimensional search data structure: the skip list.</p>"},{"location":"concepts/indexing/#understanding-k-nearest-neighbor-graphs","title":"Understanding k-Nearest Neighbor Graphs","text":"<p>The k-nearest neighbor graph actually predates its use for ANN search. Its construction is quite simple:</p> <ul> <li>Each vector in the dataset is given an associated vertex.</li> <li>Each vertex has outgoing edges to its k nearest neighbors. That is, the k closest other vertices by Euclidean distance between the two corresponding vectors. This can be thought of as a \"friend list\" for the vertex.</li> <li>For some applications (including nearest-neighbor search), the incoming edges are also added.</li> </ul> <p>Eventually, it was realized that the following greedy search method over such a graph typically results in good approximate nearest neighbors:</p> <ul> <li>Given a query vector, start at some fixed \"entry point\" vertex (e.g. the approximate center node).</li> <li>Look at that vertex's neighbors. If any of them are closer to the query vector than the current vertex, then move to that vertex.</li> <li>Repeat until a local optimum is found.</li> </ul> <p>The above algorithm also generalizes to e.g. top 10 approximate nearest neighbors.</p> <p>Computing a k-nearest neighbor graph is actually quite slow, taking quadratic time in the dataset size. It was quickly realized that near-identical performance can be achieved using a k-approximate nearest neighbor graph. That is, instead of obtaining the k-nearest neighbors for each vertex, an approximate nearest neighbor search data structure is used to build much faster. In fact, another data structure is not needed: This can be done \"incrementally\". That is, if you start with a k-ANN graph for n-1 vertices, you can extend it to a k-ANN graph for n vertices as well by using the graph to obtain the k-ANN for the new vertex.</p> <p>One downside of k-NN and k-ANN graphs alone is that one must typically build them with a large value of k to get decent results, resulting in a large index.</p>"},{"location":"concepts/indexing/#hierarchical-navigable-small-worlds-hnsw","title":"Hierarchical Navigable Small Worlds (HNSW)","text":"<p>HNSW builds on k-ANN in two main ways:</p> <ul> <li>Instead of getting the k-approximate nearest neighbors for a large value of k, it sparsifies the k-ANN graph using a carefully chosen \"edge pruning\" heuristic, allowing for the number of edges per vertex to be limited to a relatively small constant.</li> <li>The \"entry point\" vertex is chosen dynamically using a recursively constructed data structure on a subset of the data, similarly to a skip list.</li> </ul> <p>This recursive structure can be thought of as separating into layers:</p> <ul> <li>At the bottom-most layer, an k-ANN graph on the whole dataset is present.</li> <li>At the second layer, a k-ANN graph on a fraction of the dataset (e.g. 10%) is present.</li> <li>At the Lth layer, a k-ANN graph is present. It is over a (constant) fraction (e.g. 10%) of the vectors/vertices present in the L-1th layer.</li> </ul> <p>Then the greedy search routine operates as follows:</p> <ul> <li>At the top layer (using an arbitrary vertex as an entry point), use the greedy local search routine on the k-ANN graph to get an approximate nearest neighbor at that layer.</li> <li>Using the approximate nearest neighbor found in the previous layer as an entry point, find an approximate nearest neighbor in the next layer with the same method.</li> <li>Repeat until the bottom-most layer is reached. Then use the entry point to find multiple nearest neighbors (e.g. top 10).</li> </ul>"},{"location":"concepts/indexing/#index-management-and-maintenance","title":"Index Management and Maintenance","text":"<p>Embeddings for a given dataset are made searchable via an index. The index is constructed by using data structures that store the embeddings such that it's very efficient to perform scans and lookups on them. A key distinguishing feature of LanceDB is it uses a disk-based index: IVF-PQ, which is a variant of the Inverted File Index (IVF) that uses Product Quantization (PQ) to compress the embeddings.</p>"},{"location":"concepts/indexing/#reindexing-process","title":"Reindexing Process","text":"<p>Reindexing is the process of updating the index to account for new data, keeping good performance for queries. This applies to either a full-text search (FTS) index or a vector index. For ANN search, new data will always be included in query results, but queries on tables with unindexed data will fallback to slower search methods for the new parts of the table. This is another important operation to run periodically as your data grows, as it also improves performance. This is especially important if you're appending large amounts of data to an existing dataset.</p> <p>Tip</p> <p>When adding new data to a dataset that has an existing index (either FTS or vector), LanceDB doesn't immediately update the index until a reindex operation is complete.</p> <p>Both LanceDB OSS and Cloud support reindexing, but the process (at least for now) is different for each, depending on the type of index.</p> <p>When a reindex job is triggered in the background, the entire data is reindexed, but in the interim as new queries come in, LanceDB will combine results from the existing index with exhaustive kNN search on the new data. This is done to ensure that you're still searching on all your data, but it does come at a performance cost. The more data that you add without reindexing, the impact on latency (due to exhaustive search) can be noticeable.</p>"},{"location":"concepts/indexing/#vector-index-reindexing","title":"Vector Index Reindexing","text":"<ul> <li>LanceDB Cloud supports incremental reindexing, where a background process will trigger a new index build for you automatically when new data is added to a dataset</li> <li>LanceDB OSS requires you to manually trigger a reindex operation -- we are working on adding incremental reindexing to LanceDB OSS as well</li> </ul>"},{"location":"concepts/indexing/#fts-index-reindexing","title":"FTS Index Reindexing","text":"<p>FTS reindexing is supported in both LanceDB OSS and Cloud, but requires that it's manually rebuilt once you have a significant enough amount of new data added that needs to be reindexed. We updated Tantivy's default heap size from 128MB to 1GB in LanceDB to make it much faster to reindex, by up to 10x from the default settings.</p>"},{"location":"concepts/search/","title":"Vector Search in LanceDB","text":""},{"location":"concepts/search/#understanding-vector-search-fundamentals","title":"Understanding Vector Search Fundamentals","text":"<p>Vector search is a technique used to search for similar items based on their vector representations, called embeddings. It is also known as similarity search, nearest neighbor search, or approximate nearest neighbor search.</p> <p>Raw data (e.g. text, images, audio, etc.) is converted into embeddings via an embedding model, which are then stored in a vector database like LanceDB. To perform similarity search at scale, an index is created on the stored embeddings, which can then used to perform fast lookups.</p> <p></p>"},{"location":"concepts/search/#brute-force-search-implementation","title":"Brute Force Search Implementation","text":"<p>The simplest way to perform vector search is to perform a brute force search, without an index, where the distance between the query vector and all the vectors in the database are computed, with the top-k closest vectors returned. This is equivalent to a k-nearest neighbours (kNN) search in vector space.</p> <p></p> <p>As you can imagine, the brute force approach is not scalable for datasets larger than a few hundred thousand vectors, as the latency of the search grows linearly with the size of the dataset. This is where approximate nearest neighbour (ANN) algorithms come in.</p>"},{"location":"concepts/search/#approximate-nearest-neighbor-ann-search","title":"Approximate Nearest Neighbor (ANN) Search","text":"<p>Instead of performing an exhaustive search on the entire database for each and every query, approximate nearest neighbour (ANN) algorithms use an index to narrow down the search space, which significantly reduces query latency. The trade-off is that the results are not guaranteed to be the true nearest neighbors of the query, but are usually \"good enough\" for most use cases.</p>"},{"location":"concepts/storage/","title":"Storage Architecture in LanceDB","text":"<p>LanceDB is among the only vector databases built on top of multiple modular components designed from the ground-up to be efficient on disk. This gives it the unique benefit of being flexible enough to support multiple storage backends, including local NVMe, EBS, EFS and many other third-party APIs that connect to the cloud.</p> <p>It is important to understand the tradeoffs between cost and latency for your specific application and use case. This section will help you understand the tradeoffs between the different storage backends.</p>"},{"location":"concepts/storage/#storage-backend-selection-guide","title":"Storage Backend Selection Guide","text":"<p>We've prepared a simple diagram to showcase the thought process that goes into choosing a storage backend when using LanceDB OSS, Cloud or Enterprise.</p> <p></p> <p>When architecting your system, you'd typically ask yourself the following questions to decide on a storage option:</p> <ol> <li>Latency: How fast do I need results? What do the p50 and also p95 look like?</li> <li>Scalability: Can I scale up the amount of data and QPS easily?</li> <li>Cost: To serve my application, what's the all-in cost of both storage and serving infra?</li> <li>Reliability/Availability: How does replication work? Is disaster recovery addressed?</li> </ol>"},{"location":"concepts/storage/#storage-backend-comparison","title":"Storage Backend Comparison","text":"<p>This section reviews the characteristics of each storage option in four dimensions: latency, scalability, cost and reliability.</p> <p>We begin with the lowest cost option, and end with the lowest latency option.</p>"},{"location":"concepts/storage/#1-object-storage-s3-gcs-azure-blob","title":"1. Object Storage (S3 / GCS / Azure Blob)","text":"<p>Lowest cost, highest latency</p> <ul> <li>Latency \u21d2 Has the highest latency. p95 latency is also substantially worse than p50. In general you get results in the order of several hundred milliseconds</li> <li>Scalability \u21d2 Infinite on storage, however, QPS will be limited by S3 concurrency limits</li> <li>Cost \u21d2 Lowest (order of magnitude cheaper than other options)</li> <li>Reliability/Availability \u21d2 Highly available, as blob storage like S3 are critical infrastructure that form the backbone of the internet.</li> </ul> <p>Another important point to note is that LanceDB is designed to separate storage from compute, and the underlying Lance format stores the data in numerous immutable fragments. Due to these factors, LanceDB is a great storage option that addresses the N + 1 query problem. i.e., when a high query throughput is required, query processes can run in a stateless manner and be scaled up and down as needed.</p>"},{"location":"concepts/storage/#2-file-storage-efs-gcs-filestore-azure-file","title":"2. File Storage (EFS / GCS Filestore / Azure File)","text":"<p>Moderately low cost, moderately low latency (&lt;100ms)</p> <ul> <li>Latency \u21d2 Much better than object/blob storage but not as good as EBS/Local disk; &lt; 100ms p95 achievable</li> <li>Scalability \u21d2 High, but the bottleneck will be the IOPs limit, but when scaling you can provision multiple EFS volumes</li> <li>Cost \u21d2 Significantly more expensive than S3 but still very cost effective compared to in-memory dbs. Inactive data in EFS is also automatically tiered to S3-level costs.</li> <li>Reliability/Availability \u21d2 Highly available, as query nodes can go down without affecting EFS.  However, EFS does not provide replication / backup - this must be managed manually.</li> </ul> <p>A recommended best practice is to keep a copy of the data on S3 for disaster recovery scenarios. If any downtime is unacceptable, then you would need another EFS with a copy of the data. This is still much cheaper than EC2 instances holding multiple copies of the data.</p>"},{"location":"concepts/storage/#3-third-party-storage-solutions","title":"3. Third-party Storage Solutions","text":"<p>Solutions like MinIO, WekaFS, etc. that deliver S3 compatible API with much better performance than S3.</p> <p>Moderately low cost, moderately low latency (&lt;100ms)</p> <ul> <li>Latency \u21d2 Should be similar latency to EFS, better than S3 (&lt;100ms)</li> <li>Scalability \u21d2 Up to the solutions architect, who can add as many nodes to their MinIO or other third-party provider's cluster as needed</li> <li>Cost \u21d2 Definitely higher than S3. The cost can be marginally higher than EFS until you get to maybe &gt;10TB scale with high utilization</li> <li>Reliability/Availability \u21d2 These are all shareable by lots of nodes, quality/cost of replication/backup depends on the vendor</li> </ul>"},{"location":"concepts/storage/#4-block-storage-ebs-gcp-persistent-disk-azure-managed-disk","title":"4. Block Storage (EBS / GCP Persistent Disk / Azure Managed Disk)","text":"<p>Very low latency (&lt;30ms), higher cost</p> <ul> <li>Latency \u21d2 Very good, pretty close to local disk. You're looking at &lt;30ms latency in most cases</li> <li>Scalability \u21d2 EBS is not shareable between instances. If deployed via k8s, it can be shared between pods that live on the same instance, but beyond that you would need to shard data or make an additional copy</li> <li>Cost \u21d2 Higher than EFS. There are some hidden costs to EBS as well if you're paying for IO.</li> <li>Reliability/Availability \u21d2 Not shareable between instances but can be shared between pods on the same instance. Survives instance termination. No automatic backups.</li> </ul> <p>Just like EFS, an EBS or persistent disk setup requires more manual work to manage data sharding, backups and capacity.</p>"},{"location":"concepts/storage/#5-local-storage-ssdnvme","title":"5. Local Storage (SSD/NVMe)","text":"<p>Lowest latency (&lt;10ms), highest cost</p> <ul> <li>Latency \u21d2 Lowest latency with modern NVMe drives, &lt;10ms p95</li> <li>Scalability \u21d2 Difficult to scale on cloud. Also need additional copies / sharding if QPS needs to be higher</li> <li>Cost \u21d2 Highest cost; the main issue with keeping your application and storage tightly integrated is that it's just not really possible to scale this up in cloud environments</li> <li>Reliability/Availability \u21d2 If the instance goes down, so does your data. You have to be very diligent about backing up your data</li> </ul> <p>As a rule of thumb, local disk should be your storage option if you require absolutely crazy low latency and you're willing to do a bunch of data management work to make it happen.</p>"},{"location":"concepts/vectors/","title":"Vector Embeddings in LanceDB","text":""},{"location":"concepts/vectors/#understanding-vector-representations","title":"Understanding Vector Representations","text":"<p>Modern machine learning models can be trained to convert raw data into embeddings, represented as arrays (or vectors) of floating point numbers of fixed dimensionality. What makes embeddings useful in practice is that the position of an embedding in vector space captures some of the semantics of the data, depending on the type of model and how it was trained. Points that are close to each other in vector space are considered similar (or appear in similar contexts), and points that are far away are considered dissimilar.</p> <p>Large datasets of multi-modal data (text, audio, images, etc.) can be converted into embeddings with the appropriate model. Projecting the vectors' principal components in 2D space results in groups of vectors that represent similar concepts clustering together, as shown below.</p> <p></p>"},{"location":"embeddings/","title":"Working with Embeddings in LanceDB","text":"<p>Due to the nature of vector embeddings, they can be used to represent any kind of data, from text to images to audio. This makes them a very powerful tool for machine learning practitioners. However, there's no one-size-fits-all solution for generating embeddings - there are many different libraries and APIs (both commercial and open source) that can be used to generate embeddings from structured/unstructured data.</p> <p>LanceDB supports 3 methods of working with embeddings.</p> <ol> <li>You can manually generate embeddings for the data and queries. This is done outside of LanceDB.</li> <li>You can use the built-in embedding functions to embed the data and queries in the background.</li> <li>You can define your own custom embedding function    that extends the default embedding functions.</li> </ol> <p>For python users, there is also a legacy with_embeddings API. It is retained for compatibility and will be removed in a future version.</p>"},{"location":"embeddings/#quickstart","title":"Quickstart","text":"<p>To get started with embeddings, you can use the built-in embedding functions.</p>"},{"location":"embeddings/#openai-embedding-function","title":"OpenAI Embedding function","text":"<p>LanceDB registers the OpenAI embeddings function in the registry as <code>openai</code>. You can pass any supported model name to the <code>create</code>. By default it uses <code>\"text-embedding-ada-002\"</code>.</p> PythonTypeScriptRust <pre><code>import lancedb\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.embeddings import get_registry\n\ndb = lancedb.connect(\"/tmp/db\")\nfunc = get_registry().get(\"openai\").create(name=\"text-embedding-ada-002\")\n\nclass Words(LanceModel):\n    text: str = func.SourceField()\n    vector: Vector(func.ndims()) = func.VectorField()\n\ntable = db.create_table(\"words\", schema=Words, mode=\"overwrite\")\ntable.add(\n    [\n        {\"text\": \"hello world\"},\n        {\"text\": \"goodbye world\"}\n    ]\n    )\n\nquery = \"greetings\"\nactual = table.search(query).limit(1).to_pydantic(Words)[0]\nprint(actual.text)\n</code></pre> <pre><code>import * as lancedb from \"@lancedb/lancedb\";\nimport \"@lancedb/lancedb/embedding/openai\";\nimport { LanceSchema, getRegistry, register } from \"@lancedb/lancedb/embedding\";\nimport { EmbeddingFunction } from \"@lancedb/lancedb/embedding\";\nimport { type Float, Float32, Utf8 } from \"apache-arrow\";\nconst db = await lancedb.connect(databaseDir);\nconst func = getRegistry()\n  .get(\"openai\")\n  ?.create({ model: \"text-embedding-ada-002\" }) as EmbeddingFunction;\n\nconst wordsSchema = LanceSchema({\n  text: func.sourceField(new Utf8()),\n  vector: func.vectorField(),\n});\nconst tbl = await db.createEmptyTable(\"words\", wordsSchema, {\n  mode: \"overwrite\",\n});\nawait tbl.add([{ text: \"hello world\" }, { text: \"goodbye world\" }]);\n\nconst query = \"greetings\";\nconst actual = (await tbl.search(query).limit(1).toArray())[0];\n</code></pre> <pre><code>use std::{iter::once, sync::Arc};\n\nuse arrow_array::{Float64Array, Int32Array, RecordBatch, RecordBatchIterator, StringArray};\nuse arrow_schema::{DataType, Field, Schema};\nuse futures::StreamExt;\nuse lancedb::{\n    arrow::IntoArrow,\n    connect,\n    embeddings::{openai::OpenAIEmbeddingFunction, EmbeddingDefinition, EmbeddingFunction},\n    query::{ExecutableQuery, QueryBase},\n    Result,\n};\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;()&gt; {\n    let tempdir = tempfile::tempdir().unwrap();\n    let tempdir = tempdir.path().to_str().unwrap();\n    let api_key = std::env::var(\"OPENAI_API_KEY\").expect(\"OPENAI_API_KEY is not set\");\n    let embedding = Arc::new(OpenAIEmbeddingFunction::new_with_model(\n        api_key,\n        \"text-embedding-3-large\",\n    )?);\n\n    let db = connect(tempdir).execute().await?;\n    db.embedding_registry()\n        .register(\"openai\", embedding.clone())?;\n\n    let table = db\n        .create_table(\"vectors\", make_data())\n        .add_embedding(EmbeddingDefinition::new(\n            \"text\",\n            \"openai\",\n            Some(\"embeddings\"),\n        ))?\n        .execute()\n        .await?;\n\n    let query = Arc::new(StringArray::from_iter_values(once(\"something warm\")));\n    let query_vector = embedding.compute_query_embeddings(query)?;\n    let mut results = table\n        .vector_search(query_vector)?\n        .limit(1)\n        .execute()\n        .await?;\n\n    let rb = results.next().await.unwrap()?;\n    let out = rb\n        .column_by_name(\"text\")\n        .unwrap()\n        .as_any()\n        .downcast_ref::&lt;StringArray&gt;()\n        .unwrap();\n    let text = out.iter().next().unwrap().unwrap();\n    println!(\"Closest match: {}\", text);\n    Ok(())\n}\n</code></pre>"},{"location":"embeddings/#sentence-transformers-embedding-function","title":"Sentence Transformers Embedding function","text":"<p>LanceDB registers the Sentence Transformers embeddings function in the registry as <code>sentence-transformers</code>. You can pass any supported model name to the <code>create</code>. By default it uses <code>\"sentence-transformers/paraphrase-MiniLM-L6-v2\"</code>.</p> PythonTypeScriptRust <pre><code>import lancedb\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.embeddings import get_registry\n\ndb = lancedb.connect(\"/tmp/db\")\nmodel = get_registry().get(\"sentence-transformers\").create(name=\"BAAI/bge-small-en-v1.5\", device=\"cpu\")\n\nclass Words(LanceModel):\n    text: str = model.SourceField()\n    vector: Vector(model.ndims()) = model.VectorField()\n\ntable = db.create_table(\"words\", schema=Words)\ntable.add(\n    [\n        {\"text\": \"hello world\"},\n        {\"text\": \"goodbye world\"}\n    ]\n)\n\nquery = \"greetings\"\nactual = table.search(query).limit(1).to_pydantic(Words)[0]\nprint(actual.text)\n</code></pre> <p>Coming Soon!</p> <p>Coming Soon!</p>"},{"location":"embeddings/#embedding-function-with-lancedb-cloud","title":"Embedding function with LanceDB cloud","text":"<p>Embedding functions are now supported on LanceDB cloud. The embeddings will be generated on the source device and sent to the cloud. This means that the source device must have the necessary resources to generate the embeddings. Here's an example using the OpenAI embedding function:</p> <pre><code>import os\nimport lancedb\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.embeddings import get_registry\nos.environ['OPENAI_API_KEY'] = \"...\"\n\ndb = lancedb.connect(\n  uri=\"db://....\",\n  api_key=\"sk_...\",\n  region=\"us-east-1\"\n)\nfunc = get_registry().get(\"openai\").create()\n\nclass Words(LanceModel):\n    text: str = func.SourceField()\n    vector: Vector(func.ndims()) = func.VectorField()\n\ntable = db.create_table(\"words\", schema=Words)\ntable.add([\n    {\"text\": \"hello world\"},\n    {\"text\": \"goodbye world\"}\n])\n\nquery = \"greetings\"\nactual = table.search(query).limit(1).to_pydantic(Words)[0]\nprint(actual.text)\n</code></pre>"},{"location":"embeddings/custom_embedding_function/","title":"Custom Embedding Functions in LanceDB","text":"<p>To use your own custom embedding function, you can follow these 2 simple steps:</p> <ol> <li>Create your embedding function by implementing the <code>EmbeddingFunction</code> interface</li> <li>Register your embedding function in the global <code>EmbeddingFunctionRegistry</code>.</li> </ol> <p>Let us see how this looks like in action.</p> <p></p> <p><code>EmbeddingFunction</code> and <code>EmbeddingFunctionRegistry</code> handle low-level details for serializing schema and model information as metadata. To build a custom embedding function, you don't have to worry about the finer details - simply focus on setting up the model and leave the rest to LanceDB.</p>"},{"location":"embeddings/custom_embedding_function/#textembeddingfunction-interface","title":"<code>TextEmbeddingFunction</code> interface","text":"<p>There is another optional layer of abstraction available: <code>TextEmbeddingFunction</code>. You can use this abstraction if your model isn't multi-modal in nature and only needs to operate on text. In such cases, both the source and vector fields will have the same work for vectorization, so you simply just need to setup the model and rest is handled by <code>TextEmbeddingFunction</code>. You can read more about the class and its attributes in the class reference.</p> <p>Let's implement <code>SentenceTransformerEmbeddings</code> class. All you need to do is implement the <code>generate_embeddings()</code> and <code>ndims</code> function to handle the input types you expect and register the class in the global <code>EmbeddingFunctionRegistry</code></p> PythonTypeScript <pre><code>from lancedb.embeddings import register\nfrom lancedb.util import attempt_import_or_raise\n\n@register(\"sentence-transformers\")\nclass SentenceTransformerEmbeddings(TextEmbeddingFunction):\n    name: str = \"all-MiniLM-L6-v2\"\n    # set more default instance vars like device, etc.\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._ndims = None\n\n    def generate_embeddings(self, texts):\n        return self._embedding_model().encode(list(texts), ...).tolist()\n\n    def ndims(self):\n        if self._ndims is None:\n            self._ndims = len(self.generate_embeddings(\"foo\")[0])\n        return self._ndims\n\n    @cached(cache={})\n    def _embedding_model(self):\n        return sentence_transformers.SentenceTransformer(name)\n</code></pre> <pre><code>import * as lancedb from \"@lancedb/lancedb\";\nimport {\n  LanceSchema,\n  TextEmbeddingFunction,\n  getRegistry,\n  register,\n} from \"@lancedb/lancedb/embedding\";\n\n@register(\"sentence-transformers\")\nclass SentenceTransformersEmbeddings extends TextEmbeddingFunction {\n  name = \"Xenova/all-miniLM-L6-v2\";\n  #ndims!: number;\n  extractor!: FeatureExtractionPipeline;\n\n  async init() {\n    this.extractor = await pipeline(\"feature-extraction\", this.name, {\n      dtype: \"fp32\",\n    });\n    this.#ndims = await this.generateEmbeddings([\"hello\"]).then(\n      (e) =&gt; e[0].length,\n    );\n  }\n\n  ndims() {\n    return this.#ndims;\n  }\n\n  toJSON() {\n    return {\n      name: this.name,\n    };\n  }\n  async generateEmbeddings(texts: string[]) {\n    const output = await this.extractor(texts, {\n      pooling: \"mean\",\n      normalize: true,\n    });\n    return output.tolist();\n  }\n}\n</code></pre> <p>This is a stripped down version of our implementation of <code>SentenceTransformerEmbeddings</code> that removes certain optimizations and default settings.</p> <p>Use sensitive keys to prevent leaking secrets</p> <p>To prevent leaking secrets, such as API keys, you should add any sensitive parameters of an embedding function to the output of the sensitive_keys() / getSensitiveKeys() method. This prevents users from accidentally instantiating the embedding function with hard-coded secrets.</p> <p>Now you can use this embedding function to create your table schema and that's it! you can then ingest data and run queries without manually vectorizing the inputs.</p> PythonTypeScript <pre><code>from lancedb.pydantic import LanceModel, Vector\n\nregistry = EmbeddingFunctionRegistry.get_instance()\nstransformer = registry.get(\"sentence-transformers\").create()\n\nclass TextModelSchema(LanceModel):\n    vector: Vector(stransformer.ndims) = stransformer.VectorField()\n    text: str = stransformer.SourceField()\n\ntbl = db.create_table(\"table\", schema=TextModelSchema)\n\ntbl.add(pd.DataFrame({\"text\": [\"halo\", \"world\"]}))\nresult = tbl.search(\"world\").limit(5)\n</code></pre> <pre><code>const registry = getRegistry();\n\nconst sentenceTransformer = await registry\n  .get&lt;SentenceTransformersEmbeddings&gt;(\"sentence-transformers\")!\n  .create();\n\nconst schema = LanceSchema({\n  vector: sentenceTransformer.vectorField(),\n  text: sentenceTransformer.sourceField(),\n});\n\nconst db = await lancedb.connect(databaseDir);\nconst table = await db.createEmptyTable(\"table\", schema, {\n  mode: \"overwrite\",\n});\n\nawait table.add([{ text: \"hello\" }, { text: \"world\" }]);\n\nconst results = await table.search(\"greeting\").limit(1).toArray();\n</code></pre> <p>Note</p> <p>You can always implement the <code>EmbeddingFunction</code> interface directly if you want or need to, <code>TextEmbeddingFunction</code> just makes it much simpler and faster for you to do so, by setting up the boiler plat for text-specific use case</p>"},{"location":"embeddings/custom_embedding_function/#multi-modal-embedding-function-example","title":"Multi-modal embedding function example","text":"<p>You can also use the <code>EmbeddingFunction</code> interface to implement more complex workflows such as multi-modal embedding function support.</p> PythonTypeScript <p>LanceDB implements <code>OpenClipEmeddingFunction</code> class that suppports multi-modal seach. Here's the implementation that you can use as a reference to build your own multi-modal embedding functions.</p> <pre><code>@register(\"open-clip\")\nclass OpenClipEmbeddings(EmbeddingFunction):\n    name: str = \"ViT-B-32\"\n    pretrained: str = \"laion2b_s34b_b79k\"\n    device: str = \"cpu\"\n    batch_size: int = 64\n    normalize: bool = True\n    _model = PrivateAttr()\n    _preprocess = PrivateAttr()\n    _tokenizer = PrivateAttr()\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        open_clip = attempt_import_or_raise(\"open_clip\", \"open-clip\") # EmbeddingFunction util to import external libs and raise if not found\n        model, _, preprocess = open_clip.create_model_and_transforms(\n            self.name, pretrained=self.pretrained\n        )\n        model.to(self.device)\n        self._model, self._preprocess = model, preprocess\n        self._tokenizer = open_clip.get_tokenizer(self.name)\n        self._ndims = None\n\n    def ndims(self):\n        if self._ndims is None:\n            self._ndims = self.generate_text_embeddings(\"foo\").shape[0]\n        return self._ndims\n\n    def compute_query_embeddings(\n        self, query: Union[str, \"PIL.Image.Image\"], *args, **kwargs\n    ) -&gt; List[np.ndarray]:\n        \"\"\"\n        Compute the embeddings for a given user query\n\n        Parameters\n        ----------\n        query : Union[str, PIL.Image.Image]\n            The query to embed. A query can be either text or an image.\n        \"\"\"\n        if isinstance(query, str):\n            return [self.generate_text_embeddings(query)]\n        else:\n            PIL = attempt_import_or_raise(\"PIL\", \"pillow\")\n            if isinstance(query, PIL.Image.Image):\n                return [self.generate_image_embedding(query)]\n            else:\n                raise TypeError(\"OpenClip supports str or PIL Image as query\")\n\n    def generate_text_embeddings(self, text: str) -&gt; np.ndarray:\n        torch = attempt_import_or_raise(\"torch\")\n        text = self.sanitize_input(text)\n        text = self._tokenizer(text)\n        text.to(self.device)\n        with torch.no_grad():\n            text_features = self._model.encode_text(text.to(self.device))\n            if self.normalize:\n                text_features /= text_features.norm(dim=-1, keepdim=True)\n            return text_features.cpu().numpy().squeeze()\n\n    def sanitize_input(self, images: IMAGES) -&gt; Union[List[bytes], np.ndarray]:\n        \"\"\"\n        Sanitize the input to the embedding function.\n        \"\"\"\n        if isinstance(images, (str, bytes)):\n            images = [images]\n        elif isinstance(images, pa.Array):\n            images = images.to_pylist()\n        elif isinstance(images, pa.ChunkedArray):\n            images = images.combine_chunks().to_pylist()\n        return images\n\n    def compute_source_embeddings(\n        self, images: IMAGES, *args, **kwargs\n    ) -&gt; List[np.array]:\n        \"\"\"\n        Get the embeddings for the given images\n        \"\"\"\n        images = self.sanitize_input(images)\n        embeddings = []\n        for i in range(0, len(images), self.batch_size):\n            j = min(i + self.batch_size, len(images))\n            batch = images[i:j]\n            embeddings.extend(self._parallel_get(batch))\n        return embeddings\n\n    def _parallel_get(self, images: Union[List[str], List[bytes]]) -&gt; List[np.ndarray]:\n        \"\"\"\n        Issue concurrent requests to retrieve the image data\n        \"\"\"\n        with concurrent.futures.ThreadPoolExecutor() as executor:\n            futures = [\n                executor.submit(self.generate_image_embedding, image)\n                for image in images\n            ]\n            return [future.result() for future in futures]\n\n    def generate_image_embedding(\n        self, image: Union[str, bytes, \"PIL.Image.Image\"]\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Generate the embedding for a single image\n\n        Parameters\n        ----------\n        image : Union[str, bytes, PIL.Image.Image]\n            The image to embed. If the image is a str, it is treated as a uri.\n            If the image is bytes, it is treated as the raw image bytes.\n        \"\"\"\n        torch = attempt_import_or_raise(\"torch\")\n        # TODO handle retry and errors for https\n        image = self._to_pil(image)\n        image = self._preprocess(image).unsqueeze(0)\n        with torch.no_grad():\n            return self._encode_and_normalize_image(image)\n\n    def _to_pil(self, image: Union[str, bytes]):\n        PIL = attempt_import_or_raise(\"PIL\", \"pillow\")\n        if isinstance(image, bytes):\n            return PIL.Image.open(io.BytesIO(image))\n        if isinstance(image, PIL.Image.Image):\n            return image\n        elif isinstance(image, str):\n            parsed = urlparse.urlparse(image)\n            # TODO handle drive letter on windows.\n            if parsed.scheme == \"file\":\n                return PIL.Image.open(parsed.path)\n            elif parsed.scheme == \"\":\n                return PIL.Image.open(image if os.name == \"nt\" else parsed.path)\n            elif parsed.scheme.startswith(\"http\"):\n                return PIL.Image.open(io.BytesIO(url_retrieve(image)))\n            else:\n                raise NotImplementedError(\"Only local and http(s) urls are supported\")\n\n    def _encode_and_normalize_image(self, image_tensor: \"torch.Tensor\"):\n        \"\"\"\n        encode a single image tensor and optionally normalize the output\n        \"\"\"\n        image_features = self._model.encode_image(image_tensor)\n        if self.normalize:\n            image_features /= image_features.norm(dim=-1, keepdim=True)\n        return image_features.cpu().numpy().squeeze()\n</code></pre> <p>Coming Soon! See this issue to track the status!</p>"},{"location":"embeddings/default_embedding_functions/","title":"Default Embedding Functions in LanceDB","text":"<p>There are various embedding functions available out of the box with LanceDB to manage your embeddings implicitly. We're actively working on adding other popular embedding APIs and models. \ud83d\ude80</p> <p>Before jumping on the list of available models, let's understand how to get an embedding model initialized and configured to use in our code: </p> <p>Example usage</p> <pre><code>model = get_registry()\n          .get(\"openai\")\n          .create(name=\"text-embedding-ada-002\")\n</code></pre> <p>Now let's understand the above syntax:  <pre><code>model = get_registry().get(\"model_id\").create(...params)\n</code></pre> This\ud83d\udc46 line effectively creates a configured instance of an <code>embedding function</code> with <code>model</code> of choice that is ready for use.</p> <ul> <li> <p><code>get_registry()</code> :  This function call returns an instance of a <code>EmbeddingFunctionRegistry</code> object. This registry manages the registration and retrieval of embedding functions.</p> </li> <li> <p><code>.get(\"model_id\")</code> : This method call on the registry object and retrieves the embedding models functions associated with the <code>\"model_id\"</code> (1) .</p> <ol> <li>Hover over the names in table below to find out the <code>model_id</code> of different embedding functions.</li> </ol> </li> <li> <p><code>.create(...params)</code> : This method call is on the object returned by the <code>get</code> method. It instantiates an embedding model function using the specified parameters. </p> </li> </ul> What parameters does the <code>.create(...params)</code> method accepts? <p>Checkout the documentation of specific embedding models (links in the table below\ud83d\udc47) to know what parameters it takes.</p> <p>Moving on</p> <p>Now that we know how to get the desired embedding model and use it in our code, let's explore the comprehensive list of embedding models supported by LanceDB, in the tables below.</p>"},{"location":"embeddings/default_embedding_functions/#text-embedding-functions","title":"Text Embedding Functions \ud83d\udcdd","text":"<p>These functions are registered by default to handle text embeddings.</p> <ul> <li> <p>\ud83d\udd04 Embedding functions have an inbuilt rate limit handler wrapper for source and query embedding function calls that retry with exponential backoff. </p> </li> <li> <p>\ud83c\udf15 Each <code>EmbeddingFunction</code> implementation automatically takes <code>max_retries</code> as an argument which has the default value of 7. </p> </li> </ul> <p>\ud83c\udf1f Available Text Embeddings</p> Embedding Description Documentation Sentence Transformers \ud83e\udde0 SentenceTransformers is a Python framework for state-of-the-art sentence, text, and image embeddings. Huggingface Models \ud83e\udd17 We offer support for all Huggingface models. The default model is <code>colbert-ir/colbertv2.0</code>. Ollama Embeddings \ud83d\udd0d Generate embeddings via the Ollama python library. Ollama supports embedding models, making it possible to build RAG apps. OpenAI Embeddings \ud83d\udd11 OpenAI's text embeddings measure the relatedness of text strings. LanceDB supports state-of-the-art embeddings from OpenAI. Instructor Embeddings \ud83d\udcda Instructor: An instruction-finetuned text embedding model that can generate text embeddings tailored to any task and domains by simply providing the task instruction, without any finetuning. Gemini Embeddings \ud83c\udf0c Google's Gemini API generates state-of-the-art embeddings for words, phrases, and sentences. Cohere Embeddings \ud83d\udcac This will help you get started with Cohere embedding models using LanceDB. Using cohere API requires cohere package. Install it via <code>pip</code>. Jina Embeddings \ud83d\udd17 World-class embedding models to improve your search and RAG systems. You will need jina api key. AWS Bedrock Functions \u2601\ufe0f AWS Bedrock supports multiple base models for generating text embeddings. You need to setup the AWS credentials to use this embedding function. IBM Watsonx.ai \ud83d\udca1 Generate text embeddings using IBM's watsonx.ai platform. Note: watsonx.ai library is an optional dependency. VoyageAI Embeddings \ud83c\udf15 Voyage AI provides cutting-edge embedding and rerankers. This will help you get started with VoyageAI embedding models using LanceDB. Using voyageai API requires voyageai package. Install it via <code>pip</code>."},{"location":"embeddings/default_embedding_functions/#multi-modal-embedding-functions","title":"Multi-modal Embedding Functions\ud83d\uddbc\ufe0f","text":"<p>Multi-modal embedding functions allow you to query your table using both images and text. \ud83d\udcac\ud83d\uddbc\ufe0f</p> <p>\ud83c\udf10 Available Multi-modal Embeddings</p> Embedding  Description Documentation OpenClip Embeddings \ud83c\udfa8 We support CLIP model embeddings using the open source alternative, open-clip which supports various customizations. Imagebind Embeddings \ud83c\udf0c  We have support for imagebind model embeddings. You can download our version of the packaged model via - <code>pip install imagebind-packaged==0.1.2</code>. Jina Multi-modal Embeddings \ud83d\udd17 Jina embeddings can also be used to embed both text and image data, only some of the models support image data and you can check the detailed documentation. \ud83d\udc49 <p>Note</p> <p>If you'd like to request support for additional embedding functions, please feel free to open an issue on our LanceDB GitHub issue page.</p>"},{"location":"embeddings/embedding_functions/","title":"Embedding Functions in LanceDB","text":"<p>Representing multi-modal data as vector embeddings is becoming a standard practice. Embedding functions can themselves be thought of as key part of the data processing pipeline that each request has to be passed through. The assumption here is: after initial setup, these components and the underlying methodology are not expected to change for a particular project.</p> <p>For this purpose, LanceDB introduces an embedding functions API, that allow you simply set up once, during the configuration stage of your project. After this, the table remembers it, effectively making the embedding functions disappear in the background so you don't have to worry about manually passing callables, and instead, simply focus on the rest of your data engineering pipeline.</p> <p>Embedding functions on LanceDB cloud</p> <p>When using embedding functions with LanceDB cloud, the embeddings will be generated on the source device and sent to the cloud. This means that the source device must have the necessary resources to generate the embeddings.</p> <p>Warning</p> <p>Using the embedding function registry means that you don't have to explicitly generate the embeddings yourself. However, if your embedding function changes, you'll have to re-configure your table with the new embedding function and regenerate the embeddings. In the future, we plan to support the ability to change the embedding function via table metadata and have LanceDB automatically take care of regenerating the embeddings.</p>"},{"location":"embeddings/embedding_functions/#1-define-the-embedding-function","title":"1. Define the embedding function","text":"PythonTypeScriptRust <p>In the LanceDB python SDK, we define a global embedding function registry with many different embedding models and even more coming soon. Here's let's an implementation of CLIP as example.</p> <pre><code>from lancedb.embeddings import get_registry\n\nregistry = get_registry()\nclip = registry.get(\"open-clip\").create()\n</code></pre> <p>You can also define your own embedding function by implementing the <code>EmbeddingFunction</code> abstract base interface. It subclasses Pydantic Model which can be utilized to write complex schemas simply as we'll see next!</p> <p>In the TypeScript SDK, the choices are more limited. For now, only the OpenAI embedding function is available.</p> <pre><code>import * as lancedb from '@lancedb/lancedb'\nimport { getRegistry } from '@lancedb/lancedb/embeddings'\n\n// You need to provide an OpenAI API key\nconst apiKey = \"sk-...\"\n// The embedding function will create embeddings for the 'text' column\nconst func = getRegistry().get(\"openai\").create({apiKey})\n</code></pre> <p>In the Rust SDK, the choices are more limited. For now, only the OpenAI embedding function is available. But unlike the Python and TypeScript SDKs, you need manually register the OpenAI embedding function.</p> <pre><code>// Make sure to include the `openai` feature\n[dependencies]\nlancedb = {version = \"*\", features = [\"openai\"]}\n</code></pre> <pre><code>use std::{iter::once, sync::Arc};\n\nuse arrow_array::{Float64Array, Int32Array, RecordBatch, RecordBatchIterator, StringArray};\nuse arrow_schema::{DataType, Field, Schema};\nuse futures::StreamExt;\nuse lancedb::{\n    arrow::IntoArrow,\n    connect,\n    embeddings::{openai::OpenAIEmbeddingFunction, EmbeddingDefinition, EmbeddingFunction},\n    query::{ExecutableQuery, QueryBase},\n    Result,\n};\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;()&gt; {\n    let tempdir = tempfile::tempdir().unwrap();\n    let tempdir = tempdir.path().to_str().unwrap();\n    let api_key = std::env::var(\"OPENAI_API_KEY\").expect(\"OPENAI_API_KEY is not set\");\n    let embedding = Arc::new(OpenAIEmbeddingFunction::new_with_model(\n        api_key,\n        \"text-embedding-3-large\",\n    )?);\n\n    let db = connect(tempdir).execute().await?;\n    db.embedding_registry()\n        .register(\"openai\", embedding.clone())?;\n\n    let table = db\n        .create_table(\"vectors\", make_data())\n        .add_embedding(EmbeddingDefinition::new(\n            \"text\",\n            \"openai\",\n            Some(\"embeddings\"),\n        ))?\n        .execute()\n        .await?;\n\n    let query = Arc::new(StringArray::from_iter_values(once(\"something warm\")));\n    let query_vector = embedding.compute_query_embeddings(query)?;\n    let mut results = table\n        .vector_search(query_vector)?\n        .limit(1)\n        .execute()\n        .await?;\n\n    let rb = results.next().await.unwrap()?;\n    let out = rb\n        .column_by_name(\"text\")\n        .unwrap()\n        .as_any()\n        .downcast_ref::&lt;StringArray&gt;()\n        .unwrap();\n    let text = out.iter().next().unwrap().unwrap();\n    println!(\"Closest match: {}\", text);\n    Ok(())\n}\n</code></pre>"},{"location":"embeddings/embedding_functions/#2-define-the-data-model-or-schema","title":"2. Define the data model or schema","text":"PythonTypeScript <p>The embedding function defined above abstracts away all the details about the models and dimensions required to define the schema. You can simply set a field as source or vector column. Here's how:</p> <pre><code>class Pets(LanceModel):\n    vector: Vector(clip.ndims()) = clip.VectorField()\n    image_uri: str = clip.SourceField()\n</code></pre> <p><code>VectorField</code> tells LanceDB to use the clip embedding function to generate query embeddings for the <code>vector</code> column and <code>SourceField</code> ensures that when adding data, we automatically use the specified embedding function to encode <code>image_uri</code>.</p> <p>For the TypeScript SDK, a schema can be inferred from input data, or an explicit Arrow schema can be provided.</p>"},{"location":"embeddings/embedding_functions/#3-create-table-and-add-data","title":"3. Create table and add data","text":"<p>Now that we have chosen/defined our embedding function and the schema, we can create the table and ingest data without needing to explicitly generate the embeddings at all:</p> PythonTypeScript <pre><code>db = lancedb.connect(\"~/lancedb\")\ntable = db.create_table(\"pets\", schema=Pets)\n\ntable.add([{\"image_uri\": u} for u in uris])\n</code></pre> @lancedb/lancedbvectordb (deprecated) <pre><code>import * as lancedb from \"@lancedb/lancedb\";\nimport \"@lancedb/lancedb/embedding/openai\";\nimport { LanceSchema, getRegistry, register } from \"@lancedb/lancedb/embedding\";\nimport { EmbeddingFunction } from \"@lancedb/lancedb/embedding\";\nimport { type Float, Float32, Utf8 } from \"apache-arrow\";\nconst db = await lancedb.connect(databaseDir);\n\n@register(\"my_embedding\")\nclass MyEmbeddingFunction extends EmbeddingFunction&lt;string&gt; {\n  constructor(optionsRaw = {}) {\n    super();\n    const options = this.resolveVariables(optionsRaw);\n    // Initialize using options\n  }\n  ndims() {\n    return 3;\n  }\n  protected getSensitiveKeys(): string[] {\n    return [];\n  }\n  embeddingDataType(): Float {\n    return new Float32();\n  }\n  async computeQueryEmbeddings(_data: string) {\n    // This is a placeholder for a real embedding function\n    return [1, 2, 3];\n  }\n  async computeSourceEmbeddings(data: string[]) {\n    // This is a placeholder for a real embedding function\n    return Array.from({ length: data.length }).fill([\n      1, 2, 3,\n    ]) as number[][];\n  }\n}\n\nconst func = new MyEmbeddingFunction();\n\nconst data = [{ text: \"pepperoni\" }, { text: \"pineapple\" }];\n\n// Option 1: manually specify the embedding function\nconst table = await db.createTable(\"vectors\", data, {\n  embeddingFunction: {\n    function: func,\n    sourceColumn: \"text\",\n    vectorColumn: \"vector\",\n  },\n  mode: \"overwrite\",\n});\n\n// Option 2: provide the embedding function through a schema\n\nconst schema = LanceSchema({\n  text: func.sourceField(new Utf8()),\n  vector: func.vectorField(),\n});\n\nconst table2 = await db.createTable(\"vectors2\", data, {\n  schema,\n  mode: \"overwrite\",\n});\n</code></pre> <pre><code>const db = await lancedb.connect(\"data/sample-lancedb\");\nconst data = [\n    { text: \"pepperoni\"},\n    { text: \"pineapple\"}\n]\n\nconst table = await db.createTable(\"vectors\", data, embedding)\n</code></pre>"},{"location":"embeddings/embedding_functions/#4-querying-your-table","title":"4. Querying your table","text":"<p>Not only can you forget about the embeddings during ingestion, you also don't need to worry about it when you query the table:</p> PythonTypeScript <p>Our OpenCLIP query embedding function supports querying via both text and images:</p> <pre><code>results = (\n    table.search(\"dog\")\n        .limit(10)\n        .to_pandas()\n)\n</code></pre> <p>Or we can search using an image:</p> <pre><code>p = Path(\"path/to/images/samoyed_100.jpg\")\nquery_image = Image.open(p)\nresults = (\n    table.search(query_image)\n        .limit(10)\n        .to_pandas()\n)\n</code></pre> <p>Both of the above snippet returns a pandas DataFrame with the 10 closest vectors to the query.</p> @lancedb/lancedbvectordb (deprecated) <pre><code>const results = await table.search(\"What's the best pizza topping?\")\n    .limit(10)\n    .toArray()\n</code></pre> <pre><code>const results = await table\n    .search(\"What's the best pizza topping?\")\n    .limit(10)\n    .execute()\n</code></pre> <p>The above snippet returns an array of records with the top 10 nearest neighbors to the query.</p>"},{"location":"embeddings/embedding_functions/#rate-limit-handling","title":"Rate limit Handling","text":"<p><code>EmbeddingFunction</code> class wraps the calls for source and query embedding generation inside a rate limit handler that retries the requests with exponential backoff after successive failures. By default, the maximum retires is set to 7. You can tune it by setting it to a different number, or disable it by setting it to 0.</p> <p>An example of how to do this is shown below:</p> <pre><code>clip = registry.get(\"open-clip\").create() # Defaults to 7 max retries\nclip = registry.get(\"open-clip\").create(max_retries=10) # Increase max retries to 10\nclip = registry.get(\"open-clip\").create(max_retries=0) # Retries disabled\n</code></pre> <p>Note</p> <p>Embedding functions can also fail due to other errors that have nothing to do with rate limits. This is why the error is also logged.</p>"},{"location":"embeddings/embedding_functions/#some-fun-with-pydantic","title":"Some fun with Pydantic","text":"<p>LanceDB is integrated with Pydantic, which was used in the example above to define the schema in Python. It's also used behind the scenes by the embedding function API to ingest useful information as table metadata.</p> <p>You can also use the integration for adding utility operations in the schema. For example, in our multi-modal example, you can search images using text or another image. Let's define a utility function to plot the image.</p> <p><pre><code>class Pets(LanceModel):\n    vector: Vector(clip.ndims()) = clip.VectorField()\n    image_uri: str = clip.SourceField()\n\n    @property\n    def image(self):\n        return Image.open(self.image_uri)\n</code></pre> Now, you can covert your search results to a Pydantic model and use this property.</p> <pre><code>rs = table.search(query_image).limit(3).to_pydantic(Pets)\nrs[2].image\n</code></pre> <p></p> <p>Now that you have the basic idea about LanceDB embedding functions and the embedding function registry, let's dive deeper into defining your own custom functions.</p>"},{"location":"embeddings/legacy/","title":"Legacy Embedding API in LanceDB | Python Integration Guide","text":"<p>The legacy <code>with_embeddings</code> API is for Python only and is deprecated.</p>"},{"location":"embeddings/legacy/#hugging-face","title":"Hugging Face","text":"<p>The most popular open source option is to use the sentence-transformers  library, which can be installed via pip.</p> <pre><code>pip install sentence-transformers\n</code></pre> <p>The example below shows how to use the <code>paraphrase-albert-small-v2</code> model to generate embeddings  for a given document.</p> <pre><code>from sentence_transformers import SentenceTransformer\n\nname=\"paraphrase-albert-small-v2\"\nmodel = SentenceTransformer(name)\n\n# used for both training and querying\ndef embed_func(batch):\n    return [model.encode(sentence) for sentence in batch]\n</code></pre>"},{"location":"embeddings/legacy/#openai","title":"OpenAI","text":"<p>Another popular alternative is to use an external API like OpenAI's embeddings API.</p> <pre><code>import openai\nimport os\n\n# Configuring the environment variable OPENAI_API_KEY\nif \"OPENAI_API_KEY\" not in os.environ:\n# OR set the key here as a variable\nopenai.api_key = \"sk-...\"\n\nclient = openai.OpenAI()\n\ndef embed_func(c):    \n    rs = client.embeddings.create(input=c, model=\"text-embedding-ada-002\")\n    return [record.embedding for record in rs[\"data\"]]\n</code></pre>"},{"location":"embeddings/legacy/#applying-an-embedding-function-to-data","title":"Applying an embedding function to data","text":"<p>Using an embedding function, you can apply it to raw data to generate embeddings for each record.</p> <p>Say you have a pandas DataFrame with a <code>text</code> column that you want embedded, you can use the <code>with_embeddings</code> function to generate embeddings and add them to  an existing table.</p> <pre><code>    import pandas as pd\n    from lancedb.embeddings import with_embeddings\n\n    df = pd.DataFrame(\n        [\n            {\"text\": \"pepperoni\"},\n            {\"text\": \"pineapple\"}\n        ]\n    )\n    data = with_embeddings(embed_func, df)\n\n    # The output is used to create / append to a table\n    tbl = db.create_table(\"my_table\", data=data)\n</code></pre> <p>If your data is in a different column, you can specify the <code>column</code> kwarg to <code>with_embeddings</code>.</p> <p>By default, LanceDB calls the function with batches of 1000 rows. This can be configured using the <code>batch_size</code> parameter to <code>with_embeddings</code>.</p> <p>LanceDB automatically wraps the function with retry and rate-limit logic to ensure the OpenAI API call is reliable.</p>"},{"location":"embeddings/legacy/#querying-using-an-embedding-function","title":"Querying using an embedding function","text":"<p>Warning</p> <p>At query time, you must use the same embedding function you used to vectorize your data. If you use a different embedding function, the embeddings will not reside in the same vector space and the results will be nonsensical.</p> Python <pre><code>query = \"What's the best pizza topping?\"\nquery_vector = embed_func([query])[0]\nresults = (\n   tbl.search(query_vector)\n   .limit(10)\n   .to_pandas()\n)\n</code></pre> <p>The above snippet returns a pandas DataFrame with the 10 closest vectors to the query.</p>"},{"location":"embeddings/understanding_embeddings/","title":"Understanding Embeddings in LanceDB","text":"<p>The term dimension is a synonym for the number of elements in a feature vector. Each feature can be thought of as a different axis in a geometric space. </p> <p>High-dimensional data means there are many features(or attributes) in the data.</p> <p>Example</p> <ol> <li> <p>An image is a data point and it might have thousands of dimensions because each pixel could be considered as a feature. </p> </li> <li> <p>Text data, when represented by each word or character, can also lead to high dimensions, especially when considering all possible words in a language.</p> </li> </ol> <p>Embedding captures meaning and relationships within data by mapping high-dimensional data into a lower-dimensional space. It captures it by placing inputs that are more similar in meaning closer together in the embedding space. </p>"},{"location":"embeddings/understanding_embeddings/#what-are-vector-embeddings","title":"What are Vector Embeddings?","text":"<p>Vector embeddings is a way to convert complex data, like text, images, or audio into numerical coordinates (called vectors) that can be plotted in an n-dimensional space(embedding space). </p> <p>The closer these data points are related in the real world, the closer their corresponding numerical coordinates (vectors) will be to each other in the embedding space. This proximity in the embedding space reflects their semantic similarities, allowing machines to intuitively understand and process the data in a way that mirrors human perception of relationships and meaning.</p> <p>In a way, it captures the most important aspects of the data while ignoring the less important ones. As a result, tasks like searching for related content or identifying patterns become more efficient and accurate, as the embeddings make it possible to quantify how closely related different data points are and reduce the computational complexity.</p> Are vectors and embeddings the same thing? <p>When we say \"vectors\" we mean - list of numbers that represents the data.  When we say \"embeddings\" we mean - list of numbers that capture important details and relationships.</p> <p>Although the terms are often used interchangeably, \"embeddings\" highlight how the data is represented with meaning and structure, while \"vector\" simply refers to the numerical form of that representation.</p>"},{"location":"embeddings/understanding_embeddings/#embedding-vs-indexing","title":"Embedding vs Indexing","text":"<p>We already saw that creating embeddings on data is a method of creating vectors for a n-dimensional embedding space that captures the meaning and relationships inherent in the data.</p> <p>Once we have these vectors, indexing comes into play. Indexing is a method of organizing these vector embeddings, that allows us to quickly and efficiently locate and retrieve them from the entire dataset of vector embeddings.</p>"},{"location":"embeddings/understanding_embeddings/#what-types-of-dataobjects-can-be-embedded","title":"What types of data/objects can be embedded?","text":"<p>The following are common types of data that can be embedded:</p> <ol> <li>Text: Text data includes sentences, paragraphs, documents, or any written content.</li> <li>Images:  Image data encompasses photographs, illustrations, or any visual content.</li> <li>Audio: Audio data includes sounds, music, speech, or any auditory content.</li> <li>Video:  Video data consists of moving images and sound, which can convey complex information.</li> </ol> <p>Large datasets of multi-modal data (text, audio, images, etc.) can be converted into embeddings with the appropriate model.</p> <p>LanceDB vs Other traditional Vector DBs</p> <p>While many vector databases primarily focus on the storage and retrieval of vector embeddings, LanceDB uses Lance file format (operates on a disk-based architecture), which allows for the storage and management of not just embeddings but also raw file data (bytes). This capability means that users can integrate various types of data, including images and text, alongside their vector embeddings in a unified system.</p> <p>With the ability to store both vectors and associated file data, LanceDB enhances the querying process. Users can perform semantic searches that not only retrieve similar embeddings but also access related files and metadata, thus streamlining the workflow.</p>"},{"location":"embeddings/understanding_embeddings/#how-does-embedding-works","title":"How does embedding works?","text":"<p>As mentioned, after creating embedding, each data point is represented as a vector in a n-dimensional space (embedding space). The dimensionality of this space can vary depending on the complexity of the data and the specific embedding technique used.</p> <p>Points that are close to each other in vector space are considered similar (or appear in similar contexts), and points that are far away are considered dissimilar. To quantify this closeness, we use distance as a metric which can be measured in the  following way - </p> <ol> <li>Euclidean Distance (l2): It calculates the straight-line distance between two points (vectors) in a multidimensional space.</li> <li>Cosine Similarity: It measures the cosine of the angle between two vectors, providing a normalized measure of similarity based on their direction.</li> <li>Dot product: It is calculated as the sum of the products of their corresponding components. To measure relatedness it considers both the magnitude and direction of the vectors.</li> </ol>"},{"location":"embeddings/understanding_embeddings/#how-do-you-create-and-store-vector-embeddings-for-your-data","title":"How do you create and store vector embeddings for your data?","text":"<ol> <li>Creating embeddings: Choose an embedding model, it can be a pre-trained model (open-source or commercial) or you can train a custom embedding model for your scenario. Then feed your preprocessed data into the chosen model to obtain embeddings.</li> </ol> Popular choices for embedding models <p>For text data, popular choices are OpenAI\u2019s text-embedding models, Google Gemini text-embedding models, Cohere\u2019s Embed models, and SentenceTransformers, etc.</p> <p>For image data, popular choices are CLIP (Contrastive Language\u2013Image Pretraining), Imagebind embeddings by meta (supports audio, video, and image), and Jina multi-modal embeddings, etc.</p> <ol> <li>Storing vector embeddings: This effectively requires specialized databases that can handle the complexity of vector data, as traditional databases often struggle with this task. Vector databases are designed specifically for storing and querying vector embeddings. They optimize for efficient nearest-neighbor searches and provide built-in indexing mechanisms.</li> </ol> <p>Why LanceDB</p> <p>LanceDB automates the entire process of creating and storing embeddings for your data. LanceDB allows you to define and use embedding functions, which can be pre-trained models or custom models. </p> <p>This enables you to generate embeddings tailored to the nature of your data (e.g., text, images) and store both the original data and embeddings in a structured schema thus providing efficient querying capabilities for similarity searches.</p> <p>Let's quickly get started and learn how to manage embeddings in LanceDB. </p>"},{"location":"embeddings/understanding_embeddings/#bonus-as-a-developer-what-you-can-create-using-embeddings","title":"Bonus: As a developer, what you can create using embeddings?","text":"<p>As a developer, you can create a variety of innovative applications using vector embeddings. Check out the following - </p> <ul> <li> <p>Chatbots</p> <p>Develop chatbots that utilize embeddings to retrieve relevant context and generate coherent, contextually aware responses to user queries.</p> <p> Check out examples</p> </li> <li> <p>Recommendation Systems</p> <p>Develop systems that recommend content (such as articles, movies, or products) based on the similarity of keywords and descriptions, enhancing user experience.</p> <p> Check out examples</p> </li> <li> <p>Vector Search</p> <p>Build powerful applications that harness the full potential of semantic search, enabling them to retrieve relevant data quickly and effectively. </p> <p> Check out examples</p> </li> <li> <p>RAG Applications</p> <p>Combine the strengths of large language models (LLMs) with retrieval-based approaches to create more useful applications.</p> <p> Check out examples</p> </li> <li> <p>Many more examples</p> <p>Explore applied examples available as Colab notebooks or Python scripts to integrate into your applications.</p> <p> More</p> </li> </ul>"},{"location":"embeddings/variables_and_secrets/","title":"Variables and Secrets in LanceDB","text":"<p>Most embedding configuration options are saved in the table's metadata. However, this isn't always appropriate. For example, API keys should never be stored in the metadata. Additionally, other configuration options might be best set at runtime, such as the <code>device</code> configuration that controls whether to use GPU or CPU for inference. If you hardcoded this to GPU, you wouldn't be able to run the code on a server without one.</p> <p>To handle these cases, you can set variables on the embedding registry and reference them in the embedding configuration. These variables will be available during the runtime of your program, but not saved in the table's metadata. When the table is loaded from a different process, the variables must be set again.</p> <p>To set a variable, use the <code>set_var()</code> / <code>setVar()</code> method on the embedding registry. To reference a variable, use the syntax <code>$env:VARIABLE_NAME</code>. If there is a default value, you can use the syntax <code>$env:VARIABLE_NAME:DEFAULT_VALUE</code>.</p>"},{"location":"embeddings/variables_and_secrets/#using-variables-to-set-secrets","title":"Using variables to set secrets","text":"<p>Sensitive configuration, such as API keys, must either be set as environment variables or using variables on the embedding registry. If you pass in a hardcoded value, LanceDB will raise an error. Instead, if you want to set an API key via configuration, use a variable:</p> PythonTypescript <pre><code>registry = get_registry()\nregistry.set_var(\"api_key\", \"sk-...\")\n\nfunc = registry.get(\"openai\").create(api_key=\"$var:api_key\")\n</code></pre> <pre><code>const registry = getRegistry();\nregistry.setVar(\"api_key\", \"sk-...\");\n\nconst func = registry.get(\"openai\")!.create({\n  apiKey: \"$var:api_key\",\n});\n</code></pre>"},{"location":"embeddings/variables_and_secrets/#using-variables-to-set-the-device-parameter","title":"Using variables to set the device parameter","text":"<p>Many embedding functions that run locally have a <code>device</code> parameter that controls whether to use GPU or CPU for inference. Because not all computers have a GPU, it's helpful to be able to set the <code>device</code> parameter at runtime, rather than have it hard coded in the embedding configuration. To make it work even if the variable isn't set, you could provide a default value of <code>cpu</code> in the embedding configuration.</p> <p>Some embedding libraries even have a method to detect which devices are available, which could be used to dynamically set the device at runtime. For example, in Python you can check if a CUDA GPU is available using <code>torch.cuda.is_available()</code>.</p> <pre><code>import torch\n\nregistry = get_registry()\nif torch.cuda.is_available():\n    registry.set_var(\"device\", \"cuda\")\n\nfunc = registry.get(\"huggingface\").create(device=\"$var:device:cpu\")\n</code></pre>"},{"location":"embeddings/available_embedding_models/multimodal_embedding_functions/imagebind_embedding/","title":"ImageBind Embedding Models","text":"<p>We have support for imagebind model embeddings. You can download our version of the packaged model via - <code>pip install imagebind-packaged==0.1.2</code>.</p> <p>This function is registered as <code>imagebind</code> and supports Audio, Video and Text modalities(extending to Thermal,Depth,IMU data):</p> Parameter Type Default Value Description <code>name</code> <code>str</code> <code>\"imagebind_huge\"</code> Name of the model. <code>device</code> <code>str</code> <code>\"cpu\"</code> The device to run the model on. Can be <code>\"cpu\"</code> or <code>\"gpu\"</code>. <code>normalize</code> <code>bool</code> <code>False</code> set to <code>True</code> to normalize your inputs before model ingestion. <p>Below is an example demonstrating how the API works:</p> <pre><code>import lancedb\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.embeddings import get_registry\n\ndb = lancedb.connect(tmp_path)\nfunc = get_registry().get(\"imagebind\").create()\n\nclass ImageBindModel(LanceModel):\n    text: str\n    image_uri: str = func.SourceField()\n    audio_path: str\n    vector: Vector(func.ndims()) = func.VectorField()\n\n# add locally accessible image paths\ntext_list=[\"A dog.\", \"A car\", \"A bird\"]\nimage_paths=[\".assets/dog_image.jpg\", \".assets/car_image.jpg\", \".assets/bird_image.jpg\"]\naudio_paths=[\".assets/dog_audio.wav\", \".assets/car_audio.wav\", \".assets/bird_audio.wav\"]\n\n# Load data\ninputs = [\n    {\"text\": a, \"audio_path\": b, \"image_uri\": c}\n    for a, b, c in zip(text_list, audio_paths, image_paths)\n]\n\n#create table and add data\ntable = db.create_table(\"img_bind\", schema=ImageBindModel)\ntable.add(inputs)\n</code></pre> <p>Now, we can search using any modality:</p>"},{"location":"embeddings/available_embedding_models/multimodal_embedding_functions/imagebind_embedding/#image-search","title":"image search","text":"<pre><code>query_image = \"./assets/dog_image2.jpg\" #download an image and enter that path here\nactual = table.search(query_image).limit(1).to_pydantic(ImageBindModel)[0]\nprint(actual.text == \"dog\")\n</code></pre>"},{"location":"embeddings/available_embedding_models/multimodal_embedding_functions/imagebind_embedding/#audio-search","title":"audio search","text":"<pre><code>query_audio = \"./assets/car_audio2.wav\" #download an audio clip and enter path here\nactual = table.search(query_audio).limit(1).to_pydantic(ImageBindModel)[0]\nprint(actual.text == \"car\")\n</code></pre>"},{"location":"embeddings/available_embedding_models/multimodal_embedding_functions/imagebind_embedding/#text-search","title":"Text search","text":"<p>You can add any input query and fetch the result as follows: <pre><code>query = \"an animal which flies and tweets\" \nactual = table.search(query).limit(1).to_pydantic(ImageBindModel)[0]\nprint(actual.text == \"bird\")\n</code></pre></p> <p>If you have any questions about the embeddings API, supported models, or see a relevant model missing, please raise an issue on GitHub.</p>"},{"location":"embeddings/available_embedding_models/multimodal_embedding_functions/jina_multimodal_embedding/","title":"Jina Multimodal Embedding Models","text":"<p>Jina embeddings can also be used to embed both text and image data, only some of the models support image data and you can check the list under https://jina.ai/embeddings/</p> <p>Supported parameters (to be passed in <code>create</code> method) are:</p> Parameter Type Default Value Description <code>name</code> <code>str</code> <code>\"jina-clip-v1\"</code> The model ID of the jina model to use <p>Usage Example:</p> <pre><code>    import os\n    import requests\n    import lancedb\n    from lancedb.pydantic import LanceModel, Vector\n    from lancedb.embeddings import get_registry\n    import pandas as pd\n\n    os.environ['JINA_API_KEY'] = 'jina_*'\n\n    db = lancedb.connect(\"~/.lancedb\")\n    func = get_registry().get(\"jina\").create()\n\n\n    class Images(LanceModel):\n        label: str\n        image_uri: str = func.SourceField()  # image uri as the source\n        image_bytes: bytes = func.SourceField()  # image bytes as the source\n        vector: Vector(func.ndims()) = func.VectorField()  # vector column\n        vec_from_bytes: Vector(func.ndims()) = func.VectorField()  # Another vector column\n\n\n    table = db.create_table(\"images\", schema=Images)\n    labels = [\"cat\", \"cat\", \"dog\", \"dog\", \"horse\", \"horse\"]\n    uris = [\n        \"http://farm1.staticflickr.com/53/167798175_7c7845bbbd_z.jpg\",\n        \"http://farm1.staticflickr.com/134/332220238_da527d8140_z.jpg\",\n        \"http://farm9.staticflickr.com/8387/8602747737_2e5c2a45d4_z.jpg\",\n        \"http://farm5.staticflickr.com/4092/5017326486_1f46057f5f_z.jpg\",\n        \"http://farm9.staticflickr.com/8216/8434969557_d37882c42d_z.jpg\",\n        \"http://farm6.staticflickr.com/5142/5835678453_4f3a4edb45_z.jpg\",\n    ]\n    # get each uri as bytes\n    image_bytes = [requests.get(uri).content for uri in uris]\n    table.add(\n      pd.DataFrame({\"label\": labels, \"image_uri\": uris, \"image_bytes\": image_bytes})\n    )\n</code></pre>"},{"location":"embeddings/available_embedding_models/multimodal_embedding_functions/openclip_embedding/","title":"OpenClip Embedding Models","text":"<p>We support CLIP model embeddings using the open source alternative, open-clip which supports various customizations. It is registered as <code>open-clip</code> and supports the following customizations:</p> Parameter Type Default Value Description <code>name</code> <code>str</code> <code>\"ViT-B-32\"</code> The name of the model. <code>pretrained</code> <code>str</code> <code>\"laion2b_s34b_b79k\"</code> The name of the pretrained model to load. <code>device</code> <code>str</code> <code>\"cpu\"</code> The device to run the model on. Can be <code>\"cpu\"</code> or <code>\"gpu\"</code>. <code>batch_size</code> <code>int</code> <code>64</code> The number of images to process in a batch. <code>normalize</code> <code>bool</code> <code>True</code> Whether to normalize the input images before feeding them to the model. <p>This embedding function supports ingesting images as both bytes and urls. You can query them using both test and other images.</p> <p>Info</p> <p>LanceDB supports ingesting images directly from accessible links.</p> <p><pre><code>import lancedb\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.embeddings import get_registry\n\ndb = lancedb.connect(tmp_path)\nfunc = get_registry().get(\"open-clip\").create()\n\nclass Images(LanceModel):\n    label: str\n    image_uri: str = func.SourceField() # image uri as the source\n    image_bytes: bytes = func.SourceField() # image bytes as the source\n    vector: Vector(func.ndims()) = func.VectorField() # vector column \n    vec_from_bytes: Vector(func.ndims()) = func.VectorField() # Another vector column \n\ntable = db.create_table(\"images\", schema=Images)\nlabels = [\"cat\", \"cat\", \"dog\", \"dog\", \"horse\", \"horse\"]\nuris = [\n    \"http://farm1.staticflickr.com/53/167798175_7c7845bbbd_z.jpg\",\n    \"http://farm1.staticflickr.com/134/332220238_da527d8140_z.jpg\",\n    \"http://farm9.staticflickr.com/8387/8602747737_2e5c2a45d4_z.jpg\",\n    \"http://farm5.staticflickr.com/4092/5017326486_1f46057f5f_z.jpg\",\n    \"http://farm9.staticflickr.com/8216/8434969557_d37882c42d_z.jpg\",\n    \"http://farm6.staticflickr.com/5142/5835678453_4f3a4edb45_z.jpg\",\n]\n# get each uri as bytes\nimage_bytes = [requests.get(uri).content for uri in uris]\ntable.add(\n    pd.DataFrame({\"label\": labels, \"image_uri\": uris, \"image_bytes\": image_bytes})\n)\n</code></pre> Now we can search using text from both the default vector column and the custom vector column <pre><code># text search\nactual = table.search(\"man's best friend\").limit(1).to_pydantic(Images)[0]\nprint(actual.label) # prints \"dog\"\n\nfrombytes = (\n    table.search(\"man's best friend\", vector_column_name=\"vec_from_bytes\")\n    .limit(1)\n    .to_pydantic(Images)[0]\n)\nprint(frombytes.label)\n</code></pre></p> <p>Because we're using a multi-modal embedding function, we can also search using images</p> <pre><code># image search\nquery_image_uri = \"http://farm1.staticflickr.com/200/467715466_ed4a31801f_z.jpg\"\nimage_bytes = requests.get(query_image_uri).content\nquery_image = Image.open(io.BytesIO(image_bytes))\nactual = table.search(query_image).limit(1).to_pydantic(Images)[0]\nprint(actual.label == \"dog\")\n\n# image search using a custom vector column\nother = (\n    table.search(query_image, vector_column_name=\"vec_from_bytes\")\n    .limit(1)\n    .to_pydantic(Images)[0]\n)\nprint(actual.label)\n</code></pre>"},{"location":"embeddings/available_embedding_models/text_embedding_functions/aws_bedrock_embedding/","title":"AWS Bedrock Text Embedding Models","text":"<p>AWS Bedrock supports multiple base models for generating text embeddings. You need to setup the AWS credentials to use this embedding function. You can do so by using <code>awscli</code> and also add your session_token: <pre><code>aws configure\naws configure set aws_session_token \"&lt;your_session_token&gt;\"\n</code></pre> to ensure that the credentials are set up correctly, you can run the following command: <pre><code>aws sts get-caller-identity\n</code></pre></p> <p>Supported Embedding modelIDs are: * <code>amazon.titan-embed-text-v1</code> * <code>cohere.embed-english-v3</code> * <code>cohere.embed-multilingual-v3</code></p> <p>Supported parameters (to be passed in <code>create</code> method) are:</p> Parameter Type Default Value Description name str \"amazon.titan-embed-text-v1\" The model ID of the bedrock model to use. Supported base models for Text Embeddings: amazon.titan-embed-text-v1, cohere.embed-english-v3, cohere.embed-multilingual-v3 region str \"us-east-1\" Optional name of the AWS Region in which the service should be called (e.g., \"us-east-1\"). profile_name str None Optional name of the AWS profile to use for calling the Bedrock service. If not specified, the default profile will be used. assumed_role str None Optional ARN of an AWS IAM role to assume for calling the Bedrock service. If not specified, the current active credentials will be used. role_session_name str \"lancedb-embeddings\" Optional name of the AWS IAM role session to use for calling the Bedrock service. If not specified, a \"lancedb-embeddings\" name will be used. runtime bool True Optional choice of getting different client to perform operations with the Amazon Bedrock service. max_retries int 7 Optional number of retries to perform when a request fails. <p>Usage Example:</p> <pre><code>import lancedb\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.embeddings import get_registry\nimport pandas as pd\n\nmodel = get_registry().get(\"bedrock-text\").create()\n\nclass TextModel(LanceModel):\n    text: str = model.SourceField()\n    vector: Vector(model.ndims()) = model.VectorField()\n\ndf = pd.DataFrame({\"text\": [\"hello world\", \"goodbye world\"]})\ndb = lancedb.connect(\"tmp_path\")\ntbl = db.create_table(\"test\", schema=TextModel, mode=\"overwrite\")\n\ntbl.add(df)\nrs = tbl.search(\"hello\").limit(1).to_pandas()\n</code></pre>"},{"location":"embeddings/available_embedding_models/text_embedding_functions/cohere_embedding/","title":"Cohere Embedding Models","text":"<p>Using cohere API requires cohere package, which can be installed using <code>pip install cohere</code>. Cohere embeddings are used to generate embeddings for text data. The embeddings can be used for various tasks like semantic search, clustering, and classification. You also need to set the <code>COHERE_API_KEY</code> environment variable to use the Cohere API.</p> <p>Supported models are:</p> <ul> <li>embed-english-v3.0</li> <li>embed-multilingual-v3.0</li> <li>embed-english-light-v3.0</li> <li>embed-multilingual-light-v3.0</li> <li>embed-english-v2.0</li> <li>embed-english-light-v2.0</li> <li>embed-multilingual-v2.0</li> </ul> <p>Supported parameters (to be passed in <code>create</code> method) are:</p> Parameter Type Default Value Description <code>name</code> <code>str</code> <code>\"embed-english-v2.0\"</code> The model ID of the cohere model to use. Supported base models for Text Embeddings: embed-english-v3.0, embed-multilingual-v3.0, embed-english-light-v3.0, embed-multilingual-light-v3.0, embed-english-v2.0, embed-english-light-v2.0, embed-multilingual-v2.0 <code>source_input_type</code> <code>str</code> <code>\"search_document\"</code> The type of input data to be used for the source column. <code>query_input_type</code> <code>str</code> <code>\"search_query\"</code> The type of input data to be used for the query. <p>Cohere supports following input types:</p> Input Type Description \"<code>search_document</code>\" Used for embeddings stored in a vector database for search use-cases. \"<code>search_query</code>\" Used for embeddings of search queries run against a vector DB \"<code>semantic_similarity</code>\" Specifies the given text will be used for Semantic Textual Similarity (STS) \"<code>classification</code>\" Used for embeddings passed through a text classifier. \"<code>clustering</code>\" Used for the embeddings run through a clustering algorithm <p>Usage Example:</p> <pre><code>    import lancedb\n    from lancedb.pydantic import LanceModel, Vector\n    from lancedb.embeddings import EmbeddingFunctionRegistry\n\n    cohere = EmbeddingFunctionRegistry\n        .get_instance()\n        .get(\"cohere\")\n        .create(name=\"embed-multilingual-v2.0\")\n\n    class TextModel(LanceModel):\n        text: str = cohere.SourceField()\n        vector: Vector(cohere.ndims()) =  cohere.VectorField()\n\n    data = [ { \"text\": \"hello world\" },\n            { \"text\": \"goodbye world\" }]\n\n    db = lancedb.connect(\"~/.lancedb\")\n    tbl = db.create_table(\"test\", schema=TextModel, mode=\"overwrite\")\n\n    tbl.add(data)\n</code></pre>"},{"location":"embeddings/available_embedding_models/text_embedding_functions/gemini_embedding/","title":"Gemini Embedding Models","text":"<p>With Google's Gemini, you can represent text (words, sentences, and blocks of text) in a vectorized form, making it easier to compare and contrast embeddings. For example, two texts that share a similar subject matter or sentiment should have similar embeddings, which can be identified through mathematical comparison techniques such as cosine similarity. For more on how and why you should use embeddings, refer to the Embeddings guide. The Gemini Embedding Model API supports various task types:</p> Task Type Description \"<code>retrieval_query</code>\" Specifies the given text is a query in a search/retrieval setting. \"<code>retrieval_document</code>\" Specifies the given text is a document in a search/retrieval setting. Using this task type requires a title but is automatically proided by Embeddings API \"<code>semantic_similarity</code>\" Specifies the given text will be used for Semantic Textual Similarity (STS). \"<code>classification</code>\" Specifies that the embeddings will be used for classification. \"<code>clusering</code>\" Specifies that the embeddings will be used for clustering. <p>Usage Example:</p> <pre><code>import lancedb\nimport pandas as pd\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.embeddings import get_registry\n\n\nmodel = get_registry().get(\"gemini-text\").create()\n\nclass TextModel(LanceModel):\n    text: str = model.SourceField()\n    vector: Vector(model.ndims()) = model.VectorField()\n\ndf = pd.DataFrame({\"text\": [\"hello world\", \"goodbye world\"]})\ndb = lancedb.connect(\"~/.lancedb\")\ntbl = db.create_table(\"test\", schema=TextModel, mode=\"overwrite\")\n\ntbl.add(df)\nrs = tbl.search(\"hello\").limit(1).to_pandas()\n</code></pre>"},{"location":"embeddings/available_embedding_models/text_embedding_functions/huggingface_embedding/","title":"HuggingFace Embedding Models","text":"<p>We offer support for all Hugging Face models (which can be loaded via transformers library). The default model is <code>colbert-ir/colbertv2.0</code> which also has its own special callout - <code>registry.get(\"colbert\")</code>. Some Hugging Face models might require custom models defined on the HuggingFace Hub in their own modeling files. You may enable this by setting <code>trust_remote_code=True</code>. This option should only be set to True for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine. </p> <p>Example usage -  <pre><code>import lancedb\nimport pandas as pd\n\nfrom lancedb.embeddings import get_registry\nfrom lancedb.pydantic import LanceModel, Vector\n\nmodel = get_registry().get(\"huggingface\").create(name='facebook/bart-base')\n\nclass Words(LanceModel):\n    text: str = model.SourceField()\n    vector: Vector(model.ndims()) = model.VectorField()\n\ndf = pd.DataFrame({\"text\": [\"hi hello sayonara\", \"goodbye world\"]})\ntable = db.create_table(\"greets\", schema=Words)\ntable.add(df)\nquery = \"old greeting\"\nactual = table.search(query).limit(1).to_pydantic(Words)[0]\nprint(actual.text)\n</code></pre></p>"},{"location":"embeddings/available_embedding_models/text_embedding_functions/ibm_watsonx_ai_embedding/","title":"IBM watsonx.ai Embedding Model","text":"<p>Generate text embeddings using IBM's watsonx.ai platform.</p>"},{"location":"embeddings/available_embedding_models/text_embedding_functions/ibm_watsonx_ai_embedding/#supported-models","title":"Supported Models","text":"<p>You can find a list of supported models at IBM watsonx.ai Documentation. The currently supported model names are:</p> <ul> <li><code>ibm/slate-125m-english-rtrvr</code></li> <li><code>ibm/slate-30m-english-rtrvr</code></li> <li><code>sentence-transformers/all-minilm-l12-v2</code></li> <li><code>intfloat/multilingual-e5-large</code></li> </ul>"},{"location":"embeddings/available_embedding_models/text_embedding_functions/ibm_watsonx_ai_embedding/#parameters","title":"Parameters","text":"<p>The following parameters can be passed to the <code>create</code> method:</p> Parameter Type Default Value Description name str \"ibm/slate-125m-english-rtrvr\" The model ID of the watsonx.ai model to use api_key str None Optional IBM Cloud API key (or set <code>WATSONX_API_KEY</code>) project_id str None Optional watsonx project ID (or set <code>WATSONX_PROJECT_ID</code>) url str None Optional custom URL for the watsonx.ai instance params dict None Optional additional parameters for the embedding model"},{"location":"embeddings/available_embedding_models/text_embedding_functions/ibm_watsonx_ai_embedding/#usage-example","title":"Usage Example","text":"<p>First, the watsonx.ai library is an optional dependency, so must be installed seperately:</p> <pre><code>pip install ibm-watsonx-ai\n</code></pre> <p>Optionally set environment variables (if not passing credentials to <code>create</code> directly):</p> <pre><code>export WATSONX_API_KEY=\"YOUR_WATSONX_API_KEY\"\nexport WATSONX_PROJECT_ID=\"YOUR_WATSONX_PROJECT_ID\"\n</code></pre> <pre><code>import os\nimport lancedb\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.embeddings import EmbeddingFunctionRegistry\n\nwatsonx_embed = EmbeddingFunctionRegistry\n  .get_instance()\n  .get(\"watsonx\")\n  .create(\n    name=\"ibm/slate-125m-english-rtrvr\",\n    # Uncomment and set these if not using environment variables\n    # api_key=\"your_api_key_here\",\n    # project_id=\"your_project_id_here\",\n    # url=\"your_watsonx_url_here\",\n    # params={...},\n  )\n\nclass TextModel(LanceModel):\n    text: str = watsonx_embed.SourceField()\n    vector: Vector(watsonx_embed.ndims()) = watsonx_embed.VectorField()\n\ndata = [\n    {\"text\": \"hello world\"},\n    {\"text\": \"goodbye world\"},\n]\n\ndb = lancedb.connect(\"~/.lancedb\")\ntbl = db.create_table(\"watsonx_test\", schema=TextModel, mode=\"overwrite\")\n\ntbl.add(data)\n\nrs = tbl.search(\"hello\").limit(1).to_pandas()\nprint(rs)\n</code></pre>"},{"location":"embeddings/available_embedding_models/text_embedding_functions/instructor_embedding/","title":"Instructor Embedding Model","text":"<p>Instructor is an instruction-finetuned text embedding model that can generate text embeddings tailored to any task (e.g. classification, retrieval, clustering, text evaluation, etc.) and domains (e.g. science, finance, etc.) by simply providing the task instruction, without any finetuning.</p> <p>If you want to calculate customized embeddings for specific sentences, you can follow the unified template to write instructions.</p> <p>Info</p> <p>Represent the <code>domain</code> <code>text_type</code> for <code>task_objective</code>:</p> <ul> <li><code>domain</code> is optional, and it specifies the domain of the text, e.g. science, finance, medicine, etc.</li> <li><code>text_type</code> is required, and it specifies the encoding unit, e.g. sentence, document, paragraph, etc.</li> <li><code>task_objective</code> is optional, and it specifies the objective of embedding, e.g. retrieve a document, classify the sentence, etc.</li> </ul> <p>More information about the model can be found at the source URL.</p> Argument Type Default Description <code>name</code> <code>str</code> \"hkunlp/instructor-base\" The name of the model to use <code>batch_size</code> <code>int</code> <code>32</code> The batch size to use when generating embeddings <code>device</code> <code>str</code> <code>\"cpu\"</code> The device to use when generating embeddings <code>show_progress_bar</code> <code>bool</code> <code>True</code> Whether to show a progress bar when generating embeddings <code>normalize_embeddings</code> <code>bool</code> <code>True</code> Whether to normalize the embeddings <code>quantize</code> <code>bool</code> <code>False</code> Whether to quantize the model <code>source_instruction</code> <code>str</code> <code>\"represent the docuement for retreival\"</code> The instruction for the source column <code>query_instruction</code> <code>str</code> <code>\"represent the document for retreiving the most similar documents\"</code> The instruction for the query <pre><code>import lancedb\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.embeddings import get_registry, InstuctorEmbeddingFunction\n\ninstructor = get_registry().get(\"instructor\").create(\n                            source_instruction=\"represent the docuement for retreival\",\n                            query_instruction=\"represent the document for retreiving the most similar documents\"\n                            )\n\nclass Schema(LanceModel):\n    vector: Vector(instructor.ndims()) = instructor.VectorField()\n    text: str = instructor.SourceField()\n\ndb = lancedb.connect(\"~/.lancedb\")\ntbl = db.create_table(\"test\", schema=Schema, mode=\"overwrite\")\n\ntexts = [{\"text\": \"Capitalism has been dominant in the Western world since the end of feudalism, but most feel[who?] that...\"},\n        {\"text\": \"The disparate impact theory is especially controversial under the Fair Housing Act because the Act...\"},\n        {\"text\": \"Disparate impact in United States labor law refers to practices in employment, housing, and other areas that..\"}]\n\ntbl.add(texts)\n</code></pre>"},{"location":"embeddings/available_embedding_models/text_embedding_functions/jina_embedding/","title":"Jina Embedding Model","text":"<p>Jina embeddings are used to generate embeddings for text and image data. You also need to set the <code>JINA_API_KEY</code> environment variable to use the Jina API.</p> <p>You can find a list of supported models under https://jina.ai/embeddings/</p> <p>Supported parameters (to be passed in <code>create</code> method) are:</p> Parameter Type Default Value Description <code>name</code> <code>str</code> <code>\"jina-clip-v1\"</code> The model ID of the jina model to use <p>Usage Example:</p> <pre><code>    import os\n    import lancedb\n    from lancedb.pydantic import LanceModel, Vector\n    from lancedb.embeddings import EmbeddingFunctionRegistry\n\n    os.environ['JINA_API_KEY'] = 'jina_*'\n\n    jina_embed = EmbeddingFunctionRegistry.get_instance().get(\"jina\").create(name=\"jina-embeddings-v2-base-en\")\n\n\n    class TextModel(LanceModel):\n        text: str = jina_embed.SourceField()\n        vector: Vector(jina_embed.ndims()) = jina_embed.VectorField()\n\n\n    data = [{\"text\": \"hello world\"},\n            {\"text\": \"goodbye world\"}]\n\n    db = lancedb.connect(\"~/.lancedb-2\")\n    tbl = db.create_table(\"test\", schema=TextModel, mode=\"overwrite\")\n\n    tbl.add(data)\n</code></pre>"},{"location":"embeddings/available_embedding_models/text_embedding_functions/ollama_embedding/","title":"Ollama Embedding Model","text":"<p>Generate embeddings via the ollama python library. More details:</p> <ul> <li>Ollama docs on embeddings</li> <li>Ollama blog on embeddings</li> </ul> Parameter Type Default Value Description <code>name</code> <code>str</code> <code>nomic-embed-text</code> The name of the model. <code>host</code> <code>str</code> <code>http://localhost:11434</code> The Ollama host to connect to. <code>options</code> <code>ollama.Options</code> or <code>dict</code> <code>None</code> Additional model parameters listed in the documentation for the Modelfile such as <code>temperature</code>. <code>keep_alive</code> <code>float</code> or <code>str</code> <code>\"5m\"</code> Controls how long the model will stay loaded into memory following the request. <code>ollama_client_kwargs</code> <code>dict</code> <code>{}</code> kwargs that can be past to the <code>ollama.Client</code>. <pre><code>import lancedb\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.embeddings import get_registry\n\ndb = lancedb.connect(\"/tmp/db\")\nfunc = get_registry().get(\"ollama\").create(name=\"nomic-embed-text\")\n\nclass Words(LanceModel):\n    text: str = func.SourceField()\n    vector: Vector(func.ndims()) = func.VectorField()\n\ntable = db.create_table(\"words\", schema=Words, mode=\"overwrite\")\ntable.add([\n    {\"text\": \"hello world\"},\n    {\"text\": \"goodbye world\"}\n])\n\nquery = \"greetings\"\nactual = table.search(query).limit(1).to_pydantic(Words)[0]\nprint(actual.text)\n</code></pre>"},{"location":"embeddings/available_embedding_models/text_embedding_functions/openai_embedding/","title":"OpenAI Embedding Model","text":"<p>LanceDB registers the OpenAI embeddings function in the registry by default, as <code>openai</code>. Below are the parameters that you can customize when creating the instances:</p> Parameter Type Default Value Description <code>name</code> <code>str</code> <code>\"text-embedding-ada-002\"</code> The name of the model. <code>dim</code> <code>int</code> Model default For OpenAI's newer text-embedding-3 model, we can specify a dimensionality that is smaller than the 1536 size. This feature supports it <code>use_azure</code> bool <code>False</code> Set true to use Azure OpenAPI SDK <pre><code>import lancedb\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.embeddings import get_registry\n\ndb = lancedb.connect(\"/tmp/db\")\nfunc = get_registry().get(\"openai\").create(name=\"text-embedding-ada-002\")\n\nclass Words(LanceModel):\n    text: str = func.SourceField()\n    vector: Vector(func.ndims()) = func.VectorField()\n\ntable = db.create_table(\"words\", schema=Words, mode=\"overwrite\")\ntable.add(\n    [\n        {\"text\": \"hello world\"},\n        {\"text\": \"goodbye world\"}\n    ]\n    )\n\nquery = \"greetings\"\nactual = table.search(query).limit(1).to_pydantic(Words)[0]\nprint(actual.text)\n</code></pre>"},{"location":"embeddings/available_embedding_models/text_embedding_functions/sentence_transformers/","title":"Sentence Transformers Embedding Model","text":"<p>Allows you to set parameters when registering a <code>sentence-transformers</code> object.</p> <p>Info</p> <p>Sentence transformer embeddings are normalized by default. It is recommended to use normalized embeddings for similarity search.</p> Parameter Type Default Value Description <code>name</code> <code>str</code> <code>all-MiniLM-L6-v2</code> The name of the model <code>device</code> <code>str</code> <code>cpu</code> The device to run the model on (can be <code>cpu</code> or <code>gpu</code>) <code>normalize</code> <code>bool</code> <code>True</code> Whether to normalize the input text before feeding it to the model <code>trust_remote_code</code> <code>bool</code> <code>False</code> Whether to trust and execute remote code from the model's Huggingface repository Check out available sentence-transformer models here! <pre><code>- sentence-transformers/all-MiniLM-L12-v2\n- sentence-transformers/paraphrase-mpnet-base-v2\n- sentence-transformers/gtr-t5-base\n- sentence-transformers/LaBSE\n- sentence-transformers/all-MiniLM-L6-v2\n- sentence-transformers/bert-base-nli-max-tokens\n- sentence-transformers/bert-base-nli-mean-tokens\n- sentence-transformers/bert-base-nli-stsb-mean-tokens\n- sentence-transformers/bert-base-wikipedia-sections-mean-tokens\n- sentence-transformers/bert-large-nli-cls-token\n- sentence-transformers/bert-large-nli-max-tokens\n- sentence-transformers/bert-large-nli-mean-tokens\n- sentence-transformers/bert-large-nli-stsb-mean-tokens\n- sentence-transformers/distilbert-base-nli-max-tokens\n- sentence-transformers/distilbert-base-nli-mean-tokens\n- sentence-transformers/distilbert-base-nli-stsb-mean-tokens\n- sentence-transformers/distilroberta-base-msmarco-v1\n- sentence-transformers/distilroberta-base-msmarco-v2\n- sentence-transformers/nli-bert-base-cls-pooling\n- sentence-transformers/nli-bert-base-max-pooling\n- sentence-transformers/nli-bert-base\n- sentence-transformers/nli-bert-large-cls-pooling\n- sentence-transformers/nli-bert-large-max-pooling\n- sentence-transformers/nli-bert-large\n- sentence-transformers/nli-distilbert-base-max-pooling\n- sentence-transformers/nli-distilbert-base\n- sentence-transformers/nli-roberta-base\n- sentence-transformers/nli-roberta-large\n- sentence-transformers/roberta-base-nli-mean-tokens\n- sentence-transformers/roberta-base-nli-stsb-mean-tokens\n- sentence-transformers/roberta-large-nli-mean-tokens\n- sentence-transformers/roberta-large-nli-stsb-mean-tokens\n- sentence-transformers/stsb-bert-base\n- sentence-transformers/stsb-bert-large\n- sentence-transformers/stsb-distilbert-base\n- sentence-transformers/stsb-roberta-base\n- sentence-transformers/stsb-roberta-large\n- sentence-transformers/xlm-r-100langs-bert-base-nli-mean-tokens\n- sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens\n- sentence-transformers/xlm-r-base-en-ko-nli-ststb\n- sentence-transformers/xlm-r-bert-base-nli-mean-tokens\n- sentence-transformers/xlm-r-bert-base-nli-stsb-mean-tokens\n- sentence-transformers/xlm-r-large-en-ko-nli-ststb\n- sentence-transformers/bert-base-nli-cls-token\n- sentence-transformers/all-distilroberta-v1\n- sentence-transformers/multi-qa-MiniLM-L6-dot-v1\n- sentence-transformers/multi-qa-distilbert-cos-v1\n- sentence-transformers/multi-qa-distilbert-dot-v1\n- sentence-transformers/multi-qa-mpnet-base-cos-v1\n- sentence-transformers/multi-qa-mpnet-base-dot-v1\n- sentence-transformers/nli-distilroberta-base-v2\n- sentence-transformers/all-MiniLM-L6-v1\n- sentence-transformers/all-mpnet-base-v1\n- sentence-transformers/all-mpnet-base-v2\n- sentence-transformers/all-roberta-large-v1\n- sentence-transformers/allenai-specter\n- sentence-transformers/average_word_embeddings_glove.6B.300d\n- sentence-transformers/average_word_embeddings_glove.840B.300d\n- sentence-transformers/average_word_embeddings_komninos\n- sentence-transformers/average_word_embeddings_levy_dependency\n- sentence-transformers/clip-ViT-B-32-multilingual-v1\n- sentence-transformers/clip-ViT-B-32\n- sentence-transformers/distilbert-base-nli-stsb-quora-ranking\n- sentence-transformers/distilbert-multilingual-nli-stsb-quora-ranking\n- sentence-transformers/distilroberta-base-paraphrase-v1\n- sentence-transformers/distiluse-base-multilingual-cased-v1\n- sentence-transformers/distiluse-base-multilingual-cased-v2\n- sentence-transformers/distiluse-base-multilingual-cased\n- sentence-transformers/facebook-dpr-ctx_encoder-multiset-base\n- sentence-transformers/facebook-dpr-ctx_encoder-single-nq-base\n- sentence-transformers/facebook-dpr-question_encoder-multiset-base\n- sentence-transformers/facebook-dpr-question_encoder-single-nq-base\n- sentence-transformers/gtr-t5-large\n- sentence-transformers/gtr-t5-xl\n- sentence-transformers/gtr-t5-xxl\n- sentence-transformers/msmarco-MiniLM-L-12-v3\n- sentence-transformers/msmarco-MiniLM-L-6-v3\n- sentence-transformers/msmarco-MiniLM-L12-cos-v5\n- sentence-transformers/msmarco-MiniLM-L6-cos-v5\n- sentence-transformers/msmarco-bert-base-dot-v5\n- sentence-transformers/msmarco-bert-co-condensor\n- sentence-transformers/msmarco-distilbert-base-dot-prod-v3\n- sentence-transformers/msmarco-distilbert-base-tas-b\n- sentence-transformers/msmarco-distilbert-base-v2\n- sentence-transformers/msmarco-distilbert-base-v3\n- sentence-transformers/msmarco-distilbert-base-v4\n- sentence-transformers/msmarco-distilbert-cos-v5\n- sentence-transformers/msmarco-distilbert-dot-v5\n- sentence-transformers/msmarco-distilbert-multilingual-en-de-v2-tmp-lng-aligned\n- sentence-transformers/msmarco-distilbert-multilingual-en-de-v2-tmp-trained-scratch\n- sentence-transformers/msmarco-distilroberta-base-v2\n- sentence-transformers/msmarco-roberta-base-ance-firstp\n- sentence-transformers/msmarco-roberta-base-v2\n- sentence-transformers/msmarco-roberta-base-v3\n- sentence-transformers/multi-qa-MiniLM-L6-cos-v1\n- sentence-transformers/nli-mpnet-base-v2\n- sentence-transformers/nli-roberta-base-v2\n- sentence-transformers/nq-distilbert-base-v1\n- sentence-transformers/paraphrase-MiniLM-L12-v2\n- sentence-transformers/paraphrase-MiniLM-L3-v2\n- sentence-transformers/paraphrase-MiniLM-L6-v2\n- sentence-transformers/paraphrase-TinyBERT-L6-v2\n- sentence-transformers/paraphrase-albert-base-v2\n- sentence-transformers/paraphrase-albert-small-v2\n- sentence-transformers/paraphrase-distilroberta-base-v1\n- sentence-transformers/paraphrase-distilroberta-base-v2\n- sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n- sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n- sentence-transformers/paraphrase-xlm-r-multilingual-v1\n- sentence-transformers/quora-distilbert-base\n- sentence-transformers/quora-distilbert-multilingual\n- sentence-transformers/sentence-t5-base\n- sentence-transformers/sentence-t5-large\n- sentence-transformers/sentence-t5-xxl\n- sentence-transformers/sentence-t5-xl\n- sentence-transformers/stsb-distilroberta-base-v2\n- sentence-transformers/stsb-mpnet-base-v2\n- sentence-transformers/stsb-roberta-base-v2\n- sentence-transformers/stsb-xlm-r-multilingual\n- sentence-transformers/xlm-r-distilroberta-base-paraphrase-v1\n- sentence-transformers/clip-ViT-L-14\n- sentence-transformers/clip-ViT-B-16\n- sentence-transformers/use-cmlm-multilingual\n- sentence-transformers/all-MiniLM-L12-v1\n</code></pre> <p>Info</p> <p>You can also load many other model architectures from the library. For example models from sources such as BAAI, nomic, salesforce research, etc. See this HF hub page for all supported models.</p> <p>BAAI Embeddings example</p> <p>Here is an example that uses BAAI embedding model from the HuggingFace Hub supported models <pre><code>import lancedb\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.embeddings import get_registry\n\ndb = lancedb.connect(\"/tmp/db\")\nmodel = get_registry().get(\"sentence-transformers\").create(name=\"BAAI/bge-small-en-v1.5\", device=\"cpu\")\n\nclass Words(LanceModel):\n    text: str = model.SourceField()\n    vector: Vector(model.ndims()) = model.VectorField()\n\ntable = db.create_table(\"words\", schema=Words)\ntable.add(\n    [\n        {\"text\": \"hello world\"},\n        {\"text\": \"goodbye world\"}\n    ]\n)\n\nquery = \"greetings\"\nactual = table.search(query).limit(1).to_pydantic(Words)[0]\nprint(actual.text)\n</code></pre></p> <p>Visit sentence-transformers HuggingFace HUB page for more information on the available models.</p>"},{"location":"embeddings/available_embedding_models/text_embedding_functions/voyageai_embedding/","title":"VoyageAI Embedding Models","text":"<p>Voyage AI provides cutting-edge embedding and rerankers.</p> <p>Using voyageai API requires voyageai package, which can be installed using <code>pip install voyageai</code>. Voyage AI embeddings are used to generate embeddings for text data. The embeddings can be used for various tasks like semantic search, clustering, and classification. You also need to set the <code>VOYAGE_API_KEY</code> environment variable to use the VoyageAI API.</p> <p>Supported models are:</p> <ul> <li>voyage-3</li> <li>voyage-3-lite</li> <li>voyage-finance-2</li> <li>voyage-multilingual-2</li> <li>voyage-law-2</li> <li>voyage-code-2</li> </ul> <p>Supported parameters (to be passed in <code>create</code> method) are:</p> Parameter Type Default Value Description <code>name</code> <code>str</code> <code>None</code> The model ID of the model to use. Supported base models for Text Embeddings: voyage-3, voyage-3-lite, voyage-finance-2, voyage-multilingual-2, voyage-law-2, voyage-code-2 <code>input_type</code> <code>str</code> <code>None</code> Type of the input text. Default to None. Other options: query, document. <code>truncation</code> <code>bool</code> <code>True</code> Whether to truncate the input texts to fit within the context length. <p>Usage Example:</p> <pre><code>    import lancedb\n    from lancedb.pydantic import LanceModel, Vector\n    from lancedb.embeddings import EmbeddingFunctionRegistry\n\n    voyageai = EmbeddingFunctionRegistry\n        .get_instance()\n        .get(\"voyageai\")\n        .create(name=\"voyage-3\")\n\n    class TextModel(LanceModel):\n        text: str = voyageai.SourceField()\n        vector: Vector(voyageai.ndims()) =  voyageai.VectorField()\n\n    data = [ { \"text\": \"hello world\" },\n            { \"text\": \"goodbye world\" }]\n\n    db = lancedb.connect(\"~/.lancedb\")\n    tbl = db.create_table(\"test\", schema=TextModel, mode=\"overwrite\")\n\n    tbl.add(data)\n</code></pre>"},{"location":"enterprise/","title":"LanceDB Enterprise | Enterprise-Grade Vector Database","text":"<p>LanceDB Enterprise transforms your data lake into a high performance vector database that can operate at extreme scale.</p> <p>Serve millions of tables and tens of billions of rows in a single index, improve retrieval quality using hybrid search with blazing fast metadata filters, and up to 200x cheaper with object storage.</p>"},{"location":"enterprise/#benefits","title":"Benefits","text":"<ul> <li>Performance: Tens of thousands of QPS with latency in single-digit milliseconds,   hundreds of thousands of rows per second write throughput, low-latency indexing across many tables.</li> <li>Scalability: Support workloads requiring data isolation with millions of active tables, or   a single table with billions of rows.</li> <li>Truly Multimodal Storage of the actual data itself, alongside the embeddings and metadata, to   enable lightning retrieval and filtering.</li> <li>Effortless Migration: Migrating Open Source LanceDB to LanceDB Enterprise   by just using a connecting URL.</li> <li>Multi-cloud: Available on AWS, GCP and Azure. Open data layer that eliminates vendor lock-up.</li> <li>Security: Encryption at rest. SOC2 Type2 and HIPAA.</li> <li>Flexible deployment model: Bring your own cloud, account, region, or kubernetes cluster, or let LanceDB manage it for you.</li> <li>Observability: First-class integration with existing observability systems for logging, monitoring, and distributed traces using OpenTelemetry.</li> </ul>"},{"location":"enterprise/architecture/","title":"LanceDB Enterprise Architecture","text":"<p>LanceDB Enterprise has the following key components</p> <ul> <li>Query fleet</li> <li>Plan execution fleet</li> <li>Indexer fleet</li> </ul> <p></p>"},{"location":"enterprise/architecture/#query-execution","title":"Query Execution","text":"<p>The LanceDB stateless query fleet is capable of managing tens of thousands of queries per second (QPS) per table with minimal latency. This level of throughput satisfies the requirements of even the most demanding production environments.</p> <p>Each query is compiled into a distributed query plan, and being executed on the Plan Execution Fleet in parallel. Additionally, each query is auto-vectorized for recent generations of <code>X86_64</code> and <code>ARM</code> CPUs for enhanced hardware efficiency.</p>"},{"location":"enterprise/architecture/#plan-execution-fleet","title":"Plan Execution Fleet","text":"<p>Each plan execution node is equipped with high-performance NVMe SSDs that act as a hybrid cache for cloud object storage systems like AWS S3, Google GCS, and Azure Blob Storage.</p> <p>The distributed query plan enforces cache locality for both data and indices using a variant of Consistent Hashing algorithm with low cache miss rate. LanceDB can serve warm queries with latency in the single-digit to low double-digit milliseconds range.</p>"},{"location":"enterprise/architecture/#write-path","title":"Write Path","text":"<p>LanceDB Enterprise is engineered for high-throughput data ingestion and indexing. The system ensures data persistence on durable object storage before confirming any write request.</p> <p>An extensive indexing fleet, enhanced with hardware acceleration, operates asynchronously to perform partial or full indexing, data compaction, and cleanup. Furthermore, we achieve high-throughput indexing operations without compromising query performance.</p> <p>   Customer data does not go through the event queue. The queue sends events such   as create an index to the indexers to trigger actions. </p> <p>Note: Indexing scales down to zero when there is no activity on the table.</p>"},{"location":"enterprise/benchmark/","title":"Benchmarking LanceDB Enterprise","text":"<p>LanceDB designed our architecture in a way to deliver 25ms vector search latency. Even with metadata filtering, our query latency remains as low as 50ms. It is important to note that we can support thousands of QPS with such query performance.</p> Percentile vector search vector search w. filtering full-text search P50 25ms 30ms 26ms P90 26ms 39ms 37ms P99 35ms 50ms 42ms"},{"location":"enterprise/benchmark/#dataset","title":"Dataset","text":"<p>We used two datasets for this benchmark test. The dbpedia-entities-openai-1M  for vector search, and a synthetic dataset for vector search with metadata filtering. </p> Name # Vector Vector Dimension dbpedia-entities-openai-1M 1,000,000 1536 synthetic dataset 15,000,000 256"},{"location":"enterprise/benchmark/#vector-search","title":"Vector search","text":"<p>We ran vector queries with dbpedia-entities-openai-1M with warmed up cache.  The query latency is as follows:</p> Percentile Latency P50 25ms P90 26ms P99 35ms Max 49ms"},{"location":"enterprise/benchmark/#full-text-search","title":"Full-text search","text":"<p>With the same dataset, and warmed-up cache, the full-text search performance is as follows:</p> Percentile Latency P50 26ms P90 37ms P99 42ms Max 98ms"},{"location":"enterprise/benchmark/#vector-search-with-metadata-filtering","title":"Vector search with metadata filtering","text":"<p>We created a 15M-vector dataset with sufficient complexity to thoroughly test our complex metadata filtering capabilities.  Such filtering can span a wide range on the scalar columns, e.g. find Sci-fi movies since 1900. </p> <p>With a warmed-up cache, the query performance using slightly more selective filters,  e.g. find Sci-fi movies between the year of 2000 and 2012, is as follows: </p> Percentile Latency P50 30ms P90 39ms P99 50ms <p>The query performance using complex filters, e.g. find Sci-fi movies since 1900, is as follows:</p> Percentile Latency P50 65ms P90 76ms P99 100ms <p>Note: Our benchmarking tests provide consistent, up-to-date performance evaluations of LanceDB. We regularly update and re-run these benchmarks to ensure the data remains accurate and relevant. </p>"},{"location":"enterprise/deployment/","title":"Available Deployment Models","text":"<p>There are two deployment models you can choose from for LanceDB Enterprise: Managed and BYOC. In each case AWS, GCP and Azure clouds are supported.</p>"},{"location":"enterprise/deployment/#managed-deployment","title":"Managed Deployment","text":"<p>Managed deployment is a private deployment of LanceDB Enterprise. All applications will run cloud accounts managed by LanceDB in the same location as your client applications.</p> <p>This hands-off approach is recommended for users who do not wish to manage the infrastructure themselves.</p> <p>To access your deployment, LanceDB can provision a public or private load balancer. </p>"},{"location":"enterprise/deployment/#bring-your-own-cloud-byoc","title":"Bring Your Own Cloud (BYOC)","text":"<p>With this deployment model LanceDB Enterprise is installed into your own cloud account.</p> <p>This approach is recommended when: - Users' security requirements for data residency preclude them from having data leave their account - Other applications may be accessing the object storage</p> <p>To deploy, an identity will be provisioned in your account with permissions to manage the infrastructure.</p>"},{"location":"enterprise/deployment/#get-started","title":"Get Started","text":"<p>LanceDB Enterprise installation is highly configurable and customizable to your needs. To get started, contact us at contact@lancedb.com for further instructions.</p>"},{"location":"enterprise/security/","title":"LanceDB Enterprise Security &amp; Compliance","text":""},{"location":"enterprise/security/#lancedb-achieves-soc-2-type-ii-hipaa-compliance-your-data-secured","title":"LanceDB Achieves SOC 2 Type II &amp; HIPAA Compliance: Your Data, Secured","text":"<p>We're proud to announce that LanceDB has successfully completed SOC 2 Type II and HIPAA audits.  At LanceDB, we always uphold the highest standards of security, availability, and confidentiality  for our services and our  customers'data.</p>"},{"location":"enterprise/security/#future-compliance","title":"Future Compliance","text":"<p>Going forward, LanceDB will maintain SOC 2 Type II and HIPAA compliance by conducting continuous  audits to ensure our security practices remain aligned with industry standards and evolving  risks. In the meanwhile, we are actively working on GDPR compliance.  Contact us to get the letter of engagement. </p>"},{"location":"enterprise/security/#lancedb-enterprise","title":"LanceDB Enterprise","text":""},{"location":"enterprise/security/#data-security","title":"Data security","text":"<p>Customer data is strictly protected and remains within the confines of your account.  We maintain rigorous data isolation and encryption protocols to ensure confidentiality.  LanceDB Enterprise only receives telemetry data for monitoring system health.  At LanceDB, customer data security is always paramount. </p>"},{"location":"enterprise/security/#encryption","title":"Encryption","text":"<p>LanceDB Enterprise safeguards your data through encryption at rest, preventing  unauthorized access. This comprehensive encryption covers all data stored within the  object store and cache. </p>"},{"location":"examples/","title":"Example projects and recipes","text":""},{"location":"examples/#recipes-and-example-code","title":"Recipes and example code","text":"<p>LanceDB provides language APIs, allowing you to embed a database in your language of choice.</p> <ul> <li>\ud83d\udc0d Python examples</li> <li>\ud83d\udc7e JavaScript examples</li> <li>\ud83e\udd80 Rust examples (coming soon)</li> </ul> <p>Hosted LanceDB</p> <p>If you want S3 cost-efficiency and local performance via a simple serverless API, checkout LanceDB Cloud. For private deployments, high performance at extreme scale, or if you have strict security requirements, talk to us about LanceDB Enterprise. Learn more</p>"},{"location":"examples/code_documentation_qa_bot_with_langchain/","title":"Code documentation Q&amp;A bot with LangChain","text":""},{"location":"examples/code_documentation_qa_bot_with_langchain/#use-lancedbs-langchain-integration-to-build-a-qa-bot-for-your-documentation","title":"use LanceDB's LangChain integration to build a Q&amp;A bot for your documentation","text":"<p>This example is in a notebook</p>"},{"location":"examples/examples_js/","title":"Examples: JavaScript","text":"<p>To help you get started, we provide some examples, projects and applications that use the LanceDB JavaScript API. You can always find the latest examples in our VectorDB Recipes repository.</p> Example Scripts Youtube transcript search bot Langchain: Code Docs QA bot AI Agents: Reducing Hallucination TransformersJS Embedding example"},{"location":"examples/examples_python/","title":"Overview : Python Examples","text":"<p>To help you get started, we provide some examples, projects, and applications that use the LanceDB Python API. These examples are designed to get you right into the code with minimal introduction, enabling you to move from an idea to a proof of concept in minutes. </p> <p>You can find the latest examples in our VectorDB Recipes repository. </p> <p>Introduction</p> <p>Explore applied examples available as Colab notebooks or Python scripts to integrate into your applications. You can also checkout our blog posts related to the particular example for deeper understanding.</p> Explore Description Build from Scratch with LanceDB \ud83d\udee0\ufe0f\ud83d\ude80 Start building your GenAI applications from the ground up using LanceDB's efficient vector-based document retrieval capabilities! Get started quickly with a solid foundation. Multimodal Search with LanceDB \ud83e\udd39\u200d\u2642\ufe0f\ud83d\udd0d Combine text and image queries to find the most relevant results using LanceDB's multimodal capabilities. Leverage the efficient vector-based similarity search. RAG (Retrieval-Augmented Generation) with LanceDB \ud83d\udd13\ud83e\uddd0 Build RAG (Retrieval-Augmented Generation) with LanceDB for efficient vector-based information retrieval and more accurate responses from AI. Vector Search: Efficient Retrieval \ud83d\udd13\ud83d\udc40 Use LanceDB's vector search capabilities to perform efficient and accurate similarity searches, enabling rapid discovery and retrieval of relevant documents in Large datasets. Chatbot applications with LanceDB \ud83e\udd16 Create chatbots that retrieves relevant context for coherent and context-aware replies, enhancing user experience through advanced conversational AI. Evaluation: Assessing Text Performance with Precision \ud83d\udcca\ud83d\udca1 Develop evaluation applications that allows you to input reference and candidate texts to measure their performance across various metrics. AI Agents: Intelligent Collaboration \ud83e\udd16 Enable AI agents to communicate and collaborate efficiently through dense vector representations, achieving shared goals seamlessly. Recommender Systems: Personalized Discovery \ud83c\udf7f\ud83d\udcfa Deliver personalized experiences by efficiently storing and querying item embeddings with LanceDB's powerful vector database capabilities. Miscellaneous Examples\ud83c\udf1f Find other unique examples and creative solutions using LanceDB, showcasing the flexibility and broad applicability of the platform."},{"location":"examples/examples_rust/","title":"Examples: Rust","text":"<p>Our Rust SDK is now stable. Examples are coming soon.</p>"},{"location":"examples/image_embeddings_roboflow/","title":"How to Load Image Embeddings into LanceDB","text":"<p>With the rise of Large Multimodal Models (LMMs) such as GPT-4 Vision, the need for storing image embeddings is growing. The most effective way to store text and image embeddings is in a vector database such as LanceDB. Vector databases are a special kind of data store that enables efficient search over stored embeddings. </p> <p>CLIP, a multimodal model developed by OpenAI, is commonly used to calculate image embeddings. These embeddings can then be used with a vector database to build a semantic search engine that you can query using images or text. For example, you could use LanceDB and CLIP embeddings to build a search engine for a database of folders.</p> <p>In this guide, we are going to show you how to use Roboflow Inference to load image embeddings into LanceDB. Without further ado, let\u2019s get started!</p>"},{"location":"examples/image_embeddings_roboflow/#step-1-install-roboflow-inference","title":"Step #1: Install Roboflow Inference","text":"<p>Roboflow Inference enables you to run state-of-the-art computer vision models with minimal configuration. Inference supports a range of models, from fine-tuned object detection, classification, and segmentation models to foundation models like CLIP. We will use Inference to calculate CLIP image embeddings.</p> <p>Inference provides a HTTP API through which you can run vision models.</p> <p>Inference powers the Roboflow hosted API, and is available as an open source utility. In this guide, we are going to run Inference locally, which enables you to calculate CLIP embeddings on your own hardware. We will also show you how to use the hosted Roboflow CLIP API, which is ideal if you need to scale and do not want to manage a system for calculating embeddings.</p> <p>To get started, first install the Inference CLI:</p> <pre><code>pip install inference-cli\n</code></pre> <p>Next, install Docker. Refer to the official Docker installation instructions for your operating system to get Docker set up. Once Docker is ready, you can start Inference using the following command:</p> <pre><code>inference server start\n</code></pre> <p>An Inference server will start running at \u2018http://localhost:9001\u2019.</p>"},{"location":"examples/image_embeddings_roboflow/#step-2-set-up-a-lancedb-vector-database","title":"Step #2: Set Up a LanceDB Vector Database","text":"<p>Now that we have Inference running, we can set up a LanceDB vector database. You can run LanceDB in JavaScript and Python. For this guide, we will use the Python API. But, you can take the HTTP requests we make below and change them to JavaScript if required.</p> <p>For this guide, we are going to search the COCO 128 dataset, which contains a wide range of objects. The variability in objects present in this dataset makes it a good dataset to demonstrate the capabilities of vector search. If you want to use this dataset, you can download COCO 128 from Roboflow Universe. With that said, you can search whatever folder of images you want.</p> <p>Once you have a dataset ready, install LanceDB with the following command:</p> <pre><code>pip install lancedb\n</code></pre> <p>We also need to install a specific commit of <code>tantivy</code>, a dependency of the LanceDB full text search engine we will use later in this guide:</p> <pre><code>pip install tantivy\n</code></pre> <p>Create a new Python file and add the following code:</p> <pre><code>import cv2\nimport supervision as sv\nimport requests\n\nimport lancedb\n\ndb = lancedb.connect(\"./embeddings\")\n\nIMAGE_DIR = \"images/\"\nAPI_KEY = os.environ.get(\"ROBOFLOW_API_KEY\")\nSERVER_URL = \"http://localhost:9001\"\n\nresults = []\n\nfor i, image in enumerate(os.listdir(IMAGE_DIR)):\n    infer_clip_payload = {\n        #Images can be provided as urls or as base64 encoded strings\n        \"image\": {\n            \"type\": \"base64\",\n            \"value\": base64.b64encode(open(IMAGE_DIR + image, \"rb\").read()).decode(\"utf-8\"),\n        },\n    }\n\n    res = requests.post(\n        f\"{SERVER_URL}/clip/embed_image?api_key={API_KEY}\",\n        json=infer_clip_payload,\n    )\n\n    embeddings = res.json()['embeddings']\n\n    print(\"Calculated embedding for image: \", image)\n\n    image = {\"vector\": embeddings[0], \"name\": os.path.join(IMAGE_DIR, image)}\n\n    results.append(image)\n\ntbl = db.create_table(\"images\", data=results)\n\ntbl.create_fts_index(\"name\")\n</code></pre> <p>To use the code above, you will need a Roboflow API key. Learn how to retrieve a Roboflow API key. Run the following command to set up your API key in your environment:</p> <pre><code>export ROBOFLOW_API_KEY=\"\"\n</code></pre> <p>Replace the <code>IMAGE_DIR</code> value with the folder in which you are storing the images for which you want to calculate embeddings. If you want to use the Roboflow CLIP API to calculate embeddings, replace the <code>SERVER_URL</code> value with <code>https://infer.roboflow.com</code>.</p> <p>Run the script above to create a new LanceDB database. This database will be stored on your local machine. The database will be called <code>embeddings</code> and the table will be called <code>images</code>.</p> <p>The script above calculates all embeddings for a folder then creates a new table. To add additional images, use the following code:</p> <pre><code>def make_batches():\n    for i in range(5):\n        yield [\n                {\"vector\": [3.1, 4.1], \"name\": \"image1.png\"},\n                {\"vector\": [5.9, 26.5], \"name\": \"image2.png\"}\n            ]\n\ntbl = db.open_table(\"images\")\ntbl.add(make_batches())\n</code></pre> <p>Replacing the <code>make_batches()</code> function with code to load embeddings for images.</p>"},{"location":"examples/image_embeddings_roboflow/#step-3-run-a-search-query","title":"Step #3: Run a Search Query","text":"<p>We are now ready to run a search query. To run a search query, we need a text embedding that represents a text query. We can use this embedding to search our LanceDB database for an entry.</p> <p>Let\u2019s calculate a text embedding for the query \u201ccat\u201d, then run a search query:</p> <pre><code>infer_clip_payload = {\n    \"text\": \"cat\",\n}\n\nres = requests.post(\n    f\"{SERVER_URL}/clip/embed_text?api_key={API_KEY}\",\n    json=infer_clip_payload,\n)\n\nembeddings = res.json()['embeddings']\n\ndf = tbl.search(embeddings[0]).limit(3).to_list()\n\nprint(\"Results:\")\n\nfor i in df:\n    print(i[\"name\"])\n</code></pre> <p>This code will search for the three images most closely related to the prompt \u201ccat\u201d. The names of the most similar three images will be printed to the console. Here are the three top results:</p> <pre><code>dataset/images/train/000000000650_jpg.rf.1b74ba165c5a3513a3211d4a80b69e1c.jpg\ndataset/images/train/000000000138_jpg.rf.af439ef1c55dd8a4e4b142d186b9c957.jpg\ndataset/images/train/000000000165_jpg.rf.eae14d5509bf0c9ceccddbb53a5f0c66.jpg\n</code></pre> <p>Let\u2019s open the top image:</p> <p></p> <p>The top image was a cat. Our search was successful.</p>"},{"location":"examples/image_embeddings_roboflow/#conclusion","title":"Conclusion","text":"<p>LanceDB is a vector database that you can use to store and efficiently search your image embeddings. You can use Roboflow Inference, a scalable computer vision inference server, to calculate CLIP embeddings that you can store in LanceDB.</p> <p>You can use Inference and LanceDB together to build a range of applications with image embeddings, from a media search engine to a retrieval-augmented generation pipeline for use with LMMs.</p> <p>To learn more about Inference and its capabilities, refer to the Inference documentation.</p>"},{"location":"examples/modal_langchain/","title":"Modal langchain","text":"In\u00a0[\u00a0]: Copied! <pre>import pickle\nimport re\nimport zipfile\nfrom pathlib import Path\n</pre> import pickle import re import zipfile from pathlib import Path In\u00a0[\u00a0]: Copied! <pre>import requests\nfrom langchain.chains import RetrievalQA\nfrom langchain.document_loaders import UnstructuredHTMLLoader\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.llms import OpenAI\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import LanceDB\nfrom modal import Image, Secret, Stub, web_endpoint\n</pre> import requests from langchain.chains import RetrievalQA from langchain.document_loaders import UnstructuredHTMLLoader from langchain.embeddings import OpenAIEmbeddings from langchain.llms import OpenAI from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.vectorstores import LanceDB from modal import Image, Secret, Stub, web_endpoint In\u00a0[\u00a0]: Copied! <pre>import lancedb\n</pre> import lancedb In\u00a0[\u00a0]: Copied! <pre>lancedb_image = Image.debian_slim().pip_install(\n    \"lancedb\", \"langchain\", \"openai\", \"pandas\", \"tiktoken\", \"unstructured\", \"tabulate\"\n)\n</pre> lancedb_image = Image.debian_slim().pip_install(     \"lancedb\", \"langchain\", \"openai\", \"pandas\", \"tiktoken\", \"unstructured\", \"tabulate\" ) In\u00a0[\u00a0]: Copied! <pre>stub = Stub(\n    name=\"example-langchain-lancedb\",\n    image=lancedb_image,\n    secrets=[Secret.from_name(\"my-openai-secret\")],\n)\n</pre> stub = Stub(     name=\"example-langchain-lancedb\",     image=lancedb_image,     secrets=[Secret.from_name(\"my-openai-secret\")], ) In\u00a0[\u00a0]: Copied! <pre>docsearch = None\ndocs_path = Path(\"docs.pkl\")\ndb_path = Path(\"lancedb\")\n</pre> docsearch = None docs_path = Path(\"docs.pkl\") db_path = Path(\"lancedb\") In\u00a0[\u00a0]: Copied! <pre>def get_document_title(document):\n    m = str(document.metadata[\"source\"])\n    title = re.findall(\"pandas.documentation(.*).html\", m)\n    if title[0] is not None:\n        return title[0]\n    return \"\"\n</pre> def get_document_title(document):     m = str(document.metadata[\"source\"])     title = re.findall(\"pandas.documentation(.*).html\", m)     if title[0] is not None:         return title[0]     return \"\" In\u00a0[\u00a0]: Copied! <pre>def download_docs():\n    pandas_docs = requests.get(\n        \"https://eto-public.s3.us-west-2.amazonaws.com/datasets/pandas_docs/pandas.documentation.zip\"\n    )\n    with open(Path(\"pandas.documentation.zip\"), \"wb\") as f:\n        f.write(pandas_docs.content)\n\n    file = zipfile.ZipFile(Path(\"pandas.documentation.zip\"))\n    file.extractall(path=Path(\"pandas_docs\"))\n</pre> def download_docs():     pandas_docs = requests.get(         \"https://eto-public.s3.us-west-2.amazonaws.com/datasets/pandas_docs/pandas.documentation.zip\"     )     with open(Path(\"pandas.documentation.zip\"), \"wb\") as f:         f.write(pandas_docs.content)      file = zipfile.ZipFile(Path(\"pandas.documentation.zip\"))     file.extractall(path=Path(\"pandas_docs\")) In\u00a0[\u00a0]: Copied! <pre>def store_docs():\n    docs = []\n\n    if not docs_path.exists():\n        for p in Path(\"pandas_docs/pandas.documentation\").rglob(\"*.html\"):\n            if p.is_dir():\n                continue\n            loader = UnstructuredHTMLLoader(p)\n            raw_document = loader.load()\n\n            m = {}\n            m[\"title\"] = get_document_title(raw_document[0])\n            m[\"version\"] = \"2.0rc0\"\n            raw_document[0].metadata = raw_document[0].metadata | m\n            raw_document[0].metadata[\"source\"] = str(raw_document[0].metadata[\"source\"])\n            docs = docs + raw_document\n\n        with docs_path.open(\"wb\") as fh:\n            pickle.dump(docs, fh)\n    else:\n        with docs_path.open(\"rb\") as fh:\n            docs = pickle.load(fh)\n\n    return docs\n</pre> def store_docs():     docs = []      if not docs_path.exists():         for p in Path(\"pandas_docs/pandas.documentation\").rglob(\"*.html\"):             if p.is_dir():                 continue             loader = UnstructuredHTMLLoader(p)             raw_document = loader.load()              m = {}             m[\"title\"] = get_document_title(raw_document[0])             m[\"version\"] = \"2.0rc0\"             raw_document[0].metadata = raw_document[0].metadata | m             raw_document[0].metadata[\"source\"] = str(raw_document[0].metadata[\"source\"])             docs = docs + raw_document          with docs_path.open(\"wb\") as fh:             pickle.dump(docs, fh)     else:         with docs_path.open(\"rb\") as fh:             docs = pickle.load(fh)      return docs In\u00a0[\u00a0]: Copied! <pre>def qanda_langchain(query):\n    download_docs()\n    docs = store_docs()\n\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,\n        chunk_overlap=200,\n    )\n    documents = text_splitter.split_documents(docs)\n    embeddings = OpenAIEmbeddings()\n\n    db = lancedb.connect(db_path)\n    table = db.create_table(\n        \"pandas_docs\",\n        data=[\n            {\n                \"vector\": embeddings.embed_query(\"Hello World\"),\n                \"text\": \"Hello World\",\n                \"id\": \"1\",\n            }\n        ],\n        mode=\"overwrite\",\n    )\n    docsearch = LanceDB.from_documents(documents, embeddings, connection=table)\n    qa = RetrievalQA.from_chain_type(\n        llm=OpenAI(), chain_type=\"stuff\", retriever=docsearch.as_retriever()\n    )\n    return qa.run(query)\n</pre> def qanda_langchain(query):     download_docs()     docs = store_docs()      text_splitter = RecursiveCharacterTextSplitter(         chunk_size=1000,         chunk_overlap=200,     )     documents = text_splitter.split_documents(docs)     embeddings = OpenAIEmbeddings()      db = lancedb.connect(db_path)     table = db.create_table(         \"pandas_docs\",         data=[             {                 \"vector\": embeddings.embed_query(\"Hello World\"),                 \"text\": \"Hello World\",                 \"id\": \"1\",             }         ],         mode=\"overwrite\",     )     docsearch = LanceDB.from_documents(documents, embeddings, connection=table)     qa = RetrievalQA.from_chain_type(         llm=OpenAI(), chain_type=\"stuff\", retriever=docsearch.as_retriever()     )     return qa.run(query) In\u00a0[\u00a0]: Copied! <pre>@stub.function()\n@web_endpoint(method=\"GET\")\ndef web(query: str):\n    answer = qanda_langchain(query)\n    return {\n        \"answer\": answer,\n    }\n</pre> @stub.function() @web_endpoint(method=\"GET\") def web(query: str):     answer = qanda_langchain(query)     return {         \"answer\": answer,     } In\u00a0[\u00a0]: Copied! <pre>@stub.function()\ndef cli(query: str):\n    answer = qanda_langchain(query)\n    print(answer)\n</pre> @stub.function() def cli(query: str):     answer = qanda_langchain(query)     print(answer)"},{"location":"examples/multimodal_search/","title":"Image multimodal search","text":""},{"location":"examples/multimodal_search/#search-through-an-image-dataset-using-natural-language-full-text-and-sql","title":"Search through an image dataset using natural language, full text and SQL","text":"<p>This example is in a notebook</p>"},{"location":"examples/serverless_lancedb_with_s3_and_lambda/","title":"Serverless LanceDB","text":""},{"location":"examples/serverless_lancedb_with_s3_and_lambda/#store-your-data-on-s3-and-use-lambda-to-compute-embeddings-and-retrieve-queries-in-production-easily","title":"Store your data on S3 and use Lambda to compute embeddings and retrieve queries in production easily.","text":"<p>This is a great option if you're wanting to scale with your use case and save effort and costs of maintenance.</p> <p>Let's walk through how to get a simple Lambda function that queries the SIFT dataset on S3.</p> <p>Before we start, you'll need to ensure you create a secure account access to AWS. We recommend using user policies, as this way AWS can share credentials securely without you having to pass around environment variables into Lambda.</p> <p>We'll also use a container to ship our Lambda code. This is a good option for Lambda as you don't have the space limits that you would otherwise by building a package yourself.</p>"},{"location":"examples/serverless_lancedb_with_s3_and_lambda/#initial-setup-creating-a-lancedb-table-and-storing-it-remotely-on-s3","title":"Initial setup: creating a LanceDB Table and storing it remotely on S3","text":"<p>We'll use the SIFT vector dataset as an example. To make it easier, we've already made a Lance-format SIFT dataset publicly available, which we can access and use to populate our LanceDB Table. </p> <p>To do this, download the Lance files locally first from:</p> <pre><code>s3://eto-public/datasets/sift/vec_data.lance\n</code></pre> <p>Then, we can write a quick Python script to populate our LanceDB Table:</p> <pre><code>import lance\nsift_dataset = lance.dataset(\"/path/to/local/vec_data.lance\")\ndf = sift_dataset.to_table().to_pandas()\n\nimport lancedb\ndb = lancedb.connect(\".\")\ntable = db.create_table(\"vector_example\", df)\n</code></pre> <p>Once we've created our Table, we are free to move this data over to S3 so we can remotely host it.</p>"},{"location":"examples/serverless_lancedb_with_s3_and_lambda/#building-our-lambda-app-a-simple-event-handler-for-vector-search","title":"Building our Lambda app: a simple event handler for vector search","text":"<p>Now that we've got a remotely hosted LanceDB Table, we'll want to be able to query it from Lambda. To do so, let's create a new <code>Dockerfile</code> using the AWS python container base:</p> <pre><code>FROM public.ecr.aws/lambda/python:3.10\n\nRUN pip3 install --upgrade pip\nRUN pip3 install --no-cache-dir -U numpy --target \"${LAMBDA_TASK_ROOT}\"\nRUN pip3 install --no-cache-dir -U lancedb --target \"${LAMBDA_TASK_ROOT}\"\n\nCOPY app.py ${LAMBDA_TASK_ROOT}\n\nCMD [ \"app.handler\" ]\n</code></pre> <p>Now let's make a simple Lambda function that queries the SIFT dataset in <code>app.py</code>.</p> <pre><code>import json\nimport numpy as np\nimport lancedb\n\ndb = lancedb.connect(\"s3://eto-public/tables\")\ntable = db.open_table(\"vector_example\")\n\ndef handler(event, context):\n    status_code = 200\n\n    if event['query_vector'] is None:\n        status_code = 404\n        return {\n            \"statusCode\": status_code,\n            \"headers\": {\n                \"Content-Type\": \"application/json\"\n            },\n            \"body\": json.dumps({\n                \"Error \": \"No vector to query was issued\"\n            })\n        }\n\n    # Shape of SIFT is (128,1M), d=float32\n    query_vector = np.array(event['query_vector'], dtype=np.float32)\n\n    rs = table.search(query_vector).limit(2).to_list()\n\n    return {\n        \"statusCode\": status_code,\n        \"headers\": {\n            \"Content-Type\": \"application/json\"\n        },\n        \"body\": json.dumps(rs)\n    }\n</code></pre>"},{"location":"examples/serverless_lancedb_with_s3_and_lambda/#deploying-the-container-to-ecr","title":"Deploying the container to ECR","text":"<p>The next step is to build and push the container to ECR, where it can then be used to create a new Lambda function. </p> <p>It's best to follow the official AWS documentation for how to do this, which you can view here:</p> <pre><code>https://docs.aws.amazon.com/lambda/latest/dg/images-create.html#images-upload\n</code></pre>"},{"location":"examples/serverless_lancedb_with_s3_and_lambda/#final-step-setting-up-your-lambda-function","title":"Final step: setting up your Lambda function","text":"<p>Once the container is pushed, you can create a Lambda function by selecting the container. </p>"},{"location":"examples/serverless_qa_bot_with_modal_and_langchain/","title":"Serverless QA Bot with Modal and LangChain","text":""},{"location":"examples/serverless_qa_bot_with_modal_and_langchain/#use-lancedbs-langchain-integration-with-modal-to-run-a-serverless-app","title":"use LanceDB's LangChain integration with Modal to run a serverless app","text":"<p>We're going to build a QA bot for your documentation using LanceDB's LangChain integration and use Modal for deployment.</p> <p>Modal is an end-to-end compute platform for model inference, batch jobs, task queues, web apps and more. It's a great way to deploy your LanceDB models and apps.</p> <p>To get started, ensure that you have created an account and logged into Modal. To follow along, the full source code is available on Github here.</p>"},{"location":"examples/serverless_qa_bot_with_modal_and_langchain/#setting-up-modal","title":"Setting up Modal","text":"<p>We'll start by specifying our dependencies and creating a new Modal <code>Stub</code>:</p> <pre><code>lancedb_image = Image.debian_slim().pip_install(\n    \"lancedb\",\n    \"langchain\",\n    \"openai\",\n    \"pandas\",\n    \"tiktoken\",\n    \"unstructured\",\n    \"tabulate\"\n)\n\nstub = Stub(\n    name=\"example-langchain-lancedb\",\n    image=lancedb_image,\n    secrets=[Secret.from_name(\"my-openai-secret\")],\n)\n</code></pre> <p>We're using Modal's Secrets injection to secure our OpenAI key. To set your own, you can access the Modal UI and enter your key.</p>"},{"location":"examples/serverless_qa_bot_with_modal_and_langchain/#setting-up-caches-for-lancedb-and-langchain","title":"Setting up caches for LanceDB and LangChain","text":"<p>Next, we can setup some globals to cache our LanceDB database, as well as our LangChain docsource:</p> <pre><code>docsearch = None\ndocs_path = Path(\"docs.pkl\")\ndb_path = Path(\"lancedb\")\n</code></pre>"},{"location":"examples/serverless_qa_bot_with_modal_and_langchain/#downloading-our-dataset","title":"Downloading our dataset","text":"<p>We're going use a pregenerated dataset, which stores HTML files of the Pandas 2.0 documentation.  You could switch this out for your own dataset.</p> <pre><code>def download_docs():\n    pandas_docs = requests.get(\"https://eto-public.s3.us-west-2.amazonaws.com/datasets/pandas_docs/pandas.documentation.zip\")\n    with open(Path(\"pandas.documentation.zip\"), \"wb\") as f:\n        f.write(pandas_docs.content)\n\n    file = zipfile.ZipFile(Path(\"pandas.documentation.zip\"))\n    file.extractall(path=Path(\"pandas_docs\"))\n</code></pre>"},{"location":"examples/serverless_qa_bot_with_modal_and_langchain/#pre-processing-the-dataset-and-generating-metadata","title":"Pre-processing the dataset and generating metadata","text":"<p>Once we've downloaded it, we want to parse and pre-process them using LangChain, and then vectorize them and store it in LanceDB. Let's first create a function that uses LangChains <code>UnstructuredHTMLLoader</code> to parse them. We can then add our own metadata to it and store it alongside the data, we'll later be able to use this for filtering metadata.</p> <pre><code>def store_docs():\n    docs = []\n\n    if not docs_path.exists():\n        for p in Path(\"pandas_docs/pandas.documentation\").rglob(\"*.html\"):\n            if p.is_dir():\n                continue\n            loader = UnstructuredHTMLLoader(p)\n            raw_document = loader.load()\n\n            m = {}\n            m[\"title\"] = get_document_title(raw_document[0])\n            m[\"version\"] = \"2.0rc0\"\n            raw_document[0].metadata = raw_document[0].metadata | m\n            raw_document[0].metadata[\"source\"] = str(raw_document[0].metadata[\"source\"])\n            docs = docs + raw_document\n\n        with docs_path.open(\"wb\") as fh:\n            pickle.dump(docs, fh)\n    else:\n        with docs_path.open(\"rb\") as fh:\n            docs = pickle.load(fh)\n\n    return docs\n</code></pre>"},{"location":"examples/serverless_qa_bot_with_modal_and_langchain/#simple-langchain-chain-for-a-qa-bot","title":"Simple LangChain chain for a QA bot","text":"<p>Now we can create a simple LangChain chain for our QA bot. We'll use the <code>RecursiveCharacterTextSplitter</code> to split our documents into chunks, and then use the <code>OpenAIEmbeddings</code> to vectorize them.</p> <p>Lastly, we'll create a LanceDB table and store the vectorized documents in it, then create a <code>RetrievalQA</code> model from the chain and return it.</p> <pre><code>def qanda_langchain(query):\n    download_docs()\n    docs = store_docs()\n\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,\n        chunk_overlap=200,\n    )\n    documents = text_splitter.split_documents(docs)\n    embeddings = OpenAIEmbeddings()\n\n    db = lancedb.connect(db_path) \n    table = db.create_table(\"pandas_docs\", data=[\n        {\"vector\": embeddings.embed_query(\"Hello World\"), \"text\": \"Hello World\", \"id\": \"1\"}\n    ], mode=\"overwrite\")\n    docsearch = LanceDB.from_documents(documents, embeddings, connection=table)\n    qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=docsearch.as_retriever())\n    return qa.run(query)\n</code></pre>"},{"location":"examples/serverless_qa_bot_with_modal_and_langchain/#creating-our-modal-entry-points","title":"Creating our Modal entry points","text":"<p>Now we can create our Modal entry points for our CLI and web endpoint:</p> <pre><code>@stub.function()\n@web_endpoint(method=\"GET\")\ndef web(query: str):\n    answer = qanda_langchain(query)\n    return {\n        \"answer\": answer,\n    }\n\n@stub.function()\ndef cli(query: str):\n    answer = qanda_langchain(query)\n    print(answer)\n</code></pre>"},{"location":"examples/serverless_qa_bot_with_modal_and_langchain/#testing-it-out","title":"Testing it out!","text":"<p>Testing the CLI:</p> <pre><code>modal run modal_langchain.py --query \"What are the major differences in pandas 2.0?\"\n</code></pre> <p>Testing the web endpoint:</p> <pre><code>modal serve modal_langchain.py\n</code></pre> <p>In the CLI, Modal will provide you a web endpoint. Copy this endpoint URI for the next step. Once this is served, then we can hit it with <code>curl</code>. </p> <p>Note, the first time this runs, it will take a few minutes to download the dataset and vectorize it. An actual production example would pre-cache/load the dataset and vectorized documents prior</p> <pre><code>curl --get --data-urlencode \"query=What are the major differences in pandas 2.0?\" https://your-modal-endpoint-app.modal.run\n\n{\"answer\":\" The major differences in pandas 2.0 include the ability to use any numpy numeric dtype in a Index, installing optional dependencies with pip extras, and enhancements, bug fixes, and performance improvements.\"}\n</code></pre>"},{"location":"examples/serverless_website_chatbot/","title":"LanceDB Chatbot - Vercel Next.js Template","text":"<p>Use an AI chatbot with website context retrieved from a vector store like LanceDB. LanceDB is lightweight and can be embedded directly into Next.js, with data stored on-prem.</p>"},{"location":"examples/serverless_website_chatbot/#one-click-deploy-on-vercel","title":"One click deploy on Vercel","text":""},{"location":"examples/serverless_website_chatbot/#development","title":"Development","text":"<p>First, rename <code>.env.example</code> to <code>.env.local</code>, and fill out <code>OPENAI_API_KEY</code> with your OpenAI API key. You can get one here.</p> <p>Run the development server:</p> <pre><code>npm run dev\n# or\nyarn dev\n# or\npnpm dev\n</code></pre> <p>Open http://localhost:3000 with your browser to see the result.</p> <p>This project uses <code>next/font</code> to automatically optimize and load Inter, a custom Google Font.</p>"},{"location":"examples/serverless_website_chatbot/#learn-more","title":"Learn More","text":"<p>To learn more about LanceDB or Next.js, take a look at the following resources:</p> <ul> <li>LanceDB Documentation - learn about LanceDB, the developer-friendly serverless vector database.</li> <li>Next.js Documentation - learn about Next.js features and API.</li> <li>Learn Next.js - an interactive Next.js tutorial.</li> </ul>"},{"location":"examples/serverless_website_chatbot/#lancedb-on-nextjs-and-vercel","title":"LanceDB on Next.js and Vercel","text":"<p>FYI: these configurations have been pre-implemented in this template.</p> <p>Since LanceDB contains a prebuilt Node binary, you must configure <code>next.config.js</code> to exclude it from webpack. This is required for both using Next.js and deploying on Vercel. <pre><code>/** @type {import('next').NextConfig} */\nmodule.exports = ({\n  webpack(config) {\n    config.externals.push({ vectordb: 'vectordb' })\n    return config;\n  }\n})\n</code></pre></p> <p>To deploy on Vercel, we need to make sure that the NodeJS runtime static file analysis for Vercel can find the binary, since LanceDB uses dynamic imports by default. We can do this by modifying <code>package.json</code> in the <code>scripts</code> section. <pre><code>{\n  ...\n  \"scripts\": {\n    ...\n    \"vercel-build\": \"sed -i 's/nativeLib = require(`@lancedb\\\\/vectordb-\\\\${currentTarget()}`);/nativeLib = require(`@lancedb\\\\/vectordb-linux-x64-gnu`);/' node_modules/vectordb/native.js &amp;&amp; next build\",\n    ...\n  },\n  ...\n}\n</code></pre></p>"},{"location":"examples/transformerjs_embedding_search_nodejs/","title":"Vector embedding search using TransformersJS","text":""},{"location":"examples/transformerjs_embedding_search_nodejs/#embed-and-query-data-from-lancedb-using-transformersjs","title":"Embed and query data from LanceDB using TransformersJS","text":"<p>This example shows how to use the transformers.js library to perform vector embedding search using LanceDB's Javascript API.</p>"},{"location":"examples/transformerjs_embedding_search_nodejs/#setting-up","title":"Setting up","text":"<p>First, install the dependencies: <pre><code>npm install vectordb\nnpm i @xenova/transformers\n</code></pre></p> <p>We will also be using the all-MiniLM-L6-v2 model to make it compatible with Transformers.js</p> <p>Within our <code>index.js</code> file we will import the necessary libraries and define our model and database:</p> <pre><code>const lancedb = require('vectordb')\nconst { pipeline } = await import('@xenova/transformers')\nconst pipe = await pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2');\n</code></pre>"},{"location":"examples/transformerjs_embedding_search_nodejs/#creating-the-embedding-function","title":"Creating the embedding function","text":"<p>Next, we will create a function that will take in a string and return the vector embedding of that string. We will use the <code>pipe</code> function we defined earlier to get the vector embedding of the string.</p> <pre><code>// Define the function. `sourceColumn` is required for LanceDB to know\n// which column to use as input.\nconst embed_fun = {}\nembed_fun.sourceColumn = 'text'\nembed_fun.embed = async function (batch) {\n    let result = []\n    // Given a batch of strings, we will use the `pipe` function to get\n    // the vector embedding of each string.\n    for (let text of batch) {\n        // 'mean' pooling and normalizing allows the embeddings to share the\n        // same length.\n        const res = await pipe(text, { pooling: 'mean', normalize: true })\n        result.push(Array.from(res['data']))\n    }\n    return (result)\n}\n</code></pre>"},{"location":"examples/transformerjs_embedding_search_nodejs/#creating-the-database","title":"Creating the database","text":"<p>Now, we will create the LanceDB database and add the embedding function we defined earlier.</p> <pre><code>// Link a folder and create a table with data\nconst db = await lancedb.connect('data/sample-lancedb')\n\n// You can also import any other data, but make sure that you have a column\n// for the embedding function to use.\nconst data = [\n    { id: 1, text: 'Cherry', type: 'fruit' },\n    { id: 2, text: 'Carrot', type: 'vegetable' },\n    { id: 3, text: 'Potato', type: 'vegetable' },\n    { id: 4, text: 'Apple', type: 'fruit' },\n    { id: 5, text: 'Banana', type: 'fruit' }\n]\n\n// Create the table with the embedding function\nconst table = await db.createTable('food_table', data, \"create\", embed_fun)\n</code></pre>"},{"location":"examples/transformerjs_embedding_search_nodejs/#performing-the-search","title":"Performing the search","text":"<p>Now, we can perform the search using the <code>search</code> function. LanceDB automatically uses the embedding function we defined earlier to get the vector embedding of the query string.</p> <p><pre><code>// Query the table\nconst results = await table\n    .search(\"a sweet fruit to eat\")\n    .metricType(\"cosine\")\n    .limit(2)\n    .execute()\nconsole.log(results.map(r =&gt; r.text))\n</code></pre> <pre><code>[ 'Banana', 'Cherry' ]\n</code></pre></p> <p>Output of <code>results</code>: <pre><code>[\n  {\n    vector: Float32Array(384) [\n      -0.057455405592918396,\n      0.03617725893855095,\n      -0.0367760956287384,\n      ... 381 more items\n    ],\n    id: 5,\n    text: 'Banana',\n    type: 'fruit',\n    _distance: 0.4919965863227844\n  },\n  {\n    vector: Float32Array(384) [\n      0.0009714411571621895,\n      0.008223623037338257,\n      0.009571489877998829,\n      ... 381 more items\n    ],\n    id: 1,\n    text: 'Cherry',\n    type: 'fruit',\n    _distance: 0.5540297031402588\n  }\n]\n</code></pre></p>"},{"location":"examples/transformerjs_embedding_search_nodejs/#wrapping-it-up","title":"Wrapping it up","text":"<p>In this example, we showed how to use the <code>transformers.js</code> library to perform vector embedding search using LanceDB's Javascript API. You can find the full code for this example on Github!</p>"},{"location":"examples/youtube_transcript_bot/","title":"YouTube transcript search","text":""},{"location":"examples/youtube_transcript_bot/#search-through-youtube-transcripts-using-natural-language-with-lancedb","title":"Search through youtube transcripts using natural language with LanceDB","text":"<p> <p>Scripts -  </p> <p>This example is in a notebook</p>"},{"location":"examples/youtube_transcript_bot_with_nodejs/","title":"YouTube transcript QA bot with NodeJS","text":""},{"location":"examples/youtube_transcript_bot_with_nodejs/#use-lancedbs-javascript-api-and-openai-to-build-a-qa-bot-for-youtube-transcripts","title":"use LanceDB's Javascript API and OpenAI to build a QA bot for YouTube transcripts","text":"<p>This Q&amp;A bot will allow you to search through youtube transcripts using natural language! We'll introduce how to use LanceDB's Javascript API to store and manage your data easily.</p> <pre><code>npm install vectordb\n</code></pre>"},{"location":"examples/youtube_transcript_bot_with_nodejs/#download-the-data","title":"Download the data","text":"<p>For this example, we're using a sample of a HuggingFace dataset that contains YouTube transcriptions: <code>jamescalam/youtube-transcriptions</code>. Download and extract this file under the <code>data</code> folder:</p> <pre><code>wget -c https://eto-public.s3.us-west-2.amazonaws.com/datasets/youtube_transcript/youtube-transcriptions_sample.jsonl\n</code></pre>"},{"location":"examples/youtube_transcript_bot_with_nodejs/#prepare-context","title":"Prepare Context","text":"<p>Each item in the dataset contains just a short chunk of text. We'll need to merge a bunch of these chunks together on a rolling basis. For this demo, we'll look back 20 records to create a more complete context for each sentence.</p> <p>First, we need to read and parse the input file.</p> <pre><code>const lines = (await fs.readFile(INPUT_FILE_NAME, 'utf-8'))\n  .toString()\n  .split('\\n')\n  .filter(line =&gt; line.length &gt; 0)\n  .map(line =&gt; JSON.parse(line))\n\nconst data = contextualize(lines, 20, 'video_id')\n</code></pre> <p>The contextualize function groups the transcripts by video_id and then creates the expanded context for each item.</p> <pre><code>function contextualize (rows, contextSize, groupColumn) {\n  const grouped = []\n  rows.forEach(row =&gt; {\n    if (!grouped[row[groupColumn]]) {\n      grouped[row[groupColumn]] = []\n    }\n    grouped[row[groupColumn]].push(row)\n  })\n\n  const data = []\n  Object.keys(grouped).forEach(key =&gt; {\n    for (let i = 0; i &lt; grouped[key].length; i++) {\n      const start = i - contextSize &gt; 0 ? i - contextSize : 0\n      grouped[key][i].context = grouped[key].slice(start, i + 1).map(r =&gt; r.text).join(' ')\n    }\n    data.push(...grouped[key])\n  })\n  return data\n}\n</code></pre>"},{"location":"examples/youtube_transcript_bot_with_nodejs/#create-the-lancedb-table","title":"Create the LanceDB Table","text":"<p>To load our data into LanceDB, we need to create embedding (vectors) for each item. For this example, we will use the OpenAI embedding functions, which have a native integration with LanceDB.</p> <pre><code>// You need to provide an OpenAI API key, here we read it from the OPENAI_API_KEY environment variable\nconst apiKey = process.env.OPENAI_API_KEY\n// The embedding function will create embeddings for the 'context' column\nconst embedFunction = new lancedb.OpenAIEmbeddingFunction('context', apiKey)\n// Connects to LanceDB\nconst db = await lancedb.connect('data/youtube-lancedb')\nconst tbl = await db.createTable('vectors', data, embedFunction)\n</code></pre>"},{"location":"examples/youtube_transcript_bot_with_nodejs/#create-and-answer-the-prompt","title":"Create and answer the prompt","text":"<p>We will accept questions in natural language and use our corpus stored in LanceDB to answer them. First, we need to set up the OpenAI client:</p> <pre><code>const configuration = new Configuration({ apiKey })\nconst openai = new OpenAIApi(configuration)\n</code></pre> <p>Then we can prompt questions and use LanceDB to retrieve the three most relevant transcripts for this prompt.</p> <pre><code>const query = await rl.question('Prompt: ')\nconst results = await tbl\n  .search(query)\n  .select(['title', 'text', 'context'])\n  .limit(3)\n  .execute()\n</code></pre> <p>The query and the transcripts' context are appended together in a single prompt:</p> <pre><code>function createPrompt (query, context) {\n    let prompt =\n        'Answer the question based on the context below.\\n\\n' +\n        'Context:\\n'\n\n    // need to make sure our prompt is not larger than max size\n    prompt = prompt + context.map(c =&gt; c.context).join('\\n\\n---\\n\\n').substring(0, 3750)\n    prompt = prompt + `\\n\\nQuestion: ${query}\\nAnswer:`\n    return prompt\n}\n</code></pre> <p>We can now use the OpenAI Completion API to process our custom prompt and give us an answer.</p> <pre><code>const response = await openai.createCompletion({\n  model: 'text-davinci-003',\n  prompt: createPrompt(query, results),\n  max_tokens: 400,\n  temperature: 0,\n  top_p: 1,\n  frequency_penalty: 0,\n  presence_penalty: 0\n})\nconsole.log(response.data.choices[0].text)\n</code></pre>"},{"location":"examples/youtube_transcript_bot_with_nodejs/#lets-put-it-all-together-now","title":"Let's put it all together now","text":"<p>Now we can provide queries and have them answered based on your local LanceDB data.</p> <pre><code>Prompt: who was the 12th person on the moon and when did they land?\n The 12th person on the moon was Harrison Schmitt and he landed on December 11, 1972.\nPrompt: Which training method should I use for sentence transformers when I only have pairs of related sentences?\n NLI with multiple negative ranking loss.\n</code></pre>"},{"location":"examples/youtube_transcript_bot_with_nodejs/#thats-a-wrap","title":"That's a wrap","text":"<p>In this example, you learned how to use LanceDB to store and query embedding representations of your local data. The complete example code is on GitHub, and you can also download the LanceDB dataset using this link.</p>"},{"location":"examples/python_examples/aiagent/","title":"AI Agents: Intelligent Collaboration\ud83e\udd16","text":"<p>Think of a platform where AI Agents can seamlessly exchange information, coordinate over tasks, and achieve shared targets with great efficiency\ud83d\udcbb\ud83d\udcc8.</p>"},{"location":"examples/python_examples/aiagent/#vector-based-coordination-the-technical-advantage","title":"Vector-Based Coordination: The Technical Advantage","text":"<p>Leveraging LanceDB's vector-based capabilities, we can enable AI agents \ud83e\udd16 to communicate and collaborate through dense vector representations. AI agents can exchange information, coordinate on a task or work towards a common goal, just by giving queries\ud83d\udcdd.</p> AI Agents Description Links AI Agents: Reducing Hallucinationt\ud83d\udcca \ud83e\udd16\ud83d\udca1 Reduce AI hallucinations using Critique-Based Contexting! Learn by Simplifying and Automating tedious workflows by going through fitness trainer agent example.\ud83d\udcaa AI Trends Searcher: CrewAI\ud83d\udd0d\ufe0f \ud83d\udd0d\ufe0f Learn about CrewAI Agents ! Utilize the features of CrewAI - Role-based Agents, Task Management, and Inter-agent Delegation ! Make AI agents work together to do tricky stuff \ud83d\ude3a SuperAgent Autogen\ud83e\udd16 \ud83d\udcbb AI interactions with the Super Agent! Integrating Autogen, LanceDB, LangChain, LiteLLM, and Ollama to create AI agent that excels in understanding and processing complex queries.\ud83e\udd16"},{"location":"examples/python_examples/build_from_scratch/","title":"Build from Scratch with LanceDB \ud83d\udee0\ufe0f\ud83d\ude80","text":"<p>Start building your GenAI applications from the ground up using LanceDB's efficient vector-based document retrieval capabilities! \ud83d\udcd1</p> <p>Get Started in Minutes \u23f1\ufe0f</p> <p>These examples provide a solid foundation for building your own GenAI applications using LanceDB. Jump from idea to proof of concept quickly with applied examples. Get started and see what you can create! \ud83d\udcbb</p> Build From Scratch Description Links Build RAG from Scratch\ud83d\ude80\ud83d\udcbb \ud83d\udcdd Create a Retrieval-Augmented Generation (RAG) model from scratch using LanceDB. Local RAG from Scratch with Llama3\ud83d\udd25\ud83d\udca1 \ud83d\udc2b Build a local RAG model using Llama3 and LanceDB for fast and efficient text generation. Multi-Head RAG from Scratch\ud83d\udcda\ud83d\udcbb \ud83e\udd2f Develop a Multi-Head RAG model from scratch, enabling generation of text based on multiple documents."},{"location":"examples/python_examples/chatbot/","title":"Chatbot applications with LanceDB \ud83e\udd16","text":"<p>Create innovative chatbot applications that utilizes LanceDB for efficient vector-based response generation! \ud83c\udf10\u2728 </p> <p>Introduction \ud83d\udc4b\u2728</p> <p>Users can input their queries, allowing the chatbot to retrieve relevant context seamlessly. \ud83d\udd0d\ud83d\udcda This enables the generation of coherent and context-aware replies that enhance user experience. \ud83c\udf1f\ud83e\udd1d Dive into the world of advanced conversational AI and streamline interactions with powerful data management! \ud83d\ude80\ud83d\udca1</p> Chatbot Description Links Databricks DBRX Website Bot \u26a1\ufe0f Engage with the Hogwarts chatbot, that uses Open-source RAG with DBRX, LanceDB and LLama-index with Hugging Face Embeddings, to provide interactive and engaging user experiences. \u2728 CLI SDK Manual Chatbot Locally \ud83d\udcbb CLI chatbot for SDK/hardware documents using Local RAG with LLama3, Ollama, LanceDB, and Openhermes Embeddings, built with Phidata Assistant and Knowledge Base \ud83e\udd16 Youtube Transcript Search QA Bot \ud83d\udcf9 Search through youtube transcripts using natural language with a Q&amp;A bot, leveraging LanceDB for effortless data storage and management \ud83d\udcac Code Documentation Q&amp;A Bot with LangChain \ud83e\udd16 Query your own documentation easily using questions in natural language with a Q&amp;A bot, powered by LangChain and LanceDB, demonstrated with Numpy 1.26 docs \ud83d\udcda Context-aware Chatbot using Llama 2 &amp; LanceDB \ud83e\udd16 Build conversational AI with a context-aware chatbot, powered by Llama 2, LanceDB, and LangChain, that enables intuitive and meaningful conversations with your data \ud83d\udcda\ud83d\udcac Chat with csv using Hybrid Search \ud83d\udcca Chat application that interacts with CSV and Excel files using LanceDB's hybrid search capabilities, performing direct operations on large-scale columnar data efficiently \ud83d\ude80"},{"location":"examples/python_examples/evaluations/","title":"Evaluation: Assessing Text Performance with Precision \ud83d\udcca\ud83d\udca1","text":"<p>Evaluation is a comprehensive tool designed to measure the performance of text-based inputs, enabling data-driven optimization and improvement \ud83d\udcc8. </p> <p>Text Evaluation 101 \ud83d\udcda</p> <p>Using robust framework for assessing reference and candidate texts across various metrics\ud83d\udcca, ensure that the text outputs are high-quality and meet specific requirements and standards\ud83d\udcdd.</p> Evaluation Description Links Evaluating Prompts with Prompttools \ud83e\udd16 Compare, visualize &amp; evaluate embedding functions (incl. OpenAI) across metrics like latency &amp; custom evaluation \ud83d\udcc8\ud83d\udcca Evaluating RAG with RAGAs and GPT-4o \ud83d\udcca Evaluate RAG pipelines with cutting-edge metrics and tools, integrate with CI/CD for continuous performance checks, and generate responses with GPT-4o \ud83e\udd16\ud83d\udcc8"},{"location":"examples/python_examples/multimodal/","title":"Multimodal Search with LanceDB \ud83e\udd39\u200d\u2642\ufe0f\ud83d\udd0d","text":"<p>Using LanceDB's multimodal capabilities, combine text and image queries to find the most relevant results in your corpus ! \ud83d\udd13\ud83d\udca1</p> <p>Explore the Future of Search \ud83d\ude80</p> <p>LanceDB supports multimodal search by indexing and querying vector representations of text and image data \ud83e\udd16. This enables efficient retrieval of relevant documents and images using vector-based similarity search \ud83d\udcca. The platform facilitates cross-modal search, allowing for text-image and image-text retrieval, and supports scalable indexing of high-dimensional vector spaces \ud83d\udcbb.</p> Multimodal Description Links Multimodal CLIP: DiffusionDB \ud83c\udf10\ud83d\udca5 Multi-Modal Search with CLIP and LanceDB Using DiffusionDB Data for Combined Text and Image Understanding ! \ud83d\udd13 Multimodal CLIP: Youtube Videos \ud83d\udcf9\ud83d\udc40 Search Youtube videos using Multimodal CLIP, finding relevant content with ease and accuracy! \ud83c\udfaf Multimodal Image + Text Search \ud83d\udcf8\ud83d\udd0d Find relevant documents and images with a single query using LanceDB's multimodal search capabilities, to seamlessly integrate text and visuals ! \ud83c\udf09 Cambrian-1: Vision-Centric Image Exploration \ud83d\udd0d\ud83d\udc40 Learn how Cambrian-1 works, using an example of Vision-Centric exploration on images found through vector search ! Work on Flickr-8k dataset \ud83d\udd0e"},{"location":"examples/python_examples/rag/","title":"RAG (Retrieval-Augmented Generation) with LanceDB \ud83d\udd13\ud83e\uddd0","text":"<p>Build RAG (Retrieval-Augmented Generation) with  LanceDB, a powerful solution for efficient vector-based information retrieval \ud83d\udcca. </p> <p>Experience the Future of Search \ud83d\udd04</p> <p>\ud83e\udd16 RAG enables AI to retrieve relevant information from external sources and use it to generate more accurate and context-specific responses. \ud83d\udcbb LanceDB provides a robust framework for integrating LLMs with external knowledge sources \ud83d\udcdd.</p> RAG Description Links RAG with Matryoshka Embeddings and LlamaIndex \ud83e\ude86\ud83d\udd17 Utilize Matryoshka embeddings and LlamaIndex to improve the efficiency and accuracy of your RAG models. \ud83d\udcc8\u2728 Improve RAG with Re-ranking \ud83d\udcc8\ud83d\udd04 Enhance your RAG applications by implementing re-ranking strategies for more relevant document retrieval. \ud83d\udcda\ud83d\udd0d Instruct-Multitask \ud83e\udde0\ud83c\udfaf Integrate the Instruct Embedding Model with LanceDB to streamline your embedding API, reducing redundant code and overhead. \ud83c\udf10\ud83d\udcca Improve RAG with HyDE \ud83c\udf0c\ud83d\udd0d Use Hypothetical Document Embeddings for efficient, accurate, and unsupervised dense retrieval. \ud83d\udcc4\ud83d\udd0d Improve RAG with LOTR \ud83e\uddd9\u200d\u2642\ufe0f\ud83d\udcdc Enhance RAG with Lord of the Retriever (LOTR) to address 'Lost in the Middle' challenges, especially in medical data. \ud83c\udf1f\ud83d\udcdc Advanced RAG: Parent Document Retriever \ud83d\udcd1\ud83d\udd17 Use Parent Document &amp; Bigger Chunk Retriever to maintain context and relevance when generating related content. \ud83c\udfb5\ud83d\udcc4 Corrective RAG with Langgraph \ud83d\udd27\ud83d\udcca Enhance RAG reliability with Corrective RAG (CRAG) by self-reflecting and fact-checking for accurate and trustworthy results. \u2705\ud83d\udd0d Contextual Compression with RAG \ud83d\udddc\ufe0f\ud83e\udde0 Apply contextual compression techniques to condense large documents while retaining essential information. \ud83d\udcc4\ud83d\udddc\ufe0f Improve RAG with FLARE \ud83d\udd25 Enable users to ask questions directly to academic papers, focusing on ArXiv papers, with Forward-Looking Active REtrieval augmented generation.\ud83d\ude80\ud83c\udf1f Query Expansion and Reranker \ud83d\udd0d\ud83d\udd04 Enhance RAG with query expansion using Large Language Models and advanced reranking methods like Cross Encoders, ColBERT v2, and FlashRank for improved document retrieval precision and recall \ud83d\udd0d\ud83d\udcc8 RAG Fusion \u26a1\ud83c\udf10 Build RAG Fusion, utilize the RRF algorithm to rerank documents based on user queries ! Use LanceDB as vector database to store and retrieve documents related to queries via OPENAI Embeddings\u26a1\ud83c\udf10 Agentic RAG \ud83e\udd16\ud83d\udcda Build autonomous information retrieval with Agentic RAG, a framework of intelligent agents that collaborate to synthesize, summarize, and compare data across sources, that enables proactive and informed decision-making \ud83e\udd16\ud83d\udcda"},{"location":"examples/python_examples/recommendersystem/","title":"Recommender Systems: Personalized Discovery\ud83c\udf7f\ud83d\udcfa","text":"<p>Deliver personalized experiences with Recommender Systems. \ud83c\udf81</p> <p>Technical Overview\ud83d\udcdc</p> <p>\ud83d\udd0d\ufe0f LanceDB's powerful vector database capabilities can efficiently store and query item embeddings. Recommender Systems can utilize it and provide personalized recommendations based on user preferences \ud83e\udd1d and item features \ud83d\udcca and therefore enhance the user experience.\ud83d\uddc2\ufe0f </p> Recommender System Description Links Movie Recommender System\ud83c\udfac \ud83e\udd1d Use collaborative filtering to predict user preferences, assuming similar users will like similar movies, and leverage Singular Value Decomposition (SVD) from Numpy for precise matrix factorization and accurate recommendations\ud83d\udcca \ud83c\udfa5 Movie Recommendation with Genres \ud83d\udd0d Creates movie embeddings using Doc2Vec, capturing genre and characteristic nuances, and leverages VectorDB for efficient storage and querying, enabling accurate genre classification and personalized movie recommendations through similarity searches\ud83c\udfa5 \ud83d\udecd\ufe0f Product Recommender using Collaborative Filtering and LanceDB \ud83d\udcc8 Using Collaborative Filtering and LanceDB to analyze your past purchases, recommends products based on user's past purchases. Demonstrated with the Instacart dataset in our example\ud83d\uded2 \ud83d\udd0d Arxiv Search with OpenCLIP and LanceDB \ud83d\udca1 Build a semantic search engine for Arxiv papers using LanceDB, and benchmarks its performance against traditional keyword-based search on Nomic's Atlas, to demonstrate the power of semantic search in finding relevant research papers\ud83d\udcda Food Recommendation System\ud83c\udf74 \ud83c\udf54 Build a food recommendation system with LanceDB, featuring vector-based recommendations, full-text search, hybrid search, and reranking model integration for personalized and accurate food suggestions\ud83d\udc4c"},{"location":"examples/python_examples/vector_search/","title":"Vector Search: Efficient Retrieval \ud83d\udd13\ud83d\udc40","text":"<p>Vector search with LanceDB, is a solution for  efficient and accurate similarity searches in large datasets \ud83d\udcca. </p> <p>Vector Search Capabilities in LanceDB\ud83d\udd1d</p> <p>LanceDB implements vector search algorithms for efficient document retrieval and analysis \ud83d\udcca. This enables fast and accurate discovery of relevant documents, leveraging dense vector representations \ud83e\udd16. The platform supports scalable indexing and querying of high-dimensional vector spaces, facilitating precise document matching and retrieval \ud83d\udcc8.</p> Vector Search Description Links Inbuilt Hybrid Search \ud83d\udd04 Perform hybrid search in LanceDB by combining the results of semantic and full-text search via a reranking algorithm of your choice \ud83d\udcca Hybrid Search with BM25 and LanceDB \ud83d\udca1 Use Synergizes BM25's keyword-focused precision (term frequency, document length normalization, bias-free retrieval) with LanceDB's semantic understanding (contextual analysis, query intent alignment) for nuanced search results in complex datasets \ud83d\udcc8 NER-powered Semantic Search \ud83d\udd0e Extract and identify essential information from text with Named Entity Recognition (NER) methods: Dictionary-Based, Rule-Based, and Deep Learning-Based, to accurately extract and categorize entities, enabling precise semantic search results \ud83d\uddc2\ufe0f Audio Similarity Search using Vector Embeddings \ud83c\udfb5 Create vector embeddings of audio files to find similar audio content, enabling efficient audio similarity search and retrieval in LanceDB's vector store \ud83d\udcfb LanceDB Embeddings API: Multi-lingual Semantic Search \ud83c\udf0e Build a universal semantic search table with LanceDB's Embeddings API, supporting multiple languages (e.g., English, French) using cohere's multi-lingual model, for accurate cross-lingual search results \ud83d\udcc4 Facial Recognition: Face Embeddings \ud83e\udd16 Detect, crop, and embed faces using Facenet, then store and query face embeddings in LanceDB for efficient facial recognition and top-K matching results \ud83d\udc65 Sentiment Analysis: Hotel Reviews \ud83c\udfe8 Analyze customer sentiments towards the hotel industry using BERT models, storing sentiment labels, scores, and embeddings in LanceDB, enabling queries on customer opinions and potential areas for improvement \ud83d\udcac Vector Arithmetic with LanceDB \u2696\ufe0f Perform vector arithmetic on embeddings, enabling complex relationships and nuances in data to be captured, and simplifying the process of retrieving semantically similar results \ud83d\udcca Imagebind Demo \ud83d\uddbc\ufe0f Explore the multi-modal capabilities of Imagebind through a Gradio app, use LanceDB API for seamless image search and retrieval experiences \ud83d\udcf8 Search Engine using SAM &amp; CLIP \ud83d\udd0d Build a search engine within an image using SAM and CLIP models, enabling object-level search and retrieval, with LanceDB indexing and search capabilities to find the closest match between image embeddings and user queries \ud83d\udcf8 Zero Shot Object Localization and Detection with CLIP \ud83d\udd0e Perform object detection on images using OpenAI's CLIP, enabling zero-shot localization and detection of objects, with capabilities to split images into patches, parse with CLIP, and plot bounding boxes \ud83d\udcca Accelerate Vector Search with OpenVINO \ud83d\ude80 Boost vector search applications using OpenVINO, achieving significant speedups with CLIP for text-to-image and image-to-image searching, through PyTorch model optimization, FP16 and INT8 format conversion, and quantization with OpenVINO NNCF \ud83d\udcc8 Zero-Shot Image Classification with CLIP and LanceDB \ud83d\udcf8 Achieve zero-shot image classification using CLIP and LanceDB, enabling models to classify images without prior training on specific use cases, unlocking flexible and adaptable image classification capabilities \ud83d\udd13"},{"location":"guides/indexing/fts-index/","title":"Full Text Index","text":""},{"location":"guides/indexing/fts-index/#full-text-search-index","title":"Full-Text Search Index","text":"<p>LanceDB Cloud and Enterprise provide performant full-text search based on BM25, allowing you to incorporate keyword-based search in your retrieval solutions.</p> <p>The <code>create_fts_index</code> API returns immediately, but the building of the FTS index is asynchronous.</p> PythonTypeScript <pre><code>import lancedb\n\n# Connect to LanceDB\ndb = lancedb.connect(\n  uri=\"db://your-project-slug\",\n  api_key=\"your-api-key\",\n  region=\"us-east-1\"\n)\n\ntable_name = \"lancedb-cloud-quickstart\"\ntable = db.open_table(table_name)\ntable.create_fts_index(\"text\")\n</code></pre> <pre><code>import * as lancedb from \"@lancedb/lancedb\"\n\nconst db = await lancedb.connect({\n  uri: \"db://your-project-slug\",\n  apiKey: \"your-api-key\",\n  region: \"us-east-1\"\n});\n\nconst tableName = \"lancedb-cloud-quickstart\"\nconst table = openTable(tableName);\nawait table.createIndex(\"text\", {\n  config: lancedb.Index.fts()\n});\n</code></pre> <p>Check FTS index status using the methods above.</p> PythonTypeScript <pre><code>index_name = \"text_idx\"\ntable.wait_for_index([index_name])\n</code></pre> <pre><code>const indexName = \"text_idx\"\nawait table.waitForIndex([indexName], 60)\n</code></pre>"},{"location":"guides/indexing/fts-index/#fts-configuration-parameters","title":"FTS Configuration Parameters","text":"<p>LanceDB supports the following configurable parameters for full-text search:</p> Parameter Type Default Description with_position bool True Store token positions (required for phrase queries) base_tokenizer str \"simple\" Text splitting method:  - \"simple\": Split by whitespace/punctuation  - \"whitespace\": Split by whitespace only  - \"raw\": Treat as single token language str \"English\" Language for tokenization (stemming/stop words) max_token_length int 40 Maximum token size in bytes; tokens exceeding this length are omitted from the index lower_case bool True Convert tokens to lowercase stem bool False Apply stemming (e.g., \"running\" \u2192 \"run\") remove_stop_words bool False Remove common stop words ascii_folding bool False Normalize accented characters <ul> <li>The <code>max_token_length</code> parameter helps optimize indexing performance by filtering out non-linguistic content like base64 data and long URLs</li> <li>When <code>with_position</code> is disabled, phrase queries will not work, but index size is reduced and indexing is faster</li> <li><code>ascii_folding</code> is useful for handling international text (e.g., \"caf\u00e9\" \u2192 \"cafe\")</li> </ul>"},{"location":"guides/indexing/gpu-indexing/","title":"GPU-based Indexing","text":"<p>This feature is currently only available in LanceDB Enterprise. Please contact us to enable GPU indexing for your deployment.</p> <p>With GPU-powered indexing, LanceDB can create vector indices with billions of rows in a few hours.</p>"},{"location":"guides/indexing/reindexing/","title":"Incremental indexing","text":""},{"location":"guides/indexing/reindexing/#update-an-index","title":"Update an Index","text":"<p>When new data is added to a table, LanceDB Cloud automatically updates indices in the background.</p> <p>To check index status, use <code>index_stats()</code> to view the number of unindexed rows. This will be zero when indices are fully up-to-date.</p> <p>While indices are being updated, queries use brute force methods for unindexed rows, which may temporarily increase latency. To avoid this, set <code>fast_search=True</code> to search only indexed data.</p> <p>LanceDB supports incremental indexing, which means you can add new records to the table without reindexing the entire table.</p> <p>This can make the query more efficient, especially when the table is large and the new records are relatively small.</p> PythonTypeScriptRust Sync APIAsync API <pre><code>table.add([{\"vector\": [3.1, 4.1], \"text\": \"Frodo was a happy puppy\"}])\ntable.optimize()\n</code></pre> <pre><code>await async_tbl.add([{\"vector\": [3.1, 4.1], \"text\": \"Frodo was a happy puppy\"}])\nawait async_tbl.optimize()\n</code></pre> <pre><code>await tbl.add([{ vector: [3.1, 4.1], text: \"Frodo was a happy puppy\" }]);\nawait tbl.optimize();\n</code></pre> <pre><code>let more_data: Box&lt;dyn RecordBatchReader + Send&gt; = create_some_records()?;\ntbl.add(more_data).execute().await?;\ntbl.optimize(OptimizeAction::All).execute().await?;\n</code></pre> <p>Note</p> <p>New data added after creating the FTS index will appear in search results while incremental index is still progress, but with increased latency due to a flat search on the unindexed portion. LanceDB Cloud automates this merging process, minimizing the impact on search speed. </p>"},{"location":"guides/indexing/scalar-index/","title":"Scalar Index in LanceDB","text":"<p>Scalar indices organize data by scalar attributes (e.g. numbers, categorical values), enabling fast filtering of vector data. In vector databases, scalar indices accelerate the retrieval of scalar data associated with vectors, thus enhancing the query performance when searching for vectors that meet certain scalar criteria. </p> <p>Similar to many SQL databases, LanceDB supports several types of scalar indices to accelerate search over scalar columns.</p> <ul> <li><code>BTREE</code>: The most common type is BTREE. The index stores a copy of the   column in sorted order. This sorted copy allows a binary search to be used to   satisfy queries.</li> <li><code>BITMAP</code>: this index stores a bitmap for each unique value in the column. It    uses a series of bits to indicate whether a value is present in a row of a table</li> <li><code>LABEL_LIST</code>: a special index that can be used on <code>List&lt;T&gt;</code> columns to   support queries with <code>array_contains_all</code> and <code>array_contains_any</code>   using an underlying bitmap index.   For example, a column that contains lists of tags (e.g. <code>[\"tag1\", \"tag2\", \"tag3\"]</code>) can be indexed with a <code>LABEL_LIST</code> index.</li> </ul> <p>How to choose the right scalar index type</p> <p><code>BTREE</code>: This index is good for scalar columns with mostly distinct values and does best when the query is highly selective.</p> <p><code>BITMAP</code>: This index works best for low-cardinality numeric or string columns, where the number of unique values is small (i.e., less than a few thousands).</p> <p><code>LABEL_LIST</code>: This index should be used for columns containing list-type data.</p> Data Type Filter Index Type Numeric, String, Temporal <code>&lt;</code>, <code>=</code>, <code>&gt;</code>, <code>in</code>, <code>between</code>, <code>is null</code> <code>BTREE</code> Boolean, numbers or strings with fewer than 1,000 unique values <code>&lt;</code>, <code>=</code>, <code>&gt;</code>, <code>in</code>, <code>between</code>, <code>is null</code> <code>BITMAP</code> List of low cardinality of numbers or strings <code>array_has_any</code>, <code>array_has_all</code> <code>LABEL_LIST</code>"},{"location":"guides/indexing/scalar-index/#create-a-scalar-index","title":"Create a scalar index","text":"PythonTypescript Sync APIAsync API <pre><code>import lancedb\n\nfrom lancedb.index import BTree, Bitmap\n\nuri = \"data/sample-lancedb\"\ndb = lancedb.connect(uri)\nbooks = [\n    {\n        \"book_id\": 1,\n        \"publisher\": \"plenty of books\",\n        \"tags\": [\"fantasy\", \"adventure\"],\n    },\n    {\"book_id\": 2, \"publisher\": \"book town\", \"tags\": [\"non-fiction\"]},\n    {\"book_id\": 3, \"publisher\": \"oreilly\", \"tags\": [\"textbook\"]},\n]\ntable = db.create_table(\"books\", books)\ntable.create_scalar_index(\"book_id\")  # BTree by default\ntable.create_scalar_index(\"publisher\", index_type=\"BITMAP\")\n</code></pre> <pre><code>import lancedb\n\nfrom lancedb.index import BTree, Bitmap\n\nuri = \"data/sample-lancedb\"\nasync_db = await lancedb.connect_async(uri)\nbooks = [\n    {\n        \"book_id\": 1,\n        \"publisher\": \"plenty of books\",\n        \"tags\": [\"fantasy\", \"adventure\"],\n    },\n    {\"book_id\": 2, \"publisher\": \"book town\", \"tags\": [\"non-fiction\"]},\n    {\"book_id\": 3, \"publisher\": \"oreilly\", \"tags\": [\"textbook\"]},\n]\nasync_tbl = await async_db.create_table(\"books_async\", books)\nawait async_tbl.create_index(\"book_id\", config=BTree())  # BTree by default\nawait async_tbl.create_index(\"publisher\", config=Bitmap())\n</code></pre> @lancedb/lancedb <pre><code>const db = await lancedb.connect(\"data\");\nconst tbl = await db.openTable(\"my_vectors\");\n\nawait tbl.create_index(\"book_id\");\nawait tlb.create_index(\"publisher\", { config: lancedb.Index.bitmap() })\n</code></pre> <p>The following scan will be faster if the column <code>book_id</code> has a scalar index:</p> PythonTypescript Sync APIAsync API <pre><code>import lancedb\n\ntable = db.open_table(\"books\")\ntable.search().where(\"book_id = 2\").to_pandas()\n</code></pre> <pre><code>import lancedb\n\nasync_tbl = await async_db.open_table(\"books_async\")\nawait async_tbl.query().where(\"book_id = 2\").to_pandas()\n</code></pre> @lancedb/lancedb <pre><code>const db = await lancedb.connect(\"data\");\nconst tbl = await db.openTable(\"books\");\n\nawait tbl\n  .query()\n  .where(\"book_id = 2\")\n  .limit(10)\n  .toArray();\n</code></pre> <p>Scalar indices can also speed up scans containing a vector search or full text search, and a prefilter:</p> PythonTypescript Sync APIAsync API <pre><code>import lancedb\n\ndata = [\n    {\"book_id\": 1, \"vector\": [1.0, 2]},\n    {\"book_id\": 2, \"vector\": [3.0, 4]},\n    {\"book_id\": 3, \"vector\": [5.0, 6]},\n]\n\ntable = db.create_table(\"book_with_embeddings\", data)\n(table.search([1, 2]).where(\"book_id != 3\", prefilter=True).to_pandas())\n</code></pre> <pre><code>import lancedb\n\ndata = [\n    {\"book_id\": 1, \"vector\": [1.0, 2]},\n    {\"book_id\": 2, \"vector\": [3.0, 4]},\n    {\"book_id\": 3, \"vector\": [5.0, 6]},\n]\nasync_tbl = await async_db.create_table(\"book_with_embeddings_async\", data)\n(await (await async_tbl.search([1, 2])).where(\"book_id != 3\").to_pandas())\n</code></pre> @lancedb/lancedb <pre><code>const db = await lancedb.connect(\"data/lance\");\nconst tbl = await db.openTable(\"book_with_embeddings\");\n\nawait tbl.search(Array(1536).fill(1.2))\n  .where(\"book_id != 3\")  // prefilter is default behavior.\n  .limit(10)\n  .toArray();\n</code></pre>"},{"location":"guides/indexing/scalar-index/#update-a-scalar-index","title":"Update a scalar index","text":"<p>Updating the table data (adding, deleting, or modifying records) requires that you also update the scalar index. This can be done by calling <code>optimize</code>, which will trigger an update to the existing scalar index.</p> PythonTypeScriptRust Sync APIAsync API <pre><code>table.add([{\"vector\": [7, 8], \"book_id\": 4}])\ntable.optimize()\n</code></pre> <pre><code>await async_tbl.add([{\"vector\": [7, 8], \"book_id\": 4}])\nawait async_tbl.optimize()\n</code></pre> <pre><code>await tbl.add([{ vector: [7, 8], book_id: 4 }]);\nawait tbl.optimize();\n</code></pre> <pre><code>let more_data: Box&lt;dyn RecordBatchReader + Send&gt; = create_some_records()?;\ntbl.add(more_data).execute().await?;\ntbl.optimize(OptimizeAction::All).execute().await?;\n</code></pre> <p>Note</p> <p>New data added after creating the scalar index will still appear in search results if optimize is not used, but with increased latency due to a flat search on the unindexed portion. LanceDB Cloud automates the optimize process, minimizing the impact on search speed.</p>"},{"location":"guides/indexing/scalar-index/#scalar-index","title":"Scalar Index","text":"<p>LanceDB Cloud and Enterprise support several types of scalar indices to accelerate search over scalar columns:</p> <ul> <li>BTREE: The most common type, inspired by the btree data structure. Performs well for columns with many unique values and few rows per value.</li> <li>BITMAP: Stores a bitmap for each unique value. Ideal for columns with a finite number of unique values and many rows per value (e.g., categories, labels, tags).</li> <li>LABEL_LIST: Special index for <code>List&lt;T&gt;</code> columns, supporting <code>array_contains_all</code> and <code>array_contains_any</code> queries using an underlying bitmap index.</li> </ul> <p>You can create multiple scalar indices within a table.</p> <p>The <code>create_scalar_index</code> API returns immediately, but the building of the scalar index is asynchronous.</p> PythonTypeScript <pre><code>import lancedb\n\n# Connect to LanceDB\ndb = lancedb.connect(\n  uri=\"db://your-project-slug\",\n  api_key=\"your-api-key\",\n  region=\"us-east-1\"\n)\n\ntable_name = \"lancedb-cloud-quickstart\"\ntable = db.open_table(table_name)\n# Create BITMAP index (default is BTREE)\ntable.create_scalar_index(\"label\", index_type=\"BITMAP\")\n</code></pre> <pre><code>import * as lancedb from \"@lancedb/lancedb\"\n\nconst db = await lancedb.connect({\n  uri: \"db://your-project-slug\",\n  apiKey: \"your-api-key\",\n  region: \"us-east-1\"\n});\n\nconst tableName = \"lancedb-cloud-quickstart\"\nconst table = await db.openTable(tableName);\nawait table.createIndex(\"label\", {\n  config: lancedb.Index.bitmap()\n});\n</code></pre> <p>Check scalar index status using the methods above.</p> PythonTypeScript <pre><code>index_name = \"label_idx\"\ntable.wait_for_index([index_name])\n</code></pre> <pre><code>const indexName = \"label_idx\"\nawait table.waitForIndex([indexName], 60)\n</code></pre>"},{"location":"guides/indexing/scalar-index/#build-a-scalar-index-on-uuid-columns","title":"Build a Scalar Index on UUID Columns","text":"<p>LanceDB supports scalar indices on UUID columns (stored as <code>FixedSizeBinary(16)</code>), enabling efficient lookups and filtering on UUID-based primary keys.</p> <p>To use FixedSizeBinary, ensure you have: - Python SDK version 0.22.0-beta.4 or later - TypeScript SDK version 0.19.0-beta.4 or later</p> Python [expandable]TypeScript [expandable] <pre><code>import lancedb\nimport uuid\nimport pyarrow as pa\n\nclass UuidType(pa.ExtensionType):\n    def __init__(self):\n        super().__init__(pa.binary(16), \"my.uuid\")\n\n    def __arrow_ext_serialize__(self):\n        return b'uuid-metadata'\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        return UuidType()\n\npa.register_extension_type(UuidType())\n\ndef generate_random_string(length=10):\n    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n\ndef generate_uuids(num_items):\n    return [uuid.uuid4().bytes for _ in range(num_items)]\n\n# Connect to LanceDB\ndb = lancedb.connect(\n  uri=\"db://your-project-slug\",\n  api_key=\"your-api-key\",\n  region=\"us-east-1\"\n)\n\nn = 100\nuuids = generate_uuids(n)\nnames = [generate_random_string() for _ in range(n)]\n\n# Create arrays\nuuid_array = pa.array(uuids, pa.binary(16))\nname_array = pa.array(names, pa.string())\nextension_array = pa.ExtensionArray.from_storage(UuidType(), uuid_array)\n\n# Create schema\nschema = pa.schema([\n    pa.field('id', UuidType()),\n    pa.field('name', pa.string())\n])\n\n# Create table\ndata_table = pa.Table.from_arrays([extension_array, name_array], schema=schema)\ntable_name = \"index-on-uuid-test\"\ntable = db.create_table(table_name, data=data_table, mode=\"overwrite\")\n\n# Create index\ntable.create_scalar_index(\"id\")\nindex_name = \"id_idx\"\nwait_for_index(table, index_name)\n\n# Upsert example\nnew_users = [\n    {\"id\": uuid.uuid4().bytes, \"name\": \"Bobby\"},\n    {\"id\": uuid.uuid4().bytes, \"name\": \"Charlie\"},\n]\n\ntable.merge_insert(\"id\") \\\n    .when_matched_update_all() \\\n    .when_not_matched_insert_all() \\\n    .execute(new_users)\n</code></pre> <pre><code>import * as lancedb from \"@lancedb/lancedb\"\nimport { v4 as uuidv4 } from \"uuid\"\nimport { Schema, Field, FixedSizeBinary, Utf8 } from \"apache-arrow\"\n\nexport function uuidToBuffer(uuid: string): Buffer {\n  return Buffer.from(uuid.replace(/-/g, ''), 'hex');\n}\n\nexport function generateUuids(numItems: number): Buffer[] {\n  return Array.from({ length: numItems }, () =&gt; uuidToBuffer(uuidv4()));\n}\n\nexport function generateRandomString(length: number = 10): string {\n  const chars = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789';\n  return Array.from(\n      { length },\n      () =&gt; chars.charAt(Math.floor(Math.random() * chars.length))\n  ).join('');\n}\n\n// Connect to LanceDB\nconst db = await lancedb.connect({\n  uri: \"db://your-project-slug\",\n  apiKey: \"your-api-key\",\n  region: \"us-east-1\"\n});\n\nconst tableName = \"index-on-uuid-test-ts\";\nconst n = 100;\nconst uuids = generateUuids(n);\nconst names = Array.from({ length: n }, () =&gt; generateRandomString());\n\n// Create schema\nconst schema = new Schema([\n  new Field(\"id\", new FixedSizeBinary(16), true),\n  new Field(\"name\", new Utf8(), false),\n]);\n\n// Create data array\nconst data = makeArrowTable(\n  uuids.map((id, index) =&gt; ({\n    id,\n    name: names[index],\n  })),\n  { schema }\n);\n\n// Create table\nconst table = await db.createTable(tableName, data, { mode: \"overwrite\" });\nconsole.log(`Created table: ${tableName}`);\n\n// Create scalar index\nconsole.log(\"Creating scalar index on 'id' column...\");\nawait table.createIndex(\"id\", {\n  config: Index.bitmap(),\n});\n\n// Wait for index\nconst scalarIndexName = \"id_idx\";\nawait waitForIndex(table, scalarIndexName);\n\n// Upsert example\nconst newUsers = [\n  { id: uuidToBuffer(uuidv4()), name: \"Bobby\" },\n  { id: uuidToBuffer(uuidv4()), name: \"Charlie\" },\n];\n\nconsole.log(\"Performing upsert operation...\");\nawait table.mergeInsert(\"id\")\n  .whenMatchedUpdateAll()\n  .whenNotMatchedInsertAll()\n  .execute(newUsers);\n</code></pre>"},{"location":"guides/indexing/vector-index/","title":"Building an Index with LanceDB","text":"<p>LanceDB provides a comprehensive suite of indexing strategies to optimize query performance across diverse workloads:</p> <ul> <li>Vector Index: Optimized for searching high-dimensional data (like images, audio, or text embeddings) by efficiently finding the most similar vectors</li> <li>Scalar Index: Accelerates filtering and sorting of structured numeric or categorical data (e.g., timestamps, prices)</li> <li>Full-Text Search Index: Enables fast keyword-based searches by indexing words and phrases</li> </ul> <p>Scalar indices serve as a foundational optimization layer, accelerating filtering across diverse search workloads. They can be combined with:</p> <p>Vector search (prefilter or post-filter results using metadata) - Full-text search (combining keyword matching with structured filters) - SQL scans (optimizing WHERE clauses on scalar columns) - Key-value lookups (enabling rapid primary key-based retrievals)</p> <p>Compared to our open source version, LanceDB Cloud/Enterprise automates data indexing with low-latency, asynchronous indexing performed in the cloud. - Auto-Indexing: Automates index optimization for all types of indices as soon as data is updated - Automatic Index Creation: When a table contains a single vector column named <code>vector</code>, LanceDB:   - Infers the vector column from the table schema   - Creates an optimized IVF-PQ index without manual configuration - Parameter Autotuning: LanceDB analyzes your data distribution to automatically configure indexing parameters</p>"},{"location":"guides/indexing/vector-index/#vector-index","title":"Vector Index","text":"<p>LanceDB implements state-of-the-art indexing algorithms (IVF-PQ and HNSW) with acceleration from our optimized infrastructure. We support multiple distance metrics:</p> <ul> <li>L2 (default)</li> <li>Cosine</li> <li>Dot</li> <li>Hamming (for binary vectors only)</li> </ul> <p>You can create multiple vector indices within a table.</p> <p>The <code>create_index</code> API returns immediately, but the building of the vector index is asynchronous. To wait until all data is fully indexed, you can specify the<code>wait_timeout</code>parameter.</p> PythonTypeScript <pre><code>import lancedb\nfrom datetime import timedelta\n\n# Connect to LanceDB\ndb = lancedb.connect(\n  uri=\"db://your-project-slug\",\n  api_key=\"your-api-key\",\n  region=\"us-east-1\"\n)\n\n# Use the table from quickstart\ntable_name = \"lancedb-cloud-quickstart\"\ntable = db.open_table(table_name)\n\n# Create index with cosine similarity\n# Note: vector_column_name only needed for multiple vector columns or non-default names\n# Supported index types: IVF_PQ (default) and IVF_HNSW_SQ\ntable.create_index(metric=\"cosine\", vector_column_name=\"keywords_embeddings\", wait_timeout=timedelta(seconds=60))\n</code></pre> <pre><code>import * as lancedb from \"@lancedb/lancedb\"\n\nconst db = await lancedb.connect({\n  uri: \"db://your-project-slug\",\n  apiKey: \"your-api-key\",\n  region: \"us-east-1\"\n});\n\nconst tableName = \"myTable\"\nconst table = await db.openTable(tableName);\n\n// Create index with cosine similarity\n// Note: vector_column_name only needed for multiple vector columns or non-default names\n// Supported index types: IVF_PQ (default) and IVF_HNSW_SQ\nawait table.createIndex(\"keywords_embeddings\", {\n  config: lancedb.Index.ivfPq({\n    distanceType: 'cosine'\n  })\n});\n</code></pre> <ul> <li>If your vector column is named <code>vector</code> and contains more than 256 vectors, an IVF_PQ index with L2 distance is automatically created</li> <li>You can create a new index with different parameters using <code>create_index</code> - this replaces any existing index</li> <li>When using cosine similarity, distances range from 0 (identical vectors) to 2 (maximally dissimilar)</li> <li>Available index types:</li> <li><code>IVF_PQ</code>: Default index type, optimized for high-dimensional vectors</li> <li><code>IVF_HNSW_SQ</code>: Combines IVF clustering with HNSW graph for improved search quality</li> </ul>"},{"location":"guides/indexing/vector-index/#build-hnsw-index","title":"Build HNSW Index","text":"<p>There are three key parameters to set when constructing an HNSW index:</p> <ul> <li><code>metric</code>: Use an <code>l2</code> euclidean distance metric. We also support <code>dot</code> and <code>cosine</code> distance.</li> <li><code>m</code>: The number of neighbors to select for each vector in the HNSW graph.</li> <li><code>ef_construction</code>: The number of candidates to evaluate during the construction of the HNSW graph.</li> </ul> <p>We can combine the above concepts to understand how to build and query an HNSW index in LanceDB.</p>"},{"location":"guides/indexing/vector-index/#construct-index","title":"Construct index","text":"<pre><code>import lancedb\nimport numpy as np\nuri = \"/tmp/lancedb\"\ndb = lancedb.connect(uri)\n\n# Create 10,000 sample vectors\ndata = [\n    {\"vector\": row, \"item\": f\"item {i}\"}\n    for i, row in enumerate(np.random.random((10_000, 1536)).astype('float32'))\n]\n\n# Add the vectors to a table\ntbl = db.create_table(\"my_vectors\", data=data)\n\n# Create and train the HNSW index for a 1536-dimensional vector\n# Make sure you have enough data in the table for an effective training step\ntbl.create_index(index_type=IVF_HNSW_SQ)\n</code></pre>"},{"location":"guides/indexing/vector-index/#query-the-index","title":"Query the index","text":"<pre><code># Search using a random 1536-dimensional embedding\ntbl.search(np.random.random((1536))) \\\n    .limit(2) \\\n    .to_pandas()\n</code></pre>"},{"location":"guides/indexing/vector-index/#putting-it-all-together","title":"Putting it all together","text":"<p>We can combine the above concepts to understand how to build and query an IVF-PQ index in LanceDB.</p>"},{"location":"guides/indexing/vector-index/#construct-index_1","title":"Construct index","text":"<p>There are three key parameters to set when constructing an IVF-PQ index:</p> <ul> <li><code>metric</code>: Use an <code>l2</code> euclidean distance metric. We also support <code>dot</code> and <code>cosine</code> distance.</li> <li><code>num_partitions</code>: The number of partitions in the IVF portion of the index.</li> <li><code>num_sub_vectors</code>: The number of sub-vectors that will be created during Product Quantization (PQ).</li> </ul> <p>In Python, the index can be created as follows:</p> <pre><code># Create and train the index for a 1536-dimensional vector\n# Make sure you have enough data in the table for an effective training step\ntbl.create_index(metric=\"l2\", num_partitions=256, num_sub_vectors=96)\n</code></pre> <p>Note</p> <p><code>num_partitions</code>=256 and <code>num_sub_vectors</code>=96 does not work for every dataset. Those values needs to be adjusted for your particular dataset.</p> <p>The <code>num_partitions</code> is usually chosen to target a particular number of vectors per partition. <code>num_sub_vectors</code> is typically chosen based on the desired recall and the dimensionality of the vector. See here for best practices on choosing these parameters.</p>"},{"location":"guides/indexing/vector-index/#query-the-index_1","title":"Query the index","text":"<pre><code># Search using a random 1536-dimensional embedding\ntbl.search(np.random.random((1536))) \\\n    .limit(2) \\\n    .nprobes(20) \\\n    .refine_factor(10) \\\n    .to_pandas()\n</code></pre> <p>The above query will perform a search on the table <code>tbl</code> using the given query vector, with the following parameters:</p> <ul> <li><code>limit</code>: The number of results to return</li> <li><code>nprobes</code>: The number of probes determines the distribution of vector space. While a higher number enhances search accuracy, it also results in slower performance. Typically, setting <code>nprobes</code> to cover 5\u201310% of the dataset proves effective in achieving high recall with minimal latency.</li> <li><code>refine_factor</code>: Refine the results by reading extra elements and re-ranking them in memory. A higher number makes the search more accurate but also slower (see the FAQ page for more details on this).</li> <li><code>to_pandas()</code>: Convert the results to a pandas DataFrame</li> </ul> <p>And there you have it! You now understand what an IVF-PQ index is, and how to create and query it in LanceDB. To see how to create an IVF-PQ index in LanceDB, take a look at the ANN indexes section.</p>"},{"location":"guides/indexing/vector-index/#check-index-status","title":"Check Index Status","text":"<p>Vector index creation is fast - typically a few minutes for 1 million vectors with 1536 dimensions. You can check index status in two ways:</p>"},{"location":"guides/indexing/vector-index/#option-1-dashboard","title":"Option 1: Dashboard","text":"<p>Navigate to your table page - the \"Index\" column shows index status. It remains blank if no index exists or if creation is in progress. </p>"},{"location":"guides/indexing/vector-index/#option-2-api","title":"Option 2: API","text":"<p>Use <code>list_indices()</code> and <code>index_stats()</code> to check index status. The index name is formed by appending \"_idx\" to the column name. Note that <code>list_indices()</code> only returns information after the index is fully built. To wait until all data is fully indexed, you can specify the <code>wait_timeout</code> parameter on <code>create_index()</code> or call <code>wait_for_index()</code> on the table.</p> PythonTypeScript <pre><code>import time\n\nindex_name = \"keywords_embeddings_idx\"\ntable.wait_for_index([index_name])\nprint(table.index_stats(index_name))\n# IndexStatistics(num_indexed_rows=3000, num_unindexed_rows=0, index_type='IVF_PQ', \n#   distance_type='cosine', num_indices=None)\n</code></pre> <pre><code>const indexName = \"keywords_embeddings_idx\"\nawait table.waitForIndex([indexName], 60)\nconsole.log(await table.indexStats(indexName))\n// {\n//   numIndexedRows: 1000,\n//   numUnindexedRows: 0,\n//   indexType: 'IVF_PQ',\n//   distanceType: 'cosine'\n// }\n</code></pre>"},{"location":"guides/indexing/vector-index/#index-binary-vectors","title":"Index Binary Vectors","text":"<p>Binary vectors are useful for hash-based retrieval, fingerprinting, or any scenario where data can be represented as bits.</p> <p>Key points for binary vectors:</p> <ul> <li>Store as fixed-size binary data (uint8 arrays, with 8 bits per byte)</li> <li>Use Hamming distance for similarity search</li> <li>Pack binary vectors into bytes to save space</li> </ul> <p>   The dimension of binary vectors must be a multiple of 8. For example, a 128-dimensional vector is stored as a uint8 array of size 16. </p> PythonTypeScript <pre><code>import lancedb\nimport numpy as np\nimport pyarrow as pa\n\n# Connect to LanceDB\ndb = lancedb.connect(\n  uri=\"db://your-project-slug\",\n  api_key=\"your-api-key\",\n  region=\"us-east-1\"\n)\n\n# Create schema with binary vector field\ntable_name = \"test-hamming\"\nndim = 256\nschema = pa.schema([\n    pa.field(\"id\", pa.int64()),\n    # For dim=256, store every 8 bits in a byte (32 bytes total)\n    pa.field(\"vector\", pa.list_(pa.uint8(), 32)),\n])\n\ntable = db.create_table(table_name, schema=schema, mode=\"overwrite\")\n\n# Generate and add data\ndata = []\nfor i in range(1024):\n    vector = np.random.randint(0, 2, size=ndim)\n    vector = np.packbits(vector)  # Optional: pack bits to save space\n    data.append({\"id\": i, \"vector\": vector})\ntable.add(data)\n\n# Create index with Hamming distance\ntable.create_index(\n    metric=\"hamming\",\n    vector_column_name=\"vector\",\n    index_type=\"IVF_FLAT\"\n)\n\n# Search example\nquery = np.random.randint(0, 2, size=256)\nquery = np.packbits(query)\ndf = table.search(query).metric(\"hamming\").limit(10).to_pandas()\ndf.vector = df.vector.apply(np.unpackbits)\n</code></pre> <pre><code>// Create schema with binary vector field\nconst binaryTableName = \"test-hamming-ts\";\nconst ndim = 256;\nconst bytesPerVector = ndim / 8; // 32 bytes for 256 bits\n\nconst binarySchema = new Schema([\n  new Field(\"id\", new Int32(), true),\n  new Field(\"vector\", new FixedSizeList(32, new Field(\"item\", new Uint8()))),\n]);\n\n// Generate random binary vectors\nconst data = makeArrowTable(\n  Array(1_000).fill(0).map((_, i) =&gt; ({\n    id: i,\n    vector: packBits(Array(ndim).fill(0).map(() =&gt; Math.floor(Math.random() * 2))),\n  })),\n  { schema: binarySchema },\n);\n\n// Create and populate table\nconst table = await db.createTable(binaryTableName, data);\nconsole.log(`Created table: ${binaryTableName}`);\n\n// Create index with Hamming distance\nconsole.log(\"Creating binary vector index...\");\nawait table.createIndex(\"vector\", {\n  config: Index.ivfFlat({\n    distanceType: \"hamming\",\n  }),\n});\n\n// Wait for index\nconst binaryIndexName = \"vector_idx\";\nawait table.waitForIndex([binaryIndexName], 60)\n\n// Generate query and search\nconst query = packBits(Array(ndim).fill(0).map(() =&gt; Math.floor(Math.random() * 2)));\nconsole.log(\"Performing binary vector search...\");\nconst binaryResults = await table\n  .query()\n  .nearestTo(query)\n  .limit(10)\n  .toArray();\n\n// Unpack vectors for display\nconst unpackedResults = binaryResults.map(row =&gt; ({\n  ...row,\n  vector: Array.from(row.vector)\n}));\nconsole.log(\"Binary vector search results:\", unpackedResults);\n</code></pre> <p><code>IVF_FLAT</code> with Hamming distance is used for indexing binary vectors.</p>"},{"location":"guides/search/filtering/","title":"Metadata Filtering with LanceDB","text":"<p>We support rich filtering features of query results based on metadata fields.  While joint vector and metadata search at scale presents a significant challenge,  LanceDB achieves sub-100ms latency at thousands of QPS, enabling efficient vector search  with filtering capabilities even on datasets containing billions of records. </p> <p>By default, pre-filtering is applied before executing the vector search to  narrow the search space within large datasets, thereby reducing query latency.  Alternatively, post-filtering can be employed to refine results after the  vector search completes.</p> <p>Best Practices for Filtering</p> <p>Scalar Indices: We strongly recommend creating scalar indices on  columns used for filtering, whether combined with a search operation  or applied independently (e.g., for updates or deletions).</p> <p>SQL Filters: Built on DataFusion, LanceDB supports SQL filters.  For example: for a column of type LIST(T) can use <code>LABEL_LIST</code> to  create a scalar index. Then leverage DataFusion's  array functions  like <code>array_has_any</code> or <code>array_has_all</code> for optimized filtering.</p> <p>For best performance with large tables or high query volumes:</p> <ul> <li>Build a scalar index on frequently filtered columns</li> <li>Use exact column names in filters (e.g., <code>user_id</code> instead of <code>USER_ID</code>)</li> <li>Avoid complex transformations in filter expressions (keep them simple)</li> <li>When running concurrent queries, use connection pooling for better throughput</li> </ul> PythonTypeScript <pre><code>import lancedb\n\n# connect to LanceDB\ndb = lancedb.connect(\n  uri=\"db://your-project-slug\",\n  api_key=\"your-api-key\",\n  region=\"us-east-1\"\n)\n# Create sample data\ndata = [\n    {\"vector\": [3.1, 4.1], \"item\": \"foo\", \"price\": 10.0},\n    {\"vector\": [5.9, 26.5], \"item\": \"bar\", \"price\": 20.0},\n    {\"vector\": [10.2, 100.8], \"item\": \"baz\", \"price\": 30.0},\n    {\"vector\": [1.4, 9.5], \"item\": \"fred\", \"price\": 40.0},\n]\ntable = db.create_table(\"metadata_filter_example\", data=data, mode=\"overwrite\")\n\n#  Filters with vector search\nfiltered_result = (\n    table.search([100, 102])\n    .where(\"(item IN ('foo', 'bar')) AND (price &gt; 9.0)\")\n    .limit(3)\n    .to_arrow()\n)\n\n# With post-filtering\npost_filtered_result = (\n    table.search([100, 102])\n    .where(\"(item IN ('foo', 'bar')) AND (price &gt; 9.0)\", prefilter=False)\n    .limit(3)\n    .to_arrow()\n)\n\n# Filters without vector search\nfiltered_no_search_result = (\n    table.search()\n    .where(\"(item IN ('foo', 'bar', 'baz')) AND (price &gt; 9.0)\")\n    .limit(3)\n    .to_arrow()\n)\n</code></pre> <pre><code>import * as lancedb from \"@lancedb/lancedb\"\n\nconst db = await lancedb.connect({\n  uri: \"db://your-project-slug\",\n  apiKey: \"your-api-key\",\n  region: \"us-east-1\"\n});\nconst data = [\n  { vector: [3.1, 4.1], item: \"foo\", price: 10.0 },\n  { vector: [5.9, 26.5], item: \"bar\", price: 20.0 },\n  { vector: [10.2, 100.8], item: \"baz\", price: 30.0 },\n  { vector: [1.4, 9.5], item: \"fred\", price: 40.0 },\n];\n\nconst tableName = \"metadata_filter_example\";\nconst table = await db.createTable(tableName, data, {\n  mode: \"overwrite\",\n});\nconst results = await table\n  .search([100, 102])\n  .where(\"(item IN ('foo', 'bar')) AND (price &gt; 10.0)\")\n  .toArray();\n\n// With post-filtering\nconst postFilteredResult = await (table.search([100, 102]) as VectorQuery)\n  .where(\"(item IN ('foo', 'bar')) AND (price &gt; 9.0)\")\n  .postfilter()\n  .limit(3)\n  .toArray();\n\n// Filters without vector search\nconst filteredResult = await table\n  .query()\n  .where(\"(item IN ('foo', 'bar')) AND (price &gt; 9.0)\")\n  .limit(3)\n  .toArray();\n</code></pre> <p>Resource Usage Warning</p> <p>When querying large tables, omitting a <code>limit</code> clause may: - Overwhelm Resources: return excessive data. - Increase Costs: query pricing scales with data scanned and returned (LanceDB Cloud pricing).</p>"},{"location":"guides/search/filtering/#sql-filters","title":"SQL filters","text":"<p>Because it's built on top of DataFusion, LanceDB embraces the  utilization of standard SQL expressions as predicates for  filtering operations. SQL can be used during vector search,  update, and deletion operations.</p> SQL Expression Description <code>&gt;, &gt;=, &lt;, &lt;=, =</code> Comparison operators <code>AND</code>, <code>OR</code>, <code>NOT</code> Logical operators <code>IS NULL</code>, <code>IS NOT NULL</code> Null checks <code>IS TRUE</code>, <code>IS NOT TRUE</code>, <code>IS FALSE</code>, <code>IS NOT FALSE</code> Boolean checks <code>IN</code> Value matching from a set <code>LIKE</code>, <code>NOT LIKE</code> Pattern matching <code>CAST</code> Type conversion <code>regexp_match(column, pattern)</code> Regular expression matching DataFusion Functions Additional SQL functions <p>If your column name contains special characters, upper-case characters,  or is a SQL Keyword, you can use backtick (`) to escape it.  For nested fields, each segment of the path must be wrapped in  backticks.</p> <pre><code>`CUBE` = 10 AND `UpperCaseName` = '3' AND `column name with space` IS NOT NULL\nAND `nested with space`.`inner with space` &lt; 2\n</code></pre> <p>Field Name Limitation</p> <p>Field names containing periods (.) are NOT supported.</p>"},{"location":"guides/search/filtering/#filtering-sql-in-lancedb","title":"Filtering &amp; SQL in LanceDB","text":""},{"location":"guides/search/filtering/#pre-and-post-filtering","title":"Pre and Post-Filtering","text":"<p>LanceDB supports filtering of query results based on metadata fields. By default, post-filtering is performed on the top-k results returned by the vector search. However, pre-filtering is also an option that performs the filter prior to vector search. This can be useful to narrow down the search space of a very large dataset to reduce query latency.</p> <p>Note that both pre-filtering and post-filtering can yield false positives. For pre-filtering, if the filter is too selective, it might eliminate relevant items that the vector search would have otherwise identified as a good match. In this case, increasing <code>nprobes</code> parameter will help reduce such false positives. It is recommended to call <code>bypass_vector_index()</code> if you know that the filter is highly selective.</p> <p>Similarly, a highly selective post-filter can lead to false positives. Increasing both <code>nprobes</code> and <code>refine_factor</code> can mitigate this issue. When deciding between pre-filtering and post-filtering, pre-filtering is generally the safer choice if you're uncertain.</p> PythonTypeScript <pre><code># Synchronous client\nresult = tbl.search([0.5, 0.2]).where(\"id = 10\", prefilter=True).limit(1).to_arrow()\n# Asynchronous client\nresult = await async_tbl.query().where(\"id = 10\").nearest_to([0.5, 0.2]).limit(1).to_arrow()\n</code></pre> @lancedb/lancedbvectordb (deprecated) <pre><code>const _result = await tbl\n  .search(Array(1536).fill(0.5))\n  .limit(1)\n  .where(\"id = 10\")\n  .toArray();\n</code></pre> <pre><code>let result = await tbl\n  .search(Array(1536).fill(0.5))\n  .limit(1)\n  .filter(\"id = 10\")\n  .prefilter(true)\n  .execute();\n</code></pre> <p>Note</p> <p>Creating a scalar index accelerates filtering.</p>"},{"location":"guides/search/filtering/#sql-filtering","title":"SQL Filtering","text":"<p>Because it's built on top of DataFusion, LanceDB embraces the utilization of standard SQL expressions as predicates for filtering operations. SQL can be used during vector search, update, and deletion operations.</p> <p>LanceDB supports a growing list of SQL expressions:</p> <ul> <li><code>&gt;</code>, <code>&gt;=</code>, <code>&lt;</code>, <code>&lt;=</code>, <code>=</code></li> <li><code>AND</code>, <code>OR</code>, <code>NOT</code></li> <li><code>IS NULL</code>, <code>IS NOT NULL</code></li> <li><code>IS TRUE</code>, <code>IS NOT TRUE</code>, <code>IS FALSE</code>, <code>IS NOT FALSE</code></li> <li><code>IN</code></li> <li><code>LIKE</code>, <code>NOT LIKE</code></li> <li><code>CAST</code></li> <li><code>regexp_match(column, pattern)</code></li> <li>DataFusion Functions</li> </ul> <p>For example, the following filter string is acceptable:</p> PythonTypeScript <pre><code># Synchronous client\ntbl.search([100, 102]).where(\n    \"(item IN ('item 0', 'item 2')) AND (id &gt; 10)\"\n).to_arrow()\n# Asynchronous client\nawait (\n    async_tbl.query()\n    .where(\"(item IN ('item 0', 'item 2')) AND (id &gt; 10)\")\n    .nearest_to([100, 102])\n    .to_arrow()\n)\n</code></pre> @lancedb/lancedbvectordb (deprecated) <pre><code>const result = await (\n  tbl.search(Array(1536).fill(0)) as lancedb.VectorQuery\n)\n  .where(\"(item IN ('item 0', 'item 2')) AND (id &gt; 10)\")\n  .postfilter()\n  .toArray();\n</code></pre> <pre><code>await tbl\n  .search(Array(1536).fill(0))\n  .where(\"(item IN ('item 0', 'item 2')) AND (id &gt; 10)\")\n  .execute();\n</code></pre> <p>If your column name contains special characters, upper-case characters, or is a SQL Keyword, you can use backtick (<code>`</code>) to escape it. For nested fields, each segment of the path must be wrapped in backticks.</p> SQL <pre><code>`CUBE` = 10 AND `UpperCaseName` = '3' AND `column name with space` IS NOT NULL\n  AND `nested with space`.`inner with space` &lt; 2\n</code></pre> <p>Field names containing periods (<code>.</code>) are not supported.</p> <p>Literals for dates, timestamps, and decimals can be written by writing the string value after the type name. For example:</p> SQL <pre><code>date_col = date '2021-01-01'\nand timestamp_col = timestamp '2021-01-01 00:00:00'\nand decimal_col = decimal(8,3) '1.000'\n</code></pre> <p>For timestamp columns, the precision can be specified as a number in the type parameter. Microsecond precision (6) is the default.</p> SQL Time unit <code>timestamp(0)</code> Seconds <code>timestamp(3)</code> Milliseconds <code>timestamp(6)</code> Microseconds <code>timestamp(9)</code> Nanoseconds <p>LanceDB internally stores data in Apache Arrow format. The mapping from SQL types to Arrow types is:</p> SQL type Arrow type <code>boolean</code> <code>Boolean</code> <code>tinyint</code> / <code>tinyint unsigned</code> <code>Int8</code> / <code>UInt8</code> <code>smallint</code> / <code>smallint unsigned</code> <code>Int16</code> / <code>UInt16</code> <code>int</code> or <code>integer</code> / <code>int unsigned</code> or <code>integer unsigned</code> <code>Int32</code> / <code>UInt32</code> <code>bigint</code> / <code>bigint unsigned</code> <code>Int64</code> / <code>UInt64</code> <code>float</code> <code>Float32</code> <code>double</code> <code>Float64</code> <code>decimal(precision, scale)</code> <code>Decimal128</code> <code>date</code> <code>Date32</code> <code>timestamp</code> <code>Timestamp</code> <sup>1</sup> <code>string</code> <code>Utf8</code> <code>binary</code> <code>Binary</code>"},{"location":"guides/search/filtering/#filtering-without-vector-search","title":"Filtering without Vector Search","text":"<p>You can also filter your data without search:</p> PythonTypeScript <pre><code># Synchronous client\ntbl.search().where(\"id = 10\").limit(10).to_arrow()\n# Asynchronous client\nawait async_tbl.query().where(\"id = 10\").limit(10).to_arrow()\n</code></pre> @lancedb/lancedbvectordb (deprecated) <pre><code>await tbl.query().where(\"id = 10\").limit(10).toArray();\n</code></pre> <pre><code>await tbl.filter(\"id = 10\").limit(10).execute();\n</code></pre> <p>If your table is large, this could potentially return a very large amount of data. Please be sure to use a <code>limit</code> clause unless you're sure you want to return the whole result set.</p> <ol> <li> <p>See precision mapping in previous table.\u00a0\u21a9</p> </li> </ol>"},{"location":"guides/search/full-text-search/","title":"Full-text search (Native FTS)","text":"<p>LanceDB provides support for full-text search via Lance, allowing you to incorporate keyword-based search (based on BM25) in your retrieval solutions.</p> <p>Note</p> <p>The Python SDK uses tantivy-based FTS by default, need to pass <code>use_tantivy=False</code> to use native FTS.</p>"},{"location":"guides/search/full-text-search/#example","title":"Example","text":"<p>Consider that we have a LanceDB table named <code>my_table</code>, whose string column <code>text</code> we want to index and query via keyword search, the FTS index must be created before you can search via keywords.</p> PythonTypeScriptRust Sync APIAsync API <pre><code>import lancedb\n\nfrom lancedb.index import FTS\n\nuri = \"data/sample-lancedb\"\ndb = lancedb.connect(uri)\n\ntable = db.create_table(\n    \"my_table_fts\",\n    data=[\n        {\"vector\": [3.1, 4.1], \"text\": \"Frodo was a happy puppy\"},\n        {\"vector\": [5.9, 26.5], \"text\": \"There are several kittens playing\"},\n    ],\n)\n\n# passing `use_tantivy=False` to use lance FTS index\n# `use_tantivy=True` by default\ntable.create_fts_index(\"text\", use_tantivy=False)\ntable.search(\"puppy\").limit(10).select([\"text\"]).to_list()\n# [{'text': 'Frodo was a happy puppy', '_score': 0.6931471824645996}]\n# ...\n</code></pre> <pre><code>import lancedb\n\nfrom lancedb.index import FTS\n\nuri = \"data/sample-lancedb\"\nasync_db = await lancedb.connect_async(uri)\n\nasync_tbl = await async_db.create_table(\n    \"my_table_fts_async\",\n    data=[\n        {\"vector\": [3.1, 4.1], \"text\": \"Frodo was a happy puppy\"},\n        {\"vector\": [5.9, 26.5], \"text\": \"There are several kittens playing\"},\n    ],\n)\n\n# async API uses our native FTS algorithm\nawait async_tbl.create_index(\"text\", config=FTS())\nawait (await async_tbl.search(\"puppy\")).select([\"text\"]).limit(10).to_list()\n# [{'text': 'Frodo was a happy puppy', '_score': 0.6931471824645996}]\n# ...\n</code></pre> <pre><code>import * as lancedb from \"@lancedb/lancedb\";\nconst uri = \"data/sample-lancedb\"\nconst db = await lancedb.connect(uri);\n\nconst data = [\n{ vector: [3.1, 4.1], text: \"Frodo was a happy puppy\" },\n{ vector: [5.9, 26.5], text: \"There are several kittens playing\" },\n];\nconst tbl = await db.createTable(\"my_table\", data, { mode: \"overwrite\" });\nawait tbl.createIndex(\"text\", {\n    config: lancedb.Index.fts(),\n});\n\nawait tbl\n    .search(\"puppy\", \"fts\")\n    .select([\"text\"])\n    .limit(10)\n    .toArray();\n</code></pre> <pre><code>let uri = \"data/sample-lancedb\";\nlet db = connect(uri).execute().await?;\nlet initial_data: Box&lt;dyn RecordBatchReader + Send&gt; = create_some_records()?;\nlet tbl = db\n    .create_table(\"my_table\", initial_data)\n    .execute()\n    .await?;\ntbl\n    .create_index(&amp;[\"text\"], Index::FTS(FtsIndexBuilder::default()))\n    .execute()\n    .await?;\n\ntbl\n    .query()\n    .full_text_search(FullTextSearchQuery::new(\"puppy\".to_owned()))\n    .select(lancedb::query::Select::Columns(vec![\"text\".to_owned()]))\n    .limit(10)\n    .execute()\n    .await?;\n</code></pre> <p>It would search on all indexed columns by default, so it's useful when there are multiple indexed columns.</p> <p>Passing <code>fts_columns=\"text\"</code> if you want to specify the columns to search.</p> <p>Note</p> <p>LanceDB automatically searches on the existing FTS index if the input to the search is of type <code>str</code>. If you provide a vector as input, LanceDB will search the ANN index instead.</p>"},{"location":"guides/search/full-text-search/#tokenization","title":"Tokenization","text":"<p>By default the text is tokenized by splitting on punctuation and whitespaces, and would filter out words that are with length greater than 40, and lowercase all words.</p> <p>Stemming is useful for improving search results by reducing words to their root form, e.g. \"running\" to \"run\". LanceDB supports stemming for multiple languages, you can specify the tokenizer name to enable stemming by the pattern <code>tokenizer_name=\"{language_code}_stem\"</code>, e.g. <code>en_stem</code> for English.</p> <p>For example, to enable stemming for English:</p> Sync APIAsync API <pre><code>table.create_fts_index(\"text\", tokenizer_name=\"en_stem\", replace=True)\n</code></pre> <pre><code>await async_tbl.create_index(\n    \"text\", config=FTS(language=\"English\", stem=True, remove_stop_words=True)\n</code></pre> <p>the following languages are currently supported.</p> <p>The tokenizer is customizable, you can specify how the tokenizer splits the text, and how it filters out words, etc.</p> <p>For example, for language with accents, you can specify the tokenizer to use <code>ascii_folding</code> to remove accents, e.g. '\u00e9' to 'e':</p> Sync APIAsync API <pre><code>table.create_fts_index(\n    \"text\",\n    use_tantivy=False,\n    language=\"French\",\n    stem=True,\n    ascii_folding=True,\n    replace=True,\n)\n</code></pre> <pre><code>await async_tbl.create_index(\n    \"text\", config=FTS(language=\"French\", stem=True, ascii_folding=True)\n)\n</code></pre>"},{"location":"guides/search/full-text-search/#filtering","title":"Filtering","text":"<p>LanceDB full text search supports to filter the search results by a condition, both pre-filtering and post-filtering are supported.</p> <p>This can be invoked via the familiar <code>where</code> syntax.</p> <p>With pre-filtering:</p> PythonTypeScriptRust Sync APIAsync API <pre><code>table.search(\"puppy\").limit(10).where(\"text='foo'\", prefilter=True).to_list()\n</code></pre> <pre><code>await (await async_tbl.search(\"puppy\")).limit(10).where(\"text='foo'\").to_list()\n</code></pre> <pre><code>await tbl\n.search(\"puppy\")\n.select([\"id\", \"doc\"])\n.limit(10)\n.where(\"meta='foo'\")\n.prefilter(true)\n.toArray();\n</code></pre> <pre><code>table\n    .query()\n    .full_text_search(FullTextSearchQuery::new(\"puppy\".to_owned()))\n    .select(lancedb::query::Select::Columns(vec![\"doc\".to_owned()]))\n    .limit(10)\n    .only_if(\"meta='foo'\")\n    .execute()\n    .await?;\n</code></pre> <p>With post-filtering:</p> PythonTypeScriptRust Sync APIAsync API <pre><code>table.search(\"puppy\").limit(10).where(\"text='foo'\", prefilter=False).to_list()\n</code></pre> <pre><code>await (\n    (await async_tbl.search(\"puppy\"))\n    .limit(10)\n    .where(\"text='foo'\")\n    .postfilter()\n    .to_list()\n)\n</code></pre> <pre><code>await tbl\n.search(\"apple\")\n.select([\"id\", \"doc\"])\n.limit(10)\n.where(\"meta='foo'\")\n.prefilter(false)\n.toArray();\n</code></pre> <pre><code>table\n    .query()\n    .full_text_search(FullTextSearchQuery::new(words[0].to_owned()))\n    .select(lancedb::query::Select::Columns(vec![\"doc\".to_owned()]))\n    .postfilter()\n    .limit(10)\n    .only_if(\"meta='foo'\")\n    .execute()\n    .await?;\n</code></pre>"},{"location":"guides/search/full-text-search/#phrase-queries-vs-terms-queries","title":"Phrase queries vs. terms queries","text":"<p>Warn</p> <p>Lance-based FTS doesn't support queries using boolean operators <code>OR</code>, <code>AND</code>.</p> <p>For full-text search you can specify either a phrase query like <code>\"the old man and the sea\"</code>, or a terms search query like <code>old man sea</code>. For more details on the terms query syntax, see Tantivy's query parser rules.</p> <p>To search for a phrase, the index must be created with <code>with_position=True</code>:</p> Sync APIAsync API <pre><code>table.create_fts_index(\"text\", use_tantivy=False, with_position=True, replace=True)\n</code></pre> <pre><code>await async_tbl.create_index(\"text\", config=FTS(with_position=True))\n</code></pre> <p>This will allow you to search for phrases, but it will also significantly increase the index size and indexing time.</p> <p>title: \"Full-Text Search in LanceDB | Text Search Guide\" description: \"Learn how to implement full-text search in LanceDB. Includes text indexing, query syntax, and performance optimization for text search.\"</p>"},{"location":"guides/search/full-text-search/#full-text-search-with-lancedb","title":"Full Text Search with LanceDB","text":"<p>The full-text search allows you to  incorporate keyword-based search (based on BM25) in your retrieval solutions. LanceDB can deliver 26ms query latency for full-text search.  Our benchmark tests have more details.</p> PythonTypeScript <pre><code>import lancedb\n\n# connect to LanceDB\ndb = lancedb.connect(\n  uri=\"db://your-project-slug\",\n  api_key=\"your-api-key\",\n  region=\"us-east-1\"\n)\n\n# let's use the table created from quickstart\ntable_name = \"lancedb-cloud-quickstart\"\ntable = db.open_table(table_name)\n\ntable.create_fts_index(\"text\")\n# Wait for FTS index to be ready\nfts_index_name = f\"{text_column}_idx\"\nwait_for_index(table, fts_index_name)\n\nquery_text = \"football\"\nfts_results = table.search(query_text, query_type=\"fts\").select([\"text\", \"keywords\", \"label\"]).limit(5).to_pandas()\n</code></pre> <pre><code>import * as lancedb from \"@lancedb/lancedb\"\n\nconst db = await lancedb.connect({\n  uri: \"db://your-project-slug\",\n  apiKey: \"your-api-key\",\n  region: \"us-east-1\"\n});\n\n// Open table and perform search\nconst tableName = \"lancedb-cloud-quickstart\";\nconst table = await db.openTable(tableName);\nconst textColumn = \"text\";\n\n// Create a full-text search index\nawait table.createIndex(textColumn, {\n  config: Index.fts()\n});\n\n// Wait for the index to be ready\nconst ftsIndexName = textColumn + \"_idx\";\nawait waitForIndex(table, ftsIndexName);\n\nconst queryText = \"football\";\nconst ftsResults = await table.query()\n  .fullTextSearch(queryText, {\n    columns: [textColumn]\n  })\n  .select([\"text\", \"keywords\", \"label\"])\n  .limit(5)\n  .toArray();\n</code></pre> <p>Automatic Index Updates</p> <p>Newly added or modified records become searchable immediately.  The full-text search (FTS) index updates automatically in the background,  ensuring continuous search availability without blocking queries.</p>"},{"location":"guides/search/full-text-search/#advanced-search-features","title":"Advanced Search Features","text":""},{"location":"guides/search/full-text-search/#fuzzy-search","title":"Fuzzy Search","text":"<p>Fuzzy search allows you to find matches even when the search terms contain typos or slight variations.  LanceDB uses the classic Levenshtein distance  to find similar terms within a specified edit distance.</p> Parameter Type Default Description fuzziness int 0 Maximum edit distance allowed for each term. If not specified, automatically set based on term length: 0 for length \u2264 2, 1 for length \u2264 5, 2 for length &gt; 5 max_expansions int 50 Maximum number of terms to consider for fuzzy matching. Higher values may improve recall but increase search time <p>Let's create a sample table and build full-text search indices to demonstrate  fuzzy search capabilities and relevance boosting features.</p> PythonTypeScript <pre><code>import lancedb\n\n# connect to LanceDB\ndb = lancedb.connect(\n  uri=\"db://your-project-slug\",\n  api_key=\"your-api-key\",\n  region=\"us-east-1\"\n)\n\ntable_name = \"fts-fuzzy-boosting-test\"\nvectors = [np.random.randn(128) for _ in range(100)]\ntext_nouns = (\"puppy\", \"car\")\ntext2_nouns = (\"rabbit\", \"girl\", \"monkey\")\nverbs = (\"runs\", \"hits\", \"jumps\", \"drives\", \"barfs\")\nadv = (\"crazily.\", \"dutifully.\", \"foolishly.\", \"merrily.\", \"occasionally.\")\nadj = (\"adorable\", \"clueless\", \"dirty\", \"odd\", \"stupid\")\ntext = [\n    \" \".join(\n        [\n            text_nouns[random.randrange(0, len(text_nouns))],\n            verbs[random.randrange(0, 5)],\n            adv[random.randrange(0, 5)],\n            adj[random.randrange(0, 5)],\n        ]\n    )\n    for _ in range(100)\n]\ntext2 = [\n    \" \".join(\n        [\n            text2_nouns[random.randrange(0, len(text2_nouns))],\n            verbs[random.randrange(0, 5)],\n            adv[random.randrange(0, 5)],\n            adj[random.randrange(0, 5)],\n        ]\n    )\n    for _ in range(100)\n]\ncount = [random.randint(1, 10000) for _ in range(100)]\n\ntable = db.create_table(\n    table_name,\n    data=pd.DataFrame(\n        {\n            \"vector\": vectors,\n            \"id\": [i % 2 for i in range(100)],\n            \"text\": text,\n            \"text2\": text2,\n            \"count\": count,\n        }\n    ),\n    mode=\"overwrite\"\n)\n\ntable.create_fts_index(\"text\")\nwait_for_index(table, \"text_idx\")\ntable.create_fts_index(\"text2\")\nwait_for_index(table, \"text2_idx\")\n</code></pre> <pre><code>import * as lancedb from \"@lancedb/lancedb\"\n\nconst db = await lancedb.connect({\n  uri: \"db://your-project-slug\",\n  apiKey: \"your-api-key\",\n  region: \"us-east-1\"\n});\n\nconst tableName = \"fts-fuzzy-boosting-test-ts\";\n\n// Generate sample data\nconst n = 100;\nconst vectors = Array.from({ length: n }, () =&gt; \n  Array.from({ length: 128 }, () =&gt; Math.random() * 2 - 1)\n);\n\nconst textNouns = [\"puppy\", \"car\"];\nconst text2Nouns = [\"rabbit\", \"girl\", \"monkey\"];\nconst verbs = [\"runs\", \"hits\", \"jumps\", \"drives\", \"barfs\"];\nconst adverbs = [\"crazily\", \"dutifully\", \"foolishly\", \"merrily\", \"occasionally\"];\nconst adjectives = [\"adorable\", \"clueless\", \"dirty\", \"odd\", \"stupid\"];\n\nconst generateText = (nouns: string[]) =&gt; {\n  const noun = nouns[Math.floor(Math.random() * nouns.length)];\n  const verb = verbs[Math.floor(Math.random() * verbs.length)];\n  const adv = adverbs[Math.floor(Math.random() * adverbs.length)];\n  const adj = adjectives[Math.floor(Math.random() * adjectives.length)];\n  return `${noun} ${verb} ${adv} ${adj}`;\n};\n\nconst text = Array.from({ length: n }, () =&gt; generateText(textNouns));\nconst text2 = Array.from({ length: n }, () =&gt; generateText(text2Nouns));\nconst count = Array.from({ length: n }, () =&gt; Math.floor(Math.random() * 10000) + 1);\n\n// Create table with data\nconst data = makeArrowTable(\n  vectors.map((vector, i) =&gt; ({\n    vector,\n    id: i % 2,\n    text: text[i],\n    text2: text2[i],\n    count: count[i],\n  }))\n);\n\nconst table = await db.createTable(tableName, data, { mode: \"overwrite\" });\nconsole.log(`Created table: ${tableName}`);\n\n// Create FTS indices\nconsole.log(\"Creating FTS indices...\");\nawait table.createIndex(\"text\", { config: Index.fts() });\nawait waitForIndex(table, \"text_idx\");\nawait table.createIndex(\"text2\", { config: Index.fts() });\nawait waitForIndex(table, \"text2_idx\");\n</code></pre> <p>To demonstrate fuzzy search's ability to handle typos and misspellings, let's perform  a search with a deliberately misspelled word. The search engine will attempt to match  similar terms within the specified edit distance.</p> PythonTypeScript <pre><code>from lancedb.query import MatchQuery\n\nprint(\"\\n=== Match Query Examples ===\")\n# Basic match\nprint(\"\\n1. Basic Match Query for 'crazily':\")\nbasic_match_results = (\n    table.search(MatchQuery(\"crazily\", \"text\"), query_type=\"fts\")\n    .select([\"id\", \"text\"])\n    .limit(100)\n    .to_pandas()\n)\n\n# Fuzzy match (allows typos)\nprint(\"\\n2. Fuzzy Match Query for 'crazi~1' (with typo):\")\nfuzzy_results = (\n    table.search(MatchQuery(\"crazi~1\", \"text\", fuzziness=2), query_type=\"fts\")\n    .select([\"id\", \"text\"])\n    .limit(100)\n    .to_pandas()\n)\n</code></pre> <pre><code>import { MatchQuery } from \"@lancedb/lancedb\";\n\n// Basic match\nconsole.log(\"\\n1. Basic Match Query for 'crazily':\");\nconst basicMatchResults = await table.query()\n  .fullTextSearch(new MatchQuery(\"crazily\", \"text\"))\n  .select([\"id\", \"text\"])\n  .limit(100)\n  .toArray();\nconsole.log(basicMatchResults);\n\n// Fuzzy match\nconsole.log(\"\\n2. Fuzzy Match Query for 'crazi~1' (with typo):\");\nconst fuzzyResults = await table.query()\n  .fullTextSearch(new MatchQuery(\"crazi~1\", \"text\", {\n    fuzziness: 2,\n  }))  // fuzziness of 2\n  .select([\"id\", \"text\"])\n  .limit(100)\n  .toArray();\nconsole.log(fuzzyResults);\n</code></pre>"},{"location":"guides/search/full-text-search/#phrase-match","title":"Phrase Match","text":"<p>Phrase matching enables you to search for exact sequences of words. Unlike regular text search  which matches individual terms independently, phrase matching requires words to appear in the  specified order with no intervening terms. </p> <p>Phrase matching is particularly useful for:</p> <ul> <li>Searching for specific multi-word expressions</li> <li>Matching exact titles or quotes  </li> <li>Finding precise word combinations in a specific order</li> </ul> PythonTypeScript <pre><code># Exact phrase match\nfrom lancedb.query import PhraseQuery\n\nprint(\"\\n1. Exact phrase match for 'puppy runs':\")\nphrase_results = (\n    table.search(PhraseQuery(\"puppy runs\", \"text\"), query_type=\"fts\")\n    .select([\"id\", \"text\"])\n    .limit(100)\n    .to_pandas()\n)\n</code></pre> <pre><code>import { PhraseQuery } from \"@lancedb/lancedb\";\n\n// Exact phrase match\nconsole.log(\"\\n1. Exact phrase match for 'puppy runs':\");\nconst phraseResults = await table.query()\n  .fullTextSearch(new PhraseQuery(\"puppy runs\", \"text\"))\n  .select([\"id\", \"text\"])\n  .limit(100)\n  .toArray();\n</code></pre>"},{"location":"guides/search/full-text-search/#search-with-boosting","title":"Search with Boosting","text":"<p>Boosting allows you to control the relative importance of different search terms or fields  in your queries. This feature is particularly useful when you need to:</p> <ul> <li>Prioritize matches in certain columns</li> <li>Promote specific terms while demoting others </li> <li>Fine-tune relevance scoring for better search results</li> </ul> Parameter Type Default Description positive Query required The primary query terms to match and promote in results negative Query required Terms to demote in the search results negative_boost float 0.5 Multiplier for negative matches (lower values = stronger demotion) PythonTypeScript <pre><code>from lancedb.query import MatchQuery, BoostQuery, MultiMatchQuery\n\n# Boost data with 'runs' in text more than 'puppy' in text\nprint(\"\\n2. Boosting data with 'runs' in text:\")\nboosting_results = (\n  table.search(\n      BoostQuery(\n          MatchQuery(\"runs\", \"text\"),\n          MatchQuery(\"puppy\", \"text\"),\n          negative_boost=0.2,\n      ),\n      query_type=\"fts\",\n  )\n  .select([\"id\", \"text\"])\n  .limit(100)\n  .to_pandas()\n)\n\n\"\"\"Test searching across multiple fields.\"\"\"\nprint(\"\\n=== Multi Match Query Examples ===\")\n# Search across both text and text2\nprint(\"\\n1. Searching 'crazily' in both text and text2:\")\nmulti_match_results = (\n    table.search(MultiMatchQuery(\"crazily\", [\"text\", \"text2\"]), query_type=\"fts\")\n    .select([\"id\", \"text\", \"text2\"])\n    .limit(100)\n    .to_pandas()\n)\n\n# Search with field boosting\nprint(\"\\n2. Searching with boosted text2 field:\")\nmulti_match_boosting_results = (\n    table.search(\n        MultiMatchQuery(\"crazily\", [\"text\", \"text2\"], boosts=[1.0, 2.0]),\n        query_type=\"fts\",\n    )\n    .select([\"id\", \"text\", \"text2\"])\n    .limit(100)\n    .to_pandas()\n)\n</code></pre> <pre><code>import { MatchQuery, BoostQuery, MultiMatchQuery } from \"@lancedb/lancedb\";\n\n// Boosting Example\nconsole.log(\"\\n2. Boosting data with 'runs' in text:\");\nconst boostingResults = await table.query()\n  .fullTextSearch(new BoostQuery(new MatchQuery(\"runs\", \"text\"), new MatchQuery(\"puppy\", \"text\"), {\n    negativeBoost: 0.2,\n  }))\n  .select([\"id\", \"text\"])\n  .limit(100)\n  .toArray();\n\n// Multi Match Query Examples\nconsole.log(\"\\n=== Multi Match Query Examples ===\");\n\n// Search across both text fields\nconsole.log(\"\\n1. Searching 'crazily' in both text and text2:\");\nconst multiMatchResults = await table.query()\n  .fullTextSearch(new MultiMatchQuery(\"crazily\", [\"text\", \"text2\"]))\n  .select([\"id\", \"text\", \"text2\"])\n  .limit(100)\n  .toArray();\n\n// Search with field boosting\nconsole.log(\"\\n2. Searching with boosted text2 field:\");\nconst multiMatchBoostingResults = await table.query()\n  .fullTextSearch(new MultiMatchQuery(\"crazily\", [\"text\", \"text2\"], {\n    boosts: [1.0, 2.0],\n  }))\n  .select([\"id\", \"text\", \"text2\"])\n  .limit(100)\n  .toArray();\n</code></pre> <p>Best Practices</p> <ul> <li>Use fuzzy search when handling user input that may contain typos or variations</li> <li>Apply field boosting to prioritize matches in more important columns</li> <li>Combine fuzzy search with boosting for robust and precise search results</li> </ul> <p>Recommendations for optimal FTS performance:</p> <ul> <li>Create full-text search indices on text columns that will be frequently searched</li> <li>For hybrid search combining text and vectors, see our hybrid search guide</li> <li>For performance benchmarks, check our benchmark results</li> <li>For complex queries, use SQL to combine FTS with other filter conditions</li> </ul>"},{"location":"guides/search/full-text-search/#full-text-search-on-array-fields","title":"Full-Text Search on Array Fields","text":"<p>LanceDB supports full-text search on string array columns, enabling efficient keyword-based search across multiple values within a single field (e.g., tags, keywords).</p> PythonTypeScript <pre><code>import lancedb\n\n# Connect to LanceDB\ndb = lancedb.connect(\n  uri=\"db://your-project-slug\",\n  api_key=\"your-api-key\",\n  region=\"us-east-1\"\n)\n\ntable_name = \"fts-array-field-test\"\nschema = pa.schema([\n    pa.field(\"id\", pa.string()),\n    pa.field(\"tags\", pa.list_(pa.string())),\n    pa.field(\"description\", pa.string())\n])\n\n# Generate sample data\ndata = {\n    \"id\": [f\"doc_{i}\" for i in range(10)],\n    \"tags\": [\n        [\"python\", \"machine learning\", \"data science\"],\n        [\"deep learning\", \"neural networks\", \"AI\"],\n        [\"database\", \"indexing\", \"search\"],\n        [\"vector search\", \"embeddings\", \"AI\"],\n        [\"full text search\", \"indexing\", \"database\"],\n        [\"python\", \"web development\", \"flask\"],\n        [\"machine learning\", \"deep learning\", \"pytorch\"],\n        [\"database\", \"SQL\", \"postgresql\"],\n        [\"search engine\", \"elasticsearch\", \"indexing\"],\n        [\"AI\", \"transformers\", \"NLP\"]\n    ],\n    \"description\": [\n        \"Python for data science projects\",\n        \"Deep learning fundamentals\",\n        \"Database indexing techniques\",\n        \"Vector search implementations\",\n        \"Full-text search guide\",\n        \"Web development with Python\",\n        \"Machine learning with PyTorch\",\n        \"Database management systems\",\n        \"Search engine optimization\",\n        \"AI and NLP applications\"\n    ]\n}\n\n# Create table and add data\ntable = db.create_table(table_name, schema=schema, mode=\"overwrite\")\ntable_data = pa.Table.from_pydict(data, schema=schema)\ntable.add(table_data)\n\n# Create FTS index\ntable.create_fts_index(\"tags\")\nwait_for_index(table, \"tags_idx\")\n\n# Search examples\nprint(\"\\nSearching for 'learning' in tags with a typo:\")\nresult = table.search(MatchQuery(\"learnin\", column=\"tags\", fuzziness=1), query_type=\"fts\").select(['id', 'tags', 'description']).to_arrow()\n\nprint(\"\\nSearching for 'machine learning' in tags:\")\nresult = table.search(PhraseQuery(\"machine learning\", column=\"tags\"), query_type=\"fts\").select(['id', 'tags', 'description']).to_arrow()\n</code></pre> <pre><code>import * as lancedb from \"@lancedb/lancedb\"\n\nconst db = await lancedb.connect({\n  uri: \"db://your-project-slug\",\n  apiKey: \"your-api-key\",\n  region: \"us-east-1\"\n});\n\nconst tableName = \"fts-array-field-test-ts\";\n\n// Create schema\nconst schema = new Schema([\n  new Field(\"id\", new Utf8(), false),\n  new Field(\"tags\", new List(new Field(\"item\", new Utf8()))),\n  new Field(\"description\", new Utf8(), false)\n]);\n\n// Generate sample data\nconst data = makeArrowTable(\n  Array(10).fill(0).map((_, i) =&gt; ({\n    id: `doc_${i}`,\n    tags: [\n      [\"python\", \"machine learning\", \"data science\"],\n      [\"deep learning\", \"neural networks\", \"AI\"],\n      [\"database\", \"indexing\", \"search\"],\n      [\"vector search\", \"embeddings\", \"AI\"],\n      [\"full text search\", \"indexing\", \"database\"],\n      [\"python\", \"web development\", \"flask\"],\n      [\"machine learning\", \"deep learning\", \"pytorch\"],\n      [\"database\", \"SQL\", \"postgresql\"],\n      [\"search engine\", \"elasticsearch\", \"indexing\"],\n      [\"AI\", \"transformers\", \"NLP\"]\n    ][i],\n    description: [\n      \"Python for data science projects\",\n      \"Deep learning fundamentals\",\n      \"Database indexing techniques\",\n      \"Vector search implementations\",\n      \"Full-text search guide\",\n      \"Web development with Python\",\n      \"Machine learning with PyTorch\",\n      \"Database management systems\",\n      \"Search engine optimization\",\n      \"AI and NLP applications\"\n    ][i]\n  })),\n  { schema }\n);\n\n// Create table\nconst table = await db.createTable(tableName, data, { mode: \"overwrite\" });\nconsole.log(`Created table: ${tableName}`);\n\n// Create FTS index\nconsole.log(\"Creating FTS index on 'tags' column...\");\nawait table.createIndex(\"tags\", {\n  config: Index.fts()\n});\n\n// Wait for index\nconst ftsIndexName = \"tags_idx\";\nawait waitForIndex(table, ftsIndexName);\n\n// Search examples\nconsole.log(\"\\nSearching for 'learning' in tags with a typo:\");\nconst fuzzyResults = await table.query()\n  .fullTextSearch(new MatchQuery(\"learnin\", \"tags\", {\n    fuzziness: 2,\n  }))\n  .select([\"id\", \"tags\", \"description\"])\n  .toArray();\nconsole.log(fuzzyResults);\n\nconsole.log(\"\\nSearching for 'machine learning' in tags:\");\nconst phraseResults = await table.query()\n  .fullTextSearch(new PhraseQuery(\"machine learning\", \"tags\"))\n  .select([\"id\", \"tags\", \"description\"])\n  .toArray();\nconsole.log(phraseResults);\n</code></pre>"},{"location":"guides/search/full-text-search/#full-text-search-tantivy-based-fts","title":"Full-text search (Tantivy-based FTS)","text":"<p>LanceDB also provides support for full-text search via Tantivy, allowing you to incorporate keyword-based search (based on BM25) in your retrieval solutions.</p> <p>The tantivy-based FTS is only available in Python synchronous APIs and does not support building indexes on object storage or incremental indexing. If you need these features, try native FTS native FTS.</p>"},{"location":"guides/search/full-text-search/#installation","title":"Installation","text":"<p>To use full-text search, install the dependency <code>tantivy-py</code>:</p> <pre><code># Say you want to use tantivy==0.20.1\npip install tantivy==0.20.1\n</code></pre>"},{"location":"guides/search/full-text-search/#example_1","title":"Example","text":"<p>Consider that we have a LanceDB table named <code>my_table</code>, whose string column <code>content</code> we want to index and query via keyword search, the FTS index must be created before you can search via keywords.</p> <pre><code>import lancedb\n\nuri = \"data/sample-lancedb\"\ndb = lancedb.connect(uri)\n\ntable = db.create_table(\n    \"my_table\",\n    data=[\n        {\"id\": 1, \"vector\": [3.1, 4.1], \"title\": \"happy puppy\", \"content\": \"Frodo was a happy puppy\", \"meta\": \"foo\"},\n        {\"id\": 2, \"vector\": [5.9, 26.5], \"title\": \"playing kittens\", \"content\": \"There are several kittens playing around the puppy\", \"meta\": \"bar\"},\n    ],\n)\n\n# passing `use_tantivy=False` to use lance FTS index\n# `use_tantivy=True` by default\ntable.create_fts_index(\"content\", use_tantivy=True)\ntable.search(\"puppy\").limit(10).select([\"content\"]).to_list()\n# [{'text': 'Frodo was a happy puppy', '_score': 0.6931471824645996}]\n# ...\n</code></pre> <p>It would search on all indexed columns by default, so it's useful when there are multiple indexed columns.</p> <p>Note</p> <p>LanceDB automatically searches on the existing FTS index if the input to the search is of type <code>str</code>. If you provide a vector as input, LanceDB will search the ANN index instead.</p>"},{"location":"guides/search/full-text-search/#tokenization_1","title":"Tokenization","text":"<p>By default the text is tokenized by splitting on punctuation and whitespaces and then removing tokens that are longer than 40 chars. For more language specific tokenization then provide the argument tokenizer_name with the 2 letter language code followed by \"_stem\". So for english it would be \"en_stem\".</p> <pre><code>table.create_fts_index(\"content\", use_tantivy=True, tokenizer_name=\"en_stem\", replace=True)\n</code></pre> <p>the following languages are currently supported.</p>"},{"location":"guides/search/full-text-search/#index-multiple-columns","title":"Index multiple columns","text":"<p>If you have multiple string columns to index, there's no need to combine them manually -- simply pass them all as a list to <code>create_fts_index</code>:</p> <pre><code>table.create_fts_index([\"title\", \"content\"], use_tantivy=True, replace=True)\n</code></pre> <p>Note that the search API call does not change - you can search over all indexed columns at once.</p>"},{"location":"guides/search/full-text-search/#filtering_1","title":"Filtering","text":"<p>Currently the LanceDB full text search feature supports post-filtering, meaning filters are applied on top of the full text search results (see native FTS if you need pre-filtering). This can be invoked via the familiar <code>where</code> syntax:</p> <pre><code>table.search(\"puppy\").limit(10).where(\"meta='foo'\").to_list()\n</code></pre>"},{"location":"guides/search/full-text-search/#sorting","title":"Sorting","text":"<p>You can pre-sort the documents by specifying <code>ordering_field_names</code> when creating the full-text search index. Once pre-sorted, you can then specify <code>ordering_field_name</code> while searching to return results sorted by the given field. For example,</p> <pre><code>table.create_fts_index([\"content\"], use_tantivy=True, ordering_field_names=[\"id\"], replace=True)\n\n(table.search(\"puppy\", ordering_field_name=\"id\")\n .limit(20)\n .to_list())\n</code></pre> <p>Note</p> <p>If you wish to specify an ordering field at query time, you must also have specified it during indexing time. Otherwise at query time, an error will be raised that looks like <code>ValueError: The field does not exist: xxx</code></p> <p>Note</p> <p>The fields to sort on must be of typed unsigned integer, or else you will see an error during indexing that looks like <code>TypeError: argument 'value': 'float' object cannot be interpreted as an integer</code>.</p> <p>Note</p> <p>You can specify multiple fields for ordering at indexing time. But at query time only one ordering field is supported.</p>"},{"location":"guides/search/full-text-search/#phrase-queries-vs-terms-queries_1","title":"Phrase queries vs. terms queries","text":"<p>For full-text search you can specify either a phrase query like <code>\"the old man and the sea\"</code>, or a terms search query like <code>\"(Old AND Man) AND Sea\"</code>. For more details on the terms query syntax, see Tantivy's query parser rules.</p> <p>Note</p> <p>The query parser will raise an exception on queries that are ambiguous. For example, in the query <code>they could have been dogs OR cats</code>, <code>OR</code> is capitalized so it's considered a keyword query operator. But it's ambiguous how the left part should be treated. So if you submit this search query as is, you'll get <code>Syntax Error: they could have been dogs OR cats</code>.</p> <pre><code># This raises a syntax error\ntable.search(\"they could have been dogs OR cats\")\n</code></pre> <p>On the other hand, lowercasing <code>OR</code> to <code>or</code> will work, because there are no capitalized logical operators and the query is treated as a phrase query.</p> <pre><code># This works!\ntable.search(\"they could have been dogs or cats\")\n</code></pre> <p>It can be cumbersome to have to remember what will cause a syntax error depending on the type of query you want to perform. To make this simpler, when you want to perform a phrase query, you can enforce it in one of two ways:</p> <ol> <li>Place the double-quoted query inside single quotes. For example, <code>table.search('\"they could have been dogs OR cats\"')</code> is treated as a phrase query.</li> <li>Explicitly declare the <code>phrase_query()</code> method. This is useful when you have a phrase query that itself contains double quotes. For example, <code>table.search('the cats OR dogs were not really \"pets\" at all').phrase_query()</code> is treated as a phrase query.</li> </ol> <p>In general, a query that's declared as a phrase query will be wrapped in double quotes during parsing, with nested double quotes replaced by single quotes.</p>"},{"location":"guides/search/full-text-search/#configurations","title":"Configurations","text":"<p>By default, LanceDB configures a 1GB heap size limit for creating the index. You can reduce this if running on a smaller node, or increase this for faster performance while indexing a larger corpus.</p> <pre><code># configure a 512MB heap size\nheap = 1024 * 1024 * 512\ntable.create_fts_index([\"title\", \"content\"], use_tantivy=True, writer_heap_size=heap, replace=True)\n</code></pre>"},{"location":"guides/search/full-text-search/#current-limitations","title":"Current limitations","text":"<ol> <li> <p>New data added after creating the FTS index will appear in search results, but with increased latency due to a flat search on the unindexed portion. Re-indexing with <code>create_fts_index</code> will reduce latency. LanceDB Cloud automates this merging process, minimizing the impact on search speed. </p> </li> <li> <p>We currently only support local filesystem paths for the FTS index.    This is a tantivy limitation. We've implemented an object store plugin    but there's no way in tantivy-py to specify to use it.</p> </li> </ol>"},{"location":"guides/search/hybrid-search/","title":"Hybrid Search with LanceDB","text":"<p>We support hybrid search that combines semantic and full-text search via a  reranking algorithm of your choice, to get the best of both worlds. LanceDB  comes with built-in rerankers  and you can implement you own customized reranker as well. </p> <p>Explore the complete hybrid search example in our guided walkthroughs:  - Python notebook  - TypeScript example</p> PythonTypeScript <pre><code>import os\n\nimport lancedb\nimport openai\nfrom lancedb.embeddings import get_registry\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.rerankers import RRFReranker\n\n# connect to LanceDB\ndb = lancedb.connect(\n  uri=\"db://your-project-slug\",\n  api_key=\"your-api-key\",\n  region=\"us-east-1\"\n)\n\n# Configuring the environment variable OPENAI_API_KEY\nif \"OPENAI_API_KEY\" not in os.environ:\n    # OR set the key here as a variable\n    openai.api_key = \"sk-...\"\nembeddings = get_registry().get(\"openai\").create()\n\n# Define schema for documents with embeddings\nclass Documents(LanceModel):\n    text: str = embeddings.SourceField()\n    vector: Vector(embeddings.ndims()) = embeddings.VectorField()\n\n# Create a table with the defined schema\ntable_name = \"hybrid_search_example\"\ntable = db.create_table(table_name, schema=Documents, mode=\"overwrite\")\n\n# Add sample data\ndata = [\n    {\"text\": \"rebel spaceships striking from a hidden base\"},\n    {\"text\": \"have won their first victory against the evil Galactic Empire\"},\n    {\"text\": \"during the battle rebel spies managed to steal secret plans\"},\n    {\"text\": \"to the Empire's ultimate weapon the Death Star\"},\n]\ntable.add(data=data)\n\ntable.create_fts_index(\"text\")\n\n# Wait for indexes to be ready\nwait_for_index(table, \"text_idx\")\n\n# Create a reranker for hybrid search\nreranker = RRFReranker()\n\n# Perform hybrid search with reranking\nresults = (\n    table.search(\n        \"flower moon\",\n        query_type=\"hybrid\",\n        vector_column_name=\"vector\",\n        fts_columns=\"text\",\n    )\n    .rerank(reranker)\n    .limit(10)\n    .to_pandas()\n)\n\nprint(\"Hybrid search results:\")\nprint(results)\n</code></pre> <p>```typescript import * as lancedb from \"@lancedb/lancedb\"; import \"@lancedb/lancedb/embedding/openai\"; import { Utf8 } from \"apache-arrow\";</p> <p>if (!process.env.OPENAI_API_KEY) {   console.log(\"Skipping hybrid search - OPENAI_API_KEY not set\");   return { success: true, message: \"Skipped: OPENAI_API_KEY not set\" }; } const db = await lancedb.connect({   uri: \"db://your-project-slug\",   apiKey: \"your-api-key\",   region: \"us-east-1\", });</p> <p>const embedFunc = lancedb.embedding.getRegistry().get(\"openai\")?.create({   model: \"text-embedding-ada-002\", }) as lancedb.embedding.EmbeddingFunction;</p> <p>// Define schema for documents with embeddings const documentSchema = lancedb.embedding.LanceSchema({   text: embedFunc.sourceField(new Utf8()),   vector: embedFunc.vectorField(), });</p> <p>// Create a table with the defined schema const tableName = \"hybrid_search_example\"; const table = await db.createEmptyTable(tableName, documentSchema, {   mode: \"overwrite\", });</p> <p>// Add sample data const data = [   { text: \"rebel spaceships striking from a hidden base\" },   { text: \"have won their first victory against the evil Galactic Empire\" },   { text: \"during the battle rebel spies managed to steal secret plans\" },   { text: \"to the Empire's ultimate weapon the Death Star\" }, ]; await table.add(data); console.log(<code>Created table: ${tableName} with ${data.length} rows</code>);</p> <p>// Create full-text search index console.log(\"Creating full-text search index...\"); await table.createIndex(\"text\", {   config: lancedb.Index.fts(), });</p> <p>// Wait for the index to be ready await waitForIndex(table as any, \"text_idx\");</p> <p>// Perform hybrid search console.log(\"Performing hybrid search...\"); const queryVector =   await embedFunc.computeQueryEmbeddings(\"full moon in May\"); const hybridResults = await table   .query()   .fullTextSearch(\"flower moon\")   .nearestTo(queryVector)   .rerank(await lancedb.rerankers.RRFReranker.create())   .select([\"text\"])   .limit(10)   .toArray();</p> <p>console.log(\"Hybrid search results:\"); console.log(hybridResults);</p> <p>title: Hybrid Search in LanceDB | Vector &amp; Keyword Search Guide description: Learn how to implement hybrid search in LanceDB by combining vector and keyword-based search. Includes examples for reranking, score normalization, and best practices for search optimization.</p>"},{"location":"guides/search/hybrid-search/#hybrid-search","title":"Hybrid Search","text":"<p>LanceDB supports both semantic and keyword-based search (also termed full-text search, or FTS). In real world applications, it is often useful to combine these two approaches to get the best best results. For example, you may want to search for a document that is semantically similar to a query document, but also contains a specific keyword. This is an example of hybrid search, a search algorithm that combines multiple search techniques.</p>"},{"location":"guides/search/hybrid-search/#hybrid-search-in-lancedb","title":"Hybrid search in LanceDB","text":"<p>You can perform hybrid search in LanceDB by combining the results of semantic and full-text search via a reranking algorithm of your choice. LanceDB provides multiple rerankers out of the box. However, you can always write a custom reranker if your use case need more sophisticated logic .</p> Sync APIAsync API <pre><code>import os\n\nimport openai\n\nimport lancedb\n\nfrom lancedb.embeddings import get_registry\n\n\nfrom lancedb.index import FTS\n\n\nclass Documents(LanceModel):\n    vector: Vector(embeddings.ndims()) = embeddings.VectorField()\n    text: str = embeddings.SourceField()\n\ndata = [\n    {\"text\": \"rebel spaceships striking from a hidden base\"},\n    {\"text\": \"have won their first victory against the evil Galactic Empire\"},\n    {\"text\": \"during the battle rebel spies managed to steal secret plans\"},\n    {\"text\": \"to the Empire's ultimate weapon the Death Star\"},\n]\nuri = \"data/sample-lancedb\"\ndb = lancedb.connect(uri)\ntable = db.create_table(\"documents\", schema=Documents)\n# ingest docs with auto-vectorization\ntable.add(data)\n# Create a fts index before the hybrid search\ntable.create_fts_index(\"text\")\n# hybrid search with default re-ranker\ntable.search(\"flower moon\", query_type=\"hybrid\").to_pandas()\n</code></pre> <pre><code>import os\n\nimport openai\n\nimport lancedb\n\nfrom lancedb.embeddings import get_registry\n\n\nfrom lancedb.index import FTS\n\n\nclass Documents(LanceModel):\n    vector: Vector(embeddings.ndims()) = embeddings.VectorField()\n    text: str = embeddings.SourceField()\n\nuri = \"data/sample-lancedb\"\nasync_db = await lancedb.connect_async(uri)\ndata = [\n    {\"text\": \"rebel spaceships striking from a hidden base\"},\n    {\"text\": \"have won their first victory against the evil Galactic Empire\"},\n    {\"text\": \"during the battle rebel spies managed to steal secret plans\"},\n    {\"text\": \"to the Empire's ultimate weapon the Death Star\"},\n]\nasync_tbl = await async_db.create_table(\"documents_async\", schema=Documents)\n# ingest docs with auto-vectorization\nawait async_tbl.add(data)\n# Create a fts index before the hybrid search\nawait async_tbl.create_index(\"text\", config=FTS())\ntext_query = \"flower moon\"\n# hybrid search with default re-ranker\nawait (await async_tbl.search(\"flower moon\", query_type=\"hybrid\")).to_pandas()\n</code></pre> <p>Note</p> <p>You can also pass the vector and text query manually. This is useful if you're not using the embedding API or if you're using a separate embedder service.</p>"},{"location":"guides/search/hybrid-search/#explicitly-passing-the-vector-and-text-query","title":"Explicitly passing the vector and text query","text":"Sync APIAsync API <pre><code>vector_query = [0.1, 0.2, 0.3, 0.4, 0.5]\ntext_query = \"flower moon\"\n(\n    table.search(query_type=\"hybrid\")\n    .vector(vector_query)\n    .text(text_query)\n    .limit(5)\n    .to_pandas()\n)\n</code></pre> <pre><code>vector_query = [0.1, 0.2, 0.3, 0.4, 0.5]\ntext_query = \"flower moon\"\nawait (\n    async_tbl.query()\n    .nearest_to(vector_query)\n    .nearest_to_text(text_query)\n    .limit(5)\n    .to_pandas()\n)\n</code></pre> <p>By default, LanceDB uses <code>RRFReranker()</code>, which uses reciprocal rank fusion score, to combine and rerank the results of semantic and full-text search. You can customize the hyperparameters as needed or write your own custom reranker. Here's how you can use any of the available rerankers:</p>"},{"location":"guides/search/hybrid-search/#rerank-arguments","title":"<code>rerank()</code> arguments","text":"<ul> <li><code>normalize</code>: <code>str</code>, default <code>\"score\"</code>:     The method to normalize the scores. Can be \"rank\" or \"score\". If \"rank\", the scores are converted to ranks and then normalized. If \"score\", the scores are normalized directly.</li> <li><code>reranker</code>: <code>Reranker</code>, default <code>RRF()</code>.     The reranker to use. If not specified, the default reranker is used.</li> </ul>"},{"location":"guides/search/hybrid-search/#available-rerankers","title":"Available Rerankers","text":"<p>LanceDB provides a number of rerankers out of the box. You can use any of these rerankers by passing them to the <code>rerank()</code> method.  Go to Rerankers to learn more about using the available rerankers and implementing custom rerankers.</p>"},{"location":"guides/search/multivector-search/","title":"Multivector Search with LanceDB Enterprise","text":"<p>A complete example for Multivector search is in this notebook</p>"},{"location":"guides/search/multivector-search/#multivector-type","title":"Multivector Type","text":"<p>LanceDB natively supports multivector data types, enabling advanced search scenarios where  a single data item is represented by multiple embeddings (e.g., using models like ColBERT  or CoLPali). In this framework, documents and queries are encoded as collections of  contextualized vectors\u2014precomputed for documents and indexed for queries.</p> <p>Key features: - Indexing on multivector column: store and index multiple vectors per row. - Supporint query being a single vector or multiple vectors - Optimized search performance with XTR with improved recall.</p> <p>Multivector Search Limitations</p> <p>Currently, only the <code>cosine</code> metric is supported for multivector search.  The vector value type can be <code>float16</code>, <code>float32</code>, or <code>float64</code>.</p>"},{"location":"guides/search/multivector-search/#using-multivector-in-python","title":"Using Multivector in Python","text":"<p>Currently, multivector search is only support in our Python SDK.  Below is an example of using multivector search in LanceDB:</p> <p><pre><code>import lancedb\nimport numpy as np\nimport pyarrow as pa\n\n# Connect to LanceDB Cloud\ndb = lancedb.connect(\n  uri=\"db://your-project-slug\",\n  api_key=\"your-api-key\",\n  region=\"your-cloud-region\"\n)\n\n# Define schema with multivector field\nschema = pa.schema(\n    [\n        pa.field(\"id\", pa.int64()),\n        # float16, float32, and float64 are supported\n        pa.field(\"vector\", pa.list_(pa.list_(pa.float32(), 256))),\n    ]\n)\n\n# Create data with multiple vectors per document\ndata = [\n    {\n        \"id\": i,\n        \"vector\": np.random.random(size=(2, 256)).tolist(),  # Each document has 2 vectors\n    }\n    for i in range(1024)\n]\n\n# Create table\ntbl = db.create_table(\"multivector_example\", data=data, schema=schema)\n\n# Create index - only cosine similarity is supported for multi-vectors\ntbl.create_index(metric=\"cosine\", vector_column_name=\"vector\")\n\n# Query with a single vector\nquery = np.random.random(256)\nresults_single = tbl.search(query).limit(5).to_pandas()\n\n# Query with multiple vectors\nquery_multi = np.random.random(size=(2, 256))\nresults_multi = tbl.search(query_multi).limit(5).to_pandas()\n\n\n---\ntitle: Multivector Search in LanceDB | Advanced Vector Operations\ndescription: Learn how to use LanceDB's multi-vector search capabilities for complex vector operations. Includes support for multiple vectors per item, late interaction techniques, and advanced search strategies.\n---\n\n# Multivector Search in LanceDB\n\nLate interaction is a technique used in retrieval that calculates the relevance of a query to a document by comparing their multi-vector representations. The key difference between late interaction and other popular methods:\n\n![late interaction vs other methods](https://raw.githubusercontent.com/lancedb/assets/b035a0ceb2c237734e0d393054c146d289792339/docs/assets/integration/colbert-blog-interaction.svg)\n\n\n[ Illustration from https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/]\n\n&lt;b&gt;No interaction:&lt;/b&gt; Refers to independently embedding the query and document, that are compared to calcualte similarity without any interaction between them. This is typically used in vector search operations.\n\n&lt;b&gt;Partial interaction&lt;/b&gt; Refers to a specific approach where the similarity computation happens primarily between query vectors and document vectors, without extensive interaction between individual components of each. An example of this is dual-encoder models like BERT.\n\n&lt;b&gt;Early full interaction&lt;/b&gt; Refers to techniques like cross-encoders that process query and docs in pairs with full interaction across various stages of encoding. This is a powerful, but relatively slower technique. Because it requires processing query and docs in pairs, doc embeddings can't be pre-computed for fast retrieval. This is why cross encoders are typically used as reranking models combined with vector search. Learn more about [LanceDB Reranking support](https://lancedb.github.io/lancedb/reranking/).\n\n&lt;b&gt;Late interaction&lt;/b&gt; Late interaction is a technique that calculates the doc and query similarity independently and then the interaction or evaluation happens during the retrieval process. This is typically used in retrieval models like ColBERT. Unlike early interaction, It allows speeding up the retrieval process without compromising the depth of semantic analysis.\n\n## Internals of ColBERT \nLet's take a look at the steps involved in performing late interaction based retrieval using ColBERT:\n\n\u2022 ColBERT employs BERT-based encoders for both queries `(fQ)` and documents `(fD)`\n\u2022 A single BERT model is shared between query and document encoders and special tokens distinguish input types: `[Q]` for queries and `[D]` for documents\n\n**Query Encoder (fQ):**\n\u2022 Query q is tokenized into WordPiece tokens: `q1, q2, ..., ql`. `[Q]` token is prepended right after BERT's `[CLS]` token\n\u2022 If query length &lt; Nq, it's padded with [MASK] tokens up to Nq.\n\u2022 The padded sequence goes through BERT's transformer architecture\n\u2022 Final embeddings are L2-normalized.\n\n**Document Encoder (fD):**\n\u2022 Document d is tokenized into tokens `d1, d2, ..., dm`. `[D]` token is prepended after `[CLS]` token\n\u2022 Unlike queries, documents are NOT padded with `[MASK]` tokens\n\u2022 Document tokens are processed through BERT and the same linear layer\n\n**Late Interaction:**\n\u2022 Late interaction estimates relevance score `S(q,d)` using embedding `Eq` and `Ed`. Late interaction happens after independent encoding\n\u2022 For each query embedding, maximum similarity is computed against all document embeddings\n\u2022 The similarity measure can be cosine similarity or squared L2 distance\n\n**MaxSim Calculation:**\n</code></pre> S(q,d) := \u03a3 max(Eqi\u22c5EdjT)           i\u2208|Eq| j\u2208|Ed| <pre><code>\u2022 This finds the best matching document embedding for each query embedding\n\u2022 Captures relevance based on strongest local matches between contextual embeddings\n\n## LanceDB Multi Vector Type\n\nLanceDB supports multivector type, this is useful when you have multiple vectors for a single item (e.g. with ColBert and ColPali).\n\nYou can index on a column with multivector type and search on it, the query can be single vector or multiple vectors. For now, only cosine metric is supported for multivector search. The vector value type can be float16, float32 or float64. LanceDB integrateds [ConteXtualized Token Retriever(XTR)](https://arxiv.org/abs/2304.01982), which introduces a simple, yet novel, objective function that encourages the model to retrieve the most important document tokens first. \n\n```python\nimport lancedb\nimport numpy as np\nimport pyarrow as pa\n\ndb = lancedb.connect(\"data/multivector_demo\")\nschema = pa.schema(\n    [\n        pa.field(\"id\", pa.int64()),\n        # float16, float32, and float64 are supported\n        pa.field(\"vector\", pa.list_(pa.list_(pa.float32(), 256))),\n    ]\n)\ndata = [\n    {\n        \"id\": i,\n        \"vector\": np.random.random(size=(2, 256)).tolist(),\n    }\n    for i in range(1024)\n]\ntbl = db.create_table(\"my_table\", data=data, schema=schema)\n\n# only cosine similarity is supported for multi-vectors\ntbl.create_index(metric=\"cosine\")\n\n# query with single vector\nquery = np.random.random(256).astype(np.float16)\ntbl.search(query).to_arrow()\n\n# query with multiple vectors\nquery = np.random.random(size=(2, 256))\ntbl.search(query).to_arrow()\n</code></pre> Find more about vector search in LanceDB here.</p>"},{"location":"guides/search/multivector-search/#multivector-type_1","title":"Multivector type","text":"<p>LanceDB supports multivector type, this is useful when you have multiple vectors for a single item (e.g. with ColBert and ColPali).</p> <p>You can index on a column with multivector type and search on it, the query can be single vector or multiple vectors. If the query is multiple vectors <code>mq</code>, the similarity (distance) from it to any multivector <code>mv</code> in the dataset, is defined as:</p> <p></p> <p>where <code>sim</code> is the similarity function (e.g. cosine).</p> <p>For now, only <code>cosine</code> metric is supported for multivector search. The vector value type can be <code>float16</code>, <code>float32</code> or <code>float64</code>.</p> Python Sync APIAsync API <pre><code>import lancedb\nimport numpy as np\nimport pyarrow as pa\n\ndb = lancedb.connect(\"data/multivector_demo\")\nschema = pa.schema(\n    [\n        pa.field(\"id\", pa.int64()),\n        # float16, float32, and float64 are supported\n        pa.field(\"vector\", pa.list_(pa.list_(pa.float32(), 256))),\n    ]\n)\ndata = [\n    {\n        \"id\": i,\n        \"vector\": np.random.random(size=(2, 256)).tolist(),\n    }\n    for i in range(1024)\n]\ntbl = db.create_table(\"my_table\", data=data, schema=schema)\n\n# only cosine similarity is supported for multi-vectors\ntbl.create_index(metric=\"cosine\")\n\n# query with single vector\nquery = np.random.random(256).astype(np.float16)\ntbl.search(query).to_arrow()\n\n# query with multiple vectors\nquery = np.random.random(size=(2, 256))\ntbl.search(query).to_arrow()\n</code></pre> <pre><code>import lancedb\nimport numpy as np\nimport pyarrow as pa\n\ndb = await lancedb.connect_async(\"data/multivector_demo\")\nschema = pa.schema(\n    [\n        pa.field(\"id\", pa.int64()),\n        # float16, float32, and float64 are supported\n        pa.field(\"vector\", pa.list_(pa.list_(pa.float32(), 256))),\n    ]\n)\ndata = [\n    {\n        \"id\": i,\n        \"vector\": np.random.random(size=(2, 256)).tolist(),\n    }\n    for i in range(1024)\n]\ntbl = await db.create_table(\"my_table\", data=data, schema=schema)\n\n# only cosine similarity is supported for multi-vectors\nawait tbl.create_index(column=\"vector\", config=IvfPq(distance_type=\"cosine\"))\n\n# query with single vector\nquery = np.random.random(256)\nawait tbl.query().nearest_to(query).to_arrow()\n\n# query with multiple vectors\nquery = np.random.random(size=(2, 256))\nawait tbl.query().nearest_to(query).to_arrow()\n</code></pre>"},{"location":"guides/search/optimize-queries/","title":"Query Optimization with LanceDB","text":"<p>LanceDB provides two powerful tools for query analysis and optimization:</p> <ul> <li><code>explain_plan</code>: Reveals the logical query plan before execution,  helping you identify potential issues with query structure and index usage.  This tool is useful for:</li> <li>Verifying query optimization strategies</li> <li>Validating index selection</li> <li>Understanding query execution order</li> <li> <p>Detecting missing indices</p> </li> <li> <p><code>analyze_plan</code>: Executes the query and provides detailed runtime metrics,  including:</p> </li> <li>Operation duration (elapsed_compute)</li> <li>Data processing statistics (output_rows, bytes_read)</li> <li>Index effectiveness (index_comparisons, indices_loaded)</li> <li>Resource utilization (iops, requests)</li> </ul> <p>Together, these tools offer a comprehensive view of query performance,  from planning to execution. Use <code>explain_plan</code> to verify your query  structure and <code>analyze_plan</code> to measure and optimize actual performance.</p>"},{"location":"guides/search/optimize-queries/#reading-the-execution-plan","title":"Reading the Execution Plan","text":"<p>To demonstrate query performance analysis, we'll use a table containing 1.2M rows sampled from the  Wikipedia dataset. Initially,  the table has no indices, allowing us to observe the impact of optimization.</p> <p>Let's examine a vector search query that: - Filters rows where <code>identifier</code> is between 0 and 1,000,000 - Returns the top 100 matches - Projects specific columns: <code>chunk_index</code>, <code>title</code>, and <code>identifier</code></p> PythonTypeScript <pre><code># explain_plan\nquery_explain_plan = (\n    table.search(query_embed)\n    .where(\"identifier &gt; 0 AND identifier &lt; 1000000\")\n    .select([\"chunk_index\", \"title\", \"identifier\"])\n    .limit(100)\n    .explain_plan(True)\n)\n</code></pre> <pre><code>// explain_plan\nconst explainPlan = await table\n    .search(queryEmbed)\n    .where(\"identifier &gt; 0 AND identifier &lt; 1000000\")\n    .select([\"chunk_index\", \"title\", \"identifier\"])\n    .limit(100)\n    .explainPlan(true);\n</code></pre> <p>The execution plan reveals the sequence of operations performed to execute your query.  Let's examine each component of the plan: <pre><code>ProjectionExec: expr=[chunk_index@4 as chunk_index, title@5 as title, identifier@1 as identifier, _distance@3 as _distance]\n  RemoteTake: columns=\"vector, identifier, _rowid, _distance, chunk_index, title\"\n    CoalesceBatchesExec: target_batch_size=1024\n      GlobalLimitExec: skip=0, fetch=100\n        FilterExec: _distance@3 IS NOT NULL\n          SortExec: TopK(fetch=100), expr=[_distance@3 ASC NULLS LAST], preserve_partitioning=[false]\n            KNNVectorDistance: metric=l2\n              FilterExec: identifier@1 &gt; 0 AND identifier@1 &lt; 1000000\n                LanceScan: uri=***, projection=[vector, identifier], row_id=true, row_addr=false, ordered=false\n</code></pre></p> <ol> <li>Base Layer (LanceScan)</li> <li>Initial data scan loading only specified columns to minimize I/O</li> <li> <p>Unordered scan enabling parallel processing    <pre><code>LanceScan: \n- projection=[vector, identifier]\n- row_id=true, row_addr=false, ordered=false\n</code></pre></p> </li> <li> <p>First Filter</p> </li> <li> <p>Apply requested filter on <code>identifier</code> column (will reduce the number of vectors that need    KNN computation)    <pre><code>FilterExec: identifier@1 &gt; 0 AND identifier@1 &lt; 1000000   \n</code></pre></p> </li> <li> <p>Vector Search</p> </li> <li> <p>Computes L2 (Euclidean) distances between query vector and all vectors that     passed the filter    <pre><code>KNNVectorDistance: metric=l2\n</code></pre></p> </li> <li> <p>Results Processing</p> </li> <li>Filters out null distance results, which occur when vectors are NULL or when using cosine     distance metric with zero vectors.</li> <li>Sorts by distance and takes top 100 results</li> <li> <p>Processes in batches of 1024 for optimal memory usage    <pre><code>SortExec: TopK(fetch=100)\n- expr=[_distance@3 ASC NULLS LAST]\n- preserve_partitioning=[false]\nFilterExec: _distance@3 IS NOT NULL\nGlobalLimitExec: skip=0, fetch=100\nCoalesceBatchesExec: target_batch_size=1024\n</code></pre></p> </li> <li> <p>Data Retrieval</p> </li> <li><code>RemoteTake</code> is a key component of Lance's I/O cache</li> <li>Handles efficient data retrieval from remote storage locations</li> <li>Fetches specific rows and columns (e.g., <code>chunk_index</code> and <code>title</code>) needed for the final output</li> <li> <p>Optimizes network bandwidth by only retrieving required data    <pre><code>RemoteTake: columns=\"vector, identifier, _rowid, _distance, chunk_index, title\"\n</code></pre></p> </li> <li> <p>Final Output</p> </li> <li>Returns only requested columns and maintains column ordering    <pre><code>ProjectionExec: expr=[chunk_index@4 as chunk_index, title@5 as title, identifier@1 as identifier, _distance@3 as _distance]\n</code></pre></li> </ol> <p>This plan demonstrates a basic search without index optimizations: it performs a full  scan and filter before vector search. </p>"},{"location":"guides/search/optimize-queries/#performance-analysis","title":"Performance Analysis","text":"<p>Let's use <code>analyze_plan</code> to run the query and analyze the query performance,  which will help us identify potential bottlenecks:</p> PythonTypeScript <pre><code># analyze_plan\nquery_analyze_plan = (\n    table.search(query_embed)\n    .where(\"identifier &gt; 0 AND identifier &lt; 1000000\")\n    .select([ \"chunk_index\", \"title\", \"identifier\"])\n    .limit(100)\n    .analyze_plan()\n)\n</code></pre> <pre><code>// analyze_plan\nconst analyzePlan = await table\n    .search(queryEmbed)\n    .where(\"identifier &gt; 0 AND identifier &lt; 1000000\")\n    .select([ \"chunk_index\", \"title\", \"identifier\"])\n    .limit(100)\n    .analyzePlan();\n</code></pre> <pre><code>ProjectionExec: expr=[chunk_index@4 as chunk_index, title@5 as title, identifier@1 as identifier, _distance@3 as _distance], metrics=[output_rows=100, elapsed_compute=1.424\u00b5s]\n    RemoteTake: columns=\"vector, identifier, _rowid, _distance, chunk_index, title\", metrics=[output_rows=100, elapsed_compute=175.53097ms, output_batches=1, remote_takes=100]\n      CoalesceBatchesExec: target_batch_size=1024, metrics=[output_rows=100, elapsed_compute=2.748\u00b5s]\n        GlobalLimitExec: skip=0, fetch=100, metrics=[output_rows=100, elapsed_compute=1.819\u00b5s]\n          FilterExec: _distance@3 IS NOT NULL, metrics=[output_rows=100, elapsed_compute=10.275\u00b5s]\n            SortExec: TopK(fetch=100), expr=[_distance@3 ASC NULLS LAST], preserve_partitioning=[false], metrics=[output_rows=100, elapsed_compute=39.259451ms, row_replacements=546]\n              KNNVectorDistance: metric=l2, metrics=[output_rows=1099508, elapsed_compute=56.783526ms, output_batches=1076]\n                FilterExec: identifier@1 &gt; 0 AND identifier@1 &lt; 1000000, metrics=[output_rows=1099508, elapsed_compute=17.136819ms]\n                  LanceScan: uri=***, projection=[vector, identifier], row_id=true, row_addr=false, ordered=false, metrics=[output_rows=1200000, elapsed_compute=21.348178ms, bytes_read=1852931072, iops=78, requests=78]\n</code></pre> <p>The <code>analyze_plan</code> output reveals detailed performance metrics for each step of  the query execution:</p> <ol> <li>Data Loading (LanceScan)</li> <li>Scanned 1,200,000 rows from the LanceDB table</li> <li>Read 1.86GB of data in 78 I/O operations</li> <li>Only loaded the necessary columns (<code>vector</code> and <code>identifier</code>) for the initial processing</li> <li> <p>The scan was unordered, allowing for parallel processing</p> </li> <li> <p>Filtering &amp; Search</p> </li> <li>Applied a prefilter condition (<code>identifier &gt; 0 AND identifier &lt; 1000000</code>) before vector search</li> <li>This reduced the dataset from 1.2M to 1,099,508 rows, optimizing the subsequent KNN computation</li> <li>The KNN search used L2 (Euclidean) distance metric</li> <li> <p>Vector comparisons were processed in 1076 batches for efficient memory &amp; cache usage</p> </li> <li> <p>Results Processing</p> </li> <li>The KNN results were sorted by distance (TopK with fetch=100)</li> <li>Null distances were filtered out to ensure result quality</li> <li>Batches were coalesced to a target size of 1024 rows for optimal memory usage</li> <li>Additional columns (<code>chunk_index</code> and <code>title</code>) were fetched for the final results</li> <li>The remote take operation fetched these additional columns for each of the 100 results</li> <li>Final projection selected only the required columns for output</li> </ol> <p>Key observations: - The vector search operation is the primary performance bottleneck, requiring KNN computation across 1,099,508 vectors - Significant I/O overhead with 1.86GB of data read through multiple I/O requests - Query execution involves a full table scan due to lack of indices - Substantial optimization potential through proper index implementation</p> <p>Let's create the vector index on the <code>vector</code> column and the scalar index on the <code>identifier</code> columns where the filter is applied. <code>explain_plan</code> and <code>analyze_plan</code> now show  the following:</p> <p><pre><code>ProjectionExec: expr=[chunk_index@3 as chunk_index, title@4 as title, identifier@2 as identifier, _distance@0 as _distance]\n  RemoteTake: columns=\"_distance, _rowid, identifier, chunk_index, title\"\n    CoalesceBatchesExec: target_batch_size=1024\n      GlobalLimitExec: skip=0, fetch=100\n        SortExec: TopK(fetch=100), expr=[_distance@0 ASC NULLS LAST], preserve_partitioning=[false]\n          ANNSubIndex: name=vector_idx, k=100, deltas=1\n            ANNIvfPartition: uuid=83916fd5-fc45-4977-bad9-1f0737539bb9, nprobes=20, deltas=1\n            ScalarIndexQuery: query=AND(identifier &gt; 0,identifier &lt; 1000000)\n</code></pre> <pre><code>ProjectionExec: expr=[chunk_index@3 as chunk_index, title@4 as title, identifier@2 as identifier, _distance@0 as _distance], metrics=[output_rows=100, elapsed_compute=1.488\u00b5s]\n  RemoteTake: columns=\"_distance, _rowid, identifier, chunk_index, title\", metrics=[output_rows=100, elapsed_compute=113.491859ms, output_batches=1, remote_takes=100]\n    CoalesceBatchesExec: target_batch_size=1024, metrics=[output_rows=100, elapsed_compute=2.709\u00b5s]\n      GlobalLimitExec: skip=0, fetch=100, metrics=[output_rows=100, elapsed_compute=2.5\u00b5s]\n        SortExec: TopK(fetch=100), expr=[_distance@0 ASC NULLS LAST], preserve_partitioning=[false], metrics=[output_rows=100, elapsed_compute=72.322\u00b5s, row_replacements=153]\n          ANNSubIndex: name=vector_idx, k=100, deltas=1, metrics=[output_rows=2000, elapsed_compute=111.849043ms, index_comparisons=25893, indices_loaded=0, output_batches=20, parts_loaded=20]\n            ANNIvfPartition: uuid=83916fd5-fc45-4977-bad9-1f0737539bb9, nprobes=20, deltas=1, metrics=[]\n            ScalarIndexQuery: query=AND(identifier &gt; 0,identifier &lt; 1000000), metrics=[output_rows=2, elapsed_compute=86.979354ms, index_comparisons=2301824, indices_loaded=2, output_batches=1, parts_loaded=562]\n</code></pre></p> <p>Let's break down this optimized query execution plan from bottom to top: 1. Scalar Index Query (Bottom Layer):     - Performs range filter using scalar index, only 2 index files and 562 sclar index parts are loaded.     - Made 2.3M index comparisons to find matches <pre><code>ScalarIndexQuery: query=AND(identifier &gt; 0,identifier &lt; 1000000)\nmetrics=[\n    output_rows=2\n    index_comparisons=2,301,824\n    indices_loaded=2\n    output_batches=1\n    parts_loaded=562\n    elapsed_compute=86.979354ms\n]\n</code></pre></p> <ol> <li> <p>Vector Search:</p> <ul> <li>Uses IVF index with 20 probes, only needed to load 20 index parts</li> <li>Made 25,893 vector comparisons</li> <li>Found 2,000 matching vectors <pre><code>ANNSubIndex: name=vector_idx, k=100, deltas=1\nmetrics=[\n    output_rows=2,000\n    index_comparisons=25,893\n    indices_loaded=0\n    output_batches=20\n    parts_loaded=20\n    elapsed_compute=111.849043ms\n]\n</code></pre></li> </ul> </li> <li> <p>Results Processing:</p> <ul> <li>Sorts results by distance</li> <li>Limits to top 100 results</li> <li>Batches results into groups of 1024 <pre><code>SortExec: TopK(fetch=100), expr=[_distance@0 ASC NULLS LAST], preserve_partitioning=[false], metrics=[output_rows=100, elapsed_compute=72.322\u00b5s, row_replacements=153]\nGlobalLimitExec: skip=0, fetch=100, metrics=[output_rows=100, elapsed_compute=2.5\u00b5s]\nCoalesceBatchesExec: target_batch_size=1024, metrics=[output_rows=100, elapsed_compute=2.709\u00b5s]\n</code></pre></li> </ul> </li> <li> <p>Data Fetching:</p> <ul> <li>Consolidates into just 1 output batches, one remote take per row. <pre><code>RemoteTake: columns=\"_distance, _rowid, identifier, chunk_index, title\", metrics=[output_rows=100, elapsed_compute=113.491859ms, output_batches=1, remote_takes=100]\n</code></pre></li> </ul> </li> <li> <p>Final Projection:</p> <ul> <li>Returns only the specified columns: chunk_index, title, identifier, and distance</li> </ul> </li> </ol> <p><pre><code>ProjectionExec: expr=[chunk_index@3 as chunk_index, title@4 as title, identifier@2 as identifier, _distance@0 as _distance], metrics=[output_rows=100, elapsed_compute=1.488\u00b5s]\n</code></pre> The plan demonstrates a combined scalar index filter + vector similarity search,  optimized to first filter by <code>identifier</code> before performing the ANN search.</p>"},{"location":"guides/search/optimize-queries/#key-improvements","title":"Key Improvements:","text":"<ol> <li>Initial Data Access <pre><code>ScalarIndexQuery metrics:\n- indices_loaded=2\n- parts_loaded=562\n- output_batches=1\n</code></pre></li> <li>Before optimization: Full table scan of 1.2M rows, reading 1.86GB of data</li> <li>After optimization: With scalar index on the <code>identifier</code> column, only 2 indices (vector and scalar) and 562 scalar index parts were loaded</li> <li> <p>Benefit: Eliminated the need for any table scans to load the prefilter, significantly reducing I/O operations</p> </li> <li> <p>Vector Search Efficiency <pre><code>ANNSubIndex:\n- index_comparisons=25,893\n- indices_loaded=0\n- parts_loaded=20\n- output_batches=20\n</code></pre></p> </li> <li>Before optimization: L2 distance calculations performed on 1,099,508 vectors</li> <li> <p>After optimization: </p> <ul> <li>Reduced vector comparisons by 99.8% (from ~1.1M to 2K)</li> <li>Decreased output batches from 1,076 to 20</li> </ul> </li> <li> <p>Data Retrieval Optimization <pre><code>RemoteTake:\n- remote_takes=100\n- output_batches=1\n</code></pre></p> </li> <li>RemoteTake operation remains the same.</li> </ol>"},{"location":"guides/search/optimize-queries/#performance-optimization-tips","title":"Performance Optimization Tips","text":"<ol> <li>Index Implementation Guide</li> </ol> <p>When to Create Indices - Columns used in WHERE clauses - Vector columns for similarity searches - Join columns used in <code>merge_insert</code></p> <p>Index Type Selection</p> Data Type Recommended Index Use Case Vector IVF_PQ/IVF_HNSW_SQ Approximate nearest neighbor search Scalar B-Tree Range queries and sorting Categorical Bitmap Multi-value filters and set operations <code>List</code> Label_list Multi-label classification and filtering <p>More details on indexing can be found here. </p> <p>Index Coverage Monitoring</p> <p>Use <code>table.index_stats()</code> to monitor index coverage.  A well-optimized table should have <code>num_unindexed_rows ~ 0</code>.</p> <ol> <li>Query Plan Optimization</li> </ol> <p>Common Patterns and Fixes:</p> Plan Pattern Optimization LanceScan with high bytes_read or iops Add missing index Use <code>select()</code> to limit returned columns Check whether the dataset has been compacted Multiple sequential filters Reorder filter conditions <p>Regular Performance Analysis</p> <p>Regularly analyze your query plans to identify and address performance bottlenecks.  The <code>analyze_plan</code> output provides detailed metrics to guide optimization efforts.</p>"},{"location":"guides/search/optimize-queries/#first-step-use-the-right-indices","title":"First Step: Use the Right Indices","text":"<p>For vector search performance: - Create ANN index on your vector column(s) as described in the index guide - If you often filter by metadata, create scalar indices on those columns</p>"},{"location":"guides/search/query-planning/","title":"Query Planning","text":""},{"location":"guides/search/query-planning/#explaining-query-plans","title":"Explaining query plans","text":"<p>If you have slow queries or unexpected query results, it can be helpful to print the resolved query plan. You can use the <code>explain_plan</code> method to do this:</p> <ul> <li>Python Sync: LanceQueryBuilder.explain_plan</li> <li>Python Async: AsyncQueryBase.explain_plan</li> <li>Node @lancedb/lancedb: LanceQueryBuilder.explainPlan</li> </ul> <p>To understand how a query was actually executed\u2014including metrics like execution time, number of rows processed, I/O stats, and more\u2014use the analyze_plan method. This executes the query and returns a physical execution plan annotated with runtime metrics, making it especially helpful for performance tuning and debugging.</p> <ul> <li>Python Sync: LanceQueryBuilder.analyze_plan</li> <li>Python Async: AsyncQueryBase.analyze_plan</li> <li>Node @lancedb/lancedb: LanceQueryBuilder.analyzePlan</li> </ul>"},{"location":"guides/search/sql-queries/","title":"SQL Queries with LanceDB","text":"<p>Note: This is a preview feature that is only available in LanceDB Enterprise.</p> <p>Our solution includes an SQL endpoint that can be used for analytical queries and data exploration. The SQL endpoint is designed to be compatible with the Arrow FlightSQL protocol, which allows you to use any Arrow FlightSQL-compatible client to query your data.</p>"},{"location":"guides/search/sql-queries/#installing-a-client","title":"Installing a client","text":"<p>There are Flight SQL clients available for most languages and tools.  If you find that your preferred language or tool is not listed here, please reach out to us and we can help you find a solution.  The following examples demonstrate how to install the Python and TypeScript clients.</p> PythonTypeScript <pre><code># The `flightsql-dbapi` package provides a Python DB API 2 interface to the\n# LanceDB SQL endpoint. You can use it to connect to the SQL endpoint and\n# execute queries directly and get back results in pyarrow format.\n\npip install flightsql-dbapi\n</code></pre> <pre><code># LanceDB maintains a TypeScript client for the Arrow FlightSQL protocol.\n# You can use it to connect to the SQL endpoint and execute queries directly.\n# Results are returned in Arrow format or as plain JS/TS objects.\n\nnpm install --save @lancedb/flightsql-client\n</code></pre>"},{"location":"guides/search/sql-queries/#usage","title":"Usage","text":"<p>LanceDB uses the powerful DataFusion query engine to execute SQL queries.  This means that you can use a wide variety of SQL syntax and functions to query your data.  For more detailed information on the SQL syntax and functions supported by DataFusion, please refer to the DataFusion documentation.</p> PythonTypeScript <pre><code>from flightsql import FlightSQLClient\n\nclient = FlightSQLClient(\n    host=\"your-enterprise-endpoint\",\n    port=10025,\n    insecure=True,\n    token=\"DATABASE_TOKEN\",\n    metadata={\"database\": \"your-project-slug\"},\n    features={\"metadata-reflection\": \"true\"},\n)\n\ndef run_query(query: str):\n    \"\"\"Simple method to fully materialize query results\"\"\"\n    info = client.execute(query)\n    if len(info.endpoints) != 1:\n        raise Error(\"Expected exactly one endpoint\")\n    ticket = info.endpoints[0].ticket\n    reader = client.do_get(ticket)\n    return reader.read_all()\n\nprint(run_query(\"SELECT * FROM flights WHERE origin = 'SFO'\"))\n</code></pre> <p>```typescript import { Client } from \"@lancedb/flightsql-client\";</p> <p>const client = await Client.connect({   host: \"your-enterprise-endpoint:10025\",   username: \"lancedb\",   password: \"password\", });</p> <p>const result = await client.query(\"SELECT * FROM flights WHERE origin = 'SFO'\");</p> <p>// Results are returned as plain JS/TS objects and we create an interface // here for our expected structure so we can have strong typing.  This is // optional but recommended. interface FlightRecord {     origin: string;     destination: string; }</p> <p>const flights = (await result.collectToObjects()) as FlightRecord[]; console.log(flights);</p>"},{"location":"guides/search/vector-search/","title":"Vector Search with LanceDB","text":"<p>We support lightning fast vector search on massive scale data. Following performance  data shows search latency from a 1M dataset with warmed up cache.</p> Percentile Latency P50 25ms P90 26ms P99 35ms Max 49ms <p>Other than latency, users can also tune the following parameters for better search quality.  - nprobes:   the number of partitions to search (probe)  - refine factor:   a multiplier to control how many additional rows are taken during the refine step  - distance range: search for vectors within the distance range</p> <p>LanceDB delivers exceptional vector search performance with  metadata filtering. Benchmark results demonstrate 65ms query latency at scale, tested on a  15-million vector dataset.This combination of fast vector  search and precise metadata filtering enables efficient,  accurate querying of large-scale datasets.</p>"},{"location":"guides/search/vector-search/#vector-search-with-metadata-prefiltering","title":"Vector search with metadata prefiltering","text":"PythonTypeScript <pre><code>import lancedb\nfrom datasets import load_dataset\n\n# Connect to LanceDB\ndb = lancedb.connect(\n  uri=\"db://your-project-slug\",\n  api_key=\"your-api-key\",\n  region=\"us-east-1\"\n)\n\n# Load query vector from dataset\nquery_dataset = load_dataset(\"sunhaozhepy/ag_news_sbert_keywords_embeddings\", split=\"test[5000:5001]\")\nprint(f\"Query keywords: {query_dataset[0]['keywords']}\")\nquery_embed = query_dataset[\"keywords_embeddings\"][0]\n\n# Open table and perform search\ntable_name = \"lancedb-cloud-quickstart\"\ntable = db.open_table(table_name)\n\n# Vector search with filters (pre-filtering is the default)\nsearch_results = (\n    table.search(query_embed)\n    .where(\"label &gt; 2\")\n    .select([\"text\", \"keywords\", \"label\"])\n    .limit(5)\n    .to_pandas()\n)\n\nprint(\"Search results (with pre-filtering):\")\nprint(search_results)\n</code></pre> <pre><code>import * as lancedb from \"@lancedb/lancedb\";\n\n// Connect to LanceDB\nconst db = await lancedb.connect({\n  uri: \"db://your-project-slug\",\n  apiKey: \"your-api-key\",\n  region: \"us-east-1\"\n});\n\n// Generate a sample 768-dimension embedding vector (typical for BERT-based models)\n// In real applications, you would get this from an embedding model\nconst dimensions = 768;\nconst queryEmbed = Array.from({ length: dimensions }, () =&gt; Math.random() * 2 - 1);\n\n// Open table and perform search\nconst tableName = \"lancedb-cloud-quickstart\";\nconst table = await db.openTable(tableName);\n\n// Vector search with filters (pre-filtering is the default)\nconst vectorResults = await table.search(queryEmbed)\n  .where(\"label &gt; 2\")\n  .select([\"text\", \"keywords\", \"label\"])\n  .limit(5)\n  .toArray();\n\nconsole.log(\"Search results (with pre-filtering):\");\nconsole.log(vectorResults);\n</code></pre>"},{"location":"guides/search/vector-search/#vector-search-with-metadata-postfiltering","title":"Vector search with metadata postfiltering","text":"<p>By default, pre-filtering is performed to filter prior to vector search.   This can be useful to narrow down the search space of a very large dataset to   reduce query latency. Post-filtering is also an option that performs the filter   on the results returned by the vector search. You can use post-filtering as follows:</p> PythonTypeScript <pre><code>results_post_filtered = (\n    table.search(query_embed)\n    .where(\"label &gt; 1\", prefilter=False)\n    .select([\"text\", \"keywords\", \"label\"])\n    .limit(5)\n    .to_pandas()\n)\n\nprint(\"Vector search results with post-filter:\")\nprint(results_post_filtered)\n</code></pre> <pre><code>const vectorResultsWithPostFilter = await (table.search(queryEmbed) as VectorQuery)\n  .where(\"label &gt; 2\")\n  .postfilter()\n  .select([\"text\", \"keywords\", \"label\"])\n  .limit(5)\n  .toArray();\n\nconsole.log(\"Vector search results with post-filter:\");\nconsole.log(vectorResultsWithPostFilter);\n</code></pre>"},{"location":"guides/search/vector-search/#batch-query","title":"Batch query","text":"<p>LanceDB can process multiple similarity search requests  simultaneously in a single operation, rather than handling  each query individually. </p> PythonTypeScript <pre><code># Load a batch of query embeddings\nquery_dataset = load_dataset(\n    \"sunhaozhepy/ag_news_sbert_keywords_embeddings\", split=\"test[5000:5005]\"\n)\nquery_embeds = query_dataset[\"keywords_embeddings\"]\nbatch_results = table.search(query_embeds).limit(5).to_pandas()\nprint(batch_results)\n</code></pre> <pre><code> // Batch query\nconsole.log(\"Performing batch vector search...\");\nconst batchSize = 5;\nconst queryVectors = Array.from(\n  { length: batchSize },\n  () =&gt; Array.from(\n    { length: dimensions },\n    () =&gt; Math.random() * 2 - 1,\n  ),\n);\nlet batchQuery = table.search(queryVectors[0]) as VectorQuery;\nfor (let i = 1; i &lt; batchSize; i++) {\n  batchQuery = batchQuery.addQueryVector(queryVectors[i]);\n}\nconst batchResults = await batchQuery\n  .select([\"text\", \"keywords\", \"label\"])\n  .limit(5)\n  .toArray();\nconsole.log(\"Batch vector search results:\");\nconsole.log(batchResults);\n</code></pre> <p>Batch Query Results</p> <p>When processing batch queries, the results include a <code>query_index</code> field  to explicitly associate each result set with its corresponding query in  the input batch. </p>"},{"location":"guides/search/vector-search/#other-search-options","title":"Other search options","text":"<p>Fast search</p> <p>While vector indexing occurs asynchronously, newly added vectors are immediately  searchable through a fallback brute-force search mechanism. This ensures zero  latency between data insertion and searchability, though it may temporarily  increase query response times. To optimize for speed over completeness,  enable the <code>fast_search</code> flag in your query to skip searching unindexed data.</p> PythonTypeScript <pre><code># sync API\ntable.search(embedding, fast_search=True).limit(5).to_pandas()\n\n# async API\nawait table.query().nearest_to(embedding).fast_search().limit(5).to_pandas()\n</code></pre> <pre><code>await table\n  .query()\n  .nearestTo(embedding)\n  .fastSearch()\n  .limit(5)\n  .toArray();\n</code></pre> <p>Bypass Vector Index</p> <p>The bypass vector index feature prioritizes search accuracy over query speed by performing  an exhaustive search across all vectors. Instead of using the approximate nearest neighbor  (ANN) index, it compares the query vector against every vector in the table directly. </p> <p>While this approach increases query latency, especially with large datasets, it provides  exact, ground-truth results. This is particularly useful when: - Evaluating ANN index quality - Calculating recall metrics to tune <code>nprobes</code> parameter - Verifying search accuracy for critical applications - Benchmarking approximate vs exact search results</p> PythonTypeScript <pre><code># sync API\ntable.search(embedding).bypass_vector_index().limit(5).to_pandas()\n\n# async API\nawait table.query().nearest_to(embedding).bypass_vector_index().limit(5).to_pandas()\n</code></pre> <pre><code>await table\n  .query()\n  .nearestTo(embedding)\n  .bypassVectorIndex()\n  .limit(5)\n  .toArray();\n</code></pre> <p>Performance Optimization</p> <p>For system design and performance tuning: - Check our benchmark page for performance characteristics - For complex queries, see the query performance tuning guide - When deciding on distance metrics, consider your vector type and search objectives</p> <p>title: Search in LanceDB | Vector, FTS, and Hybrid Search description: Learn about LanceDB's powerful search capabilities including vector similarity search, full-text search, and hybrid search. Features comprehensive query options, filtering, and result customization.</p>"},{"location":"guides/search/vector-search/#vector-search","title":"Vector Search","text":"<p>A vector search finds the approximate or exact nearest neighbors to a given query vector.</p> <ul> <li>In a recommendation system or search engine, you can find similar records to   the one you searched.</li> <li>In LLM and other AI applications,   each data point can be represented by embeddings generated from existing models,   following which the search returns the most relevant features.</li> </ul>"},{"location":"guides/search/vector-search/#distance-metrics","title":"Distance metrics","text":"<p>Distance metrics are a measure of the similarity between a pair of vectors. Currently, LanceDB supports the following metrics:</p> Metric Description <code>l2</code> Euclidean / l2 distance <code>cosine</code> Cosine Similarity <code>dot</code> Dot Production <code>hamming</code> Hamming Distance <p>Note</p> <p>The <code>hamming</code> metric is only available for binary vectors.</p>"},{"location":"guides/search/vector-search/#exhaustive-search-knn","title":"Exhaustive search (kNN)","text":"<p>If you do not create a vector index, LanceDB exhaustively scans the entire vector space and computes the distance to every vector in order to find the exact nearest neighbors. This is effectively a kNN search.</p> PythonTypeScript Sync APIAsync API <pre><code>uri = \"data/sample-lancedb\"\ndb = lancedb.connect(uri)\ndata = [\n    {\"vector\": row, \"item\": f\"item {i}\"}\n    for i, row in enumerate(np.random.random((10_000, 1536)).astype(\"float32\"))\n]\ntbl = db.create_table(\"vector_search\", data=data)\ntbl.search(np.random.random((1536))).limit(10).to_list()\n</code></pre> <pre><code>uri = \"data/sample-lancedb\"\nasync_db = await lancedb.connect_async(uri)\ndata = [\n    {\"vector\": row, \"item\": f\"item {i}\"}\n    for i, row in enumerate(np.random.random((10_000, 1536)).astype(\"float32\"))\n]\nasync_tbl = await async_db.create_table(\"vector_search_async\", data=data)\n(await (await async_tbl.search(np.random.random((1536)))).limit(10).to_list())\n</code></pre> @lancedb/lancedbvectordb (deprecated) <pre><code>import * as lancedb from \"@lancedb/lancedb\";\n\nconst db = await lancedb.connect(databaseDir);\nconst tbl = await db.openTable(\"my_vectors\");\n\nconst results1 = await tbl.search(Array(128).fill(1.2)).limit(10).toArray();\n</code></pre> <pre><code>import * as lancedb from \"vectordb\";\n\nconst db = await lancedb.connect(\"data/sample-lancedb\");\nconst tbl = await db.openTable(\"my_vectors\");\n\nconst results_1 = await tbl.search(Array(1536).fill(1.2)).limit(10).execute();\n</code></pre> <p>By default, <code>l2</code> will be used as metric type. You can specify the metric type as <code>cosine</code> or <code>dot</code> if required.</p> PythonTypeScript Sync APIAsync API <pre><code>tbl.search(np.random.random((1536))).distance_type(\"cosine\").limit(10).to_list()\n</code></pre> <pre><code>(\n    await (await async_tbl.search(np.random.random((1536))))\n    .distance_type(\"cosine\")\n    .limit(10)\n    .to_list()\n)\n</code></pre> @lancedb/lancedbvectordb (deprecated) <pre><code>const results2 = await (\n  tbl.search(Array(128).fill(1.2)) as lancedb.VectorQuery\n)\n  .distanceType(\"cosine\")\n  .limit(10)\n  .toArray();\n</code></pre> <pre><code>const results_2 = await tbl\n  .search(Array(1536).fill(1.2))\n  .metricType(lancedb.MetricType.Cosine)\n  .limit(10)\n  .execute();\n</code></pre>"},{"location":"guides/search/vector-search/#approximate-nearest-neighbor-ann-search","title":"Approximate nearest neighbor (ANN) search","text":"<p>To perform scalable vector retrieval with acceptable latencies, it's common to build a vector index. While the exhaustive search is guaranteed to always return 100% recall, the approximate nature of an ANN search means that using an index often involves a trade-off between recall and latency.</p> <p>See the IVF_PQ index for a deeper description of how <code>IVF_PQ</code> indexes work in LanceDB.</p>"},{"location":"guides/search/vector-search/#binary-vector","title":"Binary vector","text":"<p>LanceDB supports binary vectors as a data type, and has the ability to search binary vectors with hamming distance. The binary vectors are stored as uint8 arrays (every 8 bits are stored as a byte):</p> <p>Note</p> <p>The dim of the binary vector must be a multiple of 8. A vector of dim 128 will be stored as a uint8 array of size 16.</p> Python Sync APIAsync APITypeScript <pre><code>import lancedb\nimport numpy as np\nimport pyarrow as pa\nimport pytest\n\ndb = lancedb.connect(\"data/binary_lancedb\")\nschema = pa.schema(\n    [\n        pa.field(\"id\", pa.int64()),\n        # for dim=256, lance stores every 8 bits in a byte\n        # so the vector field should be a list of 256 / 8 = 32 bytes\n        pa.field(\"vector\", pa.list_(pa.uint8(), 32)),\n    ]\n)\ntbl = db.create_table(\"my_binary_vectors\", schema=schema)\n\ndata = []\nfor i in range(1024):\n    vector = np.random.randint(0, 2, size=256)\n    # pack the binary vector into bytes to save space\n    packed_vector = np.packbits(vector)\n    data.append(\n        {\n            \"id\": i,\n            \"vector\": packed_vector,\n        }\n    )\ntbl.add(data)\n\nquery = np.random.randint(0, 2, size=256)\npacked_query = np.packbits(query)\ntbl.search(packed_query).distance_type(\"hamming\").to_arrow()\n</code></pre> <pre><code>import lancedb\nimport numpy as np\nimport pyarrow as pa\nimport pytest\n\ndb = await lancedb.connect_async(\"data/binary_lancedb\")\nschema = pa.schema(\n    [\n        pa.field(\"id\", pa.int64()),\n        # for dim=256, lance stores every 8 bits in a byte\n        # so the vector field should be a list of 256 / 8 = 32 bytes\n        pa.field(\"vector\", pa.list_(pa.uint8(), 32)),\n    ]\n)\ntbl = await db.create_table(\"my_binary_vectors\", schema=schema)\n\ndata = []\nfor i in range(1024):\n    vector = np.random.randint(0, 2, size=256)\n    # pack the binary vector into bytes to save space\n    packed_vector = np.packbits(vector)\n    data.append(\n        {\n            \"id\": i,\n            \"vector\": packed_vector,\n        }\n    )\nawait tbl.add(data)\n\nquery = np.random.randint(0, 2, size=256)\npacked_query = np.packbits(query)\nawait (await tbl.search(packed_query)).distance_type(\"hamming\").to_arrow()\n</code></pre> <pre><code>import * as lancedb from \"@lancedb/lancedb\";\n\nimport { Field, FixedSizeList, Int32, Schema, Uint8 } from \"apache-arrow\";\n\nconst schema = new Schema([\n  new Field(\"id\", new Int32(), true),\n  new Field(\"vec\", new FixedSizeList(32, new Field(\"item\", new Uint8()))),\n]);\nconst data = lancedb.makeArrowTable(\n  Array(1_000)\n    .fill(0)\n    .map((_, i) =&gt; ({\n      // the 256 bits would be store in 32 bytes,\n      // if your data is already in this format, you can skip the packBits step\n      id: i,\n      vec: lancedb.packBits(Array(256).fill(i % 2)),\n    })),\n  { schema: schema },\n);\n\nconst tbl = await db.createTable(\"binary_table\", data);\nawait tbl.createIndex(\"vec\", {\n  config: lancedb.Index.ivfFlat({\n    numPartitions: 10,\n    distanceType: \"hamming\",\n  }),\n});\n\n      const query = Array(32)\n        .fill(1)\n        .map(() =&gt; Math.floor(Math.random() * 255));\n      const results = await tbl.query().nearestTo(query).limit(10).toArrow();\n      // --8&lt;-- [end:search_binary_data\n      expect(results.numRows).toBe(10);\n    }\n  });\n});\n</code></pre>"},{"location":"guides/search/vector-search/#search-with-distance-range","title":"Search with distance range","text":"<p>You can also search for vectors within a specific distance range from the query vector. This is useful when you want to find vectors that are not just the nearest neighbors, but also those that are within a certain distance. This can be done by using the <code>distance_range</code> method.</p> PythonTypeScript Sync APIAsync API <pre><code>import lancedb\nimport numpy as np\n\ndb = lancedb.connect(\"data/distance_range_demo\")\ndata = [\n    {\n        \"id\": i,\n        \"vector\": np.random.random(256),\n    }\n    for i in range(1024)\n]\ntbl = db.create_table(\"my_table\", data=data)\nquery = np.random.random(256)\n\n# Search for the vectors within the range of [0.1, 0.5)\ntbl.search(query).distance_range(0.1, 0.5).to_arrow()\n\n# Search for the vectors with the distance less than 0.5\ntbl.search(query).distance_range(upper_bound=0.5).to_arrow()\n\n# Search for the vectors with the distance greater or equal to 0.1\ntbl.search(query).distance_range(lower_bound=0.1).to_arrow()\n</code></pre> <pre><code>import lancedb\nimport numpy as np\n\ndb = await lancedb.connect_async(\"data/distance_range_demo\")\ndata = [\n    {\n        \"id\": i,\n        \"vector\": np.random.random(256),\n    }\n    for i in range(1024)\n]\ntbl = await db.create_table(\"my_table\", data=data)\nquery = np.random.random(256)\n\n# Search for the vectors within the range of [0.1, 0.5)\nawait (await tbl.search(query)).distance_range(0.1, 0.5).to_arrow()\n\n# Search for the vectors with the distance less than 0.5\nawait (await tbl.search(query)).distance_range(upper_bound=0.5).to_arrow()\n\n# Search for the vectors with the distance greater or equal to 0.1\nawait (await tbl.search(query)).distance_range(lower_bound=0.1).to_arrow()\n</code></pre> @lancedb/lancedb <pre><code>import * as lancedb from \"@lancedb/lancedb\";\n\nconst results3 = await (\n  tbl.search(Array(128).fill(1.2)) as lancedb.VectorQuery\n)\n  .distanceType(\"cosine\")\n  .distanceRange(0.1, 0.2)\n  .limit(10)\n  .toArray();\n</code></pre>"},{"location":"guides/search/vector-search/#output-search-results","title":"Output search results","text":"<p>LanceDB returns vector search results via different formats commonly used in python. Let's create a LanceDB table with a nested schema:</p> Python Sync APIAsync API <pre><code>from datetime import datetime\n\nimport lancedb\n\nfrom lancedb.pydantic import Vector, LanceModel\n\nfrom lancedb.query import BoostQuery, MatchQuery\nimport numpy as np\nimport pyarrow as pa\n\nfrom pydantic import BaseModel\n\nclass Metadata(BaseModel):\n    source: str\n    timestamp: datetime\n\n\nclass Document(BaseModel):\n    content: str\n    meta: Metadata\n\n\nclass LanceSchema(LanceModel):\n    id: str\n    vector: Vector(1536)\n    payload: Document\n\n\n# Let's add 100 sample rows to our dataset\ndata = [\n    LanceSchema(\n        id=f\"id{i}\",\n        vector=np.random.randn(1536),\n        payload=Document(\n            content=f\"document{i}\",\n            meta=Metadata(source=f\"source{i % 10}\", timestamp=datetime.now()),\n        ),\n    )\n    for i in range(100)\n]\n\n# Synchronous client\ntbl = db.create_table(\"documents\", data=data)\n</code></pre> <pre><code>from datetime import datetime\n\nimport lancedb\n\nfrom lancedb.pydantic import Vector, LanceModel\n\nfrom lancedb.query import BoostQuery, MatchQuery\nimport numpy as np\nimport pyarrow as pa\n\nfrom pydantic import BaseModel\n\nclass Metadata(BaseModel):\n    source: str\n    timestamp: datetime\n\n\nclass Document(BaseModel):\n    content: str\n    meta: Metadata\n\n\nclass LanceSchema(LanceModel):\n    id: str\n    vector: Vector(1536)\n    payload: Document\n\n\n# Let's add 100 sample rows to our dataset\ndata = [\n    LanceSchema(\n        id=f\"id{i}\",\n        vector=np.random.randn(1536),\n        payload=Document(\n            content=f\"document{i}\",\n            meta=Metadata(source=f\"source{i % 10}\", timestamp=datetime.now()),\n        ),\n    )\n    for i in range(100)\n]\n\nasync_tbl = await async_db.create_table(\"documents_async\", data=data)\n</code></pre>"},{"location":"guides/search/vector-search/#as-a-pyarrow-table","title":"As a PyArrow table","text":"<p>Using <code>to_arrow()</code> we can get the results back as a pyarrow Table. This result table has the same columns as the LanceDB table, with the addition of an <code>_distance</code> column for vector search or a <code>score</code> column for full text search.</p> Sync APIAsync API <pre><code>tbl.search(np.random.randn(1536)).to_arrow()\n</code></pre> <pre><code>await (await async_tbl.search(np.random.randn(1536))).to_arrow()\n</code></pre>"},{"location":"guides/search/vector-search/#as-a-pandas-dataframe","title":"As a Pandas DataFrame","text":"<p>You can also get the results as a pandas dataframe.</p> Sync APIAsync API <pre><code>tbl.search(np.random.randn(1536)).to_pandas()\n</code></pre> <pre><code>await (await async_tbl.search(np.random.randn(1536))).to_pandas()\n</code></pre> <p>While other formats like Arrow/Pydantic/Python dicts have a natural way to handle nested schemas, pandas can only store nested data as a python dict column, which makes it difficult to support nested references. So for convenience, you can also tell LanceDB to flatten a nested schema when creating the pandas dataframe.</p> Sync API <pre><code>tbl.search(np.random.randn(1536)).to_pandas(flatten=True)\n</code></pre> <p>If your table has a deeply nested struct, you can control how many levels of nesting to flatten by passing in a positive integer.</p> Sync API <pre><code>tbl.search(np.random.randn(1536)).to_pandas(flatten=1)\n</code></pre> <p>Note</p> <p><code>flatten</code> is not yet supported with our asynchronous client.</p>"},{"location":"guides/search/vector-search/#as-a-list-of-python-dicts","title":"As a list of Python dicts","text":"<p>You can of course return results as a list of python dicts.</p> Sync APIAsync API <pre><code>tbl.search(np.random.randn(1536)).to_list()\n</code></pre> <pre><code>await (await async_tbl.search(np.random.randn(1536))).to_list()\n</code></pre>"},{"location":"guides/search/vector-search/#as-a-list-of-pydantic-models","title":"As a list of Pydantic models","text":"<p>We can add data using Pydantic models, and we can certainly retrieve results as Pydantic models</p> Sync API <pre><code>tbl.search(np.random.randn(1536)).to_pydantic(LanceSchema)\n</code></pre> <p>Note</p> <p><code>to_pydantic()</code> is not yet supported with our asynchronous client.</p> <p>Note that in this case the extra <code>_distance</code> field is discarded since it's not part of the LanceSchema.</p>"},{"location":"guides/storage/storage/","title":"Configuring Cloud Storage for LanceDB","text":"<p>When using LanceDB OSS, you can choose where to store your data. The tradeoffs between different storage options are discussed in the storage concepts guide. This guide shows how to configure LanceDB to use different storage options.</p>"},{"location":"guides/storage/storage/#object-stores","title":"Object Stores","text":"<p>LanceDB OSS supports object stores such as AWS S3 (and compatible stores), Azure Blob Store, and Google Cloud Storage. Which object store to use is determined by the URI scheme of the dataset path. <code>s3://</code> is used for AWS S3, <code>az://</code> is used for Azure Blob Storage, and <code>gs://</code> is used for Google Cloud Storage. These URIs are passed to the <code>connect</code> function:</p> PythonTypeScript <p>AWS S3:</p> Sync APIAsync API <pre><code>import lancedb\ndb = lancedb.connect(\"s3://bucket/path\")\n</code></pre> <pre><code>import lancedb\nasync_db = await lancedb.connect_async(\"s3://bucket/path\")\n</code></pre> <p>Google Cloud Storage:</p> Sync APIAsync API <pre><code>import lancedb\ndb = lancedb.connect(\"gs://bucket/path\")\n</code></pre> <pre><code>import lancedb\nasync_db = await lancedb.connect_async(\"gs://bucket/path\")\n</code></pre> <p>Azure Blob Storage:</p> Sync API <pre><code>import lancedb\ndb = lancedb.connect(\"az://bucket/path\")\n</code></pre> Async API <pre><code>import lancedb\nasync_db = await lancedb.connect_async(\"az://bucket/path\")\n</code></pre> <p>Note that for Azure, storage credentials must be configured. See below for more details.</p> @lancedb/lancedbvectordb (deprecated) <p>AWS S3:</p> <pre><code>import * as lancedb from \"@lancedb/lancedb\";\nconst db = await lancedb.connect(\"s3://bucket/path\");\n</code></pre> <p>Google Cloud Storage:</p> <pre><code>import * as lancedb from \"@lancedb/lancedb\";\nconst db = await lancedb.connect(\"gs://bucket/path\");\n</code></pre> <p>Azure Blob Storage:</p> <pre><code>import * as lancedb from \"@lancedb/lancedb\";\nconst db = await lancedb.connect(\"az://bucket/path\");\n</code></pre> <p>AWS S3:</p> <pre><code>const lancedb = require(\"lancedb\");\nconst db = await lancedb.connect(\"s3://bucket/path\");\n</code></pre> <p>Google Cloud Storage:</p> <pre><code>const lancedb = require(\"lancedb\");\nconst db = await lancedb.connect(\"gs://bucket/path\");\n</code></pre> <p>Azure Blob Storage:</p> <pre><code>const lancedb = require(\"lancedb\");\nconst db = await lancedb.connect(\"az://bucket/path\");\n</code></pre> <p>In most cases, when running in the respective cloud and permissions are set up correctly, no additional configuration is required. When running outside of the respective cloud, authentication credentials must be provided. Credentials and other configuration options can be set in two ways: first, by setting environment variables. And second, by passing a <code>storage_options</code> object to the <code>connect</code> function. For example, to increase the request timeout to 60 seconds, you can set the <code>TIMEOUT</code> environment variable to <code>60s</code>:</p> <pre><code>export TIMEOUT=60s\n</code></pre> <p>If you only want this to apply to one particular connection, you can pass the <code>storage_options</code> argument when opening the connection:</p> PythonTypeScript Sync APIAsync API <pre><code>import lancedb\ndb = lancedb.connect(\n    \"s3://bucket/path\",\n    storage_options={\"timeout\": \"60s\"}\n)\n</code></pre> <pre><code>import lancedb\nasync_db = await lancedb.connect_async(\n    \"s3://bucket/path\",\n    storage_options={\"timeout\": \"60s\"}\n)\n</code></pre> @lancedb/lancedbvectordb (deprecated) <pre><code>import * as lancedb from \"@lancedb/lancedb\";\n\nconst db = await lancedb.connect(\"s3://bucket/path\", {\n    storageOptions: {timeout: \"60s\"}\n});\n</code></pre> <pre><code>const lancedb = require(\"lancedb\");\nconst db = await lancedb.connect(\"s3://bucket/path\", {\n    storageOptions: {timeout: \"60s\"}\n});\n</code></pre> <p>Getting even more specific, you can set the <code>timeout</code> for only a particular table:</p> PythonTypeScript Sync API <pre><code>import lancedb\ndb = lancedb.connect(\"s3://bucket/path\")\ntable = db.create_table(\n    \"table\",\n    [{\"a\": 1, \"b\": 2}],\n    storage_options={\"timeout\": \"60s\"}\n)\n</code></pre> Async API <pre><code>import lancedb\nasync_db = await lancedb.connect_async(\"s3://bucket/path\")\nasync_table = await async_db.create_table(\n    \"table\",\n    [{\"a\": 1, \"b\": 2}],\n    storage_options={\"timeout\": \"60s\"}\n)\n</code></pre> @lancedb/lancedbvectordb (deprecated) <p> <pre><code>import * as lancedb from \"@lancedb/lancedb\";\nconst db = await lancedb.connect(\"s3://bucket/path\");\nconst table = db.createTable(\n    \"table\",\n    [{ a: 1, b: 2}],\n    {storageOptions: {timeout: \"60s\"}}\n);\n</code></pre></p> <p> <pre><code>const lancedb = require(\"lancedb\");\nconst db = await lancedb.connect(\"s3://bucket/path\");\nconst table = db.createTable(\n    \"table\",\n    [{ a: 1, b: 2}],\n    {storageOptions: {timeout: \"60s\"}}\n);\n</code></pre></p> <p>Storage option casing</p> <p>The storage option keys are case-insensitive. So <code>connect_timeout</code> and <code>CONNECT_TIMEOUT</code> are the same setting. Usually lowercase is used in the <code>storage_options</code> argument and uppercase is used for environment variables. In the <code>lancedb</code> Node package, the keys can also be provided in <code>camelCase</code> capitalization. For example, <code>connectTimeout</code> is equivalent to <code>connect_timeout</code>.</p>"},{"location":"guides/storage/storage/#general-configuration","title":"General configuration","text":"<p>There are several options that can be set for all object stores, mostly related to network client configuration.</p> Key Description <code>allow_http</code> Allow non-TLS, i.e. non-HTTPS connections. Default: <code>False</code>. <code>allow_invalid_certificates</code> Skip certificate validation on HTTPS connections. Default: <code>False</code>. <code>connect_timeout</code> Timeout for only the connect phase of a Client. Default: <code>5s</code>. <code>timeout</code> Timeout for the entire request, from connection until the response body has finished. Default: <code>30s</code>. <code>user_agent</code> User agent string to use in requests. <code>proxy_url</code> URL of a proxy server to use for requests. Default: <code>None</code>. <code>proxy_ca_certificate</code> PEM-formatted CA certificate for proxy connections. <code>proxy_excludes</code> List of hosts that bypass the proxy. This is a comma-separated list of domains and IP masks. Any subdomain of the provided domain will be bypassed. For example, <code>example.com, 192.168.1.0/24</code> would bypass <code>https://api.example.com</code>, <code>https://www.example.com</code>, and any IP in the range <code>192.168.1.0/24</code>."},{"location":"guides/storage/storage/#aws-s3","title":"AWS S3","text":"<p>To configure credentials for AWS S3, you can use the <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>, and <code>AWS_SESSION_TOKEN</code> keys. Region can also be set, but it is not mandatory when using AWS. These can be set as environment variables or passed in the <code>storage_options</code> parameter:</p> PythonTypeScript Sync APIAsync API <pre><code>import lancedb\ndb = lancedb.connect(\n    \"s3://bucket/path\",\n    storage_options={\n        \"aws_access_key_id\": \"my-access-key\",\n        \"aws_secret_access_key\": \"my-secret-key\",\n        \"aws_session_token\": \"my-session-token\",\n    }\n)\n</code></pre> <pre><code>import lancedb\nasync_db = await lancedb.connect_async(\n    \"s3://bucket/path\",\n    storage_options={\n        \"aws_access_key_id\": \"my-access-key\",\n        \"aws_secret_access_key\": \"my-secret-key\",\n        \"aws_session_token\": \"my-session-token\",\n    }\n)\n</code></pre> @lancedb/lancedbvectordb (deprecated) <pre><code>import * as lancedb from \"@lancedb/lancedb\";\nconst db = await lancedb.connect(\n    \"s3://bucket/path\",\n    {\n        storageOptions: {\n            awsAccessKeyId: \"my-access-key\",\n            awsSecretAccessKey: \"my-secret-key\",\n            awsSessionToken: \"my-session-token\",\n        }\n    }\n);\n</code></pre> <pre><code>const lancedb = require(\"lancedb\");\nconst db = await lancedb.connect(\n    \"s3://bucket/path\",\n    {\n        storageOptions: {\n            awsAccessKeyId: \"my-access-key\",\n            awsSecretAccessKey: \"my-secret-key\",\n            awsSessionToken: \"my-session-token\",\n        }\n    }\n);\n</code></pre> <p>Alternatively, if you are using AWS SSO, you can use the <code>AWS_PROFILE</code> and <code>AWS_DEFAULT_REGION</code> environment variables.</p> <p>The following keys can be used as both environment variables or keys in the <code>storage_options</code> parameter:</p> Key Description <code>aws_region</code> / <code>region</code> The AWS region the bucket is in. This can be automatically detected when using AWS S3, but must be specified for S3-compatible stores. <code>aws_access_key_id</code> / <code>access_key_id</code> The AWS access key ID to use. <code>aws_secret_access_key</code> / <code>secret_access_key</code> The AWS secret access key to use. <code>aws_session_token</code> / <code>session_token</code> The AWS session token to use. <code>aws_endpoint</code> / <code>endpoint</code> The endpoint to use for S3-compatible stores. <code>aws_virtual_hosted_style_request</code> / <code>virtual_hosted_style_request</code> Whether to use virtual hosted-style requests, where the bucket name is part of the endpoint. Meant to be used with <code>aws_endpoint</code>. Default: <code>False</code>. <code>aws_s3_express</code> / <code>s3_express</code> Whether to use S3 Express One Zone endpoints. Default: <code>False</code>. See more details below. <code>aws_server_side_encryption</code> The server-side encryption algorithm to use. Must be one of <code>\"AES256\"</code>, <code>\"aws:kms\"</code>, or <code>\"aws:kms:dsse\"</code>. Default: <code>None</code>. <code>aws_sse_kms_key_id</code> The KMS key ID to use for server-side encryption. If set, <code>aws_server_side_encryption</code> must be <code>\"aws:kms\"</code> or <code>\"aws:kms:dsse\"</code>. <code>aws_sse_bucket_key_enabled</code> Whether to use bucket keys for server-side encryption. <p>Automatic cleanup for failed writes</p> <p>LanceDB uses multi-part uploads when writing data to S3 in order to maximize write speed. LanceDB will abort these uploads when it shuts down gracefully, such as when cancelled by keyboard interrupt. However, in the rare case that LanceDB crashes, it is possible that some data will be left lingering in your account. To cleanup this data, we recommend (as AWS themselves do) that you setup a lifecycle rule to delete in-progress uploads after 7 days. See the AWS guide:</p> <p>Configuring a bucket lifecycle configuration to delete incomplete multipart uploads</p>"},{"location":"guides/storage/storage/#aws-iam-permissions","title":"AWS IAM Permissions","text":"<p>If a bucket is private, then an IAM policy must be specified to allow access to it. For many development scenarios, using broad permissions such as a PowerUser account is more than sufficient for working with LanceDB. However, in many production scenarios, you may wish to have as narrow as possible permissions.</p> <p>For read and write access, LanceDB will need a policy such as:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n              \"s3:PutObject\",\n              \"s3:GetObject\",\n              \"s3:DeleteObject\"\n            ],\n            \"Resource\": \"arn:aws:s3:::&lt;bucket&gt;/&lt;prefix&gt;/*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Resource\": \"arn:aws:s3:::&lt;bucket&gt;\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"s3:prefix\": [\n                        \"&lt;prefix&gt;/*\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n</code></pre> <p>For read-only access, LanceDB will need a policy such as:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n              \"s3:GetObject\"\n            ],\n            \"Resource\": \"arn:aws:s3:::&lt;bucket&gt;/&lt;prefix&gt;/*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Resource\": \"arn:aws:s3:::&lt;bucket&gt;\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"s3:prefix\": [\n                        \"&lt;prefix&gt;/*\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"guides/storage/storage/#dynamodb-commit-store-for-concurrent-writes","title":"DynamoDB Commit Store for concurrent writes","text":"<p>By default, S3 does not support concurrent writes. Having two or more processes writing to the same table at the same time can lead to data corruption. This is because S3, unlike other object stores, does not have any atomic put or copy operation.</p> <p>To enable concurrent writes, you can configure LanceDB to use a DynamoDB table as a commit store. This table will be used to coordinate writes between different processes. To enable this feature, you must modify your connection URI to use the <code>s3+ddb</code> scheme and add a query parameter <code>ddbTableName</code> with the name of the table to use.</p> PythonJavaScript Sync APIAsync API <pre><code>import lancedb\ndb = lancedb.connect(\n    \"s3+ddb://bucket/path?ddbTableName=my-dynamodb-table\",\n)\n</code></pre> <pre><code>import lancedb\nasync_db = await lancedb.connect_async(\n    \"s3+ddb://bucket/path?ddbTableName=my-dynamodb-table\",\n)    \n</code></pre> <pre><code>const lancedb = require(\"lancedb\");\n\nconst db = await lancedb.connect(\n    \"s3+ddb://bucket/path?ddbTableName=my-dynamodb-table\",\n);\n</code></pre> <p>The DynamoDB table must be created with the following schema:</p> <ul> <li>Hash key: <code>base_uri</code> (string)</li> <li>Range key: <code>version</code> (number)</li> </ul> <p>You can create this programmatically with:</p> PythonJavaScript <p> <pre><code>import boto3\n\ndynamodb = boto3.client(\"dynamodb\")\ntable = dynamodb.create_table(\n    TableName=table_name,\n    KeySchema=[\n        {\"AttributeName\": \"base_uri\", \"KeyType\": \"HASH\"},\n        {\"AttributeName\": \"version\", \"KeyType\": \"RANGE\"},\n    ],\n    AttributeDefinitions=[\n        {\"AttributeName\": \"base_uri\", \"AttributeType\": \"S\"},\n        {\"AttributeName\": \"version\", \"AttributeType\": \"N\"},\n    ],\n    ProvisionedThroughput={\"ReadCapacityUnits\": 1, \"WriteCapacityUnits\": 1},\n)\n</code></pre></p> <p> <pre><code>import {\n  CreateTableCommand,\n  DynamoDBClient,\n} from \"@aws-sdk/client-dynamodb\";\n\nconst dynamodb = new DynamoDBClient({\n  region: CONFIG.awsRegion,\n  credentials: {\n    accessKeyId: CONFIG.awsAccessKeyId,\n    secretAccessKey: CONFIG.awsSecretAccessKey,\n  },\n  endpoint: CONFIG.awsEndpoint,\n});\nconst command = new CreateTableCommand({\n  TableName: table_name,\n  AttributeDefinitions: [\n    {\n      AttributeName: \"base_uri\",\n      AttributeType: \"S\",\n    },\n    {\n      AttributeName: \"version\",\n      AttributeType: \"N\",\n    },\n  ],\n  KeySchema: [\n    { AttributeName: \"base_uri\", KeyType: \"HASH\" },\n    { AttributeName: \"version\", KeyType: \"RANGE\" },\n  ],\n  ProvisionedThroughput: {\n    ReadCapacityUnits: 1,\n    WriteCapacityUnits: 1,\n  },\n});\nawait client.send(command);\n</code></pre></p>"},{"location":"guides/storage/storage/#s3-compatible-stores","title":"S3-compatible stores","text":"<p>LanceDB can also connect to S3-compatible stores, such as MinIO. To do so, you must specify both region and endpoint:</p> PythonTypeScript Sync APIAsync API <pre><code>import lancedb\ndb = lancedb.connect(\n    \"s3://bucket/path\",\n    storage_options={\n        \"region\": \"us-east-1\",\n        \"endpoint\": \"http://minio:9000\",\n    }\n)\n</code></pre> <pre><code>import lancedb\nasync_db = await lancedb.connect_async(\n    \"s3://bucket/path\",\n    storage_options={\n        \"region\": \"us-east-1\",\n        \"endpoint\": \"http://minio:9000\",\n    }\n)    \n</code></pre> @lancedb/lancedbvectordb (deprecated) <pre><code>import * as lancedb from \"@lancedb/lancedb\";\nconst db = await lancedb.connect(\n    \"s3://bucket/path\",\n    {\n        storageOptions: {\n            region: \"us-east-1\",\n            endpoint: \"http://minio:9000\",\n        }\n    }\n);\n</code></pre> <pre><code>const lancedb = require(\"lancedb\");\nconst db = await lancedb.connect(\n    \"s3://bucket/path\",\n    {\n        storageOptions: {\n            region: \"us-east-1\",\n            endpoint: \"http://minio:9000\",\n        }\n    }\n);\n</code></pre> <p>This can also be done with the <code>AWS_ENDPOINT</code> and <code>AWS_DEFAULT_REGION</code> environment variables.</p> <p>Local servers</p> <p>For local development, the server often has a <code>http</code> endpoint rather than a secure <code>https</code> endpoint. In this case, you must also set the <code>ALLOW_HTTP</code> environment variable to <code>true</code> to allow non-TLS connections, or pass the storage option <code>allow_http</code> as <code>true</code>. If you do not do this, you will get an error like <code>URL scheme is not allowed</code>.</p>"},{"location":"guides/storage/storage/#s3-express","title":"S3 Express","text":"<p>LanceDB supports S3 Express One Zone endpoints, but requires additional infrastructure configuration for the compute service, such as EC2 or Lambda. Please refer to Networking requirements for S3 Express One Zone.</p> <p>To configure LanceDB to use an S3 Express endpoint, you must set the storage option <code>s3_express</code>. The bucket name in your table URI should include the suffix.</p> PythonTypeScript Sync APIAsync API <pre><code>import lancedb\ndb = lancedb.connect(\n    \"s3://my-bucket--use1-az4--x-s3/path\",\n    storage_options={\n        \"region\": \"us-east-1\",\n        \"s3_express\": \"true\",\n    }\n)\n</code></pre> <pre><code>import lancedb\nasync_db = await lancedb.connect_async(\n    \"s3://my-bucket--use1-az4--x-s3/path\",\n    storage_options={\n        \"region\": \"us-east-1\",\n        \"s3_express\": \"true\",\n    }\n)    \n</code></pre> @lancedb/lancedbvectordb (deprecated) <pre><code>import * as lancedb from \"@lancedb/lancedb\";\nconst db = await lancedb.connect(\n    \"s3://my-bucket--use1-az4--x-s3/path\",\n    {\n        storageOptions: {\n            region: \"us-east-1\",\n            s3Express: \"true\",\n        }\n    }\n);\n</code></pre> <pre><code>const lancedb = require(\"lancedb\");\nconst db = await lancedb.connect(\n    \"s3://my-bucket--use1-az4--x-s3/path\",\n    {\n        storageOptions: {\n            region: \"us-east-1\",\n            s3Express: \"true\",\n        }\n    }\n);\n</code></pre>"},{"location":"guides/storage/storage/#google-cloud-storage","title":"Google Cloud Storage","text":"<p>GCS credentials are configured by setting the <code>GOOGLE_SERVICE_ACCOUNT</code> environment variable to the path of a JSON file containing the service account credentials. Alternatively, you can pass the path to the JSON file in the <code>storage_options</code>:</p> PythonTypeScript Sync API <pre><code>import lancedb\ndb = lancedb.connect(\n    \"gs://my-bucket/my-database\",\n    storage_options={\n        \"service_account\": \"path/to/service-account.json\",\n    }\n)\n</code></pre> Async API <pre><code>import lancedb\nasync_db = await lancedb.connect_async(\n    \"gs://my-bucket/my-database\",\n    storage_options={\n        \"service_account\": \"path/to/service-account.json\",\n    }\n)    \n</code></pre> @lancedb/lancedbvectordb (deprecated) <pre><code>import * as lancedb from \"@lancedb/lancedb\";\nconst db = await lancedb.connect(\n    \"gs://my-bucket/my-database\",\n    {\n        storageOptions: {\n            serviceAccount: \"path/to/service-account.json\",\n        }\n    }\n);\n</code></pre> <pre><code>const lancedb = require(\"lancedb\");\nconst db = await lancedb.connect(\n    \"gs://my-bucket/my-database\",\n    {\n        storageOptions: {\n            serviceAccount: \"path/to/service-account.json\",\n        }\n    }\n);\n</code></pre> <p>HTTP/2 support</p> <p>By default, GCS uses HTTP/1 for communication, as opposed to HTTP/2. This improves maximum throughput significantly. However, if you wish to use HTTP/2 for some reason, you can set the environment variable <code>HTTP1_ONLY</code> to <code>false</code>.</p> <p>The following keys can be used as both environment variables or keys in the <code>storage_options</code> parameter:</p> Key Description <code>google_service_account</code> / <code>service_account</code> Path to the service account JSON file. <code>google_service_account_key</code> The serialized service account key. <code>google_application_credentials</code> Path to the application credentials."},{"location":"guides/storage/storage/#azure-blob-storage","title":"Azure Blob Storage","text":"<p>Azure Blob Storage credentials can be configured by setting the <code>AZURE_STORAGE_ACCOUNT_NAME</code>and <code>AZURE_STORAGE_ACCOUNT_KEY</code> environment variables. Alternatively, you can pass the account name and key in the <code>storage_options</code> parameter:</p> PythonTypeScript Sync API <pre><code>import lancedb\ndb = lancedb.connect(\n    \"az://my-container/my-database\",\n    storage_options={\n        account_name: \"some-account\",\n        account_key: \"some-key\",\n    }\n)\n</code></pre> Async API <pre><code>import lancedb\nasync_db = await lancedb.connect_async(\n    \"az://my-container/my-database\",\n    storage_options={\n        account_name: \"some-account\",\n        account_key: \"some-key\",\n    }\n)   \n</code></pre> @lancedb/lancedbvectordb (deprecated) <pre><code>import * as lancedb from \"@lancedb/lancedb\";\nconst db = await lancedb.connect(\n    \"az://my-container/my-database\",\n    {\n        storageOptions: {\n            accountName: \"some-account\",\n            accountKey: \"some-key\",\n        }\n    }\n);\n</code></pre> <pre><code>const lancedb = require(\"lancedb\");\nconst db = await lancedb.connect(\n    \"az://my-container/my-database\",\n    {\n        storageOptions: {\n            accountName: \"some-account\",\n            accountKey: \"some-key\",\n        }\n    }\n);\n</code></pre> <p>These keys can be used as both environment variables or keys in the <code>storage_options</code> parameter:</p> Key Description <code>azure_storage_account_name</code> The name of the azure storage account. <code>azure_storage_account_key</code> The serialized service account key. <code>azure_client_id</code> Service principal client id for authorizing requests. <code>azure_client_secret</code> Service principal client secret for authorizing requests. <code>azure_tenant_id</code> Tenant id used in oauth flows. <code>azure_storage_sas_key</code> Shared access signature. The signature is expected to be percent-encoded, much like they are provided in the azure storage explorer or azure portal. <code>azure_storage_token</code> Bearer token. <code>azure_storage_use_emulator</code> Use object store with azurite storage emulator. <code>azure_endpoint</code> Override the endpoint used to communicate with blob storage. <code>azure_use_fabric_endpoint</code> Use object store with url scheme account.dfs.fabric.microsoft.com. <code>azure_msi_endpoint</code> Endpoint to request a imds managed identity token. <code>azure_object_id</code> Object id for use with managed identity authentication. <code>azure_msi_resource_id</code> Msi resource id for use with managed identity authentication. <code>azure_federated_token_file</code> File containing token for Azure AD workload identity federation. <code>azure_use_azure_cli</code> Use azure cli for acquiring access token. <code>azure_disable_tagging</code> Disables tagging objects. This can be desirable if not supported by the backing store."},{"location":"guides/tables/","title":"Overview of Tables in LanceDB","text":"<p>A Table is a collection of Records in a LanceDB Database. Tables in Lance have a schema that defines the columns and their types. These schemas can include nested columns and can evolve over time.</p>"},{"location":"guides/tables/#key-concepts","title":"Key Concepts","text":"<ul> <li>Tables: The fundamental unit of data organization in LanceDB, containing records with a defined schema</li> <li>Schema: Defines the structure of your data, including column names, data types, and whether fields are required</li> <li>Vector Column: A special column type that stores vector embeddings for similarity search</li> <li>Metadata Columns: Regular columns that store additional information about each record</li> <li>Versioning: Tables maintain versions as they are modified, enabling point-in-time queries</li> </ul>"},{"location":"guides/tables/#operations-overview","title":"Operations Overview","text":"<p>LanceDB provides comprehensive table management capabilities:</p> <ul> <li>Creation: Create tables from various data sources including lists, DataFrames, and Pydantic models</li> <li>Modification: Add, update, or delete data with support for batch operations</li> <li>Schema Management: Add, alter, or drop columns as your data needs evolve</li> <li>Consistency Control: Configure read consistency settings for your use case</li> </ul>"},{"location":"guides/tables/#whats-next","title":"What's next?","text":"<p>Learn about creating tables, modifying data, managing schemas, and consistency settings.</p> <ol> <li> <p>The <code>vectordb</code> package is a legacy package that is deprecated in favor of <code>@lancedb/lancedb</code>. The <code>vectordb</code> package will continue to receive bug fixes and security updates until September 2024. We recommend all new projects use <code>@lancedb/lancedb</code>. See the migration guide for more information.\u00a0\u21a9</p> </li> </ol>"},{"location":"guides/tables/consistency/","title":"Consistency in LanceDB","text":"<p>In LanceDB OSS, users can set the <code>read_consistency_interval</code> parameter on connections to achieve different levels of read consistency. This parameter determines how frequently the database synchronizes with the underlying storage system to check for updates made by other processes. If another process updates a table, the database will not see the changes until the next synchronization.</p> <p>There are three possible settings for <code>read_consistency_interval</code>:</p> <ol> <li>Unset (default): The database does not check for updates to tables made by other processes. This provides the best query performance, but means that clients may not see the most up-to-date data. This setting is suitable for applications where the data does not change during the lifetime of the table reference.</li> <li>Zero seconds (Strong consistency): The database checks for updates on every read. This provides the strongest consistency guarantees, ensuring that all clients see the latest committed data. However, it has the most overhead. This setting is suitable when consistency matters more than having high QPS.</li> <li>Custom interval (Eventual consistency): The database checks for updates at a custom interval, such as every 5 seconds. This provides eventual consistency, allowing for some lag between write and read operations. Performance wise, this is a middle ground between strong consistency and no consistency check. This setting is suitable for applications where immediate consistency is not critical, but clients should see updated data eventually.</li> </ol> <p>Consistency in LanceDB Cloud</p> <p>This is only tune-able in LanceDB OSS. In LanceDB Cloud, readers are always eventually consistent.</p> PythonTypescript<sup>1</sup> <p>To set strong consistency, use <code>timedelta(0)</code>:</p> Sync APIAsync API <pre><code>from datetime import timedelta\n\nuri = \"data/sample-lancedb\"\ndb = lancedb.connect(uri, read_consistency_interval=timedelta(0))\ntbl = db.open_table(\"test_table\")\n</code></pre> <pre><code>from datetime import timedelta\n\nuri = \"data/sample-lancedb\"\nasync_db = await lancedb.connect_async(uri, read_consistency_interval=timedelta(0))\nasync_tbl = await async_db.open_table(\"test_table_async\")\n</code></pre> <p>For eventual consistency, use a custom <code>timedelta</code>:</p> Sync APIAsync API <pre><code>from datetime import timedelta\n\nuri = \"data/sample-lancedb\"\ndb = lancedb.connect(uri, read_consistency_interval=timedelta(seconds=5))\ntbl = db.open_table(\"test_table\")\n</code></pre> <pre><code>from datetime import timedelta\n\nuri = \"data/sample-lancedb\"\nasync_db = await lancedb.connect_async(\n    uri, read_consistency_interval=timedelta(seconds=5)\n)\nasync_tbl = await async_db.open_table(\"test_table_async\")\n</code></pre> <p>By default, a <code>Table</code> will never check for updates from other writers. To manually check for updates you can use <code>checkout_latest</code>:</p> Sync APIAsync API <pre><code>tbl = db.open_table(\"test_table\")\n\n# (Other writes happen to my_table from another process)\n\n# Check for updates\ntbl.checkout_latest()\n</code></pre> <pre><code>async_tbl = await async_db.open_table(\"test_table_async\")\n\n# (Other writes happen to test_table_async from another process)\n\n# Check for updates\nawait async_tbl.checkout_latest()\n</code></pre> <p>To set strong consistency, use <code>0</code>:</p> <pre><code>const db = await lancedb.connect({ uri: \"./.lancedb\", readConsistencyInterval: 0 });\nconst tbl = await db.openTable(\"my_table\");\n</code></pre> <p>For eventual consistency, specify the update interval as seconds:</p> <pre><code>const db = await lancedb.connect({ uri: \"./.lancedb\", readConsistencyInterval: 5 });\nconst tbl = await db.openTable(\"my_table\");\n</code></pre>"},{"location":"guides/tables/consistency/#whats-next","title":"What's next?","text":"<p>Learn about vector indexing to optimize your vector search performance.</p> <ol> <li> <p>The <code>vectordb</code> package is a legacy package that is deprecated in favor of <code>@lancedb/lancedb</code>. The <code>vectordb</code> package will continue to receive bug fixes and security updates until September 2024. We recommend all new projects use <code>@lancedb/lancedb</code>. See the migration guide for more information.\u00a0\u21a9</p> </li> </ol>"},{"location":"guides/tables/create/","title":"Creating Tables in LanceDB","text":"<p>Initialize a LanceDB connection and create a table</p> Python Sync APIAsync API <pre><code>import lancedb\n\nuri = \"data/sample-lancedb\"\ndb = lancedb.connect(uri)\n</code></pre> <pre><code>import lancedb\n\nuri = \"data/sample-lancedb\"\nasync_db = await lancedb.connect_async(uri)\n</code></pre> <p>LanceDB allows ingesting data from various sources - <code>dict</code>, <code>list[dict]</code>, <code>pd.DataFrame</code>, <code>pa.Table</code> or a <code>Iterator[pa.RecordBatch]</code>. Let's take a look at some of these.</p>"},{"location":"guides/tables/create/#open-existing-tables","title":"Open existing tables","text":"PythonTypescript<sup>1</sup> <p>If you forget the name of your table, you can always get a listing of all table names.</p> Sync APIAsync API <pre><code>print(db.table_names())\n</code></pre> <pre><code>print(await async_db.table_names())\n</code></pre> <p>Then, you can open any existing tables.</p> Sync APIAsync API <pre><code>tbl = db.open_table(\"test_table\")\n</code></pre> <pre><code>async_tbl = await async_db.open_table(\"test_table_async\")\n</code></pre> <p>If you forget the name of your table, you can always get a listing of all table names.</p> <pre><code>console.log(await db.tableNames());\n</code></pre> <p>Then, you can open any existing tables.</p> <pre><code>const tbl = await db.openTable(\"my_table\");\n</code></pre>"},{"location":"guides/tables/create/#from-list-of-tuples-or-dictionaries","title":"From list of tuples or dictionaries","text":"PythonTypescript<sup>1</sup> Sync APIAsync API <pre><code>data = [\n    {\"vector\": [1.1, 1.2], \"lat\": 45.5, \"long\": -122.7},\n    {\"vector\": [0.2, 1.8], \"lat\": 40.1, \"long\": -74.1},\n]\ndb.create_table(\"test_table\", data)\ndb[\"test_table\"].head()\n</code></pre> <pre><code>data = [\n    {\"vector\": [1.1, 1.2], \"lat\": 45.5, \"long\": -122.7},\n    {\"vector\": [0.2, 1.8], \"lat\": 40.1, \"long\": -74.1},\n]\nasync_tbl = await async_db.create_table(\"test_table_async\", data)\nawait async_tbl.head()\n</code></pre> <p>Note</p> <p>If the table already exists, LanceDB will raise an error by default.</p> <p><code>create_table</code> supports an optional <code>exist_ok</code> parameter. When set to True and the table exists, then it simply opens the existing table. The data you passed in will NOT be appended to the table in that case.</p> Sync APIAsync API <pre><code>db.create_table(\"test_table\", data, exist_ok=True)\n</code></pre> <pre><code>await async_db.create_table(\"test_table_async\", data, exist_ok=True)\n</code></pre> <p>Sometimes you want to make sure that you start fresh. If you want to overwrite the table, you can pass in mode=\"overwrite\" to the createTable function.</p> Sync APIAsync API <pre><code>db.create_table(\"test_table\", data, mode=\"overwrite\")\n</code></pre> <pre><code>await async_db.create_table(\"test_table_async\", data, mode=\"overwrite\")\n</code></pre> <p>You can create a LanceDB table in JavaScript using an array of records as follows.</p> @lancedb/lancedbvectordb (deprecated) <pre><code>const _tbl = await db.createTable(\n  \"myTable\",\n  [\n    { vector: [3.1, 4.1], item: \"foo\", price: 10.0 },\n    { vector: [5.9, 26.5], item: \"bar\", price: 20.0 },\n  ],\n  { mode: \"overwrite\" },\n);\n</code></pre> <p>This will infer the schema from the provided data. If you want to explicitly provide a schema, you can use <code>apache-arrow</code> to declare a schema</p> <pre><code>const schema = new arrow.Schema([\n  new arrow.Field(\n    \"vector\",\n    new arrow.FixedSizeList(\n      2,\n      new arrow.Field(\"item\", new arrow.Float32(), true),\n    ),\n  ),\n  new arrow.Field(\"item\", new arrow.Utf8(), true),\n  new arrow.Field(\"price\", new arrow.Float32(), true),\n]);\nconst data = [\n  { vector: [3.1, 4.1], item: \"foo\", price: 10.0 },\n  { vector: [5.9, 26.5], item: \"bar\", price: 20.0 },\n];\nconst tbl = await db.createTable(\"myTable\", data, {\n  schema,\n});\n</code></pre> <p>Note</p> <p><code>createTable</code> supports an optional <code>existsOk</code> parameter. When set to true and the table exists, then it simply opens the existing table. The data you passed in will NOT be appended to the table in that case.</p> <pre><code>const tbl = await db.createTable(\"myTable\", data, {\n  existOk: true,\n});\n</code></pre> <p>Sometimes you want to make sure that you start fresh. If you want to overwrite the table, you can pass in mode: \"overwrite\" to the createTable function.</p> <pre><code>const tbl = await db.createTable(\"myTable\", data, {\n  mode: \"overwrite\",\n});\n</code></pre> <pre><code>const tbl = await db.createTable(\n  \"myTable\",\n  [\n    { vector: [3.1, 4.1], item: \"foo\", price: 10.0 },\n    { vector: [5.9, 26.5], item: \"bar\", price: 20.0 },\n  ],\n  { writeMode: lancedb.WriteMode.Overwrite },\n);\n</code></pre> <p>This will infer the schema from the provided data. If you want to explicitly provide a schema, you can use apache-arrow to declare a schema</p> <pre><code>const schema = new arrow.Schema([\n  new arrow.Field(\n    \"vector\",\n    new arrow.FixedSizeList(\n      2,\n      new arrow.Field(\"item\", new arrow.Float32(), true),\n    ),\n  ),\n  new arrow.Field(\"item\", new arrow.Utf8(), true),\n  new arrow.Field(\"price\", new arrow.Float32(), true),\n]);\nconst data = [\n  { vector: [3.1, 4.1], item: \"foo\", price: 10.0 },\n  { vector: [5.9, 26.5], item: \"bar\", price: 20.0 },\n];\nconst tbl = await db.createTable({\n  name: \"myTableWithSchema\",\n  data,\n  schema,\n});\n</code></pre> <p>Warning</p> <p><code>existsOk</code> is not available in <code>vectordb</code></p> <p>If the table already exists, vectordb will raise an error by default. You can use <code>writeMode: WriteMode.Overwrite</code> to overwrite the table. But this will delete the existing table and create a new one with the same name.</p> <p>Sometimes you want to make sure that you start fresh.</p> <p>If you want to overwrite the table, you can pass in <code>writeMode: lancedb.WriteMode.Overwrite</code> to the createTable function.</p> <pre><code>const table = await con.createTable(tableName, data, {\n    writeMode: WriteMode.Overwrite\n})\n</code></pre>"},{"location":"guides/tables/create/#from-a-pandas-dataframe","title":"From a Pandas DataFrame","text":"Sync APIAsync API <pre><code>import pandas as pd\n\ndata = pd.DataFrame(\n    {\n        \"vector\": [[1.1, 1.2, 1.3, 1.4], [0.2, 1.8, 0.4, 3.6]],\n        \"lat\": [45.5, 40.1],\n        \"long\": [-122.7, -74.1],\n    }\n)\ndb.create_table(\"my_table_pandas\", data)\ndb[\"my_table_pandas\"].head()\n</code></pre> <pre><code>import pandas as pd\n\ndata = pd.DataFrame(\n    {\n        \"vector\": [[1.1, 1.2, 1.3, 1.4], [0.2, 1.8, 0.4, 3.6]],\n        \"lat\": [45.5, 40.1],\n        \"long\": [-122.7, -74.1],\n    }\n)\nasync_tbl = await async_db.create_table(\"my_table_async_pd\", data)\nawait async_tbl.head()\n</code></pre> <p>Note</p> <p>Data is converted to Arrow before being written to disk. For maximum control over how data is saved, either provide the PyArrow schema to convert to or else provide a PyArrow Table directly.</p> <p>The <code>vector</code> column needs to be a Vector (defined as pyarrow.FixedSizeList) type.</p> Sync APIAsync API <pre><code>import pyarrow as pa\n\ncustom_schema = pa.schema(\n    [\n        pa.field(\"vector\", pa.list_(pa.float32(), 4)),\n        pa.field(\"lat\", pa.float32()),\n        pa.field(\"long\", pa.float32()),\n    ]\n)\n\ntbl = db.create_table(\"my_table_custom_schema\", data, schema=custom_schema)\n</code></pre> <pre><code>import pyarrow as pa\n\ncustom_schema = pa.schema(\n    [\n        pa.field(\"vector\", pa.list_(pa.float32(), 4)),\n        pa.field(\"lat\", pa.float32()),\n        pa.field(\"long\", pa.float32()),\n    ]\n)\nasync_tbl = await async_db.create_table(\n    \"my_table_async_custom_schema\", data, schema=custom_schema\n)\n</code></pre>"},{"location":"guides/tables/create/#from-a-polars-dataframe","title":"From a Polars DataFrame","text":"<p>LanceDB supports Polars, a modern, fast DataFrame library written in Rust. Just like in Pandas, the Polars integration is enabled by PyArrow under the hood. A deeper integration between LanceDB Tables and Polars DataFrames is on the way.</p> Sync APIAsync API <pre><code>import polars as pl\n\ndata = pl.DataFrame(\n    {\n        \"vector\": [[3.1, 4.1], [5.9, 26.5]],\n        \"item\": [\"foo\", \"bar\"],\n        \"price\": [10.0, 20.0],\n    }\n)\ntbl = db.create_table(\"my_table_pl\", data)\n</code></pre> <pre><code>import polars as pl\n\ndata = pl.DataFrame(\n    {\n        \"vector\": [[3.1, 4.1], [5.9, 26.5]],\n        \"item\": [\"foo\", \"bar\"],\n        \"price\": [10.0, 20.0],\n    }\n)\nasync_tbl = await async_db.create_table(\"my_table_async_pl\", data)\n</code></pre>"},{"location":"guides/tables/create/#from-an-arrow-table","title":"From an Arrow Table","text":"<p>You can also create LanceDB tables directly from Arrow tables. LanceDB supports float16 data type!</p> PythonTypescript<sup>1</sup> Sync APIAsync API <pre><code>import pyarrow as pa\n\nimport numpy as np\n\ndim = 16\ntotal = 2\nschema = pa.schema(\n    [pa.field(\"vector\", pa.list_(pa.float16(), dim)), pa.field(\"text\", pa.string())]\n)\ndata = pa.Table.from_arrays(\n    [\n        pa.array(\n            [np.random.randn(dim).astype(np.float16) for _ in range(total)],\n            pa.list_(pa.float16(), dim),\n        ),\n        pa.array([\"foo\", \"bar\"]),\n    ],\n    [\"vector\", \"text\"],\n)\ntbl = db.create_table(\"f16_tbl\", data, schema=schema)\n</code></pre> <pre><code>import polars as pl\n\nimport numpy as np\n\ndim = 16\ntotal = 2\nschema = pa.schema(\n    [pa.field(\"vector\", pa.list_(pa.float16(), dim)), pa.field(\"text\", pa.string())]\n)\ndata = pa.Table.from_arrays(\n    [\n        pa.array(\n            [np.random.randn(dim).astype(np.float16) for _ in range(total)],\n            pa.list_(pa.float16(), dim),\n        ),\n        pa.array([\"foo\", \"bar\"]),\n    ],\n    [\"vector\", \"text\"],\n)\nasync_tbl = await async_db.create_table(\"f16_tbl_async\", data, schema=schema)\n</code></pre> @lancedb/lancedbvectordb (deprecated) <pre><code>const db = await lancedb.connect(databaseDir);\nconst dim = 16;\nconst total = 10;\nconst f16Schema = new Schema([\n  new Field(\"id\", new Int32()),\n  new Field(\n    \"vector\",\n    new FixedSizeList(dim, new Field(\"item\", new Float16(), true)),\n    false,\n  ),\n]);\nconst data = lancedb.makeArrowTable(\n  Array.from(Array(total), (_, i) =&gt; ({\n    id: i,\n    vector: Array.from(Array(dim), Math.random),\n  })),\n  { schema: f16Schema },\n);\nconst _table = await db.createTable(\"f16_tbl\", data);\n</code></pre> <pre><code>const dim = 16;\nconst total = 10;\nconst schema = new Schema([\n  new Field(\"id\", new Int32()),\n  new Field(\n    \"vector\",\n    new FixedSizeList(dim, new Field(\"item\", new Float16(), true)),\n    false,\n  ),\n]);\nconst data = lancedb.makeArrowTable(\n  Array.from(Array(total), (_, i) =&gt; ({\n    id: i,\n    vector: Array.from(Array(dim), Math.random),\n  })),\n  { schema },\n);\nconst table = await db.createTable(\"f16_tbl\", data);\n</code></pre>"},{"location":"guides/tables/create/#from-pydantic-models","title":"From Pydantic Models","text":"<p>When you create an empty table without data, you must specify the table schema. LanceDB supports creating tables by specifying a PyArrow schema or a specialized Pydantic model called <code>LanceModel</code>.</p> <p>For example, the following Content model specifies a table with 5 columns: <code>movie_id</code>, <code>vector</code>, <code>genres</code>, <code>title</code>, and <code>imdb_id</code>. When you create a table, you can pass the class as the value of the <code>schema</code> parameter to <code>create_table</code>. The <code>vector</code> column is a <code>Vector</code> type, which is a specialized Pydantic type that can be configured with the vector dimensions. It is also important to note that LanceDB only understands subclasses of <code>lancedb.pydantic.LanceModel</code> (which itself derives from <code>pydantic.BaseModel</code>).</p> Sync APIAsync API <pre><code>from lancedb.pydantic import Vector, LanceModel\n\nimport pyarrow as pa\n\nclass Content(LanceModel):\n    movie_id: int\n    vector: Vector(128)\n    genres: str\n    title: str\n    imdb_id: int\n\n    @property\n    def imdb_url(self) -&gt; str:\n        return f\"https://www.imdb.com/title/tt{self.imdb_id}\"\n\n\ntbl = db.create_table(\"movielens_small\", schema=Content)\n</code></pre> <pre><code>from lancedb.pydantic import Vector, LanceModel\n\nimport pyarrow as pa\n\nclass Content(LanceModel):\n    movie_id: int\n    vector: Vector(128)\n    genres: str\n    title: str\n    imdb_id: int\n\n    @property\n    def imdb_url(self) -&gt; str:\n        return f\"https://www.imdb.com/title/tt{self.imdb_id}\"\n\n\nasync_tbl = await async_db.create_table(\"movielens_small_async\", schema=Content)\n</code></pre>"},{"location":"guides/tables/create/#nested-schemas","title":"Nested schemas","text":"<p>Sometimes your data model may contain nested objects. For example, you may want to store the document string and the document source name as a nested Document object:</p> <pre><code>from pydantic import BaseModel\n\nclass Document(BaseModel):\n    content: str\n    source: str\n</code></pre> <p>This can be used as the type of a LanceDB table column:</p> Sync APIAsync API <pre><code>class NestedSchema(LanceModel):\n    id: str\n    vector: Vector(1536)\n    document: Document\n\n\ntbl = db.create_table(\"nested_table\", schema=NestedSchema)\n</code></pre> <pre><code>class NestedSchema(LanceModel):\n    id: str\n    vector: Vector(1536)\n    document: Document\n\n\nasync_tbl = await async_db.create_table(\"nested_table_async\", schema=NestedSchema)\n</code></pre> <p>This creates a struct column called \"document\" that has two subfields called \"content\" and \"source\":</p> <pre><code>In [28]: tbl.schema\nOut[28]:\nid: string not null\nvector: fixed_size_list&lt;item: float&gt;[1536] not null\n    child 0, item: float\ndocument: struct&lt;content: string not null, source: string not null&gt; not null\n    child 0, content: string not null\n    child 1, source: string not null\n</code></pre>"},{"location":"guides/tables/create/#validators","title":"Validators","text":"<p>Note that neither Pydantic nor PyArrow automatically validates that input data is of the correct timezone, but this is easy to add as a custom field validator:</p> <pre><code>from datetime import datetime\nfrom zoneinfo import ZoneInfo\n\nfrom lancedb.pydantic import LanceModel\nfrom pydantic import Field, field_validator, ValidationError, ValidationInfo\n\ntzname = \"America/New_York\"\ntz = ZoneInfo(tzname)\n\nclass TestModel(LanceModel):\n    dt_with_tz: datetime = Field(json_schema_extra={\"tz\": tzname})\n\n    @field_validator('dt_with_tz')\n    @classmethod\n    def tz_must_match(cls, dt: datetime) -&gt; datetime:\n        assert dt.tzinfo == tz\n        return dt\n\nok = TestModel(dt_with_tz=datetime.now(tz))\n\ntry:\n    TestModel(dt_with_tz=datetime.now(ZoneInfo(\"Asia/Shanghai\")))\n    assert 0 == 1, \"this should raise ValidationError\"\nexcept ValidationError:\n    print(\"A ValidationError was raised.\")\n    pass\n</code></pre> <p>When you run this code it should print \"A ValidationError was raised.\"</p>"},{"location":"guides/tables/create/#pydantic-custom-types","title":"Pydantic custom types","text":"<p>LanceDB does NOT yet support converting pydantic custom types. If this is something you need, please file a feature request on the LanceDB Github repo.</p>"},{"location":"guides/tables/create/#using-iterators-writing-large-datasets","title":"Using Iterators / Writing Large Datasets","text":"<p>It is recommended to use iterators to add large datasets in batches when creating your table in one go. This does not create multiple versions of your dataset unlike manually adding batches using <code>table.add()</code></p> <p>LanceDB additionally supports PyArrow's <code>RecordBatch</code> Iterators or other generators producing supported data types.</p> <p>Here's an example using using <code>RecordBatch</code> iterator for creating tables.</p> Sync APIAsync API <pre><code>import pyarrow as pa\n\ndef make_batches():\n    for i in range(5):\n        yield pa.RecordBatch.from_arrays(\n            [\n                pa.array(\n                    [[3.1, 4.1, 5.1, 6.1], [5.9, 26.5, 4.7, 32.8]],\n                    pa.list_(pa.float32(), 4),\n                ),\n                pa.array([\"foo\", \"bar\"]),\n                pa.array([10.0, 20.0]),\n            ],\n            [\"vector\", \"item\", \"price\"],\n        )\n\n\nschema = pa.schema(\n    [\n        pa.field(\"vector\", pa.list_(pa.float32(), 4)),\n        pa.field(\"item\", pa.utf8()),\n        pa.field(\"price\", pa.float32()),\n    ]\n)\ndb.create_table(\"batched_tale\", make_batches(), schema=schema)\n</code></pre> <pre><code>import pyarrow as pa\n\ndef make_batches():\n    for i in range(5):\n        yield pa.RecordBatch.from_arrays(\n            [\n                pa.array(\n                    [[3.1, 4.1, 5.1, 6.1], [5.9, 26.5, 4.7, 32.8]],\n                    pa.list_(pa.float32(), 4),\n                ),\n                pa.array([\"foo\", \"bar\"]),\n                pa.array([10.0, 20.0]),\n            ],\n            [\"vector\", \"item\", \"price\"],\n        )\n\n\nschema = pa.schema(\n    [\n        pa.field(\"vector\", pa.list_(pa.float32(), 4)),\n        pa.field(\"item\", pa.utf8()),\n        pa.field(\"price\", pa.float32()),\n    ]\n)\nawait async_db.create_table(\"batched_table\", make_batches(), schema=schema)\n</code></pre> <p>You can also use iterators of other types like Pandas DataFrame or Pylists directly in the above example.</p>"},{"location":"guides/tables/create/#creating-empty-table","title":"Creating empty table","text":"<p>You can create an empty table for scenarios where you want to add data to the table later. An example would be when you want to collect data from a stream/external file and then add it to a table in batches.</p> PythonTypescript<sup>1</sup> <p>An empty table can be initialized via a PyArrow schema.</p> Sync APIAsync API <pre><code>import lancedb\n\nimport pyarrow as pa\n\nschema = pa.schema(\n    [\n        pa.field(\"vector\", pa.list_(pa.float32(), 2)),\n        pa.field(\"item\", pa.string()),\n        pa.field(\"price\", pa.float32()),\n    ]\n)\ntbl = db.create_table(\"test_empty_table\", schema=schema)\n</code></pre> <pre><code>import lancedb\n\nimport pyarrow as pa\n\nschema = pa.schema(\n    [\n        pa.field(\"vector\", pa.list_(pa.float32(), 2)),\n        pa.field(\"item\", pa.string()),\n        pa.field(\"price\", pa.float32()),\n    ]\n)\nasync_tbl = await async_db.create_table(\"test_empty_table_async\", schema=schema)\n</code></pre> <p>Alternatively, you can also use Pydantic to specify the schema for the empty table. Note that we do not directly import <code>pydantic</code> but instead use <code>lancedb.pydantic</code> which is a subclass of <code>pydantic.BaseModel</code> that has been extended to support LanceDB specific types like <code>Vector</code>.</p> Sync APIAsync API <pre><code>import lancedb\n\nfrom lancedb.pydantic import Vector, LanceModel\n\nclass Item(LanceModel):\n    vector: Vector(2)\n    item: str\n    price: float\n\n\ntbl = db.create_table(\"test_empty_table_new\", schema=Item.to_arrow_schema())\n</code></pre> <pre><code>import lancedb\n\nfrom lancedb.pydantic import Vector, LanceModel\n\nclass Item(LanceModel):\n    vector: Vector(2)\n    item: str\n    price: float\n\n\nasync_tbl = await async_db.create_table(\n    \"test_empty_table_async_new\", schema=Item.to_arrow_schema()\n)\n</code></pre> @lancedb/lancedbvectordb (deprecated) <pre><code>const schema = new arrow.Schema([\n  new arrow.Field(\"id\", new arrow.Int32()),\n  new arrow.Field(\"name\", new arrow.Utf8()),\n]);\n\nconst emptyTbl = await db.createEmptyTable(\"empty_table\", schema);\n</code></pre> <pre><code>const schema = new arrow.Schema([\n  new arrow.Field(\"id\", new arrow.Int32()),\n  new arrow.Field(\"name\", new arrow.Utf8()),\n]);\n\nconst empty_tbl = await db.createTable({ name: \"empty_table\", schema });\n</code></pre>"},{"location":"guides/tables/create/#whats-next","title":"What's next?","text":"<p>Learn about modifying data in your tables.</p> <ol> <li> <p>The <code>vectordb</code> package is a legacy package that is deprecated in favor of <code>@lancedb/lancedb</code>. The <code>vectordb</code> package will continue to receive bug fixes and security updates until September 2024. We recommend all new projects use <code>@lancedb/lancedb</code>. See the migration guide for more information.\u00a0\u21a9\u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"guides/tables/ingestion/","title":"Ingesting Data with LanceDB","text":"<p>We support high-throughput writes, comfortably handling 4GB per second.  Our client SDK maintains 1:1 parity with the open-source version,  enabling existing users to migrate seamlessly\u2014zero refactoring required.</p> <p>LanceDB supports table creation using multiple data formats, including:</p> <ul> <li>Pandas DataFrames (example below)</li> <li>Polars DataFrames </li> <li>Apache Arrow Tables</li> </ul> <p>For the Python SDK, you can also define tables flexibly using:</p> <ul> <li>PyArrow schemas (for explicit schema control)</li> <li><code>LanceModel</code> (a Pydantic-based model for structured data validation and serialization)</li> </ul> <p>This ensures compatibility with modern data workflows while maintaining performance and type safety.</p>"},{"location":"guides/tables/ingestion/#insert-data","title":"Insert data","text":"PythonTypeScript <pre><code>import lancedb\nimport pyarrow as pa\n\n# connect to LanceDB Cloud\ndb = lancedb.connect(\n  uri=\"db://your-project-slug\",\n  api_key=\"your-api-key\",\n  region=\"us-east-1\"\n)\n\n# create an empty table with schema\ndata = [\n    {\"vector\": [3.1, 4.1], \"item\": \"foo\", \"price\": 10.0},\n    {\"vector\": [5.9, 26.5], \"item\": \"bar\", \"price\": 20.0},\n    {\"vector\": [10.2, 100.8], \"item\": \"baz\", \"price\": 30.0},\n    {\"vector\": [1.4, 9.5], \"item\": \"fred\", \"price\": 40.0},\n]\n\nschema = pa.schema([\n    pa.field(\"vector\", pa.list_(pa.float32(), 2)),\n    pa.field(\"item\", pa.utf8()),\n    pa.field(\"price\", pa.float32()),\n])\n\ntable_name = \"basic_ingestion_example\"\ntable = db.create_table(table_name, schema=schema, mode=\"overwrite\")\ntable.add(data)\n</code></pre> <pre><code>import * as lancedb from \"@lancedb/lancedb\"\nimport { Schema, Field, Float32, FixedSizeList, Utf8 } from \"apache-arrow\";\n\nconst db = await lancedb.connect({\n  uri: \"db://your-project-slug\",\n  apiKey: \"your-api-key\",\n  region: \"us-east-1\"\n});\n\nconsole.log(\"Creating table from JavaScript objects\");\nconst data = [\n    { vector: [3.1, 4.1], item: \"foo\", price: 10.0 },\n    { vector: [5.9, 26.5], item: \"bar\", price: 20.0 },\n    { vector: [10.2, 100.8], item: \"baz\", price: 30.0},\n    { vector: [1.4, 9.5], item: \"fred\", price: 40.0},\n]\n\nconst tableName = \"js_objects_example\";\nconst table = await db.createTable(tableName, data, {\n    mode: \"overwrite\"\n});\n\nconsole.log(\"\\nCreating a table with a predefined schema then add data to it\");\nconst tableName = \"schema_example\";\n\n// Define schema\n// create an empty table with schema\nconst schema = new Schema([\n    new Field(\n    \"vector\",\n    new FixedSizeList(2, new Field(\"float32\", new Float32())),\n    ),\n    new Field(\"item\", new Utf8()),\n    new Field(\"price\", new Float32()),\n]);\n\n// Create an empty table with schema\nconst table = await db.createEmptyTable(tableName, schema, {\n    mode: \"overwrite\",\n});\n\n// Add data to the schema-defined table\nconst data = [\n    { vector: [3.1, 4.1], item: \"foo\", price: 10.0 },\n    { vector: [5.9, 26.5], item: \"bar\", price: 20.0 },\n    { vector: [10.2, 100.8], item: \"baz\", price: 30.0},\n    { vector: [1.4, 9.5], item: \"fred\", price: 40.0},\n]\n\nawait table.add(data);\n</code></pre> <p>Vector Column Type</p> <p>The vector column needs to be a pyarrow.FixedSizeList type.</p>"},{"location":"guides/tables/ingestion/#using-pydantic-models","title":"Using Pydantic Models","text":"<pre><code>from lancedb.pydantic import Vector, LanceModel\n\nimport pyarrow as pa\n\n# Define a Pydantic model\nclass Content(LanceModel):\n    movie_id: int\n    vector: Vector(128)\n    genres: str\n    title: str\n    imdb_id: int\n\n    @property\n    def imdb_url(self) -&gt; str:\n        return f\"https://www.imdb.com/title/tt{self.imdb_id}\"\n\n# Create table with Pydantic model schema\ntable_name = \"pydantic_example\"\ntable = db.create_table(table_name, schema=Content, mode=\"overwrite\")\n</code></pre>"},{"location":"guides/tables/ingestion/#using-nested-models","title":"Using Nested Models","text":"<p>You can use nested Pydantic models to represent complex data structures.  For example, you may want to store the document string and the document source name as a nested Document object:</p> <pre><code>from pydantic import BaseModel\n\nclass Document(BaseModel):\n    content: str\n    source: str\n</code></pre> <p>This can be used as the type of a LanceDB table column:</p> <pre><code>class NestedSchema(LanceModel):\n    id: str\n    vector: Vector(128)\n    document: Document\n\n# Create table with nested schema\ntable_name = \"nested_model_example\"\ntable = db.create_table(table_name, schema=NestedSchema, mode=\"overwrite\")\n</code></pre> <p>This creates a struct column called <code>document</code> that has two subfields called <code>content</code> and <code>source</code>:</p> <pre><code>In [28]: table.schema\nOut[28]:\nid: string not null\nvector: fixed_size_list&lt;item: float&gt;[128] not null\n    child 0, item: float\ndocument: struct&lt;content: string not null, source: string not null&gt; not null\n    child 0, content: string not null\n    child 1, source: string not null\n</code></pre>"},{"location":"guides/tables/ingestion/#insert-large-datasets","title":"Insert large datasets","text":"<p>It is recommended to use itertators to add large datasets in batches when creating  your table in one go. Data will be automatically compacted for the best query performance.</p> PythonTypeScript <pre><code>import pyarrow as pa\n\ndef make_batches():\n    for i in range(5):  # Create 3 batches\n        yield pa.RecordBatch.from_arrays(\n            [\n                pa.array([[3.1, 4.1], [5.9, 26.5]],\n                        pa.list_(pa.float32(), 2)),\n                pa.array([f\"item{i*2+1}\", f\"item{i*2+2}\"]),\n                pa.array([float((i*2+1)*10), float((i*2+2)*10)]),\n            ],\n            [\"vector\", \"item\", \"price\"],\n        )\n\nschema = pa.schema([\n    pa.field(\"vector\", pa.list_(pa.float32(), 2)),\n    pa.field(\"item\", pa.utf8()),\n    pa.field(\"price\", pa.float32()),\n])\n# Create table with batches\ntable_name = \"batch_ingestion_example\"\ntable = db.create_table(table_name, make_batches(), schema=schema, mode=\"overwrite\")\n</code></pre> <pre><code>console.log(\"\\nBatch ingestion example with product catalog data\");\nconst tableName = \"product_catalog\";\n\n// Vector dimension for product embeddings (realistic dimension for text embeddings)\nconst vectorDim = 128;\n\n// Create random embedding vector of specified dimension\nconst createRandomEmbedding = (dim: number) =&gt; Array(dim).fill(0).map(() =&gt; Math.random() * 2 - 1);\n\n// Create table with initial batch of products\nconst initialBatch = Array(10).fill(0).map((_, i) =&gt; ({\n    product_id: `PROD-${1000 + i}`,\n    name: `Product ${i + 1}`,\n    category: [\"electronics\", \"home\", \"office\"][i % 3],\n    price: 10.99 + (i * 5.99),\n    vector: createRandomEmbedding(vectorDim)\n}));\n\nconst table = await db.createTable(tableName, initialBatch, { \n    mode: \"overwrite\"\n});\n\n// Second batch - 25 more products\nconst batch2 = Array(25).fill(0).map((_, i) =&gt; ({\n    product_id: `PROD-${2000 + i}`,\n    name: `Premium Product ${i + 1}`,\n    category: [\"electronics\", \"kitchen\", \"outdoor\", \"office\", \"gaming\"][i % 5],\n    price: 25.99 + (i * 7.49),\n    vector: createRandomEmbedding(vectorDim)\n}));\n\nawait table.add(batch2);\n\n// Third batch - 15 more products in a different category\nconst batch3 = Array(15).fill(0).map((_, i) =&gt; ({\n    product_id: `PROD-${3000 + i}`,\n    name: `Budget Product ${i + 1}`,\n    category: [\"essentials\", \"budget\", \"basics\"][i % 3],\n    price: 5.99 + (i * 2.50),\n    vector: createRandomEmbedding(vectorDim)\n}));\n\nawait table.add(batch3);\n</code></pre> <p>Explore full documentation in our SDK guides: Python and Typescript.</p> <p>title: \"Data Ingestion in LanceDB | Data Loading Guide\" description: \"Learn how to ingest data into LanceDB. Includes batch loading, streaming ingestion, and best practices for data management.\"</p>"},{"location":"guides/tables/ingestion/#ingesting-data-with-lancedb_1","title":"Ingesting Data with LanceDB","text":"<p>We support high-throughput writes, comfortably handling 4GB per second.  Our client SDK maintains 1:1 parity with the open-source version,  enabling existing users to migrate seamlessly\u2014zero refactoring required.</p> <p>LanceDB supports table creation using multiple data formats, including:</p> <ul> <li>Pandas DataFrames (example below)</li> <li>Polars DataFrames </li> <li>Apache Arrow Tables</li> </ul> <p>For the Python SDK, you can also define tables flexibly using:</p> <ul> <li>PyArrow schemas (for explicit schema control)</li> <li><code>LanceModel</code> (a Pydantic-based model for structured data validation and serialization)</li> </ul> <p>This ensures compatibility with modern data workflows while maintaining performance and type safety.</p>"},{"location":"guides/tables/ingestion/#ingest-data","title":"Ingest Data","text":"PythonTypeScript <pre><code>import lancedb\nimport pyarrow as pa\n\n# connect to LanceDB Cloud\ndb = lancedb.connect(\n  uri=\"db://your-project-slug\",\n  api_key=\"your-api-key\",\n  region=\"us-east-1\"\n)\n\n# create an empty table with schema\ndata = [\n    {\"vector\": [3.1, 4.1], \"item\": \"foo\", \"price\": 10.0},\n    {\"vector\": [5.9, 26.5], \"item\": \"bar\", \"price\": 20.0},\n    {\"vector\": [10.2, 100.8], \"item\": \"baz\", \"price\": 30.0},\n    {\"vector\": [1.4, 9.5], \"item\": \"fred\", \"price\": 40.0},\n]\n\nschema = pa.schema([\n    pa.field(\"vector\", pa.list_(pa.float32(), 2)),\n    pa.field(\"item\", pa.utf8()),\n    pa.field(\"price\", pa.float32()),\n])\n\ntable_name = \"basic_ingestion_example\"\ntable = db.create_table(table_name, schema=schema, mode=\"overwrite\")\ntable.add(data)\n</code></pre> <pre><code>import * as lancedb from \"@lancedb/lancedb\"\nimport { Schema, Field, Float32, FixedSizeList, Utf8 } from \"apache-arrow\";\n\nconst db = await lancedb.connect({\n  uri: \"db://your-project-slug\",\n  apiKey: \"your-api-key\",\n  region: \"us-east-1\"\n});\n\nconsole.log(\"Creating table from JavaScript objects\");\nconst data = [\n    { vector: [3.1, 4.1], item: \"foo\", price: 10.0 },\n    { vector: [5.9, 26.5], item: \"bar\", price: 20.0 },\n    { vector: [10.2, 100.8], item: \"baz\", price: 30.0},\n    { vector: [1.4, 9.5], item: \"fred\", price: 40.0},\n]\n\nconst tableName = \"js_objects_example\";\nconst table = await db.createTable(tableName, data, {\n    mode: \"overwrite\"\n});\n\nconsole.log(\"\\nCreating a table with a predefined schema then add data to it\");\nconst tableName = \"schema_example\";\n\n// Define schema\n// create an empty table with schema\nconst schema = new Schema([\n    new Field(\n    \"vector\",\n    new FixedSizeList(2, new Field(\"float32\", new Float32())),\n    ),\n    new Field(\"item\", new Utf8()),\n    new Field(\"price\", new Float32()),\n]);\n\n// Create an empty table with schema\nconst table = await db.createEmptyTable(tableName, schema, {\n    mode: \"overwrite\",\n});\n\n// Add data to the schema-defined table\nconst data = [\n    { vector: [3.1, 4.1], item: \"foo\", price: 10.0 },\n    { vector: [5.9, 26.5], item: \"bar\", price: 20.0 },\n    { vector: [10.2, 100.8], item: \"baz\", price: 30.0},\n    { vector: [1.4, 9.5], item: \"fred\", price: 40.0},\n]\n\nawait table.add(data);\n</code></pre> <p>Vector Column Type</p> <p>The vector column needs to be a pyarrow.FixedSizeList type.</p>"},{"location":"guides/tables/ingestion/#ingest-data-using-pydantic-models","title":"Ingest Data Using Pydantic Models","text":"<pre><code>from lancedb.pydantic import Vector, LanceModel\n\nimport pyarrow as pa\n\n# Define a Pydantic model\nclass Content(LanceModel):\n    movie_id: int\n    vector: Vector(128)\n    genres: str\n    title: str\n    imdb_id: int\n\n    @property\n    def imdb_url(self) -&gt; str:\n        return f\"https://www.imdb.com/title/tt{self.imdb_id}\"\n\n# Create table with Pydantic model schema\ntable_name = \"pydantic_example\"\ntable = db.create_table(table_name, schema=Content, mode=\"overwrite\")\n</code></pre>"},{"location":"guides/tables/ingestion/#ingest-data-using-nested-pydantic-models","title":"Ingest Data Using Nested Pydantic Models","text":"<p>You can use nested Pydantic models to represent complex data structures.  For example, you may want to store the document string and the document source name as a nested Document object:</p> <pre><code>from pydantic import BaseModel\n\nclass Document(BaseModel):\n    content: str\n    source: str\n</code></pre> <p>This can be used as the type of a LanceDB table column:</p> <pre><code>class NestedSchema(LanceModel):\n    id: str\n    vector: Vector(128)\n    document: Document\n\n# Create table with nested schema\ntable_name = \"nested_model_example\"\ntable = db.create_table(table_name, schema=NestedSchema, mode=\"overwrite\")\n</code></pre> <p>This creates a struct column called <code>document</code> that has two subfields called <code>content</code> and <code>source</code>:</p> <pre><code>In [28]: table.schema\nOut[28]:\nid: string not null\nvector: fixed_size_list&lt;item: float&gt;[128] not null\n    child 0, item: float\ndocument: struct&lt;content: string not null, source: string not null&gt; not null\n    child 0, content: string not null\n    child 1, source: string not null\n</code></pre>"},{"location":"guides/tables/ingestion/#ingest-data-in-batches","title":"Ingest Data in Batches","text":"<p>It is recommended to use itertators to add large datasets in batches when creating  your table in one go. Data will be automatically compacted for the best query performance.</p> PythonTypeScript <pre><code>import pyarrow as pa\n\ndef make_batches():\n    for i in range(5):  # Create 3 batches\n        yield pa.RecordBatch.from_arrays(\n            [\n                pa.array([[3.1, 4.1], [5.9, 26.5]],\n                        pa.list_(pa.float32(), 2)),\n                pa.array([f\"item{i*2+1}\", f\"item{i*2+2}\"]),\n                pa.array([float((i*2+1)*10), float((i*2+2)*10)]),\n            ],\n            [\"vector\", \"item\", \"price\"],\n        )\n\nschema = pa.schema([\n    pa.field(\"vector\", pa.list_(pa.float32(), 2)),\n    pa.field(\"item\", pa.utf8()),\n    pa.field(\"price\", pa.float32()),\n])\n# Create table with batches\ntable_name = \"batch_ingestion_example\"\ntable = db.create_table(table_name, make_batches(), schema=schema, mode=\"overwrite\")\n</code></pre> <pre><code>console.log(\"\\nBatch ingestion example with product catalog data\");\nconst tableName = \"product_catalog\";\n\n// Vector dimension for product embeddings (realistic dimension for text embeddings)\nconst vectorDim = 128;\n\n// Create random embedding vector of specified dimension\nconst createRandomEmbedding = (dim: number) =&gt; Array(dim).fill(0).map(() =&gt; Math.random() * 2 - 1);\n\n// Create table with initial batch of products\nconst initialBatch = Array(10).fill(0).map((_, i) =&gt; ({\n    product_id: `PROD-${1000 + i}`,\n    name: `Product ${i + 1}`,\n    category: [\"electronics\", \"home\", \"office\"][i % 3],\n    price: 10.99 + (i * 5.99),\n    vector: createRandomEmbedding(vectorDim)\n}));\n\nconst table = await db.createTable(tableName, initialBatch, { \n    mode: \"overwrite\"\n});\n\n// Second batch - 25 more products\nconst batch2 = Array(25).fill(0).map((_, i) =&gt; ({\n    product_id: `PROD-${2000 + i}`,\n    name: `Premium Product ${i + 1}`,\n    category: [\"electronics\", \"kitchen\", \"outdoor\", \"office\", \"gaming\"][i % 5],\n    price: 25.99 + (i * 7.49),\n    vector: createRandomEmbedding(vectorDim)\n}));\n\nawait table.add(batch2);\n\n// Third batch - 15 more products in a different category\nconst batch3 = Array(15).fill(0).map((_, i) =&gt; ({\n    product_id: `PROD-${3000 + i}`,\n    name: `Budget Product ${i + 1}`,\n    category: [\"essentials\", \"budget\", \"basics\"][i % 3],\n    price: 5.99 + (i * 2.50),\n    vector: createRandomEmbedding(vectorDim)\n}));\n\nawait table.add(batch3);\n</code></pre> <p>Explore full documentation in our SDK guides: Python and Typescript.</p> <ol> <li> <p>We suggest the best batch size to be 500k\u00a0\u21a9</p> </li> </ol>"},{"location":"guides/tables/schema/","title":"Schema Management in LanceDB","text":"<p>While tables must have a schema specified when they are created, you can change the schema over time. There's three methods to alter the schema of a table:</p> <ul> <li><code>add_columns</code>: Add new columns to the table</li> <li><code>alter_columns</code>: Alter the name, nullability, or data type of a column</li> <li><code>drop_columns</code>: Drop columns from the table</li> </ul>"},{"location":"guides/tables/schema/#adding-new-columns","title":"Adding new columns","text":"<p>You can add new columns to the table with the <code>add_columns</code> method. New columns are filled with values based on a SQL expression. For example, you can add a new column <code>y</code> to the table, fill it with the value of <code>x * 2</code> and set the expected data type for it.</p> PythonTypescript Sync APIAsync API <pre><code>tbl.add_columns({\"double_price\": \"cast((price * 2) as float)\"})\n</code></pre> <pre><code>await tbl.add_columns({\"double_price\": \"cast((price * 2) as float)\"})\n</code></pre> <p>API Reference: lancedb.table.Table.add_columns</p> <p><pre><code>await tbl.addColumns([\n  { name: \"double_price\", valueSql: \"cast((price * 2) as Float)\" },\n]);\n</code></pre> API Reference: lancedb.Table.addColumns</p> <p>If you want to fill it with null, you can use <code>cast(NULL as &lt;data_type&gt;)</code> as the SQL expression to fill the column with nulls, while controlling the data type of the column. Available data types are base on the DataFusion data types. You can use any of the SQL types, such as <code>BIGINT</code>:</p> <pre><code>cast(NULL as BIGINT)\n</code></pre> <p>Using Arrow data types and the <code>arrow_typeof</code> function is not yet supported.</p>"},{"location":"guides/tables/schema/#altering-existing-columns","title":"Altering existing columns","text":"<p>You can alter the name, nullability, or data type of a column with the <code>alter_columns</code> method.</p> <p>Changing the name or nullability of a column just updates the metadata. Because of this, it's a fast operation. Changing the data type of a column requires rewriting the column, which can be a heavy operation.</p> PythonTypescript Sync APIAsync API <pre><code>import pyarrow as pa\n\ntbl.alter_columns(\n    {\n        \"path\": \"double_price\",\n        \"rename\": \"dbl_price\",\n        \"data_type\": pa.float64(),\n        \"nullable\": True,\n    }\n)\n</code></pre> <pre><code>import pyarrow as pa\n\nawait tbl.alter_columns(\n    {\n        \"path\": \"double_price\",\n        \"rename\": \"dbl_price\",\n        \"data_type\": pa.float64(),\n        \"nullable\": True,\n    }\n)\n</code></pre> <p>API Reference: lancedb.table.Table.alter_columns</p> <p><pre><code>await tbl.alterColumns([\n  {\n    path: \"double_price\",\n    rename: \"dbl_price\",\n    dataType: \"float\",\n    nullable: true,\n  },\n]);\n</code></pre> API Reference: lancedb.Table.alterColumns</p>"},{"location":"guides/tables/schema/#dropping-columns","title":"Dropping columns","text":"<p>You can drop columns from the table with the <code>drop_columns</code> method. This will will remove the column from the schema.</p> PythonTypescript Sync APIAsync API <pre><code>tbl.drop_columns([\"dbl_price\"])\n</code></pre> <pre><code>await tbl.drop_columns([\"dbl_price\"])\n</code></pre> <p>API Reference: lancedb.table.Table.drop_columns</p> <p><pre><code>await tbl.dropColumns([\"dbl_price\"]);\n</code></pre> API Reference: lancedb.Table.dropColumns</p>"},{"location":"guides/tables/schema/#whats-next","title":"What's next?","text":"<p>Learn about consistency settings in LanceDB.</p> <p>title: \"Schema Evolution in LanceDB | Schema Management Guide\" description: \"Learn how to manage schema evolution in LanceDB. Includes schema updates, backward compatibility, and best practices for schema changes.\"</p>"},{"location":"guides/tables/schema/#schema-evolution-with-lancedb","title":"Schema Evolution with LanceDB","text":"<p>Schema evolution enables non-breaking modifications to a database table's structure \u2014 such as adding columns,  altering data types, or dropping fields \u2014 to adapt to evolving data requirements without service interruptions.  LanceDB supports ACID-compliant schema evolution through granular operations (add/alter/drop columns), allowing you to:</p> <ul> <li>Iterate Safely: Modify schemas in production with versioned datasets and backward compatibility</li> <li>Scale Seamlessly: Handle ML model iterations, regulatory changes, or feature additions</li> <li>Optimize Continuously: Remove unused fields or enforce new constraints without downtime</li> </ul>"},{"location":"guides/tables/schema/#schema-evolution-operations","title":"Schema Evolution Operations","text":"<p>LanceDB supports three primary schema evolution operations:</p> <ol> <li>Adding new columns: Extend your table with additional attributes</li> <li>Altering existing columns: Change column names, data types, or nullability</li> <li>Dropping columns: Remove unnecessary columns from your schema</li> </ol> <p>Schema Evolution Performance</p> <p>Schema evolution operations are applied immediately but do not typically require rewriting all data.  However, data type changes may involve more substantial operations.</p>"},{"location":"guides/tables/schema/#adding-new-columns_1","title":"Adding New Columns","text":"<p>You can add new columns to a table with the <code>add_columns</code>  method in Python or <code>addColumns</code> in TypeScript/JavaScript.  New columns are populated based on SQL expressions you provide.</p> PythonTypeScript <pre><code>table_name = \"schema_evolution_add_example\"\n\ndata = [\n    {\n        \"id\": 1,\n        \"name\": \"Laptop\",\n        \"price\": 1200.00,\n        \"vector\": np.random.random(128).tolist(),\n    },\n    {\n        \"id\": 2,\n        \"name\": \"Smartphone\",\n        \"price\": 800.00,\n        \"vector\": np.random.random(128).tolist(),\n    },\n    {\n        \"id\": 3,\n        \"name\": \"Headphones\",\n        \"price\": 150.00,\n        \"vector\": np.random.random(128).tolist(),\n    },\n    {\n        \"id\": 4,\n        \"name\": \"Monitor\",\n        \"price\": 350.00,\n        \"vector\": np.random.random(128).tolist(),\n    },\n    {\n        \"id\": 5,\n        \"name\": \"Keyboard\",\n        \"price\": 80.00,\n        \"vector\": np.random.random(128).tolist(),\n    },\n]\n\ntable = db.create_table(table_name, data, mode=\"overwrite\")\n\n# 1. Add a new column calculating a discount price\ntable.add_columns({\"discounted_price\": \"cast((price * 0.9) as float)\"})\n\n# 2. Add a status column with a default value\ntable.add_columns({\"in_stock\": \"cast(true as boolean)\"})\n\n# 3. Add a nullable column with NULL values\ntable.add_columns({\"last_ordered\": \"cast(NULL as timestamp)\"})\n</code></pre> <pre><code>const tableName = \"schema_evolution_add_example\";\n\nconst data = [\n  {\n    id: 1,\n    name: \"Laptop\",\n    price: 1200.0,\n    vector: Array.from({ length: 128 }, () =&gt; Math.random()),\n  },\n  {\n    id: 2,\n    name: \"Smartphone\",\n    price: 800.0,\n    vector: Array.from({ length: 128 }, () =&gt; Math.random()),\n  },\n  {\n    id: 3,\n    name: \"Headphones\",\n    price: 150.0,\n    vector: Array.from({ length: 128 }, () =&gt; Math.random()),\n  },\n  {\n    id: 4,\n    name: \"Monitor\",\n    price: 350.0,\n    vector: Array.from({ length: 128 }, () =&gt; Math.random()),\n  },\n  {\n    id: 5,\n    name: \"Keyboard\",\n    price: 80.0,\n    vector: Array.from({ length: 128 }, () =&gt; Math.random()),\n  },\n];\n\nconst table = await db.createTable(tableName, data, { mode: \"overwrite\" });\n\n// 1. Add a new column calculating a discount price\nawait table.addColumns([\n  { name: \"discounted_price\", valueSql: \"cast((price * 0.9) as float)\" },\n]);\n\n// 2. Add a status column with a default value\nawait table.addColumns([\n  { name: \"in_stock\", valueSql: \"cast(true as boolean)\" },\n]);\n\n// 3. Add a nullable column with NULL values\nawait table.addColumns([\n  { name: \"last_ordered\", valueSql: \"cast(NULL as timestamp)\" },\n]);\n</code></pre> <p>NULL Values in New Columns</p> <p>When adding columns that should contain NULL values, be sure to cast the NULL  to the appropriate type, e.g., <code>cast(NULL as timestamp)</code>.</p>"},{"location":"guides/tables/schema/#altering-existing-columns_1","title":"Altering Existing Columns","text":"<p>You can alter columns using the <code>alter_columns</code> method in Python or <code>alterColumns</code> in TypeScript/JavaScript. This allows you to:</p> <ul> <li>Rename a column</li> <li>Change a column's data type</li> <li>Modify nullability (whether a column can contain NULL values)</li> </ul> PythonTypeScript <pre><code>table_name = \"schema_evolution_alter_example\"\n\ndata = [\n    {\n        \"id\": 1,\n        \"name\": \"Laptop\",\n        \"price\": 1200,\n        \"discount_price\": 1080.0,\n        \"vector\": np.random.random(128).tolist(),\n    },\n    {\n        \"id\": 2,\n        \"name\": \"Smartphone\",\n        \"price\": 800,\n        \"discount_price\": 720.0,\n        \"vector\": np.random.random(128).tolist(),\n    },\n    {\n        \"id\": 3,\n        \"name\": \"Headphones\",\n        \"price\": 150,\n        \"discount_price\": 135.0,\n        \"vector\": np.random.random(128).tolist(),\n    },\n    {\n        \"id\": 4,\n        \"name\": \"Monitor\",\n        \"price\": 350,\n        \"discount_price\": 315.0,\n        \"vector\": np.random.random(128).tolist(),\n    },\n    {\n        \"id\": 5,\n        \"name\": \"Keyboard\",\n        \"price\": 80,\n        \"discount_price\": 72.0,\n        \"vector\": np.random.random(128).tolist(),\n    },\n]\nschema = pa.schema(\n    {\n        \"id\": pa.int64(),\n        \"name\": pa.string(),\n        \"price\": pa.int32(),\n        \"discount_price\": pa.float64(),\n        \"vector\": pa.list_(pa.float32(), 128),\n    }\n)\n\ntable = db.create_table(table_name, data, schema=schema, mode=\"overwrite\")\n\n# 1. Rename a column\ntable.alter_columns({\"path\": \"discount_price\", \"rename\": \"sale_price\"})\n\n# 2. Change a column's data type\ntable.alter_columns({\"path\": \"price\", \"data_type\": pa.int64()})\n\n# 3. Make a column nullable\ntable.alter_columns({\"path\": \"name\", \"nullable\": True})\n\n# 4. Multiple changes at once\ntable.alter_columns(\n    {\n        \"path\": \"sale_price\",\n        \"rename\": \"final_price\",\n        \"data_type\": pa.float64(),\n        \"nullable\": True,\n    }\n)\n</code></pre> <pre><code>const tableName = \"schema_evolution_alter_example\";\n\nconst data = [\n  {\n    id: 1,\n    name: \"Laptop\",\n    price: 1200,\n    discount_price: 1080.0,\n    vector: Array.from({ length: 128 }, () =&gt; Math.random()),\n  },\n  {\n    id: 2,\n    name: \"Smartphone\",\n    price: 800,\n    discount_price: 720.0,\n    vector: Array.from({ length: 128 }, () =&gt; Math.random()),\n  },\n  {\n    id: 3,\n    name: \"Headphones\",\n    price: 150,\n    discount_price: 135.0,\n    vector: Array.from({ length: 128 }, () =&gt; Math.random()),\n  },\n  {\n    id: 4,\n    name: \"Monitor\",\n    price: 350,\n    discount_price: 315.0,\n    vector: Array.from({ length: 128 }, () =&gt; Math.random()),\n  },\n  {\n    id: 5,\n    name: \"Keyboard\",\n    price: 80,\n    discount_price: 72.0,\n    vector: Array.from({ length: 128 }, () =&gt; Math.random()),\n  },\n];\n\nconst table = await db.createTable(tableName, data, { mode: \"overwrite\" });\n\n// 1. Rename a column\nawait table.alterColumns([\n  {\n    path: \"discount_price\",\n    rename: \"sale_price\",\n  },\n]);\n\n// 2. Make a column nullable\nawait table.alterColumns([\n  {\n    path: \"name\",\n    nullable: true,\n  },\n]);\n\n// 3. Multiple changes at once\nconsole.log(\"\\nMultiple changes to 'sale_price' column...\");\nawait table.alterColumns([\n  {\n    path: \"sale_price\",\n    rename: \"final_price\",\n    nullable: true,\n  },\n]);\n</code></pre> <p>Data Type Changes</p> <p>Changing data types requires rewriting the column data and may be resource-intensive for large tables. Renaming columns or changing nullability is more efficient as it only updates metadata.</p>"},{"location":"guides/tables/schema/#dropping-columns_1","title":"Dropping Columns","text":"<p>You can remove columns using the <code>drop_columns</code>  method in Python or [<code>dropColumns</code>] in TypeScript/JavaScript(https://lancedb.github.io/lancedb/js/classes/Table/#altercolumns).</p> PythonTypeScript <pre><code>table_name = \"schema_evolution_drop_example\"\n\ndata = [\n    {\n        \"id\": 1,\n        \"name\": \"Laptop\",\n        \"price\": 1200.00,\n        \"temp_col1\": \"X\",\n        \"temp_col2\": 100,\n        \"vector\": np.random.random(128).tolist(),\n    },\n    {\n        \"id\": 2,\n        \"name\": \"Smartphone\",\n        \"price\": 800.00,\n        \"temp_col1\": \"Y\",\n        \"temp_col2\": 200,\n        \"vector\": np.random.random(128).tolist(),\n    },\n    {\n        \"id\": 3,\n        \"name\": \"Headphones\",\n        \"price\": 150.00,\n        \"temp_col1\": \"Z\",\n        \"temp_col2\": 300,\n        \"vector\": np.random.random(128).tolist(),\n    },\n]\n\ntable = db.create_table(table_name, data, mode=\"overwrite\")\n\n# 1. Drop a single column\ntable.drop_columns([\"temp_col1\"])\n\n# 2. Drop multiple columns\ntable.drop_columns([\"temp_col2\"])\n</code></pre> <pre><code>const tableName = \"schema_evolution_drop_example\";\n\nconst data = [\n  {\n    id: 1,\n    name: \"Laptop\",\n    price: 1200.0,\n    temp_col1: \"X\",\n    temp_col2: 100,\n    vector: Array.from({ length: 128 }, () =&gt; Math.random()),\n  },\n  {\n    id: 2,\n    name: \"Smartphone\",\n    price: 800.0,\n    temp_col1: \"Y\",\n    temp_col2: 200,\n    vector: Array.from({ length: 128 }, () =&gt; Math.random()),\n  },\n  {\n    id: 3,\n    name: \"Headphones\",\n    price: 150.0,\n    temp_col1: \"Z\",\n    temp_col2: 300,\n    vector: Array.from({ length: 128 }, () =&gt; Math.random()),\n  },\n];\n\nconst table = await db.createTable(tableName, data, { mode: \"overwrite\" });\n\n// 1. Drop a single column\nawait table.dropColumns([\"temp_col1\"]);\n\n// 2. Drop multiple columns\nawait table.dropColumns([\"temp_col2\"]);\n</code></pre> <p>Irreversible Column Deletion</p> <p>Dropping columns cannot be undone. Make sure you have backups or are certain before removing columns.</p>"},{"location":"guides/tables/schema/#vector-column-considerations","title":"Vector Column Considerations","text":"<p>Vector columns (used for embeddings) have special considerations. When altering vector columns, you should ensure consistent dimensionality.</p>"},{"location":"guides/tables/schema/#converting-list-to-fixedsizelist","title":"Converting List to FixedSizeList","text":"<p>A common schema evolution task is converting a generic list column to a fixed-size list for performance:</p> <pre><code>vector_dim = 768  # Your embedding dimension\ntable.alter_columns(dict(path=\"embedding\", data_type=pa.list_(pa.float32(), vector_dim)))\n</code></pre> <ol> <li> <p>The <code>vectordb</code> package is a legacy package that is deprecated in favor of <code>@lancedb/lancedb</code>. The <code>vectordb</code> package will continue to receive bug fixes and security updates until September 2024. We recommend all new projects use <code>@lancedb/lancedb</code>. See the migration guide for more information.\u00a0\u21a9</p> </li> </ol>"},{"location":"guides/tables/update/","title":"Updating Data with LanceDB","text":"<p>Our enterprise solution efficiently manages updates across millions of tables,  supporting several hundred transactions per second (TPS) per table.</p> <p>LanceDB supports various data modification operations:</p> <ul> <li>Basic Operations: Update and delete existing data</li> <li>Merge Insert Operations: Upsert, insert-if-not-exists, and replace range</li> </ul>"},{"location":"guides/tables/update/#basic-operations","title":"Basic Operations","text":""},{"location":"guides/tables/update/#update","title":"Update","text":"<p>Update existing rows that match a condition.</p> PythonTypeScript <pre><code>import lancedb\n\n# Connect to LanceDB\ndb = lancedb.connect(\n  uri=\"db://your-project-slug\",\n  api_key=\"your-api-key\",\n  region=\"us-east-1\"\n)\n\ndata = [\n    {\"vector\": [3.1, 4.1], \"item\": \"foo\", \"price\": 10.0},\n    {\"vector\": [5.9, 26.5], \"item\": \"bar\", \"price\": 20.0},\n    {\"vector\": [2.9, 18.2], \"item\": \"zoo\", \"price\": 30.0},\n]\n\ntable = db.create_table(\"update_table_example\", data, mode=\"overwrite\")\n\n# Update with direct values\ntable.update(where=\"price &lt; 20.0\", values={\"vector\": [2, 2], \"item\": \"foo-updated\"})\n\n# Update using SQL expression\ntable.update(where=\"price &lt; 20.0\", values_sql={\"price\": \"price * 1.1\"})\n</code></pre> <pre><code>import * as lancedb from \"@lancedb/lancedb\"\n\n// Connect to LanceDB\nconst db = await lancedb.connect({\n  uri: \"db://your-project-slug\",\n  apiKey: \"your-api-key\",\n  region: \"us-east-1\"\n});\n\n// Create a table with sample data\nconst data = [\n  { vector: [3.1, 4.1], item: \"foo\", price: 10.0 },\n  { vector: [5.9, 26.5], item: \"bar\", price: 20.0 },\n  { vector: [2.9, 18.2], item: \"zoo\", price: 30.0},\n];\n\nconst table = await db.createTable(\"update_table_example\", data, { mode: \"overwrite\" });\n\n// Update with direct values\nawait table.update({\n  where: \"price &lt; 20.0\",\n  values: { vector: [2, 2], item: \"foo-updated\" },\n});\n\n// Update using SQL expression\nawait table.update({\n  where: \"price &lt; 20.0\", \n  valuesSql: { price: \"price * 1.1\" },\n});\n</code></pre> <p>Index Updates</p> <p>When rows are updated, LanceDB will update the existing index on the column.  The updated data is available for search immediately.</p> <p>Check out the details of <code>update</code> from Python SDK and TypeScript SDK.</p>"},{"location":"guides/tables/update/#delete","title":"Delete","text":"<p>Remove rows that match a condition.</p> PythonTypeScript <pre><code>import lancedb\n\n# Connect to LanceDB\ndb = lancedb.connect(\n  uri=\"db://your-project-slug\",\n  api_key=\"your-api-key\",\n  region=\"us-east-1\"\n)\ntable = db.open_table(\"update_table_example\")\n\n# delete data\npredicate = \"price = 30.0\"\ntable.delete(predicate)\n</code></pre> <pre><code>import * as lancedb from \"@lancedb/lancedb\"\n\n// Connect to LanceDB\nconst db = await lancedb.connect({\n  uri: \"db://your-project-slug\",\n  apiKey: \"your-api-key\",\n  region: \"us-east-1\"\n});\ntable = await db.openTable(\"update_table_example\");\n\n// delete data\nconst predicate = \"price = 30.0\";\nawait table.delete(predicate);\n</code></pre> <p>Permanent Deletion</p> <p>Delete operations are permanent and cannot be undone. Always ensure you have backups or are certain before deleting data.</p> <p>Check out the details of <code>delete</code> from Python SDK and TypeScript SDK.</p>"},{"location":"guides/tables/update/#merge-operations","title":"Merge Operations","text":"<p>The merge insert command is a flexible API that can be used to perform upsert,  insert_if_not_exists, and replace_range operations.</p> <p>Update Performance Optimization</p> <p>For optimal update performance: - Use batch updates when possible (add multiple rows at once) - Consider creating a scalar index on your ID column if updating by ID frequently - For large-scale updates, consider using multiple concurrent connections</p> <p>Embedding Functions</p> <p>Like the create table and add APIs, the merge insert API will automatically compute embeddings  if the table has an embedding definition in its schema. If the input data doesn't contain the  source column, or the vector column is already filled, the embeddings won't be computed.</p>"},{"location":"guides/tables/update/#upsert","title":"Upsert","text":"<p><code>upsert</code> updates rows if they exist and inserts them if they don't. To do this with merge insert,  enable both <code>when_matched_update_all()</code> and <code>when_not_matched_insert_all()</code>.</p> PythonTypeScript <pre><code>import lancedb\n\n# Connect to LanceDB\ndb = lancedb.connect(\n  uri=\"db://your-project-slug\",\n  api_key=\"your-api-key\",\n  region=\"us-east-1\"\n)\n\n# Create example table\nusers_table_name = \"users_example\"\ntable = db.create_table(\n    users_table_name,\n    [\n        {\"id\": 0, \"name\": \"Alice\"},\n        {\"id\": 1, \"name\": \"Bob\"},\n    ],\n    mode=\"overwrite\",\n)\nprint(f\"Created users table with {table.count_rows()} rows\")\n\n# Prepare data for upsert\nnew_users = [\n    {\"id\": 1, \"name\": \"Bobby\"},  # Will update existing record\n    {\"id\": 2, \"name\": \"Charlie\"},  # Will insert new record\n]\n\n# Upsert by id\n(\n    users_table.merge_insert(\"id\")\n    .when_matched_update_all()\n    .when_not_matched_insert_all()\n    .execute(new_users)\n)\n\n# Verify results - should be 3 records total\nprint(f\"Total users: {users_table.count_rows()}\")  # 3\n</code></pre> <pre><code>import * as lancedb from \"@lancedb/lancedb\"\n\n// Connect to LanceDB\nconst db = await lancedb.connect({\n  uri: \"db://your-project-slug\",\n  apiKey: \"your-api-key\",\n  region: \"us-east-1\"\n});\n\n// Create example table\nconst table = await db.createTable(\"users\", [\n  { id: 0, name: \"Alice\" },\n  { id: 1, name: \"Bob\" },\n]);\n\n// Prepare data for upsert\nconst newUsers = [\n  { id: 1, name: \"Bobby\" },  // Will update existing record\n  { id: 2, name: \"Charlie\" },  // Will insert new record\n];\n\n// Upsert by id\nawait table\n  .mergeInsert(\"id\")\n  .whenMatchedUpdateAll()\n  .whenNotMatchedInsertAll()\n  .execute(newUsers);\n\n// Verify results - should be 3 records total\nconst count = await table.countRows();\nconsole.log(`Total users: ${count}`);  // 3\n</code></pre>"},{"location":"guides/tables/update/#insert-if-not-exists","title":"Insert-if-not-exists","text":"<p>This will only insert rows that do not have a match in the target table, thus  preventing duplicate rows. To do this with merge insert, enable just  <code>when_not_matched_insert_all()</code>.</p> PythonTypeScript <pre><code>import lancedb\n\n# Connect to LanceDB\ndb = lancedb.connect(\n  uri=\"db://your-project-slug\",\n  api_key=\"your-api-key\",\n  region=\"us-east-1\"\n)\n\n# Create example table\ntable = db.create_table(\n    \"domains\",\n    [\n        {\"domain\": \"google.com\", \"name\": \"Google\"},\n        {\"domain\": \"github.com\", \"name\": \"GitHub\"},\n    ],\n)\n\n# Prepare new data - one existing and one new record\nnew_domains = [\n    {\"domain\": \"google.com\", \"name\": \"Google\"},\n    {\"domain\": \"facebook.com\", \"name\": \"Facebook\"},\n]\n\n# Insert only if domain doesn't exist\ntable.merge_insert(\"domain\").when_not_matched_insert_all().execute(new_domains)\n\n# Verify count - should be 3 (original 2 plus 1 new)\nprint(f\"Total domains: {table.count_rows()}\")  # 3\n</code></pre> <pre><code>import * as lancedb from \"@lancedb/lancedb\"\n\n// Connect to LanceDB\nconst db = await lancedb.connect({\n  uri: \"db://your-project-slug\",\n  apiKey: \"your-api-key\",\n  region: \"us-east-1\"\n});\n\n// Create example table\nconst table = await db.createTable(\n  \"domains\", \n  [\n    { domain: \"google.com\", name: \"Google\" },\n    { domain: \"github.com\", name: \"GitHub\" },\n  ]\n);\n\n// Prepare new data - one existing and one new record\nconst newDomains = [\n  { domain: \"google.com\", name: \"Google\" },\n  { domain: \"facebook.com\", name: \"Facebook\" },\n];\n\n// Insert only if domain doesn't exist\nawait table.merge_insert(\"domain\")\n  .whenNotMatchedInsertAll()\n  .execute(newDomains);\n\n// Verify count - should be 3 (original 2 plus 1 new)\nconst count = await table.countRows();\nconsole.log(`Total domains: ${count}`);  // 3\n</code></pre>"},{"location":"guides/tables/update/#replace-range","title":"Replace range","text":"<p>You can also replace a range of rows in the target table with the input data.  For example, if you have a table of document chunks, where each chunk has both  a <code>doc_id</code> and a <code>chunk_id</code>, you can replace all chunks for a given <code>doc_id</code> with updated chunks. </p> <p>This can be tricky otherwise because if you try to use <code>upsert</code> when the new data has fewer  chunks you will end up with extra chunks. To avoid this, add another clause to delete any chunks  for the document that are not in the new data, with <code>when_not_matched_by_source_delete</code>.</p> PythonTypeScript <pre><code>import lancedb\n\n# Connect to LanceDB\ndb = lancedb.connect(\n  uri=\"db://your-project-slug\",\n  api_key=\"your-api-key\",\n  region=\"us-east-1\"\n)\n\n# Create example table with document chunks\ntable = db.create_table(\n    \"chunks\",\n    [\n        {\"doc_id\": 0, \"chunk_id\": 0, \"text\": \"Hello\"},\n        {\"doc_id\": 0, \"chunk_id\": 1, \"text\": \"World\"},\n        {\"doc_id\": 1, \"chunk_id\": 0, \"text\": \"Foo\"},\n        {\"doc_id\": 1, \"chunk_id\": 1, \"text\": \"Bar\"},\n        {\"doc_id\": 2, \"chunk_id\": 0, \"text\": \"Baz\"},\n    ],\n)\n\n# New data - replacing all chunks for doc_id 1 with just one chunk\nnew_chunks = [\n    {\"doc_id\": 1, \"chunk_id\": 0, \"text\": \"Zoo\"},\n]\n\n# Replace all chunks for doc_id 1\n(\n    table.merge_insert([\"doc_id\"])\n    .when_matched_update_all()\n    .when_not_matched_insert_all()\n    .when_not_matched_by_source_delete(\"doc_id = 1\")\n    .execute(new_chunks)\n)\n\n# Verify count for doc_id = 1 - should be 2 \nprint(f\"Chunks for doc_id = 1: {table.count_rows('doc_id = 1')}\")  # 2\n</code></pre> <pre><code>import * as lancedb from \"@lancedb/lancedb\"\n\n// Connect to LanceDB\nconst db = await lancedb.connect({\n  uri: \"db://your-project-slug\",\n  apiKey: \"your-api-key\",\n  region: \"us-east-1\"\n});\n\n// Create example table with document chunks\nconst table = await db.createTable(\n  \"chunks\", \n  [\n    { doc_id: 0, chunk_id: 0, text: \"Hello\" },\n    { doc_id: 0, chunk_id: 1, text: \"World\" },\n    { doc_id: 1, chunk_id: 0, text: \"Foo\" },\n    { doc_id: 1, chunk_id: 1, text: \"Bar\" },\n    { doc_id: 2, chunk_id: 0, text: \"Baz\" },\n  ]\n);\n\n// New data - replacing all chunks for doc_id 1 with just one chunk\nconst newChunks = [\n  { doc_id: 1, chunk_id: 0, text: \"Zoo\" }\n];\n\n// Replace all chunks for doc_id 1\nawait table.merge_insert([\"doc_id\"])\n  .whenMatchedUpdateAll()\n  .whenNotMatchedInsertAll()\n  .whenNotMatchedBySourceDelete(\"doc_id = 1\")\n  .execute(newChunks);\n\n// Verify count for doc_id = 1 - should be 2 \nconst count = await table.countRows(\"doc_id = 1\");\nconsole.log(`Chunks for doc_id =1: ${count}`);  // 2\n</code></pre> <p>For more detailed information, refer to the <code>merge_insert</code> from Python SDK and <code>mergeInsert from TypeScript SDK</code></p> <p>title: Merge Insert in LanceDB | Upsert, Insert-if-Not-Exists &amp; Replace Range description: Learn how to use the merge insert command in LanceDB for upserts, insert-if-not-exists, and range replacements. Covers performance tips, embedding functions, and API usage in Python and TypeScript.</p> <p>The merge insert command is a flexible API that can be used to perform:</p> <ol> <li>Upsert</li> <li>Insert-if-not-exists</li> <li>Replace range</li> </ol> <p>It works by joining the input data with the target table on a key you provide. Often this key is a unique row id key. You can then specify what to do when there is a match and when there is not a match. For example, for upsert you want to update if the row has a match and insert if the row doesn't have a match. Whereas for insert-if-not-exists you only want to insert if the row doesn't have a match.</p> <p>You can also read more in the API reference:</p> <ul> <li>Python<ul> <li>Sync: lancedb.table.Table.merge_insert</li> <li>Async: lancedb.table.AsyncTable.merge_insert</li> </ul> </li> <li>Typescript: lancedb.Table.mergeInsert</li> </ul> <p>Use scalar indices to speed up merge insert</p> <p>The merge insert command needs to perform a join between the input data and the target table on the <code>on</code> key you provide. This requires scanning that entire column, which can be expensive for large tables. To speed up this operation, you can create a scalar index on the <code>on</code> column, which will allow LanceDB to find matches without having to scan the whole tables.</p> <p>Read more about scalar indices in Building a Scalar Index guide.</p> <p>Embedding Functions</p> <p>Like the create table and add APIs, the merge insert API will automatically compute embeddings if the table has a embedding definition in its schema. If the input data doesn't contain the source column, or the vector column is already filled, then the embeddings won't be computed. See the Embedding Functions guide for more information.</p>"},{"location":"guides/tables/update/#upsert_1","title":"Upsert","text":"<p>Upsert updates rows if they exist and inserts them if they don't. To do this with merge insert, enable both <code>when_matched_update_all()</code> and <code>when_not_matched_insert_all()</code>.</p> PythonTypescript Sync APIAsync API <pre><code>table = db.create_table(\n    \"users\",\n    [\n        {\"id\": 0, \"name\": \"Alice\"},\n        {\"id\": 1, \"name\": \"Bob\"},\n    ],\n)\nnew_users = [\n    {\"id\": 1, \"name\": \"Bobby\"},\n    {\"id\": 2, \"name\": \"Charlie\"},\n]\nres = (\n    table.merge_insert(\"id\")\n    .when_matched_update_all()\n    .when_not_matched_insert_all()\n    .execute(new_users)\n)\ntable.count_rows()  # 3\nres  # {'num_inserted_rows': 1, 'num_updated_rows': 1, 'num_deleted_rows': 0}\n</code></pre> <pre><code>table = await db.create_table(\n    \"users\",\n    [\n        {\"id\": 0, \"name\": \"Alice\"},\n        {\"id\": 1, \"name\": \"Bob\"},\n    ],\n)\nnew_users = [\n    {\"id\": 1, \"name\": \"Bobby\"},\n    {\"id\": 2, \"name\": \"Charlie\"},\n]\nres = await (\n    table.merge_insert(\"id\")\n    .when_matched_update_all()\n    .when_not_matched_insert_all()\n    .execute(new_users)\n)\nawait table.count_rows()  # 3\nres\n# MergeResult(version=2, num_updated_rows=1,\n# num_inserted_rows=1, num_deleted_rows=0)\n</code></pre> @lancedb/lancedb <pre><code>const table = await db.createTable(\"users\", [\n  { id: 0, name: \"Alice\" },\n  { id: 1, name: \"Bob\" },\n]);\n\nconst newUsers = [\n  { id: 1, name: \"Bobby\" },\n  { id: 2, name: \"Charlie\" },\n];\nawait table\n  .mergeInsert(\"id\")\n  .whenMatchedUpdateAll()\n  .whenNotMatchedInsertAll()\n  .execute(newUsers);\n\nawait table.countRows(); // 3\n</code></pre> <p>Providing subsets of columns</p> <p>If a column is nullable, it can be omitted from input data and it will be considered <code>null</code>. Columns can also be provided in any order.</p>"},{"location":"guides/tables/update/#insert-if-not-exists_1","title":"Insert-if-not-exists","text":"<p>To avoid inserting duplicate rows, you can use the insert-if-not-exists command. This will only insert rows that do not have a match in the target table. To do this with merge insert, enable just <code>when_not_matched_insert_all()</code>.</p> PythonTypescript Sync APIAsync API <pre><code>table = db.create_table(\n    \"domains\",\n    [\n        {\"domain\": \"google.com\", \"name\": \"Google\"},\n        {\"domain\": \"github.com\", \"name\": \"GitHub\"},\n    ],\n)\nnew_domains = [\n    {\"domain\": \"google.com\", \"name\": \"Google\"},\n    {\"domain\": \"facebook.com\", \"name\": \"Facebook\"},\n]\nres = (\n    table.merge_insert(\"domain\").when_not_matched_insert_all().execute(new_domains)\n)\ntable.count_rows()  # 3\nres\n# MergeResult(version=2, num_updated_rows=0,\n# num_inserted_rows=1, num_deleted_rows=0)\n</code></pre> <pre><code>    table = await db.create_table(\n        \"domains\",\n        [\n            {\"domain\": \"google.com\", \"name\": \"Google\"},\n            {\"domain\": \"github.com\", \"name\": \"GitHub\"},\n        ],\n    )\n    new_domains = [\n        {\"domain\": \"google.com\", \"name\": \"Google\"},\n        {\"domain\": \"facebook.com\", \"name\": \"Facebook\"},\n    ]\n    res = await (\n        table.merge_insert(\"domain\").when_not_matched_insert_all().execute(new_domains)\n    )\n    await table.count_rows()  # 3\n    res\n    # MergeResult(version=2, num_updated_rows=0,\n    # num_inserted_rows=1, num_deleted_rows=0)\n    assert await table.count_rows() == 3\n    assert res.version == 2\n    assert res.num_inserted_rows == 1\n    assert res.num_deleted_rows == 0\n    assert res.num_updated_rows == 0\n\n\ndef test_replace_range(mem_db):\n    db = mem_db\n    table = db.create_table(\n        \"chunks\",\n        [\n            {\"doc_id\": 0, \"chunk_id\": 0, \"text\": \"Hello\"},\n            {\"doc_id\": 0, \"chunk_id\": 1, \"text\": \"World\"},\n            {\"doc_id\": 1, \"chunk_id\": 0, \"text\": \"Foo\"},\n            {\"doc_id\": 1, \"chunk_id\": 1, \"text\": \"Bar\"},\n        ],\n    )\n    new_chunks = [\n        {\"doc_id\": 1, \"chunk_id\": 0, \"text\": \"Baz\"},\n    ]\n    res = (\n        table.merge_insert([\"doc_id\", \"chunk_id\"])\n        .when_matched_update_all()\n        .when_not_matched_insert_all()\n        .when_not_matched_by_source_delete(\"doc_id = 1\")\n        .execute(new_chunks)\n    )\n    table.count_rows(\"doc_id = 1\")  # 1\n    res\n    # MergeResult(version=2, num_updated_rows=1,\n    # num_inserted_rows=0, num_deleted_rows=1)\n    assert table.count_rows(\"doc_id = 1\") == 1\n    assert res.version == 2\n    assert res.num_inserted_rows == 0\n    assert res.num_deleted_rows == 1\n    assert res.num_updated_rows == 1\n\n\n@pytest.mark.asyncio\nasync def test_replace_range_async(mem_db_async):\n    db = mem_db_async\n    table = await db.create_table(\n        \"chunks\",\n        [\n            {\"doc_id\": 0, \"chunk_id\": 0, \"text\": \"Hello\"},\n            {\"doc_id\": 0, \"chunk_id\": 1, \"text\": \"World\"},\n            {\"doc_id\": 1, \"chunk_id\": 0, \"text\": \"Foo\"},\n            {\"doc_id\": 1, \"chunk_id\": 1, \"text\": \"Bar\"},\n        ],\n    )\n    new_chunks = [\n        {\"doc_id\": 1, \"chunk_id\": 0, \"text\": \"Baz\"},\n    ]\n    res = await (\n        table.merge_insert([\"doc_id\", \"chunk_id\"])\n        .when_matched_update_all()\n        .when_not_matched_insert_all()\n        .when_not_matched_by_source_delete(\"doc_id = 1\")\n        .execute(new_chunks)\n    )\n    await table.count_rows(\"doc_id = 1\")  # 1\n    res\n    # MergeResult(version=2, num_updated_rows=1,\n    # num_inserted_rows=0, num_deleted_rows=1)\n    assert await table.count_rows(\"doc_id = 1\") == 1\n    assert res.version == 2\n    assert res.num_inserted_rows == 0\n    assert res.num_deleted_rows == 1\n    assert res.num_updated_rows == 1\n</code></pre> @lancedb/lancedb <pre><code>const table2 = await db.createTable(\"domains\", [\n  { domain: \"google.com\", name: \"Google\" },\n  { domain: \"github.com\", name: \"GitHub\" },\n]);\n\nconst newDomains = [\n  { domain: \"google.com\", name: \"Google\" },\n  { domain: \"facebook.com\", name: \"Facebook\" },\n];\nawait table2\n  .mergeInsert(\"domain\")\n  .whenNotMatchedInsertAll()\n  .execute(newDomains);\nawait table2.countRows(); // 3\n</code></pre>"},{"location":"guides/tables/update/#replace-range_1","title":"Replace range","text":"<p>You can also replace a range of rows in the target table with the input data. For example, if you have a table of document chunks, where each chunk has both a <code>doc_id</code> and a <code>chunk_id</code>, you can replace all chunks for a given <code>doc_id</code> with updated chunks. This can be tricky otherwise because if you try to use upsert when the new data has fewer chunks you will end up with extra chunks. To avoid this, add another clause to delete any chunks for the document that are not in the new data, with <code>when_not_matched_by_source_delete</code>.</p> PythonTypescript Sync APIAsync API <pre><code>    table = db.create_table(\n        \"chunks\",\n        [\n            {\"doc_id\": 0, \"chunk_id\": 0, \"text\": \"Hello\"},\n            {\"doc_id\": 0, \"chunk_id\": 1, \"text\": \"World\"},\n            {\"doc_id\": 1, \"chunk_id\": 0, \"text\": \"Foo\"},\n            {\"doc_id\": 1, \"chunk_id\": 1, \"text\": \"Bar\"},\n        ],\n    )\n    new_chunks = [\n        {\"doc_id\": 1, \"chunk_id\": 0, \"text\": \"Baz\"},\n    ]\n    res = (\n        table.merge_insert([\"doc_id\", \"chunk_id\"])\n        .when_matched_update_all()\n        .when_not_matched_insert_all()\n        .when_not_matched_by_source_delete(\"doc_id = 1\")\n        .execute(new_chunks)\n    )\n    table.count_rows(\"doc_id = 1\")  # 1\n    res\n    # MergeResult(version=2, num_updated_rows=1,\n    # num_inserted_rows=0, num_deleted_rows=1)\n    assert table.count_rows(\"doc_id = 1\") == 1\n    assert res.version == 2\n    assert res.num_inserted_rows == 0\n    assert res.num_deleted_rows == 1\n    assert res.num_updated_rows == 1\n\n\n@pytest.mark.asyncio\nasync def test_replace_range_async(mem_db_async):\n    db = mem_db_async\n    table = await db.create_table(\n        \"chunks\",\n        [\n            {\"doc_id\": 0, \"chunk_id\": 0, \"text\": \"Hello\"},\n            {\"doc_id\": 0, \"chunk_id\": 1, \"text\": \"World\"},\n            {\"doc_id\": 1, \"chunk_id\": 0, \"text\": \"Foo\"},\n            {\"doc_id\": 1, \"chunk_id\": 1, \"text\": \"Bar\"},\n        ],\n    )\n    new_chunks = [\n        {\"doc_id\": 1, \"chunk_id\": 0, \"text\": \"Baz\"},\n    ]\n    res = await (\n        table.merge_insert([\"doc_id\", \"chunk_id\"])\n        .when_matched_update_all()\n        .when_not_matched_insert_all()\n        .when_not_matched_by_source_delete(\"doc_id = 1\")\n        .execute(new_chunks)\n    )\n    await table.count_rows(\"doc_id = 1\")  # 1\n    res\n    # MergeResult(version=2, num_updated_rows=1,\n    # num_inserted_rows=0, num_deleted_rows=1)\n    assert await table.count_rows(\"doc_id = 1\") == 1\n    assert res.version == 2\n    assert res.num_inserted_rows == 0\n    assert res.num_deleted_rows == 1\n    assert res.num_updated_rows == 1\n</code></pre> <pre><code>table = await db.create_table(\n    \"chunks\",\n    [\n        {\"doc_id\": 0, \"chunk_id\": 0, \"text\": \"Hello\"},\n        {\"doc_id\": 0, \"chunk_id\": 1, \"text\": \"World\"},\n        {\"doc_id\": 1, \"chunk_id\": 0, \"text\": \"Foo\"},\n        {\"doc_id\": 1, \"chunk_id\": 1, \"text\": \"Bar\"},\n    ],\n)\nnew_chunks = [\n    {\"doc_id\": 1, \"chunk_id\": 0, \"text\": \"Baz\"},\n]\nres = await (\n    table.merge_insert([\"doc_id\", \"chunk_id\"])\n    .when_matched_update_all()\n    .when_not_matched_insert_all()\n    .when_not_matched_by_source_delete(\"doc_id = 1\")\n    .execute(new_chunks)\n)\nawait table.count_rows(\"doc_id = 1\")  # 1\nres\n# MergeResult(version=2, num_updated_rows=1,\n# num_inserted_rows=0, num_deleted_rows=1)\nassert await table.count_rows(\"doc_id = 1\") == 1\nassert res.version == 2\nassert res.num_inserted_rows == 0\nassert res.num_deleted_rows == 1\nassert res.num_updated_rows == 1\n</code></pre> @lancedb/lancedb <pre><code>const table3 = await db.createTable(\"chunks\", [\n  { doc_id: 0, chunk_id: 0, text: \"Hello\" },\n  { doc_id: 0, chunk_id: 1, text: \"World\" },\n  { doc_id: 1, chunk_id: 0, text: \"Foo\" },\n  { doc_id: 1, chunk_id: 1, text: \"Bar\" },\n]);\n\nconst newChunks = [{ doc_id: 1, chunk_id: 0, text: \"Baz\" }];\n\nawait table3\n  .mergeInsert([\"doc_id\", \"chunk_id\"])\n  .whenMatchedUpdateAll()\n  .whenNotMatchedInsertAll()\n  .whenNotMatchedBySourceDelete({ where: \"doc_id = 1\" })\n  .execute(newChunks);\n\nawait table3.countRows(\"doc_id = 1\"); // 1\n</code></pre> <p>title: Modifying Tables in LanceDB description: Learn how to add, update, and delete data in LanceDB tables.</p>"},{"location":"guides/tables/update/#modifying-tables-in-lancedb","title":"Modifying Tables in LanceDB","text":""},{"location":"guides/tables/update/#adding-to-a-table","title":"Adding to a table","text":"<p>After a table has been created, you can always add more data to it using the <code>add</code> method</p> PythonTypescript<sup>1</sup> <p>You can add any of the valid data structures accepted by LanceDB table, i.e, <code>dict</code>, <code>list[dict]</code>, <code>pd.DataFrame</code>, or <code>Iterator[pa.RecordBatch]</code>. Below are some examples.</p> <pre><code>await tbl.add(\n    [\n        {vector: [1.3, 1.4], item: \"fizz\", price: 100.0},\n        {vector: [9.5, 56.2], item: \"buzz\", price: 200.0}\n    ]\n)\n</code></pre>"},{"location":"guides/tables/update/#add-a-pandas-dataframe","title":"Add a Pandas DataFrame","text":"Sync APIAsync API <pre><code>df = pd.DataFrame(\n    {\n        \"vector\": [[1.3, 1.4], [9.5, 56.2]],\n        \"item\": [\"banana\", \"apple\"],\n        \"price\": [5.0, 7.0],\n    }\n)\n\ntbl.add(df)\n</code></pre> <pre><code>df = pd.DataFrame(\n    {\n        \"vector\": [[1.3, 1.4], [9.5, 56.2]],\n        \"item\": [\"banana\", \"apple\"],\n        \"price\": [5.0, 7.0],\n    }\n)\nawait async_tbl.add(df)\n</code></pre>"},{"location":"guides/tables/update/#add-a-polars-dataframe","title":"Add a Polars DataFrame","text":"Sync APIAsync API <pre><code>df = pl.DataFrame(\n    {\n        \"vector\": [[1.3, 1.4], [9.5, 56.2]],\n        \"item\": [\"banana\", \"apple\"],\n        \"price\": [5.0, 7.0],\n    }\n)\n\ntbl.add(df)\n</code></pre> <pre><code>df = pl.DataFrame(\n    {\n        \"vector\": [[1.3, 1.4], [9.5, 56.2]],\n        \"item\": [\"banana\", \"apple\"],\n        \"price\": [5.0, 7.0],\n    }\n)\nawait async_tbl.add(df)\n</code></pre>"},{"location":"guides/tables/update/#add-an-iterator","title":"Add an Iterator","text":"<p>You can also add a large dataset batch in one go using Iterator of any supported data types.</p> Sync APIAsync API <pre><code>def make_batches_for_add():\n    for i in range(5):\n        yield [\n            {\"vector\": [3.1, 4.1], \"item\": \"peach\", \"price\": 6.0},\n            {\"vector\": [5.9, 26.5], \"item\": \"pear\", \"price\": 5.0},\n        ]\n\n\ntbl.add(make_batches_for_add())\n</code></pre> <pre><code>def make_batches_for_add():\n    for i in range(5):\n        yield [\n            {\"vector\": [3.1, 4.1], \"item\": \"peach\", \"price\": 6.0},\n            {\"vector\": [5.9, 26.5], \"item\": \"pear\", \"price\": 5.0},\n        ]\n\n\nawait async_tbl.add(make_batches_for_add())\n</code></pre>"},{"location":"guides/tables/update/#add-a-pyarrow-table","title":"Add a PyArrow table","text":"<p>If you have data coming in as a PyArrow table, you can add it directly to the LanceDB table.</p> Sync APIAsync API <pre><code>pa_table = pa.Table.from_arrays(\n    [\n        pa.array([[9.1, 6.7], [9.9, 31.2]], pa.list_(pa.float32(), 2)),\n        pa.array([\"mango\", \"orange\"]),\n        pa.array([7.0, 4.0]),\n    ],\n    [\"vector\", \"item\", \"price\"],\n)\ntbl.add(pa_table)\n</code></pre> <pre><code>pa_table = pa.Table.from_arrays(\n    [\n        pa.array([[9.1, 6.7], [9.9, 31.2]], pa.list_(pa.float32(), 2)),\n        pa.array([\"mango\", \"orange\"]),\n        pa.array([7.0, 4.0]),\n    ],\n    [\"vector\", \"item\", \"price\"],\n)\nawait async_tbl.add(pa_table)\n</code></pre>"},{"location":"guides/tables/update/#add-a-pydantic-model","title":"Add a Pydantic Model","text":"<p>Assuming that a table has been created with the correct schema as shown in Creating Tables, you can add data items that are valid Pydantic models to the table.</p> Sync APIAsync API <pre><code>pydantic_model_items = [\n    Item(vector=[8.1, 4.7], item=\"pineapple\", price=10.0),\n    Item(vector=[6.9, 9.3], item=\"avocado\", price=9.0),\n]\ntbl.add(pydantic_model_items)\n</code></pre> <pre><code>pydantic_model_items = [\n    Item(vector=[8.1, 4.7], item=\"pineapple\", price=10.0),\n    Item(vector=[6.9, 9.3], item=\"avocado\", price=9.0),\n]\nawait async_tbl.add(pydantic_model_items)\n</code></pre> Ingesting Pydantic models with LanceDB embedding API <p>When using LanceDB's embedding API, you can add Pydantic models directly to the table. LanceDB will automatically convert the <code>vector</code> field to a vector before adding it to the table. You need to specify the default value of <code>vector</code> field as None to allow LanceDB to automatically vectorize the data.</p> Sync APIAsync API <pre><code>import lancedb\n\nfrom lancedb.pydantic import Vector, LanceModel\n\nfrom lancedb.embeddings import get_registry\n\nembed_fcn = get_registry().get(\"huggingface\").create(name=\"BAAI/bge-small-en-v1.5\")\n\nclass Schema(LanceModel):\n    text: str = embed_fcn.SourceField()\n    vector: Vector(embed_fcn.ndims()) = embed_fcn.VectorField(default=None)\n\ntbl = db.create_table(\"my_table_with_embedding\", schema=Schema, mode=\"overwrite\")\nmodels = [Schema(text=\"hello\"), Schema(text=\"world\")]\ntbl.add(models)\n</code></pre> <pre><code>import lancedb\n\nfrom lancedb.pydantic import Vector, LanceModel\n\nfrom lancedb.embeddings import get_registry\n\nembed_fcn = get_registry().get(\"huggingface\").create(name=\"BAAI/bge-small-en-v1.5\")\n\nclass Schema(LanceModel):\n    text: str = embed_fcn.SourceField()\n    vector: Vector(embed_fcn.ndims()) = embed_fcn.VectorField(default=None)\n\nasync_tbl = await async_db.create_table(\n    \"my_table_async_with_embedding\", schema=Schema, mode=\"overwrite\"\n)\nmodels = [Schema(text=\"hello\"), Schema(text=\"world\")]\nawait async_tbl.add(models)\n</code></pre>"},{"location":"guides/tables/update/#upserting-into-a-table","title":"Upserting into a table","text":"<p>Upserting lets you insert new rows or update existing rows in a table. To upsert in LanceDB, use the merge insert API.</p> PythonTypescript<sup>1</sup> Sync APIAsync API <p><pre><code>table = db.create_table(\n    \"users\",\n    [\n        {\"id\": 0, \"name\": \"Alice\"},\n        {\"id\": 1, \"name\": \"Bob\"},\n    ],\n)\nnew_users = [\n    {\"id\": 1, \"name\": \"Bobby\"},\n    {\"id\": 2, \"name\": \"Charlie\"},\n]\nres = (\n    table.merge_insert(\"id\")\n    .when_matched_update_all()\n    .when_not_matched_insert_all()\n    .execute(new_users)\n)\ntable.count_rows()  # 3\nres  # {'num_inserted_rows': 1, 'num_updated_rows': 1, 'num_deleted_rows': 0}\n</code></pre> API Reference: lancedb.table.Table.merge_insert</p> <p><pre><code>table = await db.create_table(\n    \"users\",\n    [\n        {\"id\": 0, \"name\": \"Alice\"},\n        {\"id\": 1, \"name\": \"Bob\"},\n    ],\n)\nnew_users = [\n    {\"id\": 1, \"name\": \"Bobby\"},\n    {\"id\": 2, \"name\": \"Charlie\"},\n]\nres = await (\n    table.merge_insert(\"id\")\n    .when_matched_update_all()\n    .when_not_matched_insert_all()\n    .execute(new_users)\n)\nawait table.count_rows()  # 3\nres\n# MergeResult(version=2, num_updated_rows=1,\n# num_inserted_rows=1, num_deleted_rows=0)\n</code></pre> API Reference: lancedb.table.AsyncTable.merge_insert</p> @lancedb/lancedb <p><pre><code>const table = await db.createTable(\"users\", [\n  { id: 0, name: \"Alice\" },\n  { id: 1, name: \"Bob\" },\n]);\n\nconst newUsers = [\n  { id: 1, name: \"Bobby\" },\n  { id: 2, name: \"Charlie\" },\n];\nawait table\n  .mergeInsert(\"id\")\n  .whenMatchedUpdateAll()\n  .whenNotMatchedInsertAll()\n  .execute(newUsers);\n\nawait table.countRows(); // 3\n</code></pre> API Reference: lancedb.Table.mergeInsert</p> <p>Read more in the guide on merge insert.</p>"},{"location":"guides/tables/update/#deleting-from-a-table","title":"Deleting from a table","text":"<p>Use the <code>delete()</code> method on tables to delete rows from a table. To choose which rows to delete, provide a filter that matches on the metadata columns. This can delete any number of rows that match the filter.</p> PythonTypescript<sup>1</sup> Sync APIAsync API <pre><code>tbl.delete('item = \"fizz\"')\n</code></pre> <pre><code>await async_tbl.delete('item = \"fizz\"')\n</code></pre> <pre><code>await tbl.delete('item = \"fizz\"')\n</code></pre>"},{"location":"guides/tables/update/#deleting-row-with-specific-column-value","title":"Deleting row with specific column value","text":"Sync APIAsync API <pre><code>data = [\n    {\"x\": 1, \"vector\": [1, 2]},\n    {\"x\": 2, \"vector\": [3, 4]},\n    {\"x\": 3, \"vector\": [5, 6]},\n]\n# Synchronous client\ntbl = db.create_table(\"delete_row\", data)\ntbl.to_pandas()\n#   x      vector\n# 0  1  [1.0, 2.0]\n# 1  2  [3.0, 4.0]\n# 2  3  [5.0, 6.0]\n\ntbl.delete(\"x = 2\")\ntbl.to_pandas()\n#   x      vector\n# 0  1  [1.0, 2.0]\n# 1  3  [5.0, 6.0]\n</code></pre> <pre><code>data = [\n    {\"x\": 1, \"vector\": [1, 2]},\n    {\"x\": 2, \"vector\": [3, 4]},\n    {\"x\": 3, \"vector\": [5, 6]},\n]\nasync_db = await lancedb.connect_async(uri)\nasync_tbl = await async_db.create_table(\"delete_row_async\", data)\nawait async_tbl.to_pandas()\n#   x      vector\n# 0  1  [1.0, 2.0]\n# 1  2  [3.0, 4.0]\n# 2  3  [5.0, 6.0]\n\nawait async_tbl.delete(\"x = 2\")\nawait async_tbl.to_pandas()\n#   x      vector\n# 0  1  [1.0, 2.0]\n# 1  3  [5.0, 6.0]\n</code></pre>"},{"location":"guides/tables/update/#delete-from-a-list-of-values","title":"Delete from a list of values","text":"Sync APIAsync API <pre><code>to_remove = [1, 5]\nto_remove = \", \".join(str(v) for v in to_remove)\n\ntbl.delete(f\"x IN ({to_remove})\")\ntbl.to_pandas()\n#   x      vector\n# 0  3  [5.0, 6.0]\n</code></pre> <pre><code>to_remove = [1, 5]\nto_remove = \", \".join(str(v) for v in to_remove)\n\nawait async_tbl.delete(f\"x IN ({to_remove})\")\nawait async_tbl.to_pandas()\n#   x      vector\n# 0  3  [5.0, 6.0]\n</code></pre>"},{"location":"guides/tables/update/#deleting-row-with-specific-column-value_1","title":"Deleting row with specific column value","text":"<pre><code>const con = await lancedb.connect(\"./.lancedb\")\nconst data = [\n  {id: 1, vector: [1, 2]},\n  {id: 2, vector: [3, 4]},\n  {id: 3, vector: [5, 6]},\n];\nconst tbl = await con.createTable(\"my_table\", data)\nawait tbl.delete(\"id = 2\")\nawait tbl.countRows() // Returns 2\n</code></pre>"},{"location":"guides/tables/update/#delete-from-a-list-of-values_1","title":"Delete from a list of values","text":"<pre><code>const to_remove = [1, 5];\nawait tbl.delete(`id IN (${to_remove.join(\",\")})`)\nawait tbl.countRows() // Returns 1\n</code></pre>"},{"location":"guides/tables/update/#drop-a-table","title":"Drop a table","text":"<p>Use the <code>drop_table()</code> method on the database to remove a table.</p> PythonTypeScript Sync APIAsync API <pre><code>db.drop_table(\"my_table\")\n</code></pre> <pre><code>await db.drop_table(\"my_table_async\")\n</code></pre> <p>This permanently removes the table and is not recoverable, unlike deleting rows.   By default, if the table does not exist an exception is raised. To suppress this,   you can pass in <code>ignore_missing=True</code>.</p> <pre><code>await db.dropTable(\"myTable\");\n</code></pre> <p>This permanently removes the table and is not recoverable, unlike deleting rows.   If the table does not exist an exception is raised.</p>"},{"location":"guides/tables/update/#updating-a-table","title":"Updating a table","text":"<p>This can be used to update zero to all rows depending on how many rows match the where clause. The update queries follow the form of a SQL UPDATE statement. The <code>where</code> parameter is a SQL filter that matches on the metadata columns. The <code>values</code> or <code>values_sql</code> parameters are used to provide the new values for the columns.</p> Parameter Type Description <code>where</code> <code>str</code> The SQL where clause to use when updating rows. For example, <code>'x = 2'</code> or <code>'x IN (1, 2, 3)'</code>. The filter must not be empty, or it will error. <code>values</code> <code>dict</code> The values to update. The keys are the column names and the values are the values to set. <code>values_sql</code> <code>dict</code> The values to update. The keys are the column names and the values are the SQL expressions to set. For example, <code>{'x': 'x + 1'}</code> will increment the value of the <code>x</code> column by 1. <p>SQL syntax</p> <p>See SQL filters for more information on the supported SQL syntax.</p> <p>Warning</p> <p>Updating nested columns is not yet supported.</p> PythonTypescript<sup>1</sup> <p>API Reference: lancedb.table.Table.update</p> Sync APIAsync API <pre><code>import lancedb\n\nimport pandas as pd\n\n# Create a table from a pandas DataFrame\ndata = pd.DataFrame({\"x\": [1, 2, 3], \"vector\": [[1, 2], [3, 4], [5, 6]]})\n\ntbl = db.create_table(\"test_table\", data, mode=\"overwrite\")\n# Update the table where x = 2\ntbl.update(where=\"x = 2\", values={\"vector\": [10, 10]})\n# Get the updated table as a pandas DataFrame\ndf = tbl.to_pandas()\nprint(df)\n</code></pre> <pre><code>import lancedb\n\nimport pandas as pd\n\n# Create a table from a pandas DataFrame\ndata = pd.DataFrame({\"x\": [1, 2, 3], \"vector\": [[1, 2], [3, 4], [5, 6]]})\n\nasync_tbl = await async_db.create_table(\"update_table_async\", data)\n# Update the table where x = 2\nawait async_tbl.update({\"vector\": [10, 10]}, where=\"x = 2\")\n# Get the updated table as a pandas DataFrame\ndf = await async_tbl.to_pandas()\n# Print the DataFrame\nprint(df)\n</code></pre> <p>Output <pre><code>    x  vector\n0  1  [1.0, 2.0]\n1  3  [5.0, 6.0]\n2  2  [10.0, 10.0]\n</code></pre></p> @lancedb/lancedbvectordb (deprecated) <p>API Reference: lancedb.Table.update</p> <pre><code>import * as lancedb from \"@lancedb/lancedb\";\n\nconst db = await lancedb.connect(\"./.lancedb\");\n\nconst data = [\n    {x: 1, vector: [1, 2]},\n    {x: 2, vector: [3, 4]},\n    {x: 3, vector: [5, 6]},\n];\nconst tbl = await db.createTable(\"my_table\", data)\n\nawait tbl.update({ \n    values: { vector: [10, 10] },\n    where: \"x = 2\"\n});\n</code></pre> <p>API Reference: vectordb.Table.update</p> <pre><code>const lancedb = require(\"vectordb\");\n\nconst db = await lancedb.connect(\"./.lancedb\");\n\nconst data = [\n    {x: 1, vector: [1, 2]},\n    {x: 2, vector: [3, 4]},\n    {x: 3, vector: [5, 6]},\n];\nconst tbl = await db.createTable(\"my_table\", data)\n\nawait tbl.update({ \n    where: \"x = 2\", \n    values: { vector: [10, 10] } \n});\n</code></pre>"},{"location":"guides/tables/update/#updating-using-a-sql-query","title":"Updating using a sql query","text":"<p>The <code>values</code> parameter is used to provide the new values for the columns as literal values. You can also use the <code>values_sql</code> / <code>valuesSql</code> parameter to provide SQL expressions for the new values. For example, you can use <code>values_sql=\"x + 1\"</code> to increment the value of the <code>x</code> column by 1.</p> PythonTypescript<sup>1</sup> Sync APIAsync API <pre><code># Update the table where x = 2\ntbl.update(values_sql={\"x\": \"x + 1\"})\nprint(tbl.to_pandas())\n</code></pre> <pre><code># Update the table where x = 2\nawait async_tbl.update(updates_sql={\"x\": \"x + 1\"})\nprint(await async_tbl.to_pandas())\n</code></pre> <p>Output <pre><code>    x  vector\n0  2  [1.0, 2.0]\n1  4  [5.0, 6.0]\n2  3  [10.0, 10.0]\n</code></pre></p> @lancedb/lancedbvectordb (deprecated) <p>Coming Soon!</p> <pre><code>await tbl.update({ valuesSql: { x: \"x + 1\" } })\n</code></pre> <p>Note</p> <p>When rows are updated, they are moved out of the index. The row will still show up in ANN queries, but the query will not be as fast as it would be if the row was in the index. If you update a large proportion of rows, consider rebuilding the index afterwards.</p>"},{"location":"guides/tables/update/#handling-bad-vectors","title":"Handling bad vectors","text":"<p>In LanceDB Python, you can use the <code>on_bad_vectors</code> parameter to choose how invalid vector values are handled. Invalid vectors are vectors that are not valid because:</p> <ol> <li>They are the wrong dimension</li> <li>They contain NaN values</li> <li>They are null but are on a non-nullable field</li> </ol> <p>By default, LanceDB will raise an error if it encounters a bad vector. You can also choose one of the following options:</p> <ul> <li><code>drop</code>: Ignore rows with bad vectors</li> <li><code>fill</code>: Replace bad values (NaNs) or missing values (too few dimensions) with     the fill value specified in the <code>fill_value</code> parameter. An input like     <code>[1.0, NaN, 3.0]</code> will be replaced with <code>[1.0, 0.0, 3.0]</code> if <code>fill_value=0.0</code>.</li> <li><code>null</code>: Replace bad vectors with null (only works if the column is nullable).     A bad vector <code>[1.0, NaN, 3.0]</code> will be replaced with <code>null</code> if the column is     nullable. If the vector column is non-nullable, then bad vectors will cause an     error</li> </ul>"},{"location":"guides/tables/update/#whats-next","title":"What's next?","text":"<p>Learn about managing schemas in your tables.</p> <ol> <li> <p>The <code>vectordb</code> package is a legacy package that is deprecated in favor of <code>@lancedb/lancedb</code>. The <code>vectordb</code> package will continue to receive bug fixes and security updates until September 2024. We recommend all new projects use <code>@lancedb/lancedb</code>. See the migration guide for more information.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"guides/tables/versioning/","title":"Version Control in LanceDB","text":"<p>LanceDB redefines data management for AI/ML workflows with built-in,  automatic versioning powered by the Lance columnar format.  Every table mutation\u2014appends, updates, deletions, or schema changes \u2014 is tracked with  zero configuration, enabling: - Time-Travel Debugging: Pinpoint production issues by querying historical table states. - Atomic Rollbacks: Revert terabyte-scale datasets to any prior version in seconds. - ML Reproducibility: Exactly reproduce training snapshots (vectors + metadata). - Branching Workflows: Conduct A/B tests on embeddings/models via lightweight table clones.</p> PythonTypeScript <pre><code>import lancedb\nimport pandas as pd\nimport numpy as np\nimport pyarrow as pa\nfrom sentence_transformers import SentenceTransformer\n\n# Connect to LanceDB\ndb = lancedb.connect(\n  uri=\"db://your-project-slug\",\n  api_key=\"your-api-key\",\n  region=\"us-east-1\"\n)\n\n# Create a table with initial data\ntable_name = \"quotes_versioning_example\"\ndata = [\n    {\"id\": 1, \"author\": \"Richard\", \"quote\": \"Wubba Lubba Dub Dub!\"},\n    {\"id\": 2, \"author\": \"Morty\", \"quote\": \"Rick, what's going on?\"},\n    {\n        \"id\": 3,\n        \"author\": \"Richard\",\n        \"quote\": \"I turned myself into a pickle, Morty!\",\n    },\n]\n\n# Define schema\nschema = pa.schema(\n    [\n        pa.field(\"id\", pa.int64()),\n        pa.field(\"author\", pa.string()),\n        pa.field(\"quote\", pa.string()),\n    ]\n)\n\ntable = db.create_table(table_name, data, schema=schema, mode=\"overwrite\")\n\n# View the initial version\nversions = table.list_versions()\nprint(f\"Number of versions after creation: {len(versions)}\")\nprint(f\"Current version: {table.version}\")\n</code></pre> <pre><code>import * as lancedb from \"@lancedb/lancedb\";\nimport {\n  Schema,\n  Field,\n  Utf8,\n  Int64,\n} from \"apache-arrow\";\n\n// Connect to LanceDB\nconst db = await lancedb.connect({\n  uri: \"db://your-project-slug\",\n  apiKey: \"your-api-key\",\n  region: \"us-east-1\"\n});\n\n// Create a table with initial data\nconst tableName = \"quotes_versioning_example-ts\";\n\nconst data = [\n  {\n    id: 1,\n    author: \"Richard\",\n    quote: \"Wubba Lubba Dub Dub!\",\n  },\n  {\n    id: 2,\n    author: \"Morty\",\n    quote: \"Rick, what's going on?\",\n  },\n  {\n    id: 3,\n    author: \"Richard\",\n    quote: \"I turned myself into a pickle, Morty!\",\n  },\n];\n\nconst schema = new Schema([\n  new Field(\"author\", new Utf8()),\n  new Field(\"quote\", new Utf8()),\n  new Field(\"id\", new Int64()),\n]);\n\nconst table = await db.createTable(tableName, data, {\n  schema,\n  mode: \"overwrite\",\n});\n\n// View the initial version\nconst versions = await table.listVersions();\nconst versionCountInitial = versions.length;\nconst initialVersion = await table.version();\nconsole.log(`Number of versions after creation: ${versionCountInitial}`);\nconsole.log(`Current version: ${initialVersion}`);\n</code></pre>"},{"location":"guides/tables/versioning/#modifying-data","title":"Modifying Data","text":"<p>When you modify data through operations like update or delete, LanceDB automatically creates new versions.</p> PythonTypeScript <pre><code># Add more data\n# Make changes to the table\ntable.update(where=\"author='Richard'\", values={\"author\": \"Richard Daniel Sanchez\"})\nrows_after_update = table.count_rows()\nprint(f\"Number of rows after update: {rows_after_update}\")\n\n# Add more data\nmore_data = [\n    {\n        \"id\": 4,\n        \"author\": \"Richard Daniel Sanchez\",\n        \"quote\": \"That's the way the news goes!\",\n    },\n    {\"id\": 5, \"author\": \"Morty\", \"quote\": \"Aww geez, Rick!\"},\n]\ntable.add(more_data)\n\n# Check versions after modifications\nversions = table.list_versions()\nversion_count_after_mod = len(versions)\nversion_after_mod = table.version\nprint(f\"Number of versions after modifications: {version_count_after_mod}\")\nprint(f\"Current version: {version_after_mod}\")\n</code></pre> <pre><code>  // Make changes to the table\n  await table.update({\n    where: \"author='Richard'\",\n    values: { author: \"Richard Daniel Sanchez\" },\n  });\n  const rowsAfterUpdate = await table.countRows();\n  console.log(`Number of rows after update: ${rowsAfterUpdate}`);\n  results[\"rows_after_update\"] = rowsAfterUpdate;\n\n  // Add more data\n  const moreData = [\n    {\n      id: 4,\n      author: \"Richard Daniel Sanchez\",\n      quote: \"That's the way the news goes!\",\n    },\n    {\n      id: 5,\n      author: \"Morty\",\n      quote: \"Aww geez, Rick!\",\n    },\n  ];\n  await table.add(moreData);\n\n  // Check versions after modifications\n  const versionsAfterMod = await table.listVersions();\n  const versionCountAfterMod = versionsAfterMod.length;\n  const versionAfterMod = await table.version();\n  console.log(`Number of versions after modifications: ${versionCountAfterMod}`);\n  console.log(`Current version: ${versionAfterMod}`);\n</code></pre>"},{"location":"guides/tables/versioning/#schema-evolution","title":"Schema Evolution","text":"<p>LanceDB's versioning system automatically tracks  every schema modification. This is critical when handling  evolving embedding models. For example, adding a new  vector_minilm column creates a fresh version, enabling seamless A/B testing  between embedding generations without recreating the table. </p> PythonTypeScript <pre><code>import pyarrow as pa\n\n# Get data from table\ndf = table.search().limit(5).to_pandas()\n\n# Let's use \"all-MiniLM-L6-v2\" model to embed the quotes\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\")\n\n# Generate embeddings for each quote and pair with IDs\nvectors = model.encode(\n    df[\"quote\"].tolist(), convert_to_numpy=True, normalize_embeddings=True\n)\nvector_dim = vectors[0].shape[0]\nprint(f\"Vector dimension: {vector_dim}\")\n\n# Add IDs to vectors array with proper column names\nvectors_with_ids = [\n    {\"id\": i + 1, \"vector_minilm\": vec.tolist()} for i, vec in enumerate(vectors)\n]\n\n# Add vector column and merge data\ntable.add_columns(\n  {\"vector_minilm\": f\"arrow_cast(NULL, 'FixedSizeList({vector_dim}, Float32)')\"}\n)\n\ntable.merge_insert(\n  \"id\"\n).when_matched_update_all().when_not_matched_insert_all().execute(vectors_with_ids)\n\n# Check versions after schema change\nversions = table.list_versions()\nversion_count_after_embed = len(versions)\nversion_after_embed = table.version\nprint(f\"Number of versions after adding embeddings: {version_count_after_embed}\")\nprint(f\"Current version: {version_after_embed}\")\n\n# Verify the schema change\n# The table should now include a vector_minilm column containing\n# embeddings generated by the all-MiniLM-L6-v2 model\nprint(table.schema)\n</code></pre> <pre><code>// Get data from table\nconst df = await table.query().limit(5).toArray()\n\nlet vectorDim = 0;\ntry {\n  // Let's use all-MiniLM-L6-v2 model to embed the quotes\n  console.log(\"Generating embeddings with transformers...\");\n  const { pipeline } = await import(\"@xenova/transformers\");\n  const extractor = await pipeline(\n    \"feature-extraction\",\n    \"Xenova/all-MiniLM-L6-v2\",\n  );\n\n  // Generate embeddings for all quotes\n  const quotes = df.map((row) =&gt; row.quote);\n  const outputs = await Promise.all(\n    quotes.map((quote) =&gt;\n      extractor(quote, {\n        pooling: \"mean\",\n        normalize: true,\n      }),\n    ),\n  );\n  const embeddings = outputs.map((output) =&gt; Array.from(output.data));\n  vectorDim = embeddings[0].length;\n  console.log(`Vector dimension: ${vectorDim}`);\n\n  // Create embedding_with_id for all quotes\n  const embedding_with_id = df.map((row, i) =&gt; ({\n    id: row.id,\n    vector_minilm: embeddings[i],\n  }));\n\n  // Add the vector column to the table\n  await table.addColumns([\n    {\n      name: \"vector_minilm\",\n      valueSql: `arrow_cast(NULL, 'FixedSizeList(${vectorDim}, Float32)')`,\n    },\n  ]);\n\n  // Update all rows with their embeddings\n  await table\n    .mergeInsert(\"id\")\n    .whenMatchedUpdateAll()\n    .whenNotMatchedInsertAll()\n    .execute(embedding_with_id);\n} catch (error) {\n  console.log(\n    \"Failed to load transformers, using dummy vectors instead:\",\n    error,\n  );\n  // Create dummy embeddings for all quotes\n  const dummyEmbeddings = df.map(() =&gt; Array(vectorDim).fill(10));\n  const embedding_with_id = df.map((row, i) =&gt; ({\n    id: row.id,\n    vector: dummyEmbeddings[i],\n  }));\n  await table\n    .mergeInsert(\"id\")\n    .whenMatchedUpdateAll()\n    .whenNotMatchedInsertAll()\n    .execute(embedding_with_id);\n}\n\n// Check versions after embedding addition\nconst versionsAfterEmbed = await table.listVersions();\nconst versionCountAfterEmbed = versionsAfterEmbed.length;\nconst versionAfterEmbed = await table.version();\nconsole.log(\n  `Number of versions after adding embeddings: ${versionCountAfterEmbed}`,\n);\nconsole.log(`Current version: ${versionAfterEmbed}`);\n\n// Verify the schema change\n// The table should now include a vector_minilm column containing\n// embeddings generated by the all-MiniLM-L6-v2 model\nconsole.log(await table.schema())\n</code></pre>"},{"location":"guides/tables/versioning/#rollback-to-previous-versions","title":"Rollback to Previous Versions","text":"<p>LanceDB supports fast rollbacks to any previous version without data duplication.</p> PythonTypeScript <pre><code># Let's see all versions\nversions = table.list_versions()\nfor v in versions:\n    print(f\"Version {v['version']}, created at {v['timestamp']}\")\n\n# Let's roll back to before we added the vector column\n# We'll use the version after modifications but before adding embeddings\ntable.restore(version_after_mod)\n\n# Notice we have one more version now, not less!\nversions = table.list_versions()\nversion_count_after_rollback = len(versions)\nprint(f\"Total number of versions after rollback: {version_count_after_rollback}\")\n</code></pre> <pre><code>// Let's see all versions\nconst allVersions = await table.listVersions();\nallVersions.forEach(v =&gt; {\n  console.log(`Version ${v.version}, created at ${v.timestamp}`);\n});\n\n// Let's roll back to before we added the vector column\n// We'll use the version after modifications but before adding embeddings\nawait table.checkout(versionAfterMod);\nawait table.restore();\n\n// Notice we have one more version now, not less!\nconst versionsAfterRollback = await table.listVersions();\nconst versionCountAfterRollback = versionsAfterRollback.length;\nconsole.log(\n  `Total number of versions after rollback: ${versionCountAfterRollback}`,\n);\n</code></pre>"},{"location":"guides/tables/versioning/#making-changes-from-previous-versions","title":"Making Changes from Previous Versions","text":"<p>After restoring a table to an earlier version, you can continue making modifications. In this example,  we rolled back to a version before adding embeddings. This allows us to experiment with different  embedding models and compare their performance. Here's how to switch to a different model and add new embeddings:</p> PythonTypeScript <pre><code># Let's switch to the all-mpnet-base-v2 model to embed the quotes\nmodel = SentenceTransformer(\"all-mpnet-base-v2\", device=\"cpu\")\n\n# Generate embeddings for each quote and pair with IDs\nvectors = model.encode(\n    df[\"quote\"].tolist(), convert_to_numpy=True, normalize_embeddings=True\n)\nvector_dim = vectors[0].shape[0]\nprint(f\"Vector dimension: {vector_dim}\")\n\n# Add IDs to vectors array with proper column names\nvectors_with_ids = [\n    {\"id\": i + 1, \"vector_mpnet\": vec.tolist()} for i, vec in enumerate(vectors)\n]\n\n# Add vector column and merge data\ntable.add_columns(\n    {\"vector_mpnet\": f\"arrow_cast(NULL, 'FixedSizeList({vector_dim}, Float32)')\"}\n)\n\ntable.merge_insert(\n    \"id\"\n).when_matched_update_all().when_not_matched_insert_all().execute(vectors_with_ids)\n\n# Check versions after schema change\nversions = table.list_versions()\nversion_count_after_alter_embed = len(versions)\nversion_after_alter_embed = table.version\nprint(\n    f\"Number of versions after switching model: {version_count_after_alter_embed}\"\n)\nprint(f\"Current version: {version_after_alter_embed}\")\n\n# The table should now include a vector_mpnet column containing\n# embeddings generated by the all-mpnet-base-v2 model\nprint(table.schema)\n</code></pre> <pre><code>try {\n  // Let's switch to the all-mpnet-base-v2 model to embed the quotes\n  console.log(\"Generating embeddings with transformers...\");\n  const { pipeline } = await import(\"@xenova/transformers\");\n  const extractor = await pipeline(\n    \"feature-extraction\",\n    \"Xenova/all-mpnet-base-v2\",\n  );\n\n  // Generate embeddings for all quotes\n  const quotes = df.map((row) =&gt; row.quote);\n  const outputs = await Promise.all(\n    quotes.map((quote) =&gt;\n      extractor(quote, {\n        pooling: \"mean\",\n        normalize: true,\n      }),\n    ),\n  );\n  const embeddings = outputs.map((output) =&gt; Array.from(output.data));\n  vectorDim = embeddings[0].length;\n  console.log(`Vector dimension: ${vectorDim}`);\n\n  // Create embedding_with_id for all quotes\n  const embedding_with_id = df.map((row, i) =&gt; ({\n    id: row.id,\n    vector_mpnet: embeddings[i],\n  }));\n\n  // Add the vector column to the table\n  await table.addColumns([\n    {\n      name: \"vector_mpnet\",\n      valueSql: `arrow_cast(NULL, 'FixedSizeList(${vectorDim}, Float32)')`,\n    },\n  ]);\n\n  // Update all rows with their embeddings\n  await table\n    .mergeInsert(\"id\")\n    .whenMatchedUpdateAll()\n    .whenNotMatchedInsertAll()\n    .execute(embedding_with_id);\n} catch (error) {\n  console.log(\n    \"Failed to load transformers, using dummy vectors instead:\",\n    error,\n  );\n  // Create dummy embeddings for all quotes\n  const dummyEmbeddings = df.map(() =&gt; Array(vectorDim).fill(100));\n  const embedding_with_id = df.map((row, i) =&gt; ({\n    id: row.id,\n    vector_mpnet: dummyEmbeddings[i],\n  }));\n  // Add the vector column to the table\n  await table.addColumns([\n    {\n      name: \"vector_mpnet\",\n      valueSql: `arrow_cast(NULL, 'FixedSizeList(${vectorDim}, Float32)')`,\n    },\n  ]);\n\n  await table\n    .mergeInsert(\"id\")\n    .whenMatchedUpdateAll()\n    .whenNotMatchedInsertAll()\n    .execute(embedding_with_id);\n}\n\n// Check versions after schema change\nconst versionsAfterSchemaChange = await table.listVersions();\nconst versionCountAfterSchemaChange = versionsAfterSchemaChange.length;\nconsole.log(\n  `Total number of versions after schema change: ${versionCountAfterSchemaChange}`,\n);\n\n// The table should now include a vector_mpnet column containing\n// embeddings generated by the all-mpnet-base-v2 model\nconsole.log(await table.schema())\n</code></pre>"},{"location":"guides/tables/versioning/#delete-data-from-the-table","title":"Delete data from the table","text":"PythonTypeScript <pre><code># Go back to the latest version\ntable.checkout_latest()\n# Let's delete data from the table\ntable.delete(\"author != 'Richard Daniel Sanchez'\")\nrows_after_deletion = table.count_rows()\nprint(f\"Number of rows after deletion: {rows_after_deletion}\")\n</code></pre> <pre><code>// Go back to the latest version\nawait table.checkoutLatest();\n\n// Let's delete data from the table\nawait table.delete(\"author != 'Richard Daniel Sanchez'\");\nconst rowsAfterDeletion = await table.countRows();\nconsole.log(`Number of rows after deletion: ${rowsAfterDeletion}`);\n</code></pre>"},{"location":"guides/tables/versioning/#version-history-and-operations","title":"Version History and Operations","text":"<p>Throughout this guide, we've demonstrated various operations that create new versions in LanceDB.  Here's a summary of the version history we created:</p> <p>System Operations</p> <p>System operations like index updates and table compaction automatically increment the table version number. These background processes are tracked in the version history, though their version numbers are omitted from this example for clarity.</p> <ol> <li>Initial Creation (Version 1)</li> <li>Created the table with initial quotes data</li> <li> <p>Set up the basic schema with <code>id</code>, <code>author</code>, and <code>quote</code> columns</p> </li> <li> <p>First Update (Version 2)</p> </li> <li>Updated author names from \"Richard\" to \"Richard Daniel Sanchez\"</li> <li> <p>Modified existing records while maintaining data integrity</p> </li> <li> <p>Data Append (Version 3)</p> </li> <li>Added new quotes from Richard Daniel Sanchez and Morty</li> <li> <p>Expanded the dataset with additional content</p> </li> <li> <p>Schema Evolution (Version 4)</p> </li> <li>Added a new <code>vector_minilm</code> column for embeddings</li> <li> <p>Modified the table structure to support vector search</p> </li> <li> <p>Embedding Merge (Version 5)</p> </li> <li>Populated the <code>vector_minilm</code> column with embeddings</li> <li> <p>Combined vector data with existing records</p> </li> <li> <p>Version Rollback (Version 6)</p> </li> <li>Restored to Version 3 (pre-vector state)</li> <li> <p>Demonstrated time-travel capabilities</p> </li> <li> <p>Alternative Schema (Version 7)</p> </li> <li>Added a new <code>vector_mpnet</code> column</li> <li> <p>Showed support for multiple embedding models</p> </li> <li> <p>Alternative Embedding Merge (Version 8)</p> </li> <li>Populated the <code>vector_mpnet</code> column</li> <li> <p>Implemented a different embedding strategy</p> </li> <li> <p>Data Cleanup (Version 9)</p> </li> <li>Performed selective deletion of records</li> <li>Maintained only Richard Daniel Sanchez quotes</li> </ol> <p>Each version represents a distinct state of your data, allowing you to: - Track changes over time - Compare different embedding strategies - Revert to previous states - Maintain data lineage for ML reproducibility</p>"},{"location":"guides/tuning_retrievers/1_query_types/","title":"Query Types in LanceDB | Search Strategy Guide","text":""},{"location":"guides/tuning_retrievers/1_query_types/#improving-retriever-performance","title":"Improving retriever performance","text":"<p>Try it yourself: </p> <p>VectorDBs are used as retrievers in recommender or chatbot-based systems for retrieving relevant data based on user queries. For example, retrievers are a critical component of Retrieval Augmented Generation (RAG) acrhitectures. In this section, we will discuss how to improve the performance of retrievers.</p> <p>There are serveral ways to improve the performance of retrievers. Some of the common techniques are:</p> <ul> <li>Using different query types</li> <li>Using hybrid search</li> <li>Fine-tuning the embedding models</li> <li>Using different embedding models</li> </ul> <p>Using different embedding models is something that's very specific to the use case and the data. So we will not discuss it here. In this section, we will discuss the first three techniques.</p> <p>Note</p> <p>We'll be using a simple metric called \"hit-rate\" for evaluating the performance of the retriever across this guide. Hit-rate is the percentage of queries for which the retriever returned the correct answer in the top-k results. For example, if the retriever returned the correct answer in the top-3 results for 70% of the queries, then the hit-rate@3 is 0.7.</p>"},{"location":"guides/tuning_retrievers/1_query_types/#the-dataset","title":"The dataset","text":"<p>We'll be using a QA dataset generated using a LLama2 review paper. The dataset contains 221 query, context and answer triplets. The queries and answers are generated using GPT-4 based on a given query. Full script used to generate the dataset can be found on this repo. It can be downloaded from here.</p>"},{"location":"guides/tuning_retrievers/1_query_types/#using-different-query-types","title":"Using different query types","text":"<p>Let's setup the embeddings and the dataset first. We'll use the LanceDB's <code>huggingface</code> embeddings integration for this guide. </p> <pre><code>import lancedb\nimport pandas as pd\nfrom lancedb.embeddings import get_registry\nfrom lancedb.pydantic import Vector, LanceModel\n\ndb = lancedb.connect(\"~/lancedb/query_types\")\ndf = pd.read_csv(\"data_qa.csv\")\n\nembed_fcn = get_registry().get(\"huggingface\").create(name=\"BAAI/bge-small-en-v1.\")\n\nclass Schema(LanceModel):\n    context: str = embed_fcn.SourceField()\n    vector: Vector(embed_fcn.ndims()) = embed_fcn.VectorField()\n\ntable = db.create_table(\"qa\", schema=Schema)\ntable.add(df[[\"context\"]].to_dict(orient=\"records\"))\n\nqueries = df[\"query\"].tolist()\n</code></pre> <p>Now that we have the dataset and embeddings table set up, here's how you can run different query types on the dataset:</p> <ul> <li> <p> Vector Search: </p> <p><pre><code>table.search(quries[0], query_type=\"vector\").limit(5).to_pandas()\n</code></pre> By default, LanceDB uses vector search query type for searching and it automatically converts the input query to a vector before searching when using embedding API. So, the following statement is equivalent to the above statement:</p> <pre><code>table.search(quries[0]).limit(5).to_pandas()\n</code></pre> <p>Vector or semantic search is useful when you want to find documents that are similar to the query in terms of meaning.</p> </li> </ul> <ul> <li> <p> Full-text Search: </p> <p>FTS requires creating an index on the column you want to search on. <code>replace=True</code> will replace the existing index if it exists. Once the index is created, you can search using the <code>fts</code> query type. <pre><code>table.create_fts_index(\"context\", replace=True)\ntable.search(quries[0], query_type=\"fts\").limit(5).to_pandas()\n</code></pre></p> <p>Full-text search is useful when you want to find documents that contain the query terms.</p> </li> </ul> <ul> <li> <p> Hybrid Search: </p> <p>Hybrid search is a combination of vector and full-text search. Here's how you can run a hybrid search query on the dataset: <pre><code>table.search(quries[0], query_type=\"hybrid\").limit(5).to_pandas()\n</code></pre> Hybrid search requires a reranker to combine and rank the results from vector and full-text search. We'll cover reranking as a concept in the next section.</p> <p>Hybrid search is useful when you want to combine the benefits of both vector and full-text search.</p> <p>Note</p> <p>By default, it uses <code>LinearCombinationReranker</code> that combines the scores from vector and full-text search using a weighted linear combination. It is the simplest reranker implementation available in LanceDB. You can also use other rerankers like <code>CrossEncoderReranker</code> or <code>CohereReranker</code> for reranking the results. Learn more about rerankers here.</p> </li> </ul>"},{"location":"guides/tuning_retrievers/1_query_types/#hit-rate-evaluation-results","title":"Hit rate evaluation results","text":"<p>Now that we have seen how to run different query types on the dataset, let's evaluate the hit-rate of each query type on the dataset. For brevity, the entire evaluation script is not shown here. You can find the complete evaluation and benchmarking utility scripts here.</p> <p>Here are the hit-rate results for the dataset:</p> Query Type Hit-rate@5 Vector Search 0.640 Full-text Search 0.595 Hybrid Search (w/ LinearCombinationReranker) 0.645 <p>Choosing query type is very specific to the use case and the data. This synthetic dataset has been generated to be semantically challenging, i.e, the queries don't have a lot of keywords in common with the context. So, vector search performs better than full-text search. However, in real-world scenarios, full-text search might perform better than vector search. Hybrid search is a good choice when you want to combine the benefits of both vector and full-text search.</p>"},{"location":"guides/tuning_retrievers/1_query_types/#evaluation-results-on-other-datasets","title":"Evaluation results on other datasets","text":"<p>The hit-rate results can vary based on the dataset and the query type. Here are the hit-rate results for the other datasets using the same embedding function.</p> <ul> <li> <p> SQuAD Dataset: </p> Query Type Hit-rate@5 Vector Search 0.822 Full-text Search 0.835 Hybrid Search (w/ LinearCombinationReranker) 0.8874 </li> <li> <p> Uber10K sec filing Dataset: </p> Query Type Hit-rate@5 Vector Search 0.608 Full-text Search 0.82 Hybrid Search (w/ LinearCombinationReranker) 0.80 </li> </ul> <p>In these standard datasets, FTS seems to perform much better than vector search because the queries have a lot of keywords in common with the context. So, in general choosing the query type is very specific to the use case and the data.</p>"},{"location":"guides/tuning_retrievers/2_reranking/","title":"Reranking in LanceDB | Search Result Optimization","text":"<p>Continuing from the previous section, we can now rerank the results using more complex rerankers.</p> <p>Try it yourself: </p>"},{"location":"guides/tuning_retrievers/2_reranking/#reranking-search-results","title":"Reranking search results","text":"<p>You can rerank any search results using a reranker. The syntax for reranking is as follows:</p> <p><pre><code>from lancedb.rerankers import LinearCombinationReranker\n\nreranker = LinearCombinationReranker()\ntable.search(quries[0], query_type=\"hybrid\").rerank(reranker=reranker).limit(5).to_pandas()\n</code></pre> Based on the <code>query_type</code>, the <code>rerank()</code> function can accept other arguments as well. For example, hybrid search accepts a <code>normalize</code> param to determine the score normalization method.</p> <p>Note</p> <p>LanceDB provides a <code>Reranker</code> base class that can be extended to implement custom rerankers. Each reranker must implement the <code>rerank_hybrid</code> method. <code>rerank_vector</code> and <code>rerank_fts</code> methods are optional. For example, the <code>LinearCombinationReranker</code> only implements the <code>rerank_hybrid</code> method and so it can only be used for reranking hybrid search results.</p>"},{"location":"guides/tuning_retrievers/2_reranking/#choosing-a-reranker","title":"Choosing a Reranker","text":"<p>There are many rerankers available in LanceDB like <code>CrossEncoderReranker</code>, <code>CohereReranker</code>, and <code>ColBERT</code>. The choice of reranker depends on the dataset and the application. You can even implement you own custom reranker by extending the <code>Reranker</code> class. For more details about each available reranker and performance comparison, refer to the rerankers documentation.</p> <p>In this example, we'll use the <code>CohereReranker</code> to rerank the search results. It requires  <code>cohere</code> to be installed and <code>COHERE_API_KEY</code> to be set in the environment. To get your API key, sign up on Cohere.</p> <pre><code>from lancedb.rerankers import CohereReranker\n\n# use Cohere reranker v3\nreranker = CohereReranker(model_name=\"rerank-english-v3.0\") # default model is \"rerank-english-v2.0\"\n</code></pre>"},{"location":"guides/tuning_retrievers/2_reranking/#reranking-search-results_1","title":"Reranking search results","text":"<p>Now we can rerank all query type results using the <code>CohereReranker</code>:</p> <pre><code># rerank hybrid search results\ntable.search(quries[0], query_type=\"hybrid\").rerank(reranker=reranker).limit(5).to_pandas()\n\n# rerank vector search results\ntable.search(quries[0], query_type=\"vector\").rerank(reranker=reranker).limit(5).to_pandas()\n\n# rerank fts search results\ntable.search(quries[0], query_type=\"fts\").rerank(reranker=reranker).limit(5).to_pandas()\n</code></pre> <p>Each reranker can accept additional arguments. For example, <code>CohereReranker</code> accepts <code>top_k</code> and <code>batch_size</code> params to control the number of documents to rerank and the batch size for reranking respectively. Similarly, a custom reranker can accept any number of arguments based on the implementation. For example, a reranker can accept a <code>filter</code> that implements some custom logic to filter out documents before reranking.</p>"},{"location":"guides/tuning_retrievers/2_reranking/#results","title":"Results","text":"<p>Let us take a look at the same datasets from the previous sections, using the same embedding table but with Cohere reranker applied to all query types.</p> <p>Note</p> <p>When reranking fts or vector search results, the search results are over-fetched by a factor of 2 and then reranked. From the reranked set, <code>top_k</code> (5 in this case) results are taken. This is done because reranking will have no effect on the hit-rate if we only fetch the <code>top_k</code> results.</p>"},{"location":"guides/tuning_retrievers/2_reranking/#synthetic-llama2-paper-dataset","title":"Synthetic LLama2 paper dataset","text":"Query Type Hit-rate@5 Vector 0.640 FTS 0.595 Reranked vector 0.677 Reranked fts 0.672 Hybrid 0.759"},{"location":"guides/tuning_retrievers/2_reranking/#uber10k-sec-filing-dataset","title":"Uber10K sec filing Dataset","text":"Query Type Hit-rate@5 Vector 0.608 FTS 0.824 Reranked vector 0.671 Reranked fts 0.843 Hybrid 0.849"},{"location":"guides/tuning_retrievers/3_embed_tuning/","title":"Embedding Tuning in LanceDB | Model Optimization Guide","text":""},{"location":"guides/tuning_retrievers/3_embed_tuning/#finetuning-the-embedding-model","title":"Finetuning the Embedding Model","text":"<p>Try it yourself: </p> <p>Another way to improve retriever performance is to fine-tune the embedding model itself. Fine-tuning the embedding model can help in learning better representations for the documents and queries in the dataset. This can be particularly useful when the dataset is very different from the pre-trained data used to train the embedding model.</p> <p>We'll use the same dataset as in the previous sections. Start off by splitting the dataset into training and validation sets: <pre><code>from sklearn.model_selection import train_test_split\n\ntrain_df, validation_df = train_test_split(\"data_qa.csv\", test_size=0.2, random_state=42)\n\ntrain_df.to_csv(\"data_train.csv\", index=False)\nvalidation_df.to_csv(\"data_val.csv\", index=False)\n</code></pre></p> <p>You can use any tuning API to fine-tune embedding models. In this example, we'll utilise Llama-index as it also comes with utilities for synthetic data generation and training the model. </p> <p>We parse the dataset as llama-index text nodes and generate synthetic QA pairs from each node: <pre><code>from llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.readers.file import PagedCSVReader\nfrom llama_index.finetuning import generate_qa_embedding_pairs\nfrom llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n\ndef load_corpus(file):\n    loader = PagedCSVReader(encoding=\"utf-8\")\n    docs = loader.load_data(file=Path(file))\n\n    parser = SentenceSplitter()\n    nodes = parser.get_nodes_from_documents(docs)\n\n    return nodes\n\nfrom llama_index.llms.openai import OpenAI\n\n\ntrain_dataset = generate_qa_embedding_pairs(\n    llm=OpenAI(model=\"gpt-3.5-turbo\"), nodes=train_nodes, verbose=False\n)\nval_dataset = generate_qa_embedding_pairs(\n    llm=OpenAI(model=\"gpt-3.5-turbo\"), nodes=val_nodes, verbose=False\n)\n</code></pre></p> <p>Now we'll use <code>SentenceTransformersFinetuneEngine</code> engine to fine-tune the model. You can also use <code>sentence-transformers</code> or <code>transformers</code> library to fine-tune the model:</p> <p><pre><code>from llama_index.finetuning import SentenceTransformersFinetuneEngine\n\nfinetune_engine = SentenceTransformersFinetuneEngine(\n    train_dataset,\n    model_id=\"BAAI/bge-small-en-v1.5\",\n    model_output_path=\"tuned_model\",\n    val_dataset=val_dataset,\n)\nfinetune_engine.finetune()\nembed_model = finetune_engine.get_finetuned_model()\n</code></pre> This saves the fine tuned embedding model in <code>tuned_model</code> folder.</p>"},{"location":"guides/tuning_retrievers/3_embed_tuning/#evaluation-results","title":"Evaluation results","text":"<p>In order to eval the retriever, you can either use this model to ingest the data into LanceDB directly or llama-index's LanceDB integration to create a <code>VectorStoreIndex</code> and use it as a retriever.  On performing the same hit-rate evaluation as before, we see a significant improvement in the hit-rate across all query types.</p>"},{"location":"guides/tuning_retrievers/3_embed_tuning/#baseline","title":"Baseline","text":"Query Type Hit-rate@5 Vector Search 0.640 Full-text Search 0.595 Reranked Vector Search 0.677 Reranked Full-text Search 0.672 Hybrid Search (w/ CohereReranker) 0.759"},{"location":"guides/tuning_retrievers/3_embed_tuning/#fine-tuned-model-2-iterations","title":"Fine-tuned model ( 2 iterations )","text":"Query Type Hit-rate@5 Vector Search 0.672 Full-text Search 0.595 Reranked Vector Search 0.754 Reranked Full-text Search 0.672 Hybrid Search (w/ CohereReranker) 0.768"},{"location":"integrations/","title":"Integrations","text":"<p>LanceDB supports ingesting from and exporting to your favorite data formats across the Python and JavaScript ecosystems.</p> <p></p>"},{"location":"integrations/#tools","title":"Tools","text":"<p>LanceDB is integrated with a lot of popular AI tools, with more coming soon. Get started using these examples and quick links.</p> Integrations  LlamaIndex LlamaIndex is a simple, flexible data framework for connecting custom data sources to large language models. Llama index integrates with LanceDB as the serverless VectorDB. Lean More LangchainLangchain allows building applications with LLMs through composability Lean More Langchain TS Javascript bindings for Langchain. It integrates with LanceDB's serverless vectordb allowing you to build powerful AI applications through composibility using only serverless functions. Learn More Voxel51  It is an open source toolkit that enables you to build better computer vision workflows by improving the quality of your datasets and delivering insights about your models.Learn More PromptTools  Offers a set of free, open-source tools for testing and experimenting with models, prompts, and configurations. The core idea is to enable developers to evaluate prompts using familiar interfaces like code and notebooks. You can use it to experiment with different configurations of LanceDB, and test how LanceDB integrates with the LLM of your choice.Learn More"},{"location":"integrations/dlt/","title":"dlt","text":"<p>dlt is an open-source library that you can add to your Python scripts to load data from various and often messy data sources into well-structured, live datasets. dlt's integration with LanceDB lets you ingest data from any source (databases, APIs, CSVs, dataframes, JSONs, and more) into LanceDB with a few lines of simple python code. The integration enables automatic normalization of nested data, schema inference, incremental loading and embedding the data. dlt also has integrations with several other tools like dbt, airflow, dagster etc. that can be inserted into your LanceDB workflow.</p>"},{"location":"integrations/dlt/#how-to-ingest-data-into-lancedb","title":"How to ingest data into LanceDB","text":"<p>In this example, we will be fetching movie information from the Open Movie Database (OMDb) API and loading it into a local LanceDB instance. To implement it, you will need an API key for the OMDb API (which can be created freely here).</p> <ol> <li> <p>Install <code>dlt</code> with LanceDB extras: <pre><code>pip install dlt[lancedb]\n</code></pre></p> </li> <li> <p>Inside an empty directory, initialize a <code>dlt</code> project with: <pre><code>dlt init rest_api lancedb\n</code></pre>     This will add all the files necessary to create a <code>dlt</code> pipeline that can ingest data from any REST API (ex: OMDb API) and load into LanceDB.     <pre><code>\u251c\u2500\u2500 .dlt\n\u2502   \u251c\u2500\u2500 config.toml\n\u2502   \u2514\u2500\u2500 secrets.toml\n\u251c\u2500\u2500 rest_api\n\u251c\u2500\u2500 rest_api_pipeline.py\n\u2514\u2500\u2500 requirements.txt\n</code></pre></p> <p>dlt has a list of pre-built sources like SQL databases, REST APIs, Google Sheets, Notion etc., that can be used out-of-the-box by running <code>dlt init &lt;source_name&gt; lancedb</code>. Since dlt is a python library, it is also very easy to modify these pre-built sources or to write your own custom source from scratch.</p> </li> <li> <p>Specify necessary credentials and/or embedding model details: </p> <p>In order to fetch data from the OMDb API, you will need to pass a valid API key into your pipeline. Depending on whether you're using LanceDB OSS or LanceDB cloud, you also may need to provide the necessary credentials to connect to the LanceDB instance. These can be pasted inside <code>.dlt/sercrets.toml</code>. </p> <p>dlt's LanceDB integration also allows you to automatically embed the data during ingestion. Depending on the embedding model chosen, you may need to paste the necessary credentials inside <code>.dlt/sercrets.toml</code>: <pre><code>[sources.rest_api]\napi_key = \"api_key\" # Enter the API key for the OMDb API\n\n[destination.lancedb]\nembedding_model_provider = \"sentence-transformers\"\nembedding_model = \"all-MiniLM-L6-v2\"\n[destination.lancedb.credentials]\nuri = \".lancedb\"\napi_key = \"api_key\" # API key to connect to LanceDB Cloud. Leave out if you are using LanceDB OSS.\nembedding_model_provider_api_key = \"embedding_model_provider_api_key\" # Not needed for providers that don't need authentication (ollama, sentence-transformers).\n</code></pre> See here for more information and for a list of available models and model providers.  </p> </li> <li> <p>Write the pipeline code inside <code>rest_api_pipeline.py</code>: </p> <p>The following code shows how you can configure dlt's REST API source to connect to the OMDb API, fetch all movies with the word \"godzilla\" in the title, and load it into a LanceDB table. The REST API source allows you to pull data from any API with minimal code, to learn more read the dlt docs.</p> <pre><code># Import necessary modules\nimport dlt\nfrom rest_api import rest_api_source\n\n# Configure the REST API source\nmovies_source = rest_api_source(\n    {\n        \"client\": {\n            \"base_url\": \"https://www.omdbapi.com/\",\n            \"auth\": { # authentication strategy for the OMDb API\n                \"type\": \"api_key\",\n                \"name\": \"apikey\",\n                \"api_key\": dlt.secrets[\"sources.rest_api.api_token\"], # read API credentials directly from secrets.toml\n                \"location\": \"query\"\n            },\n            \"paginator\": { # pagination strategy for the OMDb API \n                \"type\": \"page_number\",\n                \"base_page\": 1,\n                \"total_path\": \"totalResults\",\n                \"maximum_page\": 5\n            }\n        },\n        \"resources\": [ # list of API endpoints to request\n            {\n                \"name\": \"movie_search\",\n                \"endpoint\": {\n                    \"path\": \"/\",\n                    \"params\": {\n                        \"s\": \"godzilla\",\n                        \"type\": \"movie\"\n                    }\n                }\n            }\n        ]\n    })\n\n\nif __name__ == \"__main__\":\n    # Create a pipeline object\n    pipeline = dlt.pipeline(\n        pipeline_name='movies_pipeline',\n        destination='lancedb', # this tells dlt to load the data into LanceDB\n        dataset_name='movies_data_pipeline',\n    )\n\n    # Run the pipeline\n    load_info = pipeline.run(movies_source)\n\n    # pretty print the information on data that was loaded\n    print(load_info)\n</code></pre> <p>The script above will ingest the data into LanceDB as it is, i.e. without creating any embeddings. If we want to embed one of the fields (for example, <code>\"Title\"</code> that contains the movie titles), then we will use dlt's <code>lancedb_adapter</code> and modify the script as follows:  </p> <ul> <li>Add the following import statement:     <pre><code>from dlt.destinations.adapters import lancedb_adapter\n</code></pre></li> <li>Modify the pipeline run like this:     <pre><code>load_info = pipeline.run(\n    lancedb_adapter(\n        movies_source,\n        embed=\"Title\",\n    )\n)\n</code></pre> This will use the embedding model specified inside <code>.dlt/secrets.toml</code> to embed the field <code>\"Title\"</code>.</li> </ul> </li> <li> <p>Install necessary dependencies: <pre><code>pip install -r requirements.txt\n</code></pre></p> <p>Note: You may need to install the dependencies for your embedding models separately. <pre><code>pip install sentence-transformers\n</code></pre></p> </li> <li> <p>Run the pipeline:     Finally, running the following command will ingest the data into your LanceDB instance.     <pre><code>python custom_source.py\n</code></pre></p> </li> </ol> <p>For more information and advanced usage of dlt's LanceDB integration, read the dlt documentation.</p>"},{"location":"integrations/genkit/","title":"GenKit Integration with LanceDB | AI Generation Framework","text":""},{"location":"integrations/genkit/#genkitx-lancedb","title":"genkitx-lancedb","text":"<p>This is a lancedb plugin for genkit framework. It allows you to use LanceDB for ingesting and rereiving data using genkit framework.</p>"},{"location":"integrations/genkit/#installation","title":"Installation","text":"<pre><code>pnpm install genkitx-lancedb\n</code></pre>"},{"location":"integrations/genkit/#usage","title":"Usage","text":"<p>Adding LanceDB plugin to your genkit instance.</p> <pre><code>import { lancedbIndexerRef, lancedb, lancedbRetrieverRef, WriteMode } from 'genkitx-lancedb';\nimport { textEmbedding004, vertexAI } from '@genkit-ai/vertexai';\nimport { gemini } from '@genkit-ai/vertexai';\nimport { z, genkit } from 'genkit';\nimport { Document } from 'genkit/retriever';\nimport { chunk } from 'llm-chunk';\nimport { readFile } from 'fs/promises';\nimport path from 'path';\nimport pdf from 'pdf-parse/lib/pdf-parse';\n\nconst ai = genkit({\n  plugins: [\n    // vertexAI provides the textEmbedding004 embedder\n    vertexAI(),\n\n    // the local vector store requires an embedder to translate from text to vector\n    lancedb([\n      {\n        dbUri: '.db', // optional lancedb uri, default to .db\n        tableName: 'table', // optional table name, default to table\n        embedder: textEmbedding004,\n      },\n    ]),\n  ],\n});\n</code></pre> <p>You can run this app with the following command: <pre><code>genkit start -- tsx --watch src/index.ts\n</code></pre></p> <p>This'll add LanceDB as a retriever and indexer to the genkit instance. You can see it in the GUI view </p> <p>Testing retrieval on a sample table Let's see the raw retrieval results</p> <p> On running this query, you'll 5 results fetched from the lancedb table, where each result looks something like this: </p>"},{"location":"integrations/genkit/#creating-a-custom-rag-flow","title":"Creating a custom RAG flow","text":"<p>Now that we've seen how you can use LanceDB for in a genkit pipeline, let's refine the flow and create a RAG. A RAG flow will consist of an index and a retreiver with its outputs postprocessed an fed into an LLM for final response</p>"},{"location":"integrations/genkit/#creating-custom-indexer-flows","title":"Creating custom indexer flows","text":"<p>You can also create custom indexer flows, utilizing more options and features provided by LanceDB.</p> <pre><code>export const menuPdfIndexer = lancedbIndexerRef({\n   // Using all defaults, for dbUri, tableName, and embedder, etc\n});\n\nconst chunkingConfig = {\n  minLength: 1000,\n  maxLength: 2000,\n  splitter: 'sentence',\n  overlap: 100,\n  delimiters: '',\n} as any;\n\n\nasync function extractTextFromPdf(filePath: string) {\n  const pdfFile = path.resolve(filePath);\n  const dataBuffer = await readFile(pdfFile);\n  const data = await pdf(dataBuffer);\n  return data.text;\n}\n\nexport const indexMenu = ai.defineFlow(\n  {\n    name: 'indexMenu',\n    inputSchema: z.string().describe('PDF file path'),\n    outputSchema: z.void(),\n  },\n  async (filePath: string) =&gt; {\n    filePath = path.resolve(filePath);\n\n    // Read the pdf.\n    const pdfTxt = await ai.run('extract-text', () =&gt;\n      extractTextFromPdf(filePath)\n    );\n\n    // Divide the pdf text into segments.\n    const chunks = await ai.run('chunk-it', async () =&gt;\n      chunk(pdfTxt, chunkingConfig)\n    );\n\n    // Convert chunks of text into documents to store in the index.\n    const documents = chunks.map((text) =&gt; {\n      return Document.fromText(text, { filePath });\n    });\n\n    // Add documents to the index.\n    await ai.index({\n      indexer: menuPdfIndexer,\n      documents,\n      options: {\n        writeMode: WriteMode.Overwrite,\n      } as any\n    });\n  }\n);\n</code></pre> <p></p> <p>In your console, you can see the logs</p> <p></p>"},{"location":"integrations/genkit/#creating-custom-retriever-flows","title":"Creating custom retriever flows","text":"<p>You can also create custom retriever flows, utilizing more options and features provided by LanceDB. <pre><code>export const menuRetriever = lancedbRetrieverRef({\n  tableName: \"table\", // Use the same table name as the indexer.\n  displayName: \"Menu\", // Use a custom display name.\n\nexport const menuQAFlow = ai.defineFlow(\n  { name: \"Menu\", inputSchema: z.string(), outputSchema: z.string() },\n  async (input: string) =&gt; {\n    // retrieve relevant documents\n    const docs = await ai.retrieve({\n      retriever: menuRetriever,\n      query: input,\n      options: { \n        k: 3,\n      },\n    });\n\n    const extractedContent = docs.map(doc =&gt; {\n      if (doc.content &amp;&amp; Array.isArray(doc.content) &amp;&amp; doc.content.length &gt; 0) {\n        if (doc.content[0].media &amp;&amp; doc.content[0].media.url) {\n          return doc.content[0].media.url;\n        }\n      }\n      return \"No content found\";\n    });\n\n    console.log(\"Extracted content:\", extractedContent);\n\n    const { text } = await ai.generate({\n      model: gemini('gemini-2.0-flash'),\n      prompt: `\nYou are acting as a helpful AI assistant that can answer \nquestions about the food available on the menu at Genkit Grub Pub.\n\nUse only the context provided to answer the question.\nIf you don't know, do not make up an answer.\nDo not add or change items on the menu.\n\nContext:\n${extractedContent.join('\\n\\n')}\n\nQuestion: ${input}`,\n      docs,\n    });\n\n    return text;\n  }\n);\n</code></pre> Now using our retrieval flow, we can ask question about the ingsted PDF </p>"},{"location":"integrations/langchain/","title":"LangChain Integration with LanceDB | Vector Store & Retriever","text":"<p>LangChain is a framework designed for building applications with large language models (LLMs) by chaining together various components. It supports a range of functionalities including memory, agents, and chat models, enabling developers to create context-aware applications.</p> <p></p> <p>LangChain streamlines these stages (in figure above) by providing pre-built components and tools for integration, memory management, and deployment, allowing developers to focus on application logic rather than underlying complexities.</p> <p>Integration of Langchain with LanceDB enables applications to retrieve the most relevant data by comparing query vectors against stored vectors, facilitating effective information retrieval. It results in better and context aware replies and actions by the LLMs.</p>"},{"location":"integrations/langchain/#quick-start","title":"Quick Start","text":"<p>You can load your document data using langchain's loaders, for this example we are using <code>TextLoader</code> and <code>OpenAIEmbeddings</code> as the embedding model. Checkout Complete example here - LangChain demo <pre><code>import os\nfrom langchain.document_loaders import TextLoader\nfrom langchain.vectorstores import LanceDB\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_text_splitters import CharacterTextSplitter\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\nloader = TextLoader(\"../../modules/state_of_the_union.txt\") # Replace with your data path\ndocuments = loader.load()\n\ndocuments = CharacterTextSplitter().split_documents(documents)\nembeddings = OpenAIEmbeddings()\n\ndocsearch = LanceDB.from_documents(documents, embeddings)\nquery = \"What did the president say about Ketanji Brown Jackson\"\ndocs = docsearch.similarity_search(query)\nprint(docs[0].page_content)\n</code></pre></p>"},{"location":"integrations/langchain/#documentation","title":"Documentation","text":"<p>In the above example <code>LanceDB</code> vector store class object is created using <code>from_documents()</code> method  which is a <code>classmethod</code> and returns the initialized class object. </p> <p>You can also use <code>LanceDB.from_texts(texts: List[str],embedding: Embeddings)</code> class method.  </p> <p>The exhaustive list of parameters for <code>LanceDB</code> vector store are : </p> Name type Purpose default <code>connection</code> (Optional) <code>Any</code> <code>lancedb.db.LanceDBConnection</code> connection object to use.  If not provided, a new connection will be created. <code>None</code> <code>embedding</code> (Optional) <code>Embeddings</code> Langchain embedding model. Provided by user. <code>uri</code> (Optional) <code>str</code> It specifies the directory location of LanceDB database and establishes a connection that can be used to interact with the database. <code>/tmp/lancedb</code> <code>vector_key</code> (Optional) <code>str</code> Column name to use for vector's in the table. <code>'vector'</code> <code>id_key</code> (Optional) <code>str</code> Column name to use for id's in the table. <code>'id'</code> <code>text_key</code> (Optional) <code>str</code> Column name to use for text in the table. <code>'text'</code> <code>table_name</code> (Optional) <code>str</code> Name of your table in the database. <code>'vectorstore'</code> <code>api_key</code> (Optional <code>str</code>) API key to use for LanceDB cloud database. <code>None</code> <code>region</code> (Optional) <code>str</code> Region to use for LanceDB cloud database. Only for LanceDB Cloud : <code>None</code>. <code>mode</code> (Optional) <code>str</code> Mode to use for adding data to the table. Valid values are \"append\" and \"overwrite\". <code>'overwrite'</code> <code>table</code> (Optional) <code>Any</code> You can connect to an existing table of LanceDB, created outside of langchain, and utilize it. <code>None</code> <code>distance</code> (Optional) <code>str</code> The choice of distance metric used to calculate the similarity between vectors. <code>'l2'</code> <code>reranker</code> (Optional) <code>Any</code> The reranker to use for LanceDB. <code>None</code> <code>relevance_score_fn</code> (Optional) <code>Callable[[float], float]</code> Langchain relevance score function to be used. <code>None</code> <code>limit</code> <code>int</code> Set the maximum number of results to return. <code>DEFAULT_K</code> (it is 4) <pre><code>db_url = \"db://lang_test\" # url of db you created\napi_key = \"xxxxx\" # your API key\nregion=\"us-east-1-dev\"  # your selected region\n\nvector_store = LanceDB(\n    uri=db_url,\n    api_key=api_key, #(dont include for local API)\n    region=region, #(dont include for local API)\n    embedding=embeddings,\n    table_name='langchain_test' # Optional\n    )\n</code></pre>"},{"location":"integrations/langchain/#methods","title":"Methods","text":""},{"location":"integrations/langchain/#add_texts","title":"add_texts()","text":"<p>This method turn texts into embedding and add it to the database.</p> Name Purpose defaults <code>texts</code> <code>Iterable</code> of strings to add to the vectorstore. Provided by user <code>metadatas</code> Optional <code>list[dict()]</code> of metadatas associated with the texts. <code>None</code> <code>ids</code> Optional <code>list</code> of ids to associate with the texts. <code>None</code> <code>kwargs</code> Other keyworded arguments provided by the user. - <p>It returns list of ids of the added texts.</p> <pre><code>vector_store.add_texts(texts = ['test_123'], metadatas =[{'source' :'wiki'}]) \n\n#Additionaly, to explore the table you can load it into a df or save it in a csv file:\n\ntbl = vector_store.get_table()\nprint(\"tbl:\", tbl)\npd_df = tbl.to_pandas()\npd_df.to_csv(\"docsearch.csv\", index=False)\n\n# you can also create a new vector store object using an older connection object:\nvector_store = LanceDB(connection=tbl, embedding=embeddings)\n</code></pre>"},{"location":"integrations/langchain/#create_index","title":"create_index()","text":"<p>This method creates a scalar(for non-vector cols) or a vector index on a table.</p> Name type Purpose defaults <code>vector_col</code> <code>Optional[str]</code> Provide if you want to create index on a vector column. <code>None</code> <code>col_name</code> <code>Optional[str]</code> Provide if you want to create index on a non-vector column. <code>None</code> <code>metric</code> <code>Optional[str]</code> Provide the metric to use for vector index. choice of metrics: 'l2', 'dot', 'cosine'. <code>l2</code> <code>num_partitions</code> <code>Optional[int]</code> Number of partitions to use for the index. <code>256</code> <code>num_sub_vectors</code> <code>Optional[int]</code> Number of sub-vectors to use for the index. <code>96</code> <code>index_cache_size</code> <code>Optional[int]</code> Size of the index cache. <code>None</code> <code>name</code> <code>Optional[str]</code> Name of the table to create index on. <code>None</code> <p>For index creation make sure your table has enough data in it. An ANN index is ususally not needed for datasets ~100K vectors. For large-scale (&gt;1M) or higher dimension vectors, it is beneficial to create an ANN index.</p> <pre><code># for creating vector index\nvector_store.create_index(vector_col='vector', metric = 'cosine')\n\n# for creating scalar index(for non-vector columns)\nvector_store.create_index(col_name='text')\n</code></pre>"},{"location":"integrations/langchain/#similarity_search","title":"similarity_search()","text":"<p>This method performs similarity search based on text query.</p> Name Type Purpose Default <code>query</code> <code>str</code> A <code>str</code> representing the text query that you want to search for in the vector store. N/A <code>k</code> <code>Optional[int]</code> It specifies the number of documents to return. <code>None</code> <code>filter</code> <code>Optional[Dict[str, str]]</code> It is used to filter the search results by specific metadata criteria. <code>None</code> <code>fts</code> <code>Optional[bool]</code> It indicates whether to perform a full-text search (FTS). <code>False</code> <code>name</code> <code>Optional[str]</code> It is used for specifying the name of the table to query. If not provided, it uses the default table set during the initialization of the LanceDB instance. <code>None</code> <code>kwargs</code> <code>Any</code> Other keyworded arguments provided by the user. N/A <p>Return documents most similar to the query without relevance scores.</p> <pre><code>docs = docsearch.similarity_search(query)\nprint(docs[0].page_content)\n</code></pre>"},{"location":"integrations/langchain/#similarity_search_by_vector","title":"similarity_search_by_vector()","text":"<p>The method returns documents that are most similar to the specified embedding (query) vector. </p> Name Type Purpose Default <code>embedding</code> <code>List[float]</code> The embedding vector you want to use to search for similar documents in the vector store. N/A <code>k</code> <code>Optional[int]</code> It specifies the number of documents to return. <code>None</code> <code>filter</code> <code>Optional[Dict[str, str]]</code> It is used to filter the search results by specific metadata criteria. <code>None</code> <code>name</code> <code>Optional[str]</code> It is used for specifying the name of the table to query. If not provided, it uses the default table set during the initialization of the LanceDB instance. <code>None</code> <code>kwargs</code> <code>Any</code> Other keyworded arguments provided by the user. N/A <p>It does not provide relevance scores.</p> <pre><code>docs = docsearch.similarity_search_by_vector(query)\nprint(docs[0].page_content)\n</code></pre>"},{"location":"integrations/langchain/#similarity_search_with_score","title":"similarity_search_with_score()","text":"<p>Returns documents most similar to the query string along with their relevance scores.</p> Name Type Purpose Default <code>query</code> <code>str</code> A <code>str</code> representing the text query you want to search for in the vector store. This query will be converted into an embedding using the specified embedding function. N/A <code>k</code> <code>Optional[int]</code> It specifies the number of documents to return. <code>None</code> <code>filter</code> <code>Optional[Dict[str, str]]</code> It is used to filter the search results by specific metadata criteria. This allows you to narrow down the search results based on certain metadata attributes associated with the documents. <code>None</code> <code>kwargs</code> <code>Any</code> Other keyworded arguments provided by the user. N/A <p>It gets called by base class's <code>similarity_search_with_relevance_scores</code> which selects relevance score based on our <code>_select_relevance_score_fn</code>.</p> <pre><code>docs = docsearch.similarity_search_with_relevance_scores(query)\nprint(\"relevance score - \", docs[0][1])\nprint(\"text- \", docs[0][0].page_content[:1000])\n</code></pre>"},{"location":"integrations/langchain/#similarity_search_by_vector_with_relevance_scores","title":"similarity_search_by_vector_with_relevance_scores()","text":"<p>Similarity search using query vector.</p> Name Type Purpose Default <code>embedding</code> <code>List[float]</code> The embedding vector you want to use to search for similar documents in the vector store. N/A <code>k</code> <code>Optional[int]</code> It specifies the number of documents to return. <code>None</code> <code>filter</code> <code>Optional[Dict[str, str]]</code> It is used to filter the search results by specific metadata criteria. <code>None</code> <code>name</code> <code>Optional[str]</code> It is used for specifying the name of the table to query. <code>None</code> <code>kwargs</code> <code>Any</code> Other keyworded arguments provided by the user. N/A <p>The method returns documents most similar to the specified embedding (query) vector, along with their relevance scores.</p> <pre><code>docs = docsearch.similarity_search_by_vector_with_relevance_scores(query_embedding)\nprint(\"relevance score - \", docs[0][1])\nprint(\"text- \", docs[0][0].page_content[:1000])\n</code></pre>"},{"location":"integrations/langchain/#max_marginal_relevance_search","title":"max_marginal_relevance_search()","text":"<p>This method returns docs selected using the maximal marginal relevance(MMR). Maximal marginal relevance optimizes for similarity to query AND diversity among selected documents.</p> Name Type Purpose Default <code>query</code> <code>str</code> Text to look up documents similar to. N/A <code>k</code> <code>Optional[int]</code> Number of Documents to return. <code>4</code> <code>fetch_k</code> <code>Optional[int]</code> Number of Documents to fetch to pass to MMR algorithm. <code>None</code> <code>lambda_mult</code> <code>float</code> Number between 0 and 1 that determines the degree of diversity among the results with 0 corresponding to maximum diversity and 1 to minimum diversity. <code>0.5</code> <code>filter</code> <code>Optional[Dict[str, str]]</code> Filter by metadata. <code>None</code> <code>kwargs</code> Other keyworded arguments provided by the user. - <p>Similarly, <code>max_marginal_relevance_search_by_vector()</code> function returns docs most similar to the embedding passed to the function using MMR. instead of a string query you need to pass the embedding to be searched for. </p> <pre><code>result = docsearch.max_marginal_relevance_search(\n        query=\"text\"\n    )\nresult_texts = [doc.page_content for doc in result]\nprint(result_texts)\n\n## search by vector :\nresult = docsearch.max_marginal_relevance_search_by_vector(\n        embeddings.embed_query(\"text\")\n    )\nresult_texts = [doc.page_content for doc in result]\nprint(result_texts)\n</code></pre>"},{"location":"integrations/langchain/#add_images","title":"add_images()","text":"<p>This method ddds images by automatically creating their embeddings and adds them to the vectorstore.</p> Name Type Purpose Default <code>uris</code> <code>List[str]</code> File path to the image N/A <code>metadatas</code> <code>Optional[List[dict]]</code> Optional list of metadatas <code>None</code> <code>ids</code> <code>Optional[List[str]]</code> Optional list of IDs <code>None</code> <p>It returns list of IDs of the added images.</p> <pre><code>vec_store.add_images(uris=image_uris) \n# here image_uris are local fs paths to the images.\n</code></pre>"},{"location":"integrations/llamaIndex/","title":"Llama-Index","text":""},{"location":"integrations/llamaIndex/#quick-start","title":"Quick start","text":"<p>You would need to install the integration via <code>pip install llama-index-vector-stores-lancedb</code> in order to use it.  You can run the below script to try it out : <pre><code>import logging\nimport sys\n\n# Uncomment to see debug logs\n# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\nfrom llama_index.core import SimpleDirectoryReader, Document, StorageContext\nfrom llama_index.core import VectorStoreIndex\nfrom llama_index.vector_stores.lancedb import LanceDBVectorStore\nimport textwrap\nimport openai\n\nopenai.api_key = \"sk-...\"\n\ndocuments = SimpleDirectoryReader(\"./data/your-data-dir/\").load_data()\nprint(\"Document ID:\", documents[0].doc_id, \"Document Hash:\", documents[0].hash)\n\n## For LanceDB cloud :\n# vector_store = LanceDBVectorStore( \n#     uri=\"db://db_name\", # your remote DB URI\n#     api_key=\"sk_..\", # lancedb cloud api key\n#     region=\"your-region\" # the region you configured\n#     ...\n# )\n\nvector_store = LanceDBVectorStore(\n    uri=\"./lancedb\", mode=\"overwrite\", query_type=\"vector\"\n)\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\nindex = VectorStoreIndex.from_documents(\n    documents, storage_context=storage_context\n)\nlance_filter = \"metadata.file_name = 'paul_graham_essay.txt' \"\nretriever = index.as_retriever(vector_store_kwargs={\"where\": lance_filter})\nresponse = retriever.retrieve(\"What did the author do growing up?\")\n</code></pre></p> <p>Checkout Complete example here - LlamaIndex demo</p>"},{"location":"integrations/llamaIndex/#filtering","title":"Filtering","text":"<p>For metadata filtering, you can use a Lance SQL-like string filter as demonstrated in the example above. Additionally, you can also filter using the <code>MetadataFilters</code> class from LlamaIndex: <pre><code>from llama_index.core.vector_stores import (\n    MetadataFilters,\n    FilterOperator,\n    FilterCondition,\n    MetadataFilter,\n)\n\nquery_filters = MetadataFilters(\n    filters=[\n        MetadataFilter(\n            key=\"creation_date\", operator=FilterOperator.EQ, value=\"2024-05-23\"\n        ),\n        MetadataFilter(\n            key=\"file_size\", value=75040, operator=FilterOperator.GT\n        ),\n    ],\n    condition=FilterCondition.AND,\n)\n</code></pre></p>"},{"location":"integrations/llamaIndex/#hybrid-search","title":"Hybrid Search","text":"<p>For complete documentation, refer here. This example uses the <code>colbert</code> reranker. Make sure to install necessary dependencies for the reranker you choose. <pre><code>from lancedb.rerankers import ColbertReranker\n\nreranker = ColbertReranker()\nvector_store._add_reranker(reranker)\n\nquery_engine = index.as_query_engine(\n    filters=query_filters,\n    vector_store_kwargs={\n        \"query_type\": \"hybrid\",\n    }\n)\n\nresponse = query_engine.query(\"How much did Viaweb charge per month?\")\n</code></pre></p> <p>In the above snippet, you can change/specify query_type again when creating the engine/retriever.</p>"},{"location":"integrations/llamaIndex/#api-reference","title":"API reference","text":"<p>The exhaustive list of parameters for <code>LanceDBVectorStore</code> vector store are : - <code>connection</code>: Optional, <code>lancedb.db.LanceDBConnection</code> connection object to use. If not provided, a new connection will be created. - <code>uri</code>: Optional[str], the uri of your database. Defaults to <code>\"/tmp/lancedb\"</code>. - <code>table_name</code> : Optional[str], Name of your table in the database. Defaults to <code>\"vectors\"</code>. - <code>table</code>: Optional[Any], <code>lancedb.db.LanceTable</code> object to be passed. Defaults to <code>None</code>.  - <code>vector_column_name</code>: Optional[Any], Column name to use for vector's in the table. Defaults to <code>'vector'</code>.  - <code>doc_id_key</code>: Optional[str], Column name to use for document id's in the table. Defaults to <code>'doc_id'</code>. - <code>text_key</code>: Optional[str], Column name to use for text in the table. Defaults to <code>'text'</code>. - <code>api_key</code>: Optional[str], API key to use for LanceDB cloud database. Defaults to <code>None</code>. - <code>region</code>: Optional[str], Region to use for LanceDB cloud database. Only for LanceDB Cloud, defaults to <code>None</code>. - <code>nprobes</code> : Optional[int], Set the number of probes to use. Only applicable if ANN index is created on the table else its ignored. Defaults to <code>20</code>. - <code>refine_factor</code> : Optional[int], Refine the results by reading extra elements and re-ranking them in memory. Defaults to <code>None</code>. - <code>reranker</code>: Optional[Any], The reranker to use for LanceDB.         Defaults to <code>None</code>. - <code>overfetch_factor</code>: Optional[int], The factor by which to fetch more results.         Defaults to <code>1</code>. - <code>mode</code>: Optional[str], The mode to use for LanceDB.             Defaults to <code>\"overwrite\"</code>. - <code>query_type</code>:Optional[str], The type of query to use for LanceDB.             Defaults to <code>\"vector\"</code>.</p>"},{"location":"integrations/llamaIndex/#methods","title":"Methods","text":"<ul> <li> <p>from_table(cls, table: lancedb.db.LanceTable) -&gt; <code>LanceDBVectorStore</code> : (class method) Creates instance from lancedb table. </p> </li> <li> <p>_add_reranker(self, reranker: lancedb.rerankers.Reranker) -&gt; <code>None</code> : Add a reranker to an existing vector store. </p> <ul> <li>Usage :     <pre><code>from lancedb.rerankers import ColbertReranker\nreranker = ColbertReranker()\nvector_store._add_reranker(reranker)\n</code></pre></li> </ul> </li> <li>_table_exists(self, tbl_name: <code>Optional[str]</code> = <code>None</code>) -&gt; <code>bool</code> : Returns <code>True</code> if <code>tbl_name</code> exists in database.</li> <li> <p>create_index(   self, scalar: <code>Optional[bool]</code> = False, col_name: <code>Optional[str]</code> = None, num_partitions: <code>Optional[int]</code> = 256, num_sub_vectors: <code>Optional[int]</code> = 96, index_cache_size: <code>Optional[int]</code> = None, metric: <code>Optional[str]</code> = \"l2\", ) -&gt; <code>None</code> : Creates a scalar(for non-vector cols) or a vector index on a table.         Make sure your vector column has enough data before creating an index on it.</p> </li> <li> <p>add(self, nodes: <code>List[BaseNode]</code>, **add_kwargs: <code>Any</code>, ) -&gt; <code>List[str]</code> : adds Nodes to the table</p> </li> <li> <p>delete(self, ref_doc_id: <code>str</code>) -&gt; <code>None</code>: Delete nodes using with node_ids.</p> </li> <li>delete_nodes(self, node_ids: <code>List[str]</code>) -&gt; <code>None</code> : Delete nodes using with node_ids.</li> <li>query(         self,         query: <code>VectorStoreQuery</code>,         **kwargs: <code>Any</code>,     ) -&gt; <code>VectorStoreQueryResult</code>:         Query index(<code>VectorStoreIndex</code>) for top k most similar nodes. Accepts llamaIndex <code>VectorStoreQuery</code> object.</li> </ul>"},{"location":"integrations/phidata/","title":"phidata Integration with LanceDB | AI Assistant Framework Guide","text":"<p>phidata is a framework for building AI Assistants with long-term memory, contextual knowledge, and the ability to take actions using function calling. It helps turn general-purpose LLMs into specialized assistants tailored to your use case by extending its capabilities using memory, knowledge, and tools. </p> <ul> <li>Memory: Stores chat history in a database and enables LLMs to have long-term conversations.</li> <li>Knowledge: Stores information in a vector database and provides LLMs with business context. (Here we will use LanceDB)</li> <li>Tools: Enable LLMs to take actions like pulling data from an API, sending emails or querying a database, etc.</li> </ul> <p></p> <p>Memory &amp; knowledge make LLMs smarter while tools make them autonomous.</p> <p>LanceDB is a vector database and its integration into phidata makes it easy for us to provide a knowledge base to LLMs. It enables us to store information as embeddings and search for the results similar to ours using query. </p> What is Knowledge Base? <p>Knowledge Base is a database of information that the Assistant can search to improve its responses. This information is stored in a vector database and provides LLMs with business context, which makes them respond in a context-aware manner.</p> <p>While any type of storage can act as a knowledge base, vector databases offer the best solution for retrieving relevant results from dense information quickly.</p> <p>Let's see how using LanceDB inside phidata helps in making LLM more useful:</p>"},{"location":"integrations/phidata/#prerequisites-install-and-import-necessary-dependencies","title":"Prerequisites: install and import necessary dependencies","text":"<p>Create a virtual environment</p> <ol> <li>install virtualenv package     <pre><code>pip install virtualenv\n</code></pre></li> <li>Create a directory for your project and go to the directory and create a virtual environment inside it.     <pre><code>mkdir phi\n</code></pre> <pre><code>cd phi\n</code></pre> <pre><code>python -m venv phidata_\n</code></pre></li> </ol> <p>Activating virtual environment</p> <ol> <li>from inside the project directory, run the following command to activate the virtual environment.     <pre><code>phidata_/Scripts/activate\n</code></pre></li> </ol> <p>Install the following packages in the virtual environment <pre><code>pip install lancedb phidata youtube_transcript_api openai ollama numpy pandas\n</code></pre></p> <p>Create python files and import necessary libraries</p> <p>You need to create two files - <code>transcript.py</code> and <code>ollama_assistant.py</code> or <code>openai_assistant.py</code></p> openai_assistant.pyollama_assistant.pytranscript.py <pre><code>import os, openai\nfrom rich.prompt import Prompt\nfrom phi.assistant import Assistant\nfrom phi.knowledge.text import TextKnowledgeBase\nfrom phi.vectordb.lancedb import LanceDb\nfrom phi.llm.openai import OpenAIChat\nfrom phi.embedder.openai import OpenAIEmbedder\nfrom transcript import extract_transcript\n\nif \"OPENAI_API_KEY\" not in os.environ:\n# OR set the key here as a variable\n    openai.api_key = \"sk-...\"\n\n# The code below creates a file \"transcript.txt\" in the directory, the txt file will be used below\nyoutube_url = \"https://www.youtube.com/watch?v=Xs33-Gzl8Mo\" \nsegment_duration = 20\ntranscript_text,dict_transcript = extract_transcript(youtube_url,segment_duration)\n</code></pre> <pre><code>from rich.prompt import Prompt\nfrom phi.assistant import Assistant\nfrom phi.knowledge.text import TextKnowledgeBase\nfrom phi.vectordb.lancedb import LanceDb\nfrom phi.llm.ollama import Ollama\nfrom phi.embedder.ollama import OllamaEmbedder\nfrom transcript import extract_transcript\n\n# The code below creates a file \"transcript.txt\" in the directory, the txt file will be used below\nyoutube_url = \"https://www.youtube.com/watch?v=Xs33-Gzl8Mo\"\nsegment_duration = 20\ntranscript_text,dict_transcript = extract_transcript(youtube_url,segment_duration)\n</code></pre> <pre><code>from youtube_transcript_api import YouTubeTranscriptApi\nimport re\n\ndef smodify(seconds):\n    hours, remainder = divmod(seconds, 3600)\n    minutes, seconds = divmod(remainder, 60)\n    return f\"{int(hours):02}:{int(minutes):02}:{int(seconds):02}\"\n\ndef extract_transcript(youtube_url,segment_duration):\n    # Extract video ID from the URL\n    video_id = re.search(r'(?&lt;=v=)[\\w-]+', youtube_url)\n    if not video_id:\n        video_id = re.search(r'(?&lt;=be/)[\\w-]+', youtube_url)\n    if not video_id:\n        return None\n\n    video_id = video_id.group(0)\n\n    # Attempt to fetch the transcript\n    try:\n        # Try to get the official transcript\n        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n    except Exception:\n        # If no official transcript is found, try to get auto-generated transcript\n        try:\n            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n            for transcript in transcript_list:\n                transcript = transcript.translate('en').fetch()\n        except Exception:\n            return None\n\n    # Format the transcript into 120s chunks\n    transcript_text,dict_transcript = format_transcript(transcript,segment_duration)\n    # Open the file in write mode, which creates it if it doesn't exist\n    with open(\"transcript.txt\", \"w\",encoding=\"utf-8\") as file:\n        file.write(transcript_text)\n    return transcript_text,dict_transcript\n\ndef format_transcript(transcript,segment_duration):\n    chunked_transcript = []\n    chunk_dict = []\n    current_chunk = []\n    current_time = 0\n    # 2 minutes in seconds\n    start_time_chunk = 0  # To track the start time of the current chunk\n\n    for segment in transcript:\n        start_time = segment['start']\n        end_time_x = start_time + segment['duration']\n        text = segment['text']\n\n        # Add text to the current chunk\n        current_chunk.append(text)\n\n        # Update the current time with the duration of the current segment\n        # The duration of the current segment is given by segment['start'] - start_time_chunk\n        if current_chunk:\n            current_time = start_time - start_time_chunk\n\n        # If current chunk duration reaches or exceeds 2 minutes, save the chunk\n        if current_time &gt;= segment_duration:\n            # Use the start time of the first segment in the current chunk as the timestamp\n            chunked_transcript.append(f\"[{smodify(start_time_chunk)} to {smodify(end_time_x)}] \" + \" \".join(current_chunk))\n            current_chunk = re.sub(r'[\\xa0\\n]', lambda x: '' if x.group() == '\\xa0' else ' ', \"\\n\".join(current_chunk))\n            chunk_dict.append({\"timestamp\":f\"[{smodify(start_time_chunk)} to {smodify(end_time_x)}]\", \"text\": \"\".join(current_chunk)})\n            current_chunk = []  # Reset the chunk\n            start_time_chunk = start_time + segment['duration'] # Update the start time for the next chunk\n            current_time = 0  # Reset current time\n\n    # Add any remaining text in the last chunk\n    if current_chunk:\n        chunked_transcript.append(f\"[{smodify(start_time_chunk)} to {smodify(end_time_x)}] \" + \" \".join(current_chunk))\n        current_chunk = re.sub(r'[\\xa0\\n]', lambda x: '' if x.group() == '\\xa0' else ' ', \"\\n\".join(current_chunk))\n        chunk_dict.append({\"timestamp\":f\"[{smodify(start_time_chunk)} to {smodify(end_time_x)}]\", \"text\": \"\".join(current_chunk)})\n\n    return \"\\n\\n\".join(chunked_transcript), chunk_dict\n</code></pre> <p>Warning</p> <p>If creating Ollama assistant, download and install Ollama from here and then run the Ollama instance in the background. Also, download the required models using <code>ollama pull &lt;model-name&gt;</code>. Check out the models here </p> <p>Run the following command to deactivate the virtual environment if needed <pre><code>deactivate\n</code></pre></p>"},{"location":"integrations/phidata/#step-1-create-a-knowledge-base-for-ai-assistant-using-lancedb","title":"Step 1 - Create a Knowledge Base for AI Assistant using LanceDB","text":"openai_assistant.pyollama_assistant.py <pre><code># Create knowledge Base with OpenAIEmbedder in LanceDB\nknowledge_base = TextKnowledgeBase(\n    path=\"transcript.txt\",\n    vector_db=LanceDb(\n        embedder=OpenAIEmbedder(api_key = openai.api_key),\n        table_name=\"transcript_documents\",\n        uri=\"./t3mp/.lancedb\",\n    ),\n    num_documents = 10\n)\n</code></pre> <pre><code># Create knowledge Base with OllamaEmbedder in LanceDB\nknowledge_base = TextKnowledgeBase(\n    path=\"transcript.txt\",\n    vector_db=LanceDb(\n        embedder=OllamaEmbedder(model=\"nomic-embed-text\",dimensions=768),\n        table_name=\"transcript_documents\",\n        uri=\"./t2mp/.lancedb\",\n    ),\n    num_documents = 10\n)\n</code></pre> <p>Check out the list of embedders supported by phidata and their usage here.</p> <p>Here we have used <code>TextKnowledgeBase</code>, which loads text/docx files to the knowledge base.</p> <p>Let's see all the parameters that <code>TextKnowledgeBase</code> takes -</p> Name Type Purpose Default <code>path</code> <code>Union[str, Path]</code> Path to text file(s). It can point to a single text file or a directory of text files. provided by user <code>formats</code> <code>List[str]</code> File formats accepted by this knowledge base. <code>[\".txt\"]</code> <code>vector_db</code> <code>VectorDb</code> Vector Database for the Knowledge Base. phidata provides a wrapper around many vector DBs, you can import it like this - <code>from phi.vectordb.lancedb import LanceDb</code> provided by user <code>num_documents</code> <code>int</code> Number of results (documents/vectors) that vector search should return. <code>5</code> <code>reader</code> <code>TextReader</code> phidata provides many types of reader objects which read data, clean it and create chunks of data, encapsulate each chunk inside an object of the <code>Document</code> class, and return <code>List[Document]</code>. <code>TextReader()</code> <code>optimize_on</code> <code>int</code> It is used to specify the number of documents on which to optimize the vector database. Supposed to create an index. <code>1000</code> Wonder! What is <code>Document</code> class? <p>We know that, before storing the data in vectorDB, we need to split the data into smaller chunks upon which embeddings will be created and these embeddings along with the chunks will be stored in vectorDB. When the user queries over the vectorDB, some of these embeddings will be returned as the result based on the semantic similarity with the query.</p> <p>When the user queries over vectorDB, the queries are converted into embeddings, and a nearest neighbor search is performed over these query embeddings which returns the embeddings that correspond to most semantically similar chunks(parts of our data) present in vectorDB. </p> <p>Here, a \"Document\" is a class in phidata. Since there is an option to let phidata create and manage embeddings, it splits our data into smaller chunks(as expected). It does not directly create embeddings on it. Instead, it takes each chunk and encapsulates it inside the object of the <code>Document</code> class along with various other metadata related to the chunk. Then embeddings are created on these <code>Document</code> objects and stored in vectorDB.</p> <pre><code>class Document(BaseModel):\n    \"\"\"Model for managing a document\"\"\"\n\n    content: str # &lt;--- here data of chunk is stored \n    id: Optional[str] = None\n    name: Optional[str] = None\n    meta_data: Dict[str, Any] = {}\n    embedder: Optional[Embedder] = None\n    embedding: Optional[List[float]] = None\n    usage: Optional[Dict[str, Any]] = None\n</code></pre> <p>However, using phidata you can load many other types of data in the knowledge base(other than text). Check out phidata Knowledge Base for more information.</p> <p>Let's dig deeper into the <code>vector_db</code> parameter and see what parameters <code>LanceDb</code> takes -</p> Name Type Purpose Default <code>embedder</code> <code>Embedder</code> phidata provides many Embedders that abstract the interaction with embedding APIs and utilize it to generate embeddings. Check out other embedders here <code>OpenAIEmbedder</code> <code>distance</code> <code>List[str]</code> The choice of distance metric used to calculate the similarity between vectors, which directly impacts search results and performance in vector databases. <code>Distance.cosine</code> <code>connection</code> <code>lancedb.db.LanceTable</code> LanceTable can be accessed through <code>.connection</code>. You can connect to an existing table of LanceDB, created outside of phidata, and utilize it. If not provided, it creates a new table using <code>table_name</code> parameter and adds it to <code>connection</code>. <code>None</code> <code>uri</code> <code>str</code> It specifies the directory location of LanceDB database and establishes a connection that can be used to interact with the database. <code>\"/tmp/lancedb\"</code> <code>table_name</code> <code>str</code> If <code>connection</code> is not provided, it initializes and connects to a new LanceDB table with a specified(or default) name in the database present at <code>uri</code>. <code>\"phi\"</code> <code>nprobes</code> <code>int</code> It refers to the number of partitions that the search algorithm examines to find the nearest neighbors of a given query vector. Higher values will yield better recall (more likely to find vectors if they exist) at the expense of latency. <code>20</code> <p>Note</p> <p>Since we just initialized the KnowledgeBase. The VectorDB table that corresponds to this Knowledge Base is not yet populated with our data. It will be populated in Step 3, once we perform the <code>load</code> operation. </p> <p>You can check the state of the LanceDB table using - <code>knowledge_base.vector_db.connection.to_pandas()</code></p> <p>Now that the Knowledge Base is initialized, , we can go to step 2.</p>"},{"location":"integrations/phidata/#step-2-create-an-assistant-with-our-choice-of-llm-and-reference-to-the-knowledge-base","title":"Step 2 -  Create an assistant with our choice of LLM and reference to the knowledge base.","text":"openai_assistant.pyollama_assistant.py <pre><code># define an assistant with gpt-4o-mini llm and reference to the knowledge base created above\nassistant = Assistant(\n    llm=OpenAIChat(model=\"gpt-4o-mini\", max_tokens=1000, temperature=0.3,api_key = openai.api_key),\n    description=\"\"\"You are an Expert in explaining youtube video transcripts. You are a bot that takes transcript of a video and answer the question based on it.\n\n    This is transcript for the above timestamp: {relevant_document}\n    The user input is: {user_input}\n    generate highlights only when asked.\n    When asked to generate highlights from the video, understand the context for each timestamp and create key highlight points, answer in following way - \n    [timestamp] - highlight 1\n    [timestamp] - highlight 2\n    ... so on\n\n    Your task is to understand the user question, and provide an answer using the provided contexts. Your answers are correct, high-quality, and written by an domain expert. If the provided context does not contain the answer, simply state,'The provided context does not have the answer.'\"\"\",\n    knowledge_base=knowledge_base,\n    add_references_to_prompt=True,\n)\n</code></pre> <pre><code># define an assistant with llama3.1 llm and reference to the knowledge base created above\nassistant = Assistant(\n    llm=Ollama(model=\"llama3.1\"),\n    description=\"\"\"You are an Expert in explaining youtube video transcripts. You are a bot that takes transcript of a video and answer the question based on it.\n\n    This is transcript for the above timestamp: {relevant_document}\n    The user input is: {user_input}\n    generate highlights only when asked.\n    When asked to generate highlights from the video, understand the context for each timestamp and create key highlight points, answer in following way - \n    [timestamp] - highlight 1\n    [timestamp] - highlight 2\n    ... so on\n\n    Your task is to understand the user question, and provide an answer using the provided contexts. Your answers are correct, high-quality, and written by an domain expert. If the provided context does not contain the answer, simply state,'The provided context does not have the answer.'\"\"\",\n    knowledge_base=knowledge_base,\n    add_references_to_prompt=True,\n)\n</code></pre> <p>Assistants add memory, knowledge, and tools to LLMs. Here we will add only knowledge in this example. </p> <p>Whenever we will give a query to LLM, the assistant will retrieve relevant information from our Knowledge Base(table in LanceDB) and pass it to LLM along with the user query in a structured way. </p> <ul> <li>The <code>add_references_to_prompt=True</code> always adds information from the knowledge base to the prompt, regardless of whether it is relevant to the question.</li> </ul> <p>To know more about an creating assistant in phidata, check out phidata docs here.</p>"},{"location":"integrations/phidata/#step-3-load-data-to-knowledge-base","title":"Step 3 - Load data to Knowledge Base.","text":"<p><pre><code># load out data into the knowledge_base (populating the LanceTable)\nassistant.knowledge_base.load(recreate=False)\n</code></pre> The above code loads the data to the Knowledge Base(LanceDB Table) and now it is ready to be used by the assistant. </p> Name Type Purpose Default <code>recreate</code> <code>bool</code> If True, it drops the existing table and recreates the table in the vectorDB. <code>False</code> <code>upsert</code> <code>bool</code> If True and the vectorDB supports upsert, it will upsert documents to the vector db. <code>False</code> <code>skip_existing</code> <code>bool</code> If True, skips documents that already exist in the vectorDB when inserting. <code>True</code> What is upsert? <p>Upsert is a database operation that combines \"update\" and \"insert\". It updates existing records if a document with the same identifier does exist, or inserts new records if no matching record exists. This is useful for maintaining the most current information without manually checking for existence.</p> <p>During the Load operation, phidata directly interacts with the LanceDB library and performs the loading of the table with our data in the following steps -     </p> <ol> <li> <p>Creates and initializes the table if it does not exist.</p> </li> <li> <p>Then it splits our data into smaller chunks.</p> How do they create chunks? <p>phidata provides many types of Knowledge Bases based on the type of data. Most of them  has a property method called <code>document_lists</code> of type <code>Iterator[List[Document]]</code>. During the load operation, this property method is invoked. It traverses on the data provided by us (in this case, a text file(s)) using <code>reader</code>. Then it reads, creates chunks, and encapsulates each chunk inside a <code>Document</code> object and yields lists of <code>Document</code> objects that contain our data.</p> </li> <li> <p>Then embeddings are created on these chunks are inserted into the LanceDB Table </p> How do they insert your data as different rows in LanceDB Table? <p>The chunks of your data are in the form - lists of <code>Document</code> objects. It was yielded in the step above.</p> <p>for each <code>Document</code> in <code>List[Document]</code>, it does the following operations:</p> <ul> <li>Creates embedding on <code>Document</code>.</li> <li>Cleans the content attribute(chunks of our data is here) of <code>Document</code>.</li> <li> <p>Prepares data by creating <code>id</code> and loading <code>payload</code> with the metadata related to this chunk. (1)</p> <ol> <li>Three columns will be added to the table - <code>\"id\"</code>, <code>\"vector\"</code>, and <code>\"payload\"</code> (payload contains various metadata including <code>content</code>)</li> </ol> </li> <li> <p>Then add this data to LanceTable. </p> </li> </ul> </li> <li> <p>Now the internal state of <code>knowledge_base</code> is changed (embeddings are created and loaded in the table ) and it ready to be used by assistant.</p> </li> </ol>"},{"location":"integrations/phidata/#step-4-start-a-cli-chatbot-with-access-to-the-knowledge-base","title":"Step 4 - Start a cli chatbot with access to the Knowledge base","text":"<pre><code># start cli chatbot with knowledge base\nassistant.print_response(\"Ask me about something from the knowledge base\")\nwhile True:\n    message = Prompt.ask(f\"[bold] :sunglasses: User [/bold]\")\n    if message in (\"exit\", \"bye\"):\n        break\n    assistant.print_response(message, markdown=True)\n</code></pre> <p>For more information and amazing cookbooks of phidata, read the phidata documentation and also visit LanceDB x phidata docmentation.</p>"},{"location":"integrations/prompttools/","title":"PromptTools Integration with LanceDB | Evaluation Framework Guide","text":"<p>PromptTools offers a set of free, open-source tools for testing and experimenting with models, prompts, and configurations. The core idea is to enable developers to evaluate prompts using familiar interfaces like code and notebooks. You can use it to experiment with different configurations of LanceDB, and test how LanceDB integrates with the LLM of your choice.</p> <p></p> <p></p>"},{"location":"integrations/voxel51/","title":"FiftyOne","text":"<p>FiftyOne is an open source toolkit that enables users to curate better data and build better models. It includes tools for data exploration, visualization, and management, as well as features for collaboration and sharing. </p> <p>Any developers, data scientists, and researchers who work with computer vision and machine learning can use FiftyOne to improve the quality of their datasets and deliver insights about their models.</p> <p></p> <p>FiftyOne provides an API to create LanceDB tables and run similarity queries, both programmatically in Python and via point-and-click in the App.</p> <p>Let's get started and see how to use LanceDB to create a similarity index on your FiftyOne datasets.</p>"},{"location":"integrations/voxel51/#overview","title":"Overview","text":"<p>Embeddings are foundational to all of the vector search features. In FiftyOne, embeddings are managed by the FiftyOne Brain that provides powerful machine learning techniques designed to transform how you curate your data from an art into a measurable science. </p> <p>Have you ever wanted to find the images most similar to an image in your dataset?</p> <p>The FiftyOne Brain makes computing visual similarity really easy. You can compute the similarity of samples in your dataset using an embedding model and store the results in the brain key. </p> <p>You can then sort your samples by similarity or use this information to find potential duplicate images.</p> <p>Here we will be doing the following : </p> <ol> <li> <p>Create Index - In order to run similarity queries against our media, we need to index the data. We can do this via the <code>compute_similarity()</code> function. </p> <ul> <li>In the function, specify the model you want to use to generate the embedding vectors, and what vector search engine you want to use on the backend (here LanceDB). </li> </ul> <p>Tip</p> <p>You can also give the similarity index a name(<code>brain_key</code>), which is useful if you want to run vector searches against multiple indexes.</p> </li> <li> <p>Query - Once you have generated your similarity index, you can query your dataset with <code>sort_by_similarity()</code>. The query can be any of the following:</p> <ul> <li>An ID (sample or patch)</li> <li>A query vector of same dimension as the index</li> <li>A list of IDs (samples or patches)</li> <li>A text prompt (search semantically)</li> </ul> </li> </ol>"},{"location":"integrations/voxel51/#prerequisites-install-necessary-dependencies","title":"Prerequisites: install necessary dependencies","text":"<ol> <li> <p>Create and activate a virtual environment</p> <p>Install virtualenv package and run the following command in your project directory. <pre><code>python -m venv fiftyone_\n</code></pre> From inside the project directory run the following to activate the virtual environment.</p> WindowsmacOS/Linux <pre><code>fiftyone_/Scripts/activate\n</code></pre> <pre><code>source fiftyone_/Scripts/activate\n</code></pre> </li> <li> <p>Install the following packages in the virtual environment</p> <p>To install FiftyOne, ensure you have activated any virtual environment that you are using, then run <pre><code>pip install fiftyone\n</code></pre></p> </li> </ol>"},{"location":"integrations/voxel51/#understand-basic-workflow","title":"Understand basic workflow","text":"<p>The basic workflow shown below uses LanceDB to create a similarity index on your FiftyOne datasets:</p> <ol> <li> <p>Load a dataset into FiftyOne.</p> </li> <li> <p>Compute embedding vectors for samples or patches in your dataset, or select a model to use to generate embeddings.</p> </li> <li> <p>Use the <code>compute_similarity()</code> method to generate a LanceDB table for the samples or object patches embeddings in a dataset by setting the parameter <code>backend=\"lancedb\"</code> and specifying a <code>brain_key</code> of your choice.</p> </li> <li> <p>Use this LanceDB table to query your data with <code>sort_by_similarity()</code>.</p> </li> <li> <p>If desired, delete the table.</p> </li> </ol>"},{"location":"integrations/voxel51/#quick-example","title":"Quick Example","text":"<p>Let's jump on a quick example that demonstrates this workflow.</p> <p><pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\n# Step 1: Load your data into FiftyOne\ndataset = foz.load_zoo_dataset(\"quickstart\")\n</code></pre> Make sure you install torch (guide here) before proceeding. </p> <pre><code># Steps 2 and 3: Compute embeddings and create a similarity index\nlancedb_index = fob.compute_similarity(\n    dataset, \n    model=\"clip-vit-base32-torch\",\n    brain_key=\"lancedb_index\",\n    backend=\"lancedb\",\n)\n</code></pre> <p>Note</p> <p>Running the code above will download the clip model (2.6Gb)</p> <p>Once the similarity index has been generated, we can query our data in FiftyOne by specifying the <code>brain_key</code>:</p> <p><pre><code># Step 4: Query your data\nquery = dataset.first().id  # query by sample ID\nview = dataset.sort_by_similarity(\n    query, \n    brain_key=\"lancedb_index\",\n    k=10,  # limit to 10 most similar samples\n)\n</code></pre> The returned result are of type - <code>DatasetView</code>.</p> <p>Note</p> <p><code>DatasetView</code> does not hold its contents in-memory. Views simply store the rule(s) that are applied to extract the content of interest from the underlying Dataset when the view is iterated/aggregated on.</p> <p>This means, for example, that the contents of a <code>DatasetView</code> may change as the underlying Dataset is modified.</p> Can you query a view instead of dataset? <p>Yes, you can also query a view.</p> <p>Performing a similarity search on a <code>DatasetView</code> will only return results from the view; if the view contains samples that were not included in the index, they will never be included in the result.</p> <p>This means that you can index an entire Dataset once and then perform searches on subsets of the dataset by constructing views that contain the images of interest.</p> <pre><code># Step 5 (optional): Cleanup\n\n# Delete the LanceDB table\nlancedb_index.cleanup()\n\n# Delete run record from FiftyOne\ndataset.delete_brain_run(\"lancedb_index\")\n</code></pre>"},{"location":"integrations/voxel51/#using-lancedb-backend","title":"Using LanceDB backend","text":"<p>By default, calling <code>compute_similarity()</code> or <code>sort_by_similarity()</code> will use an sklearn backend.</p> <p>To use the LanceDB backend, simply set the optional <code>backend</code> parameter of <code>compute_similarity()</code> to <code>\"lancedb\"</code>:</p> <pre><code>import fiftyone.brain as fob\n#... rest of the code\nfob.compute_similarity(..., backend=\"lancedb\", ...)\n</code></pre> <p>Alternatively, you can configure FiftyOne to use the LanceDB backend by setting the following environment variable.</p> <p>In your terminal, set the environment variable using:</p> WindowsmacOS/Linux <pre><code>$Env:FIFTYONE_BRAIN_DEFAULT_SIMILARITY_BACKEND=\"lancedb\" //powershell\n\nset FIFTYONE_BRAIN_DEFAULT_SIMILARITY_BACKEND=lancedb //cmd\n</code></pre> <pre><code>export FIFTYONE_BRAIN_DEFAULT_SIMILARITY_BACKEND=lancedb\n</code></pre> <p>Note</p> <p>This will only run during the terminal session. Once terminal is closed, environment variable is deleted.</p> <p>Alternatively, you can permanently configure FiftyOne to use the LanceDB backend creating a <code>brain_config.json</code> at <code>~/.fiftyone/brain_config.json</code>. The JSON file may contain any desired subset of config fields that you wish to customize.</p> <p><pre><code>{\n    \"default_similarity_backend\": \"lancedb\"\n}\n</code></pre> This will override the default <code>brain_config</code> and will set it according to your customization. You can check the configuration by running the following code : </p> <pre><code>import fiftyone.brain as fob\n# Print your current brain config\nprint(fob.brain_config)\n</code></pre>"},{"location":"integrations/voxel51/#lancedb-config-parameters","title":"LanceDB config parameters","text":"<p>The LanceDB backend supports query parameters that can be used to customize your similarity queries. These parameters include:</p> Name Purpose Default table_name The name of the LanceDB table to use. If none is provided, a new table will be created <code>None</code> metric The embedding distance metric to use when creating a new table. The supported values are (\"cosine\", \"euclidean\") <code>\"cosine\"</code> uri The database URI to use. In this Database URI, tables will be created. <code>\"/tmp/lancedb\"</code> <p>There are two ways to specify/customize the parameters:</p> <ol> <li> <p>Using <code>brain_config.json</code> file </p> <pre><code>{\n    \"similarity_backends\": {\n        \"lancedb\": {\n            \"table_name\": \"your-table\",\n            \"metric\": \"euclidean\",\n            \"uri\": \"/tmp/lancedb\"\n        }\n    }\n}\n</code></pre> </li> <li> <p>Directly passing to <code>compute_similarity()</code> to configure a specific new index : </p> <pre><code>lancedb_index = fob.compute_similarity(\n    ...\n    backend=\"lancedb\",\n    brain_key=\"lancedb_index\",\n    table_name=\"your-table\",\n    metric=\"euclidean\",\n    uri=\"/tmp/lancedb\",\n)\n</code></pre> </li> </ol> <p>For a much more in depth walkthrough of the integration, visit the LanceDB x Voxel51 docs page.</p>"},{"location":"javascript/","title":"Index","text":"<p>vectordb / Exports</p>"},{"location":"javascript/#lancedb","title":"LanceDB","text":"<p>A JavaScript / Node.js library for LanceDB.</p>"},{"location":"javascript/#installation","title":"Installation","text":"<pre><code>npm install vectordb\n</code></pre> <p>This will download the appropriate native library for your platform. We currently support:</p> <ul> <li>Linux (x86_64 and aarch64)</li> <li>MacOS (Intel and ARM/M1/M2)</li> <li>Windows (x86_64 only)</li> </ul> <p>We do not yet support musl-based Linux (such as Alpine Linux) or aarch64 Windows.</p>"},{"location":"javascript/#usage","title":"Usage","text":""},{"location":"javascript/#basic-example","title":"Basic Example","text":"<p><pre><code>const lancedb = require('vectordb');\nconst db = await lancedb.connect('data/sample-lancedb');\nconst table = await db.createTable(\"my_table\",\n      [{ id: 1, vector: [0.1, 1.0], item: \"foo\", price: 10.0 },\n      { id: 2, vector: [3.9, 0.5], item: \"bar\", price: 20.0 }])\nconst results = await table.search([0.1, 0.3]).limit(20).execute();\nconsole.log(results);\n</code></pre> The examples folder contains complete examples.</p>"},{"location":"javascript/#development","title":"Development","text":"<p>To build everything fresh:</p> <pre><code>npm install\nnpm run build\n</code></pre> <p>Then you should be able to run the tests with:</p> <pre><code>npm test\n</code></pre>"},{"location":"javascript/#fix-lints","title":"Fix lints","text":"<p>To run the linter and have it automatically fix all errors</p> <pre><code>npm run lint -- --fix\n</code></pre> <p>To build documentation</p> <pre><code>npx typedoc --plugin typedoc-plugin-markdown --out ../docs/src/javascript src/index.ts\n</code></pre>"},{"location":"javascript/modules/","title":"Legacy SDK (vectordb)","text":"<p>vectordb / Exports</p>"},{"location":"javascript/modules/#vectordb","title":"vectordb","text":""},{"location":"javascript/modules/#table-of-contents","title":"Table of contents","text":""},{"location":"javascript/modules/#enumerations","title":"Enumerations","text":"<ul> <li>IndexStatus</li> <li>MetricType</li> <li>WriteMode</li> </ul>"},{"location":"javascript/modules/#classes","title":"Classes","text":"<ul> <li>DefaultWriteOptions</li> <li>LocalConnection</li> <li>LocalTable</li> <li>MakeArrowTableOptions</li> <li>OpenAIEmbeddingFunction</li> <li>Query</li> </ul>"},{"location":"javascript/modules/#interfaces","title":"Interfaces","text":"<ul> <li>AwsCredentials</li> <li>CleanupStats</li> <li>ColumnAlteration</li> <li>CompactionMetrics</li> <li>CompactionOptions</li> <li>Connection</li> <li>ConnectionOptions</li> <li>CreateTableOptions</li> <li>EmbeddingFunction</li> <li>IndexStats</li> <li>IvfPQIndexConfig</li> <li>MergeInsertArgs</li> <li>Table</li> <li>UpdateArgs</li> <li>UpdateSqlArgs</li> <li>VectorIndex</li> <li>WriteOptions</li> </ul>"},{"location":"javascript/modules/#type-aliases","title":"Type Aliases","text":"<ul> <li>VectorIndexParams</li> </ul>"},{"location":"javascript/modules/#functions","title":"Functions","text":"<ul> <li>connect</li> <li>convertToTable</li> <li>isWriteOptions</li> <li>makeArrowTable</li> </ul>"},{"location":"javascript/modules/#type-aliases_1","title":"Type Aliases","text":""},{"location":"javascript/modules/#vectorindexparams","title":"VectorIndexParams","text":"<p>\u01ac VectorIndexParams: <code>IvfPQIndexConfig</code></p>"},{"location":"javascript/modules/#defined-in","title":"Defined in","text":"<p>index.ts:1336</p>"},{"location":"javascript/modules/#functions_1","title":"Functions","text":""},{"location":"javascript/modules/#connect","title":"connect","text":"<p>\u25b8 connect(<code>uri</code>): <code>Promise</code>\\&lt;<code>Connection</code>&gt;</p> <p>Connect to a LanceDB instance at the given URI.</p> <p>Accepted formats:</p> <ul> <li><code>/path/to/database</code> - local database</li> <li><code>s3://bucket/path/to/database</code> or <code>gs://bucket/path/to/database</code> - database on cloud storage</li> <li><code>db://host:port</code> - remote database (LanceDB cloud)</li> </ul>"},{"location":"javascript/modules/#parameters","title":"Parameters","text":"Name Type Description <code>uri</code> <code>string</code> The uri of the database. If the database uri starts with <code>db://</code> then it connects to a remote database."},{"location":"javascript/modules/#returns","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>Connection</code>&gt;</p> <p><code>See</code></p> <p>ConnectionOptions for more details on the URI format.</p>"},{"location":"javascript/modules/#defined-in_1","title":"Defined in","text":"<p>index.ts:188</p> <p>\u25b8 connect(<code>opts</code>): <code>Promise</code>\\&lt;<code>Connection</code>&gt;</p> <p>Connect to a LanceDB instance with connection options.</p>"},{"location":"javascript/modules/#parameters_1","title":"Parameters","text":"Name Type Description <code>opts</code> <code>Partial</code>\\&lt;<code>ConnectionOptions</code>&gt; The ConnectionOptions to use when connecting to the database."},{"location":"javascript/modules/#returns_1","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>Connection</code>&gt;</p>"},{"location":"javascript/modules/#defined-in_2","title":"Defined in","text":"<p>index.ts:194</p>"},{"location":"javascript/modules/#converttotable","title":"convertToTable","text":"<p>\u25b8 convertToTable\\&lt;<code>T</code>&gt;(<code>data</code>, <code>embeddings?</code>, <code>makeTableOptions?</code>): <code>Promise</code>\\&lt;<code>ArrowTable</code>&gt;</p>"},{"location":"javascript/modules/#type-parameters","title":"Type parameters","text":"Name <code>T</code>"},{"location":"javascript/modules/#parameters_2","title":"Parameters","text":"Name Type <code>data</code> <code>Record</code>\\&lt;<code>string</code>, <code>unknown</code>&gt;[] <code>embeddings?</code> <code>EmbeddingFunction</code>\\&lt;<code>T</code>&gt; <code>makeTableOptions?</code> <code>Partial</code>\\&lt;<code>MakeArrowTableOptions</code>&gt;"},{"location":"javascript/modules/#returns_2","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>ArrowTable</code>&gt;</p>"},{"location":"javascript/modules/#defined-in_3","title":"Defined in","text":"<p>arrow.ts:465</p>"},{"location":"javascript/modules/#iswriteoptions","title":"isWriteOptions","text":"<p>\u25b8 isWriteOptions(<code>value</code>): value is WriteOptions</p>"},{"location":"javascript/modules/#parameters_3","title":"Parameters","text":"Name Type <code>value</code> <code>any</code>"},{"location":"javascript/modules/#returns_3","title":"Returns","text":"<p>value is WriteOptions</p>"},{"location":"javascript/modules/#defined-in_4","title":"Defined in","text":"<p>index.ts:1362</p>"},{"location":"javascript/modules/#makearrowtable","title":"makeArrowTable","text":"<p>\u25b8 makeArrowTable(<code>data</code>, <code>options?</code>): <code>ArrowTable</code></p> <p>An enhanced version of the makeTable function from Apache Arrow that supports nested fields and embeddings columns.</p> <p>This function converts an array of Record (row-major JS objects) to an Arrow Table (a columnar structure) <p>Note that it currently does not support nulls.</p> <p>If a schema is provided then it will be used to determine the resulting array types.  Fields will also be reordered to fit the order defined by the schema.</p> <p>If a schema is not provided then the types will be inferred and the field order will be controlled by the order of properties in the first record.</p> <p>If the input is empty then a schema must be provided to create an empty table.</p> <p>When a schema is not specified then data types will be inferred.  The inference rules are as follows:</p> <ul> <li>boolean =&gt; Bool</li> <li>number =&gt; Float64</li> <li>String =&gt; Utf8</li> <li>Buffer =&gt; Binary</li> <li>Record =&gt; Struct <li>Array =&gt; List"},{"location":"javascript/modules/#parameters_4","title":"Parameters","text":"Name Type Description <code>data</code> <code>Record</code>\\&lt;<code>string</code>, <code>any</code>&gt;[] input data <code>options?</code> <code>Partial</code>\\&lt;<code>MakeArrowTableOptions</code>&gt; options to control the makeArrowTable call."},{"location":"javascript/modules/#returns_4","title":"Returns","text":"<p><code>ArrowTable</code></p> <p><code>Example</code></p> <pre><code>import { fromTableToBuffer, makeArrowTable } from \"../arrow\";\nimport { Field, FixedSizeList, Float16, Float32, Int32, Schema } from \"apache-arrow\";\n\nconst schema = new Schema([\n  new Field(\"a\", new Int32()),\n  new Field(\"b\", new Float32()),\n  new Field(\"c\", new FixedSizeList(3, new Field(\"item\", new Float16()))),\n ]);\n const table = makeArrowTable([\n   { a: 1, b: 2, c: [1, 2, 3] },\n   { a: 4, b: 5, c: [4, 5, 6] },\n   { a: 7, b: 8, c: [7, 8, 9] },\n ], { schema });\n</code></pre> <p>By default it assumes that the column named <code>vector</code> is a vector column and it will be converted into a fixed size list array of type float32. The <code>vectorColumns</code> option can be used to support other vector column names and data types.</p> <pre><code>const schema = new Schema([\n   new Field(\"a\", new Float64()),\n   new Field(\"b\", new Float64()),\n   new Field(\n     \"vector\",\n     new FixedSizeList(3, new Field(\"item\", new Float32()))\n   ),\n ]);\n const table = makeArrowTable([\n   { a: 1, b: 2, vector: [1, 2, 3] },\n   { a: 4, b: 5, vector: [4, 5, 6] },\n   { a: 7, b: 8, vector: [7, 8, 9] },\n ]);\n assert.deepEqual(table.schema, schema);\n</code></pre> <p>You can specify the vector column types and names using the options as well</p> <pre><code>const schema = new Schema([\n   new Field('a', new Float64()),\n   new Field('b', new Float64()),\n   new Field('vec1', new FixedSizeList(3, new Field('item', new Float16()))),\n   new Field('vec2', new FixedSizeList(3, new Field('item', new Float16())))\n ]);\nconst table = makeArrowTable([\n   { a: 1, b: 2, vec1: [1, 2, 3], vec2: [2, 4, 6] },\n   { a: 4, b: 5, vec1: [4, 5, 6], vec2: [8, 10, 12] },\n   { a: 7, b: 8, vec1: [7, 8, 9], vec2: [14, 16, 18] }\n ], {\n   vectorColumns: {\n     vec1: { type: new Float16() },\n     vec2: { type: new Float16() }\n   }\n }\nassert.deepEqual(table.schema, schema)\n</code></pre>"},{"location":"javascript/modules/#defined-in_5","title":"Defined in","text":"<p>arrow.ts:198</p>"},{"location":"javascript/classes/DefaultWriteOptions/","title":"DefaultWriteOptions","text":"<p>vectordb / Exports / DefaultWriteOptions</p>"},{"location":"javascript/classes/DefaultWriteOptions/#class-defaultwriteoptions","title":"Class: DefaultWriteOptions","text":"<p>Write options when creating a Table.</p>"},{"location":"javascript/classes/DefaultWriteOptions/#implements","title":"Implements","text":"<ul> <li><code>WriteOptions</code></li> </ul>"},{"location":"javascript/classes/DefaultWriteOptions/#table-of-contents","title":"Table of contents","text":""},{"location":"javascript/classes/DefaultWriteOptions/#constructors","title":"Constructors","text":"<ul> <li>constructor</li> </ul>"},{"location":"javascript/classes/DefaultWriteOptions/#properties","title":"Properties","text":"<ul> <li>writeMode</li> </ul>"},{"location":"javascript/classes/DefaultWriteOptions/#constructors_1","title":"Constructors","text":""},{"location":"javascript/classes/DefaultWriteOptions/#constructor","title":"constructor","text":"<p>\u2022 new DefaultWriteOptions()</p>"},{"location":"javascript/classes/DefaultWriteOptions/#properties_1","title":"Properties","text":""},{"location":"javascript/classes/DefaultWriteOptions/#writemode","title":"writeMode","text":"<p>\u2022 writeMode: <code>WriteMode</code> = <code>WriteMode.Create</code></p> <p>A WriteMode to use on this operation</p>"},{"location":"javascript/classes/DefaultWriteOptions/#implementation-of","title":"Implementation of","text":"<p>WriteOptions.writeMode</p>"},{"location":"javascript/classes/DefaultWriteOptions/#defined-in","title":"Defined in","text":"<p>index.ts:1359</p>"},{"location":"javascript/classes/LocalConnection/","title":"LocalConnection","text":"<p>vectordb / Exports / LocalConnection</p>"},{"location":"javascript/classes/LocalConnection/#class-localconnection","title":"Class: LocalConnection","text":"<p>A connection to a LanceDB database.</p>"},{"location":"javascript/classes/LocalConnection/#implements","title":"Implements","text":"<ul> <li><code>Connection</code></li> </ul>"},{"location":"javascript/classes/LocalConnection/#table-of-contents","title":"Table of contents","text":""},{"location":"javascript/classes/LocalConnection/#constructors","title":"Constructors","text":"<ul> <li>constructor</li> </ul>"},{"location":"javascript/classes/LocalConnection/#properties","title":"Properties","text":"<ul> <li>_db</li> <li>_options</li> </ul>"},{"location":"javascript/classes/LocalConnection/#accessors","title":"Accessors","text":"<ul> <li>uri</li> </ul>"},{"location":"javascript/classes/LocalConnection/#methods","title":"Methods","text":"<ul> <li>createTable</li> <li>createTableImpl</li> <li>dropTable</li> <li>openTable</li> <li>tableNames</li> <li>withMiddleware</li> </ul>"},{"location":"javascript/classes/LocalConnection/#constructors_1","title":"Constructors","text":""},{"location":"javascript/classes/LocalConnection/#constructor","title":"constructor","text":"<p>\u2022 new LocalConnection(<code>db</code>, <code>options</code>)</p>"},{"location":"javascript/classes/LocalConnection/#parameters","title":"Parameters","text":"Name Type <code>db</code> <code>any</code> <code>options</code> <code>ConnectionOptions</code>"},{"location":"javascript/classes/LocalConnection/#defined-in","title":"Defined in","text":"<p>index.ts:739</p>"},{"location":"javascript/classes/LocalConnection/#properties_1","title":"Properties","text":""},{"location":"javascript/classes/LocalConnection/#_db","title":"_db","text":"<p>\u2022 <code>Private</code> <code>Readonly</code> _db: <code>any</code></p>"},{"location":"javascript/classes/LocalConnection/#defined-in_1","title":"Defined in","text":"<p>index.ts:737</p>"},{"location":"javascript/classes/LocalConnection/#_options","title":"_options","text":"<p>\u2022 <code>Private</code> <code>Readonly</code> _options: () =&gt; <code>ConnectionOptions</code></p>"},{"location":"javascript/classes/LocalConnection/#type-declaration","title":"Type declaration","text":"<p>\u25b8 (): <code>ConnectionOptions</code></p>"},{"location":"javascript/classes/LocalConnection/#returns","title":"Returns","text":"<p><code>ConnectionOptions</code></p>"},{"location":"javascript/classes/LocalConnection/#defined-in_2","title":"Defined in","text":"<p>index.ts:736</p>"},{"location":"javascript/classes/LocalConnection/#accessors_1","title":"Accessors","text":""},{"location":"javascript/classes/LocalConnection/#uri","title":"uri","text":"<p>\u2022 <code>get</code> uri(): <code>string</code></p>"},{"location":"javascript/classes/LocalConnection/#returns_1","title":"Returns","text":"<p><code>string</code></p>"},{"location":"javascript/classes/LocalConnection/#implementation-of","title":"Implementation of","text":"<p>Connection.uri</p>"},{"location":"javascript/classes/LocalConnection/#defined-in_3","title":"Defined in","text":"<p>index.ts:744</p>"},{"location":"javascript/classes/LocalConnection/#methods_1","title":"Methods","text":""},{"location":"javascript/classes/LocalConnection/#createtable","title":"createTable","text":"<p>\u25b8 createTable\\&lt;<code>T</code>&gt;(<code>name</code>, <code>data?</code>, <code>optsOrEmbedding?</code>, <code>opt?</code>): <code>Promise</code>\\&lt;<code>Table</code>\\&lt;<code>T</code>&gt;&gt;</p> <p>Creates a new Table, optionally initializing it with new data.</p>"},{"location":"javascript/classes/LocalConnection/#type-parameters","title":"Type parameters","text":"Name <code>T</code>"},{"location":"javascript/classes/LocalConnection/#parameters_1","title":"Parameters","text":"Name Type <code>name</code> <code>string</code> | <code>CreateTableOptions</code>\\&lt;<code>T</code>&gt; <code>data?</code> <code>Table</code>\\&lt;<code>any</code>&gt; | <code>Record</code>\\&lt;<code>string</code>, <code>unknown</code>&gt;[] <code>optsOrEmbedding?</code> <code>WriteOptions</code> | <code>EmbeddingFunction</code>\\&lt;<code>T</code>&gt; <code>opt?</code> <code>WriteOptions</code>"},{"location":"javascript/classes/LocalConnection/#returns_2","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>Table</code>\\&lt;<code>T</code>&gt;&gt;</p>"},{"location":"javascript/classes/LocalConnection/#implementation-of_1","title":"Implementation of","text":"<p>Connection.createTable</p>"},{"location":"javascript/classes/LocalConnection/#defined-in_4","title":"Defined in","text":"<p>index.ts:788</p>"},{"location":"javascript/classes/LocalConnection/#createtableimpl","title":"createTableImpl","text":"<p>\u25b8 <code>Private</code> createTableImpl\\&lt;<code>T</code>&gt;(<code>\u00abdestructured\u00bb</code>): <code>Promise</code>\\&lt;<code>Table</code>\\&lt;<code>T</code>&gt;&gt;</p>"},{"location":"javascript/classes/LocalConnection/#type-parameters_1","title":"Type parameters","text":"Name <code>T</code>"},{"location":"javascript/classes/LocalConnection/#parameters_2","title":"Parameters","text":"Name Type <code>\u00abdestructured\u00bb</code> <code>Object</code> \u203a\u00a0<code>data?</code> <code>Table</code>\\&lt;<code>any</code>&gt; | <code>Record</code>\\&lt;<code>string</code>, <code>unknown</code>&gt;[] \u203a\u00a0<code>embeddingFunction?</code> <code>EmbeddingFunction</code>\\&lt;<code>T</code>&gt; \u203a\u00a0<code>name</code> <code>string</code> \u203a\u00a0<code>schema?</code> <code>Schema</code>\\&lt;<code>any</code>&gt; \u203a\u00a0<code>writeOptions?</code> <code>WriteOptions</code>"},{"location":"javascript/classes/LocalConnection/#returns_3","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>Table</code>\\&lt;<code>T</code>&gt;&gt;</p>"},{"location":"javascript/classes/LocalConnection/#defined-in_5","title":"Defined in","text":"<p>index.ts:822</p>"},{"location":"javascript/classes/LocalConnection/#droptable","title":"dropTable","text":"<p>\u25b8 dropTable(<code>name</code>): <code>Promise</code>\\&lt;<code>void</code>&gt;</p> <p>Drop an existing table.</p>"},{"location":"javascript/classes/LocalConnection/#parameters_3","title":"Parameters","text":"Name Type Description <code>name</code> <code>string</code> The name of the table to drop."},{"location":"javascript/classes/LocalConnection/#returns_4","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>void</code>&gt;</p>"},{"location":"javascript/classes/LocalConnection/#implementation-of_2","title":"Implementation of","text":"<p>Connection.dropTable</p>"},{"location":"javascript/classes/LocalConnection/#defined-in_6","title":"Defined in","text":"<p>index.ts:876</p>"},{"location":"javascript/classes/LocalConnection/#opentable","title":"openTable","text":"<p>\u25b8 openTable(<code>name</code>): <code>Promise</code>\\&lt;<code>Table</code>\\&lt;<code>number</code>[]&gt;&gt;</p> <p>Open a table in the database.</p>"},{"location":"javascript/classes/LocalConnection/#parameters_4","title":"Parameters","text":"Name Type Description <code>name</code> <code>string</code> The name of the table."},{"location":"javascript/classes/LocalConnection/#returns_5","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>Table</code>\\&lt;<code>number</code>[]&gt;&gt;</p>"},{"location":"javascript/classes/LocalConnection/#implementation-of_3","title":"Implementation of","text":"<p>Connection.openTable</p>"},{"location":"javascript/classes/LocalConnection/#defined-in_7","title":"Defined in","text":"<p>index.ts:760</p> <p>\u25b8 openTable\\&lt;<code>T</code>&gt;(<code>name</code>, <code>embeddings</code>): <code>Promise</code>\\&lt;<code>Table</code>\\&lt;<code>T</code>&gt;&gt;</p> <p>Open a table in the database.</p>"},{"location":"javascript/classes/LocalConnection/#type-parameters_2","title":"Type parameters","text":"Name <code>T</code>"},{"location":"javascript/classes/LocalConnection/#parameters_5","title":"Parameters","text":"Name Type Description <code>name</code> <code>string</code> The name of the table. <code>embeddings</code> <code>EmbeddingFunction</code>\\&lt;<code>T</code>&gt; An embedding function to use on this Table"},{"location":"javascript/classes/LocalConnection/#returns_6","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>Table</code>\\&lt;<code>T</code>&gt;&gt;</p>"},{"location":"javascript/classes/LocalConnection/#implementation-of_4","title":"Implementation of","text":"<p>Connection.openTable</p>"},{"location":"javascript/classes/LocalConnection/#defined-in_8","title":"Defined in","text":"<p>index.ts:768</p> <p>\u25b8 openTable\\&lt;<code>T</code>&gt;(<code>name</code>, <code>embeddings?</code>): <code>Promise</code>\\&lt;<code>Table</code>\\&lt;<code>T</code>&gt;&gt;</p>"},{"location":"javascript/classes/LocalConnection/#type-parameters_3","title":"Type parameters","text":"Name <code>T</code>"},{"location":"javascript/classes/LocalConnection/#parameters_6","title":"Parameters","text":"Name Type <code>name</code> <code>string</code> <code>embeddings?</code> <code>EmbeddingFunction</code>\\&lt;<code>T</code>&gt;"},{"location":"javascript/classes/LocalConnection/#returns_7","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>Table</code>\\&lt;<code>T</code>&gt;&gt;</p>"},{"location":"javascript/classes/LocalConnection/#implementation-of_5","title":"Implementation of","text":"<p>Connection.openTable</p>"},{"location":"javascript/classes/LocalConnection/#defined-in_9","title":"Defined in","text":"<p>index.ts:772</p>"},{"location":"javascript/classes/LocalConnection/#tablenames","title":"tableNames","text":"<p>\u25b8 tableNames(): <code>Promise</code>\\&lt;<code>string</code>[]&gt;</p> <p>Get the names of all tables in the database.</p>"},{"location":"javascript/classes/LocalConnection/#returns_8","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>string</code>[]&gt;</p>"},{"location":"javascript/classes/LocalConnection/#implementation-of_6","title":"Implementation of","text":"<p>Connection.tableNames</p>"},{"location":"javascript/classes/LocalConnection/#defined-in_10","title":"Defined in","text":"<p>index.ts:751</p>"},{"location":"javascript/classes/LocalConnection/#withmiddleware","title":"withMiddleware","text":"<p>\u25b8 withMiddleware(<code>middleware</code>): <code>Connection</code></p> <p>Instrument the behavior of this Connection with middleware.</p> <p>The middleware will be called in the order they are added.</p> <p>Currently this functionality is only supported for remote Connections.</p>"},{"location":"javascript/classes/LocalConnection/#parameters_7","title":"Parameters","text":"Name Type <code>middleware</code> <code>HttpMiddleware</code>"},{"location":"javascript/classes/LocalConnection/#returns_9","title":"Returns","text":"<p><code>Connection</code></p> <ul> <li>this Connection instrumented by the passed middleware</li> </ul>"},{"location":"javascript/classes/LocalConnection/#implementation-of_7","title":"Implementation of","text":"<p>Connection.withMiddleware</p>"},{"location":"javascript/classes/LocalConnection/#defined-in_11","title":"Defined in","text":"<p>index.ts:880</p>"},{"location":"javascript/classes/LocalTable/","title":"LocalTable","text":"<p>vectordb / Exports / LocalTable</p>"},{"location":"javascript/classes/LocalTable/#class-localtablet","title":"Class: LocalTable\\&lt;T&gt;","text":"<p>A LanceDB Table is the collection of Records. Each Record has one or more vector fields.</p>"},{"location":"javascript/classes/LocalTable/#type-parameters","title":"Type parameters","text":"Name Type <code>T</code> <code>number</code>[]"},{"location":"javascript/classes/LocalTable/#implements","title":"Implements","text":"<ul> <li><code>Table</code>\\&lt;<code>T</code>&gt;</li> </ul>"},{"location":"javascript/classes/LocalTable/#table-of-contents","title":"Table of contents","text":""},{"location":"javascript/classes/LocalTable/#constructors","title":"Constructors","text":"<ul> <li>constructor</li> </ul>"},{"location":"javascript/classes/LocalTable/#properties","title":"Properties","text":"<ul> <li>_embeddings</li> <li>_isElectron</li> <li>_name</li> <li>_options</li> <li>_tbl</li> <li>where</li> </ul>"},{"location":"javascript/classes/LocalTable/#accessors","title":"Accessors","text":"<ul> <li>name</li> <li>schema</li> </ul>"},{"location":"javascript/classes/LocalTable/#methods","title":"Methods","text":"<ul> <li>add</li> <li>addColumns</li> <li>alterColumns</li> <li>checkElectron</li> <li>cleanupOldVersions</li> <li>compactFiles</li> <li>countRows</li> <li>createIndex</li> <li>createScalarIndex</li> <li>delete</li> <li>dropColumns</li> <li>filter</li> <li>getSchema</li> <li>indexStats</li> <li>listIndices</li> <li>mergeInsert</li> <li>overwrite</li> <li>search</li> <li>update</li> <li>withMiddleware</li> </ul>"},{"location":"javascript/classes/LocalTable/#constructors_1","title":"Constructors","text":""},{"location":"javascript/classes/LocalTable/#constructor","title":"constructor","text":"<p>\u2022 new LocalTable\\&lt;<code>T</code>&gt;(<code>tbl</code>, <code>name</code>, <code>options</code>)</p>"},{"location":"javascript/classes/LocalTable/#type-parameters_1","title":"Type parameters","text":"Name Type <code>T</code> <code>number</code>[]"},{"location":"javascript/classes/LocalTable/#parameters","title":"Parameters","text":"Name Type <code>tbl</code> <code>any</code> <code>name</code> <code>string</code> <code>options</code> <code>ConnectionOptions</code>"},{"location":"javascript/classes/LocalTable/#defined-in","title":"Defined in","text":"<p>index.ts:892</p> <p>\u2022 new LocalTable\\&lt;<code>T</code>&gt;(<code>tbl</code>, <code>name</code>, <code>options</code>, <code>embeddings</code>)</p>"},{"location":"javascript/classes/LocalTable/#type-parameters_2","title":"Type parameters","text":"Name Type <code>T</code> <code>number</code>[]"},{"location":"javascript/classes/LocalTable/#parameters_1","title":"Parameters","text":"Name Type Description <code>tbl</code> <code>any</code> <code>name</code> <code>string</code> <code>options</code> <code>ConnectionOptions</code> <code>embeddings</code> <code>EmbeddingFunction</code>\\&lt;<code>T</code>&gt; An embedding function to use when interacting with this table"},{"location":"javascript/classes/LocalTable/#defined-in_1","title":"Defined in","text":"<p>index.ts:899</p>"},{"location":"javascript/classes/LocalTable/#properties_1","title":"Properties","text":""},{"location":"javascript/classes/LocalTable/#_embeddings","title":"_embeddings","text":"<p>\u2022 <code>Private</code> <code>Optional</code> <code>Readonly</code> _embeddings: <code>EmbeddingFunction</code>\\&lt;<code>T</code>&gt;</p>"},{"location":"javascript/classes/LocalTable/#defined-in_2","title":"Defined in","text":"<p>index.ts:889</p>"},{"location":"javascript/classes/LocalTable/#_iselectron","title":"_isElectron","text":"<p>\u2022 <code>Private</code> <code>Readonly</code> _isElectron: <code>boolean</code></p>"},{"location":"javascript/classes/LocalTable/#defined-in_3","title":"Defined in","text":"<p>index.ts:888</p>"},{"location":"javascript/classes/LocalTable/#_name","title":"_name","text":"<p>\u2022 <code>Private</code> <code>Readonly</code> _name: <code>string</code></p>"},{"location":"javascript/classes/LocalTable/#defined-in_4","title":"Defined in","text":"<p>index.ts:887</p>"},{"location":"javascript/classes/LocalTable/#_options","title":"_options","text":"<p>\u2022 <code>Private</code> <code>Readonly</code> _options: () =&gt; <code>ConnectionOptions</code></p>"},{"location":"javascript/classes/LocalTable/#type-declaration","title":"Type declaration","text":"<p>\u25b8 (): <code>ConnectionOptions</code></p>"},{"location":"javascript/classes/LocalTable/#returns","title":"Returns","text":"<p><code>ConnectionOptions</code></p>"},{"location":"javascript/classes/LocalTable/#defined-in_5","title":"Defined in","text":"<p>index.ts:890</p>"},{"location":"javascript/classes/LocalTable/#_tbl","title":"_tbl","text":"<p>\u2022 <code>Private</code> _tbl: <code>any</code></p>"},{"location":"javascript/classes/LocalTable/#defined-in_6","title":"Defined in","text":"<p>index.ts:886</p>"},{"location":"javascript/classes/LocalTable/#where","title":"where","text":"<p>\u2022 where: (<code>value</code>: <code>string</code>) =&gt; <code>Query</code>\\&lt;<code>T</code>&gt;</p>"},{"location":"javascript/classes/LocalTable/#type-declaration_1","title":"Type declaration","text":"<p>\u25b8 (<code>value</code>): <code>Query</code>\\&lt;<code>T</code>&gt;</p> <p>Creates a filter query to find all rows matching the specified criteria</p>"},{"location":"javascript/classes/LocalTable/#parameters_2","title":"Parameters","text":"Name Type Description <code>value</code> <code>string</code> The filter criteria (like SQL where clause syntax)"},{"location":"javascript/classes/LocalTable/#returns_1","title":"Returns","text":"<p><code>Query</code>\\&lt;<code>T</code>&gt;</p>"},{"location":"javascript/classes/LocalTable/#defined-in_7","title":"Defined in","text":"<p>index.ts:938</p>"},{"location":"javascript/classes/LocalTable/#accessors_1","title":"Accessors","text":""},{"location":"javascript/classes/LocalTable/#name","title":"name","text":"<p>\u2022 <code>get</code> name(): <code>string</code></p>"},{"location":"javascript/classes/LocalTable/#returns_2","title":"Returns","text":"<p><code>string</code></p>"},{"location":"javascript/classes/LocalTable/#implementation-of","title":"Implementation of","text":"<p>Table.name</p>"},{"location":"javascript/classes/LocalTable/#defined-in_8","title":"Defined in","text":"<p>index.ts:918</p>"},{"location":"javascript/classes/LocalTable/#schema","title":"schema","text":"<p>\u2022 <code>get</code> schema(): <code>Promise</code>\\&lt;<code>Schema</code>\\&lt;<code>any</code>&gt;&gt;</p>"},{"location":"javascript/classes/LocalTable/#returns_3","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>Schema</code>\\&lt;<code>any</code>&gt;&gt;</p>"},{"location":"javascript/classes/LocalTable/#implementation-of_1","title":"Implementation of","text":"<p>Table.schema</p>"},{"location":"javascript/classes/LocalTable/#defined-in_9","title":"Defined in","text":"<p>index.ts:1171</p>"},{"location":"javascript/classes/LocalTable/#methods_1","title":"Methods","text":""},{"location":"javascript/classes/LocalTable/#add","title":"add","text":"<p>\u25b8 add(<code>data</code>): <code>Promise</code>\\&lt;<code>number</code>&gt;</p> <p>Insert records into this Table.</p>"},{"location":"javascript/classes/LocalTable/#parameters_3","title":"Parameters","text":"Name Type Description <code>data</code> <code>Table</code>\\&lt;<code>any</code>&gt; | <code>Record</code>\\&lt;<code>string</code>, <code>unknown</code>&gt;[] Records to be inserted into the Table"},{"location":"javascript/classes/LocalTable/#returns_4","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>number</code>&gt;</p> <p>The number of rows added to the table</p>"},{"location":"javascript/classes/LocalTable/#implementation-of_2","title":"Implementation of","text":"<p>Table.add</p>"},{"location":"javascript/classes/LocalTable/#defined-in_10","title":"Defined in","text":"<p>index.ts:946</p>"},{"location":"javascript/classes/LocalTable/#addcolumns","title":"addColumns","text":"<p>\u25b8 addColumns(<code>newColumnTransforms</code>): <code>Promise</code>\\&lt;<code>void</code>&gt;</p> <p>Add new columns with defined values.</p>"},{"location":"javascript/classes/LocalTable/#parameters_4","title":"Parameters","text":"Name Type Description <code>newColumnTransforms</code> { <code>name</code>: <code>string</code> ; <code>valueSql</code>: <code>string</code>  }[] pairs of column names and the SQL expression to use to calculate the value of the new column. These expressions will be evaluated for each row in the table, and can reference existing columns in the table."},{"location":"javascript/classes/LocalTable/#returns_5","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>void</code>&gt;</p>"},{"location":"javascript/classes/LocalTable/#implementation-of_3","title":"Implementation of","text":"<p>Table.addColumns</p>"},{"location":"javascript/classes/LocalTable/#defined-in_11","title":"Defined in","text":"<p>index.ts:1195</p>"},{"location":"javascript/classes/LocalTable/#altercolumns","title":"alterColumns","text":"<p>\u25b8 alterColumns(<code>columnAlterations</code>): <code>Promise</code>\\&lt;<code>void</code>&gt;</p> <p>Alter the name or nullability of columns.</p>"},{"location":"javascript/classes/LocalTable/#parameters_5","title":"Parameters","text":"Name Type Description <code>columnAlterations</code> <code>ColumnAlteration</code>[] One or more alterations to apply to columns."},{"location":"javascript/classes/LocalTable/#returns_6","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>void</code>&gt;</p>"},{"location":"javascript/classes/LocalTable/#implementation-of_4","title":"Implementation of","text":"<p>Table.alterColumns</p>"},{"location":"javascript/classes/LocalTable/#defined-in_12","title":"Defined in","text":"<p>index.ts:1201</p>"},{"location":"javascript/classes/LocalTable/#checkelectron","title":"checkElectron","text":"<p>\u25b8 <code>Private</code> checkElectron(): <code>boolean</code></p>"},{"location":"javascript/classes/LocalTable/#returns_7","title":"Returns","text":"<p><code>boolean</code></p>"},{"location":"javascript/classes/LocalTable/#defined-in_13","title":"Defined in","text":"<p>index.ts:1183</p>"},{"location":"javascript/classes/LocalTable/#cleanupoldversions","title":"cleanupOldVersions","text":"<p>\u25b8 cleanupOldVersions(<code>olderThan?</code>, <code>deleteUnverified?</code>): <code>Promise</code>\\&lt;<code>CleanupStats</code>&gt;</p> <p>Clean up old versions of the table, freeing disk space.</p>"},{"location":"javascript/classes/LocalTable/#parameters_6","title":"Parameters","text":"Name Type Description <code>olderThan?</code> <code>number</code> The minimum age in minutes of the versions to delete. If not provided, defaults to two weeks. <code>deleteUnverified?</code> <code>boolean</code> Because they may be part of an in-progress transaction, uncommitted files newer than 7 days old are not deleted by default. This means that failed transactions can leave around data that takes up disk space for up to 7 days. You can override this safety mechanism by setting this option to <code>true</code>, only if you promise there are no in progress writes while you run this operation. Failure to uphold this promise can lead to corrupted tables."},{"location":"javascript/classes/LocalTable/#returns_8","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>CleanupStats</code>&gt;</p>"},{"location":"javascript/classes/LocalTable/#defined-in_14","title":"Defined in","text":"<p>index.ts:1130</p>"},{"location":"javascript/classes/LocalTable/#compactfiles","title":"compactFiles","text":"<p>\u25b8 compactFiles(<code>options?</code>): <code>Promise</code>\\&lt;<code>CompactionMetrics</code>&gt;</p> <p>Run the compaction process on the table.</p> <p>This can be run after making several small appends to optimize the table for faster reads.</p>"},{"location":"javascript/classes/LocalTable/#parameters_7","title":"Parameters","text":"Name Type Description <code>options?</code> <code>CompactionOptions</code> Advanced options configuring compaction. In most cases, you can omit this arguments, as the default options are sensible for most tables."},{"location":"javascript/classes/LocalTable/#returns_9","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>CompactionMetrics</code>&gt;</p> <p>Metrics about the compaction operation.</p>"},{"location":"javascript/classes/LocalTable/#defined-in_15","title":"Defined in","text":"<p>index.ts:1153</p>"},{"location":"javascript/classes/LocalTable/#countrows","title":"countRows","text":"<p>\u25b8 countRows(<code>filter?</code>): <code>Promise</code>\\&lt;<code>number</code>&gt;</p> <p>Returns the number of rows in this table.</p>"},{"location":"javascript/classes/LocalTable/#parameters_8","title":"Parameters","text":"Name Type <code>filter?</code> <code>string</code>"},{"location":"javascript/classes/LocalTable/#returns_10","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>number</code>&gt;</p>"},{"location":"javascript/classes/LocalTable/#implementation-of_5","title":"Implementation of","text":"<p>Table.countRows</p>"},{"location":"javascript/classes/LocalTable/#defined-in_16","title":"Defined in","text":"<p>index.ts:1021</p>"},{"location":"javascript/classes/LocalTable/#createindex","title":"createIndex","text":"<p>\u25b8 createIndex(<code>indexParams</code>): <code>Promise</code>\\&lt;<code>any</code>&gt;</p> <p>Create an ANN index on this Table vector index.</p>"},{"location":"javascript/classes/LocalTable/#parameters_9","title":"Parameters","text":"Name Type Description <code>indexParams</code> <code>IvfPQIndexConfig</code> The parameters of this Index,"},{"location":"javascript/classes/LocalTable/#returns_11","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>any</code>&gt;</p> <p><code>See</code></p> <p>VectorIndexParams.</p>"},{"location":"javascript/classes/LocalTable/#implementation-of_6","title":"Implementation of","text":"<p>Table.createIndex</p>"},{"location":"javascript/classes/LocalTable/#defined-in_17","title":"Defined in","text":"<p>index.ts:1003</p>"},{"location":"javascript/classes/LocalTable/#createscalarindex","title":"createScalarIndex","text":"<p>\u25b8 createScalarIndex(<code>column</code>, <code>replace?</code>): <code>Promise</code>\\&lt;<code>void</code>&gt;</p> <p>Create a scalar index on this Table for the given column</p>"},{"location":"javascript/classes/LocalTable/#parameters_10","title":"Parameters","text":"Name Type Description <code>column</code> <code>string</code> The column to index <code>replace?</code> <code>boolean</code> If false, fail if an index already exists on the column it is always set to true for remote connections Scalar indices, like vector indices, can be used to speed up scans. A scalar index can speed up scans that contain filter expressions on the indexed column. For example, the following scan will be faster if the column <code>my_col</code> has a scalar index: <code>ts const con = await lancedb.connect('./.lancedb'); const table = await con.openTable('images'); const results = await table.where('my_col = 7').execute();</code> Scalar indices can also speed up scans containing a vector search and a prefilter: <code>ts const con = await lancedb.connect('././lancedb'); const table = await con.openTable('images'); const results = await table.search([1.0, 2.0]).where('my_col != 7').prefilter(true);</code> Scalar indices can only speed up scans for basic filters using equality, comparison, range (e.g. <code>my_col BETWEEN 0 AND 100</code>), and set membership (e.g. <code>my_col IN (0, 1, 2)</code>) Scalar indices can be used if the filter contains multiple indexed columns and the filter criteria are AND'd or OR'd together (e.g. <code>my_col &lt; 0 AND other_col&gt; 100</code>) Scalar indices may be used if the filter contains non-indexed columns but, depending on the structure of the filter, they may not be usable. For example, if the column <code>not_indexed</code> does not have a scalar index then the filter <code>my_col = 0 OR not_indexed = 1</code> will not be able to use any scalar index on <code>my_col</code>."},{"location":"javascript/classes/LocalTable/#returns_12","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>void</code>&gt;</p> <p><code>Examples</code></p> <pre><code>const con = await lancedb.connect('././lancedb')\nconst table = await con.openTable('images')\nawait table.createScalarIndex('my_col')\n</code></pre>"},{"location":"javascript/classes/LocalTable/#implementation-of_7","title":"Implementation of","text":"<p>Table.createScalarIndex</p>"},{"location":"javascript/classes/LocalTable/#defined-in_18","title":"Defined in","text":"<p>index.ts:1011</p>"},{"location":"javascript/classes/LocalTable/#delete","title":"delete","text":"<p>\u25b8 delete(<code>filter</code>): <code>Promise</code>\\&lt;<code>void</code>&gt;</p> <p>Delete rows from this table.</p>"},{"location":"javascript/classes/LocalTable/#parameters_11","title":"Parameters","text":"Name Type Description <code>filter</code> <code>string</code> A filter in the same format used by a sql WHERE clause."},{"location":"javascript/classes/LocalTable/#returns_13","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>void</code>&gt;</p>"},{"location":"javascript/classes/LocalTable/#implementation-of_8","title":"Implementation of","text":"<p>Table.delete</p>"},{"location":"javascript/classes/LocalTable/#defined-in_19","title":"Defined in","text":"<p>index.ts:1030</p>"},{"location":"javascript/classes/LocalTable/#dropcolumns","title":"dropColumns","text":"<p>\u25b8 dropColumns(<code>columnNames</code>): <code>Promise</code>\\&lt;<code>void</code>&gt;</p> <p>Drop one or more columns from the dataset</p> <p>This is a metadata-only operation and does not remove the data from the underlying storage. In order to remove the data, you must subsequently call <code>compact_files</code> to rewrite the data without the removed columns and then call <code>cleanup_files</code> to remove the old files.</p>"},{"location":"javascript/classes/LocalTable/#parameters_12","title":"Parameters","text":"Name Type Description <code>columnNames</code> <code>string</code>[] The names of the columns to drop. These can be nested column references (e.g. \"a.b.c\") or top-level column names (e.g. \"a\")."},{"location":"javascript/classes/LocalTable/#returns_14","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>void</code>&gt;</p>"},{"location":"javascript/classes/LocalTable/#implementation-of_9","title":"Implementation of","text":"<p>Table.dropColumns</p>"},{"location":"javascript/classes/LocalTable/#defined-in_20","title":"Defined in","text":"<p>index.ts:1205</p>"},{"location":"javascript/classes/LocalTable/#filter","title":"filter","text":"<p>\u25b8 filter(<code>value</code>): <code>Query</code>\\&lt;<code>T</code>&gt;</p> <p>Creates a filter query to find all rows matching the specified criteria</p>"},{"location":"javascript/classes/LocalTable/#parameters_13","title":"Parameters","text":"Name Type Description <code>value</code> <code>string</code> The filter criteria (like SQL where clause syntax)"},{"location":"javascript/classes/LocalTable/#returns_15","title":"Returns","text":"<p><code>Query</code>\\&lt;<code>T</code>&gt;</p>"},{"location":"javascript/classes/LocalTable/#implementation-of_10","title":"Implementation of","text":"<p>Table.filter</p>"},{"location":"javascript/classes/LocalTable/#defined-in_21","title":"Defined in","text":"<p>index.ts:934</p>"},{"location":"javascript/classes/LocalTable/#getschema","title":"getSchema","text":"<p>\u25b8 <code>Private</code> getSchema(): <code>Promise</code>\\&lt;<code>Schema</code>\\&lt;<code>any</code>&gt;&gt;</p>"},{"location":"javascript/classes/LocalTable/#returns_16","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>Schema</code>\\&lt;<code>any</code>&gt;&gt;</p>"},{"location":"javascript/classes/LocalTable/#defined-in_22","title":"Defined in","text":"<p>index.ts:1176</p>"},{"location":"javascript/classes/LocalTable/#indexstats","title":"indexStats","text":"<p>\u25b8 indexStats(<code>indexName</code>): <code>Promise</code>\\&lt;<code>IndexStats</code>&gt;</p> <p>Get statistics about an index.</p>"},{"location":"javascript/classes/LocalTable/#parameters_14","title":"Parameters","text":"Name Type <code>indexName</code> <code>string</code>"},{"location":"javascript/classes/LocalTable/#returns_17","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>IndexStats</code>&gt;</p>"},{"location":"javascript/classes/LocalTable/#implementation-of_11","title":"Implementation of","text":"<p>Table.indexStats</p>"},{"location":"javascript/classes/LocalTable/#defined-in_23","title":"Defined in","text":"<p>index.ts:1167</p>"},{"location":"javascript/classes/LocalTable/#listindices","title":"listIndices","text":"<p>\u25b8 listIndices(): <code>Promise</code>\\&lt;<code>VectorIndex</code>[]&gt;</p> <p>List the indicies on this table.</p>"},{"location":"javascript/classes/LocalTable/#returns_18","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>VectorIndex</code>[]&gt;</p>"},{"location":"javascript/classes/LocalTable/#implementation-of_12","title":"Implementation of","text":"<p>Table.listIndices</p>"},{"location":"javascript/classes/LocalTable/#defined-in_24","title":"Defined in","text":"<p>index.ts:1163</p>"},{"location":"javascript/classes/LocalTable/#mergeinsert","title":"mergeInsert","text":"<p>\u25b8 mergeInsert(<code>on</code>, <code>data</code>, <code>args</code>): <code>Promise</code>\\&lt;<code>void</code>&gt;</p> <p>Runs a \"merge insert\" operation on the table</p> <p>This operation can add rows, update rows, and remove rows all in a single transaction. It is a very generic tool that can be used to create behaviors like \"insert if not exists\", \"update or insert (i.e. upsert)\", or even replace a portion of existing data with new data (e.g. replace all data where month=\"january\")</p> <p>The merge insert operation works by combining new data from a source table with existing data in a target table by using a join.  There are three categories of records.</p> <p>\"Matched\" records are records that exist in both the source table and the target table. \"Not matched\" records exist only in the source table (e.g. these are new data) \"Not matched by source\" records exist only in the target table (this is old data)</p> <p>The MergeInsertArgs can be used to customize what should happen for each category of data.</p> <p>Please note that the data may appear to be reordered as part of this operation.  This is because updated rows will be deleted from the dataset and then reinserted at the end with the new values.</p>"},{"location":"javascript/classes/LocalTable/#parameters_15","title":"Parameters","text":"Name Type Description <code>on</code> <code>string</code> a column to join on. This is how records from the source table and target table are matched. <code>data</code> <code>Table</code>\\&lt;<code>any</code>&gt; | <code>Record</code>\\&lt;<code>string</code>, <code>unknown</code>&gt;[] the new data to insert <code>args</code> <code>MergeInsertArgs</code> parameters controlling how the operation should behave"},{"location":"javascript/classes/LocalTable/#returns_19","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>void</code>&gt;</p>"},{"location":"javascript/classes/LocalTable/#implementation-of_13","title":"Implementation of","text":"<p>Table.mergeInsert</p>"},{"location":"javascript/classes/LocalTable/#defined-in_25","title":"Defined in","text":"<p>index.ts:1065</p>"},{"location":"javascript/classes/LocalTable/#overwrite","title":"overwrite","text":"<p>\u25b8 overwrite(<code>data</code>): <code>Promise</code>\\&lt;<code>number</code>&gt;</p> <p>Insert records into this Table, replacing its contents.</p>"},{"location":"javascript/classes/LocalTable/#parameters_16","title":"Parameters","text":"Name Type Description <code>data</code> <code>Table</code>\\&lt;<code>any</code>&gt; | <code>Record</code>\\&lt;<code>string</code>, <code>unknown</code>&gt;[] Records to be inserted into the Table"},{"location":"javascript/classes/LocalTable/#returns_20","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>number</code>&gt;</p> <p>The number of rows added to the table</p>"},{"location":"javascript/classes/LocalTable/#implementation-of_14","title":"Implementation of","text":"<p>Table.overwrite</p>"},{"location":"javascript/classes/LocalTable/#defined-in_26","title":"Defined in","text":"<p>index.ts:977</p>"},{"location":"javascript/classes/LocalTable/#search","title":"search","text":"<p>\u25b8 search(<code>query</code>): <code>Query</code>\\&lt;<code>T</code>&gt;</p> <p>Creates a search query to find the nearest neighbors of the given search term</p>"},{"location":"javascript/classes/LocalTable/#parameters_17","title":"Parameters","text":"Name Type Description <code>query</code> <code>T</code> The query search term"},{"location":"javascript/classes/LocalTable/#returns_21","title":"Returns","text":"<p><code>Query</code>\\&lt;<code>T</code>&gt;</p>"},{"location":"javascript/classes/LocalTable/#implementation-of_15","title":"Implementation of","text":"<p>Table.search</p>"},{"location":"javascript/classes/LocalTable/#defined-in_27","title":"Defined in","text":"<p>index.ts:926</p>"},{"location":"javascript/classes/LocalTable/#update","title":"update","text":"<p>\u25b8 update(<code>args</code>): <code>Promise</code>\\&lt;<code>void</code>&gt;</p> <p>Update rows in this table.</p>"},{"location":"javascript/classes/LocalTable/#parameters_18","title":"Parameters","text":"Name Type Description <code>args</code> <code>UpdateArgs</code> | <code>UpdateSqlArgs</code> see UpdateArgs and UpdateSqlArgs for more details"},{"location":"javascript/classes/LocalTable/#returns_22","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>void</code>&gt;</p>"},{"location":"javascript/classes/LocalTable/#implementation-of_16","title":"Implementation of","text":"<p>Table.update</p>"},{"location":"javascript/classes/LocalTable/#defined-in_28","title":"Defined in","text":"<p>index.ts:1043</p>"},{"location":"javascript/classes/LocalTable/#withmiddleware","title":"withMiddleware","text":"<p>\u25b8 withMiddleware(<code>middleware</code>): <code>Table</code>\\&lt;<code>T</code>&gt;</p> <p>Instrument the behavior of this Table with middleware.</p> <p>The middleware will be called in the order they are added.</p> <p>Currently this functionality is only supported for remote tables.</p>"},{"location":"javascript/classes/LocalTable/#parameters_19","title":"Parameters","text":"Name Type <code>middleware</code> <code>HttpMiddleware</code>"},{"location":"javascript/classes/LocalTable/#returns_23","title":"Returns","text":"<p><code>Table</code>\\&lt;<code>T</code>&gt;</p> <ul> <li>this Table instrumented by the passed middleware</li> </ul>"},{"location":"javascript/classes/LocalTable/#implementation-of_17","title":"Implementation of","text":"<p>Table.withMiddleware</p>"},{"location":"javascript/classes/LocalTable/#defined-in_29","title":"Defined in","text":"<p>index.ts:1209</p>"},{"location":"javascript/classes/MakeArrowTableOptions/","title":"MakeArrowTableOptions","text":"<p>vectordb / Exports / MakeArrowTableOptions</p>"},{"location":"javascript/classes/MakeArrowTableOptions/#class-makearrowtableoptions","title":"Class: MakeArrowTableOptions","text":"<p>Options to control the makeArrowTable call.</p>"},{"location":"javascript/classes/MakeArrowTableOptions/#table-of-contents","title":"Table of contents","text":""},{"location":"javascript/classes/MakeArrowTableOptions/#constructors","title":"Constructors","text":"<ul> <li>constructor</li> </ul>"},{"location":"javascript/classes/MakeArrowTableOptions/#properties","title":"Properties","text":"<ul> <li>dictionaryEncodeStrings</li> <li>embeddings</li> <li>schema</li> <li>vectorColumns</li> </ul>"},{"location":"javascript/classes/MakeArrowTableOptions/#constructors_1","title":"Constructors","text":""},{"location":"javascript/classes/MakeArrowTableOptions/#constructor","title":"constructor","text":"<p>\u2022 new MakeArrowTableOptions(<code>values?</code>)</p>"},{"location":"javascript/classes/MakeArrowTableOptions/#parameters","title":"Parameters","text":"Name Type <code>values?</code> <code>Partial</code>\\&lt;<code>MakeArrowTableOptions</code>&gt;"},{"location":"javascript/classes/MakeArrowTableOptions/#defined-in","title":"Defined in","text":"<p>arrow.ts:98</p>"},{"location":"javascript/classes/MakeArrowTableOptions/#properties_1","title":"Properties","text":""},{"location":"javascript/classes/MakeArrowTableOptions/#dictionaryencodestrings","title":"dictionaryEncodeStrings","text":"<p>\u2022 dictionaryEncodeStrings: <code>boolean</code> = <code>false</code></p> <p>If true then string columns will be encoded with dictionary encoding</p> <p>Set this to true if your string columns tend to repeat the same values often.  For more precise control use the <code>schema</code> property to specify the data type for individual columns.</p> <p>If <code>schema</code> is provided then this property is ignored.</p>"},{"location":"javascript/classes/MakeArrowTableOptions/#defined-in_1","title":"Defined in","text":"<p>arrow.ts:96</p>"},{"location":"javascript/classes/MakeArrowTableOptions/#embeddings","title":"embeddings","text":"<p>\u2022 <code>Optional</code> embeddings: <code>EmbeddingFunction</code>\\&lt;<code>any</code>&gt;</p>"},{"location":"javascript/classes/MakeArrowTableOptions/#defined-in_2","title":"Defined in","text":"<p>arrow.ts:85</p>"},{"location":"javascript/classes/MakeArrowTableOptions/#schema","title":"schema","text":"<p>\u2022 <code>Optional</code> schema: <code>Schema</code>\\&lt;<code>any</code>&gt;</p>"},{"location":"javascript/classes/MakeArrowTableOptions/#defined-in_3","title":"Defined in","text":"<p>arrow.ts:63</p>"},{"location":"javascript/classes/MakeArrowTableOptions/#vectorcolumns","title":"vectorColumns","text":"<p>\u2022 vectorColumns: <code>Record</code>\\&lt;<code>string</code>, <code>VectorColumnOptions</code>&gt;</p>"},{"location":"javascript/classes/MakeArrowTableOptions/#defined-in_4","title":"Defined in","text":"<p>arrow.ts:81</p>"},{"location":"javascript/classes/OpenAIEmbeddingFunction/","title":"OpenAIEmbeddingFunction","text":"<p>vectordb / Exports / OpenAIEmbeddingFunction</p>"},{"location":"javascript/classes/OpenAIEmbeddingFunction/#class-openaiembeddingfunction","title":"Class: OpenAIEmbeddingFunction","text":"<p>An embedding function that automatically creates vector representation for a given column.</p>"},{"location":"javascript/classes/OpenAIEmbeddingFunction/#implements","title":"Implements","text":"<ul> <li><code>EmbeddingFunction</code>\\&lt;<code>string</code>&gt;</li> </ul>"},{"location":"javascript/classes/OpenAIEmbeddingFunction/#table-of-contents","title":"Table of contents","text":""},{"location":"javascript/classes/OpenAIEmbeddingFunction/#constructors","title":"Constructors","text":"<ul> <li>constructor</li> </ul>"},{"location":"javascript/classes/OpenAIEmbeddingFunction/#properties","title":"Properties","text":"<ul> <li>_modelName</li> <li>_openai</li> <li>sourceColumn</li> </ul>"},{"location":"javascript/classes/OpenAIEmbeddingFunction/#methods","title":"Methods","text":"<ul> <li>embed</li> </ul>"},{"location":"javascript/classes/OpenAIEmbeddingFunction/#constructors_1","title":"Constructors","text":""},{"location":"javascript/classes/OpenAIEmbeddingFunction/#constructor","title":"constructor","text":"<p>\u2022 new OpenAIEmbeddingFunction(<code>sourceColumn</code>, <code>openAIKey</code>, <code>modelName?</code>)</p>"},{"location":"javascript/classes/OpenAIEmbeddingFunction/#parameters","title":"Parameters","text":"Name Type Default value <code>sourceColumn</code> <code>string</code> <code>undefined</code> <code>openAIKey</code> <code>string</code> <code>undefined</code> <code>modelName</code> <code>string</code> <code>'text-embedding-ada-002'</code>"},{"location":"javascript/classes/OpenAIEmbeddingFunction/#defined-in","title":"Defined in","text":"<p>embedding/openai.ts:22</p>"},{"location":"javascript/classes/OpenAIEmbeddingFunction/#properties_1","title":"Properties","text":""},{"location":"javascript/classes/OpenAIEmbeddingFunction/#_modelname","title":"_modelName","text":"<p>\u2022 <code>Private</code> <code>Readonly</code> _modelName: <code>string</code></p>"},{"location":"javascript/classes/OpenAIEmbeddingFunction/#defined-in_1","title":"Defined in","text":"<p>embedding/openai.ts:20</p>"},{"location":"javascript/classes/OpenAIEmbeddingFunction/#_openai","title":"_openai","text":"<p>\u2022 <code>Private</code> <code>Readonly</code> _openai: <code>OpenAI</code></p>"},{"location":"javascript/classes/OpenAIEmbeddingFunction/#defined-in_2","title":"Defined in","text":"<p>embedding/openai.ts:19</p>"},{"location":"javascript/classes/OpenAIEmbeddingFunction/#sourcecolumn","title":"sourceColumn","text":"<p>\u2022 sourceColumn: <code>string</code></p> <p>The name of the column that will be used as input for the Embedding Function.</p>"},{"location":"javascript/classes/OpenAIEmbeddingFunction/#implementation-of","title":"Implementation of","text":"<p>EmbeddingFunction.sourceColumn</p>"},{"location":"javascript/classes/OpenAIEmbeddingFunction/#defined-in_3","title":"Defined in","text":"<p>embedding/openai.ts:56</p>"},{"location":"javascript/classes/OpenAIEmbeddingFunction/#methods_1","title":"Methods","text":""},{"location":"javascript/classes/OpenAIEmbeddingFunction/#embed","title":"embed","text":"<p>\u25b8 embed(<code>data</code>): <code>Promise</code>\\&lt;<code>number</code>[][]&gt;</p> <p>Creates a vector representation for the given values.</p>"},{"location":"javascript/classes/OpenAIEmbeddingFunction/#parameters_1","title":"Parameters","text":"Name Type <code>data</code> <code>string</code>[]"},{"location":"javascript/classes/OpenAIEmbeddingFunction/#returns","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>number</code>[][]&gt;</p>"},{"location":"javascript/classes/OpenAIEmbeddingFunction/#implementation-of_1","title":"Implementation of","text":"<p>EmbeddingFunction.embed</p>"},{"location":"javascript/classes/OpenAIEmbeddingFunction/#defined-in_4","title":"Defined in","text":"<p>embedding/openai.ts:43</p>"},{"location":"javascript/classes/Query/","title":"Query","text":"<p>vectordb / Exports / Query</p>"},{"location":"javascript/classes/Query/#class-queryt","title":"Class: Query\\&lt;T&gt;","text":"<p>A builder for nearest neighbor queries for LanceDB.</p>"},{"location":"javascript/classes/Query/#type-parameters","title":"Type parameters","text":"Name Type <code>T</code> <code>number</code>[]"},{"location":"javascript/classes/Query/#table-of-contents","title":"Table of contents","text":""},{"location":"javascript/classes/Query/#constructors","title":"Constructors","text":"<ul> <li>constructor</li> </ul>"},{"location":"javascript/classes/Query/#properties","title":"Properties","text":"<ul> <li>_embeddings</li> <li>_fastSearch</li> <li>_filter</li> <li>_limit</li> <li>_metricType</li> <li>_nprobes</li> <li>_prefilter</li> <li>_query</li> <li>_queryVector</li> <li>_refineFactor</li> <li>_select</li> <li>_tbl</li> <li>where</li> </ul>"},{"location":"javascript/classes/Query/#methods","title":"Methods","text":"<ul> <li>execute</li> <li>fastSearch</li> <li>filter</li> <li>isElectron</li> <li>limit</li> <li>metricType</li> <li>nprobes</li> <li>prefilter</li> <li>refineFactor</li> <li>select</li> </ul>"},{"location":"javascript/classes/Query/#constructors_1","title":"Constructors","text":""},{"location":"javascript/classes/Query/#constructor","title":"constructor","text":"<p>\u2022 new Query\\&lt;<code>T</code>&gt;(<code>query?</code>, <code>tbl?</code>, <code>embeddings?</code>)</p>"},{"location":"javascript/classes/Query/#type-parameters_1","title":"Type parameters","text":"Name Type <code>T</code> <code>number</code>[]"},{"location":"javascript/classes/Query/#parameters","title":"Parameters","text":"Name Type <code>query?</code> <code>T</code> <code>tbl?</code> <code>any</code> <code>embeddings?</code> <code>EmbeddingFunction</code>\\&lt;<code>T</code>&gt;"},{"location":"javascript/classes/Query/#defined-in","title":"Defined in","text":"<p>query.ts:39</p>"},{"location":"javascript/classes/Query/#properties_1","title":"Properties","text":""},{"location":"javascript/classes/Query/#_embeddings","title":"_embeddings","text":"<p>\u2022 <code>Protected</code> <code>Optional</code> <code>Readonly</code> _embeddings: <code>EmbeddingFunction</code>\\&lt;<code>T</code>&gt;</p>"},{"location":"javascript/classes/Query/#defined-in_1","title":"Defined in","text":"<p>query.ts:37</p>"},{"location":"javascript/classes/Query/#_fastsearch","title":"_fastSearch","text":"<p>\u2022 <code>Private</code> _fastSearch: <code>boolean</code></p>"},{"location":"javascript/classes/Query/#defined-in_2","title":"Defined in","text":"<p>query.ts:36</p>"},{"location":"javascript/classes/Query/#_filter","title":"_filter","text":"<p>\u2022 <code>Private</code> <code>Optional</code> _filter: <code>string</code></p>"},{"location":"javascript/classes/Query/#defined-in_3","title":"Defined in","text":"<p>query.ts:33</p>"},{"location":"javascript/classes/Query/#_limit","title":"_limit","text":"<p>\u2022 <code>Private</code> <code>Optional</code> _limit: <code>number</code></p>"},{"location":"javascript/classes/Query/#defined-in_4","title":"Defined in","text":"<p>query.ts:29</p>"},{"location":"javascript/classes/Query/#_metrictype","title":"_metricType","text":"<p>\u2022 <code>Private</code> <code>Optional</code> _metricType: <code>MetricType</code></p>"},{"location":"javascript/classes/Query/#defined-in_5","title":"Defined in","text":"<p>query.ts:34</p>"},{"location":"javascript/classes/Query/#_nprobes","title":"_nprobes","text":"<p>\u2022 <code>Private</code> _nprobes: <code>number</code></p>"},{"location":"javascript/classes/Query/#defined-in_6","title":"Defined in","text":"<p>query.ts:31</p>"},{"location":"javascript/classes/Query/#_prefilter","title":"_prefilter","text":"<p>\u2022 <code>Private</code> _prefilter: <code>boolean</code></p>"},{"location":"javascript/classes/Query/#defined-in_7","title":"Defined in","text":"<p>query.ts:35</p>"},{"location":"javascript/classes/Query/#_query","title":"_query","text":"<p>\u2022 <code>Private</code> <code>Optional</code> <code>Readonly</code> _query: <code>T</code></p>"},{"location":"javascript/classes/Query/#defined-in_8","title":"Defined in","text":"<p>query.ts:26</p>"},{"location":"javascript/classes/Query/#_queryvector","title":"_queryVector","text":"<p>\u2022 <code>Private</code> <code>Optional</code> _queryVector: <code>number</code>[]</p>"},{"location":"javascript/classes/Query/#defined-in_9","title":"Defined in","text":"<p>query.ts:28</p>"},{"location":"javascript/classes/Query/#_refinefactor","title":"_refineFactor","text":"<p>\u2022 <code>Private</code> <code>Optional</code> _refineFactor: <code>number</code></p>"},{"location":"javascript/classes/Query/#defined-in_10","title":"Defined in","text":"<p>query.ts:30</p>"},{"location":"javascript/classes/Query/#_select","title":"_select","text":"<p>\u2022 <code>Private</code> <code>Optional</code> _select: <code>string</code>[]</p>"},{"location":"javascript/classes/Query/#defined-in_11","title":"Defined in","text":"<p>query.ts:32</p>"},{"location":"javascript/classes/Query/#_tbl","title":"_tbl","text":"<p>\u2022 <code>Private</code> <code>Optional</code> <code>Readonly</code> _tbl: <code>any</code></p>"},{"location":"javascript/classes/Query/#defined-in_12","title":"Defined in","text":"<p>query.ts:27</p>"},{"location":"javascript/classes/Query/#where","title":"where","text":"<p>\u2022 where: (<code>value</code>: <code>string</code>) =&gt; <code>Query</code>\\&lt;<code>T</code>&gt;</p>"},{"location":"javascript/classes/Query/#type-declaration","title":"Type declaration","text":"<p>\u25b8 (<code>value</code>): <code>Query</code>\\&lt;<code>T</code>&gt;</p> <p>A filter statement to be applied to this query.</p>"},{"location":"javascript/classes/Query/#parameters_1","title":"Parameters","text":"Name Type Description <code>value</code> <code>string</code> A filter in the same format used by a sql WHERE clause."},{"location":"javascript/classes/Query/#returns","title":"Returns","text":"<p><code>Query</code>\\&lt;<code>T</code>&gt;</p>"},{"location":"javascript/classes/Query/#defined-in_13","title":"Defined in","text":"<p>query.ts:90</p>"},{"location":"javascript/classes/Query/#methods_1","title":"Methods","text":""},{"location":"javascript/classes/Query/#execute","title":"execute","text":"<p>\u25b8 execute\\&lt;<code>T</code>&gt;(): <code>Promise</code>\\&lt;<code>T</code>[]&gt;</p> <p>Execute the query and return the results as an Array of Objects</p>"},{"location":"javascript/classes/Query/#type-parameters_2","title":"Type parameters","text":"Name Type <code>T</code> <code>Record</code>\\&lt;<code>string</code>, <code>unknown</code>&gt;"},{"location":"javascript/classes/Query/#returns_1","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>T</code>[]&gt;</p>"},{"location":"javascript/classes/Query/#defined-in_14","title":"Defined in","text":"<p>query.ts:127</p>"},{"location":"javascript/classes/Query/#fastsearch","title":"fastSearch","text":"<p>\u25b8 fastSearch(<code>value</code>): <code>Query</code>\\&lt;<code>T</code>&gt;</p> <p>Skip searching un-indexed data. This can make search faster, but will miss any data that is not yet indexed.</p>"},{"location":"javascript/classes/Query/#parameters_2","title":"Parameters","text":"Name Type <code>value</code> <code>boolean</code>"},{"location":"javascript/classes/Query/#returns_2","title":"Returns","text":"<p><code>Query</code>\\&lt;<code>T</code>&gt;</p>"},{"location":"javascript/classes/Query/#defined-in_15","title":"Defined in","text":"<p>query.ts:119</p>"},{"location":"javascript/classes/Query/#filter","title":"filter","text":"<p>\u25b8 filter(<code>value</code>): <code>Query</code>\\&lt;<code>T</code>&gt;</p> <p>A filter statement to be applied to this query.</p>"},{"location":"javascript/classes/Query/#parameters_3","title":"Parameters","text":"Name Type Description <code>value</code> <code>string</code> A filter in the same format used by a sql WHERE clause."},{"location":"javascript/classes/Query/#returns_3","title":"Returns","text":"<p><code>Query</code>\\&lt;<code>T</code>&gt;</p>"},{"location":"javascript/classes/Query/#defined-in_16","title":"Defined in","text":"<p>query.ts:85</p>"},{"location":"javascript/classes/Query/#iselectron","title":"isElectron","text":"<p>\u25b8 <code>Private</code> isElectron(): <code>boolean</code></p>"},{"location":"javascript/classes/Query/#returns_4","title":"Returns","text":"<p><code>boolean</code></p>"},{"location":"javascript/classes/Query/#defined-in_17","title":"Defined in","text":"<p>query.ts:155</p>"},{"location":"javascript/classes/Query/#limit","title":"limit","text":"<p>\u25b8 limit(<code>value</code>): <code>Query</code>\\&lt;<code>T</code>&gt;</p> <p>Sets the number of results that will be returned default value is 10</p>"},{"location":"javascript/classes/Query/#parameters_4","title":"Parameters","text":"Name Type Description <code>value</code> <code>number</code> number of results"},{"location":"javascript/classes/Query/#returns_5","title":"Returns","text":"<p><code>Query</code>\\&lt;<code>T</code>&gt;</p>"},{"location":"javascript/classes/Query/#defined-in_18","title":"Defined in","text":"<p>query.ts:58</p>"},{"location":"javascript/classes/Query/#metrictype","title":"metricType","text":"<p>\u25b8 metricType(<code>value</code>): <code>Query</code>\\&lt;<code>T</code>&gt;</p> <p>The MetricType used for this Query.</p>"},{"location":"javascript/classes/Query/#parameters_5","title":"Parameters","text":"Name Type Description <code>value</code> <code>MetricType</code> The metric to the."},{"location":"javascript/classes/Query/#returns_6","title":"Returns","text":"<p><code>Query</code>\\&lt;<code>T</code>&gt;</p> <p><code>See</code></p> <p>MetricType for the different options</p>"},{"location":"javascript/classes/Query/#defined-in_19","title":"Defined in","text":"<p>query.ts:105</p>"},{"location":"javascript/classes/Query/#nprobes","title":"nprobes","text":"<p>\u25b8 nprobes(<code>value</code>): <code>Query</code>\\&lt;<code>T</code>&gt;</p> <p>The number of probes used. A higher number makes search more accurate but also slower.</p>"},{"location":"javascript/classes/Query/#parameters_6","title":"Parameters","text":"Name Type Description <code>value</code> <code>number</code> The number of probes used."},{"location":"javascript/classes/Query/#returns_7","title":"Returns","text":"<p><code>Query</code>\\&lt;<code>T</code>&gt;</p>"},{"location":"javascript/classes/Query/#defined-in_20","title":"Defined in","text":"<p>query.ts:76</p>"},{"location":"javascript/classes/Query/#prefilter","title":"prefilter","text":"<p>\u25b8 prefilter(<code>value</code>): <code>Query</code>\\&lt;<code>T</code>&gt;</p>"},{"location":"javascript/classes/Query/#parameters_7","title":"Parameters","text":"Name Type <code>value</code> <code>boolean</code>"},{"location":"javascript/classes/Query/#returns_8","title":"Returns","text":"<p><code>Query</code>\\&lt;<code>T</code>&gt;</p>"},{"location":"javascript/classes/Query/#defined-in_21","title":"Defined in","text":"<p>query.ts:110</p>"},{"location":"javascript/classes/Query/#refinefactor","title":"refineFactor","text":"<p>\u25b8 refineFactor(<code>value</code>): <code>Query</code>\\&lt;<code>T</code>&gt;</p> <p>Refine the results by reading extra elements and re-ranking them in memory.</p>"},{"location":"javascript/classes/Query/#parameters_8","title":"Parameters","text":"Name Type Description <code>value</code> <code>number</code> refine factor to use in this query."},{"location":"javascript/classes/Query/#returns_9","title":"Returns","text":"<p><code>Query</code>\\&lt;<code>T</code>&gt;</p>"},{"location":"javascript/classes/Query/#defined-in_22","title":"Defined in","text":"<p>query.ts:67</p>"},{"location":"javascript/classes/Query/#select","title":"select","text":"<p>\u25b8 select(<code>value</code>): <code>Query</code>\\&lt;<code>T</code>&gt;</p> <p>Return only the specified columns.</p>"},{"location":"javascript/classes/Query/#parameters_9","title":"Parameters","text":"Name Type Description <code>value</code> <code>string</code>[] Only select the specified columns. If not specified, all columns will be returned."},{"location":"javascript/classes/Query/#returns_10","title":"Returns","text":"<p><code>Query</code>\\&lt;<code>T</code>&gt;</p>"},{"location":"javascript/classes/Query/#defined-in_23","title":"Defined in","text":"<p>query.ts:96</p>"},{"location":"javascript/enums/IndexStatus/","title":"IndexStatus","text":"<p>vectordb / Exports / IndexStatus</p>"},{"location":"javascript/enums/IndexStatus/#enumeration-indexstatus","title":"Enumeration: IndexStatus","text":""},{"location":"javascript/enums/IndexStatus/#table-of-contents","title":"Table of contents","text":""},{"location":"javascript/enums/IndexStatus/#enumeration-members","title":"Enumeration Members","text":"<ul> <li>Done</li> <li>Failed</li> <li>Indexing</li> <li>Pending</li> </ul>"},{"location":"javascript/enums/IndexStatus/#enumeration-members_1","title":"Enumeration Members","text":""},{"location":"javascript/enums/IndexStatus/#done","title":"Done","text":"<p>\u2022 Done = <code>\"done\"</code></p>"},{"location":"javascript/enums/IndexStatus/#defined-in","title":"Defined in","text":"<p>index.ts:713</p>"},{"location":"javascript/enums/IndexStatus/#failed","title":"Failed","text":"<p>\u2022 Failed = <code>\"failed\"</code></p>"},{"location":"javascript/enums/IndexStatus/#defined-in_1","title":"Defined in","text":"<p>index.ts:714</p>"},{"location":"javascript/enums/IndexStatus/#indexing","title":"Indexing","text":"<p>\u2022 Indexing = <code>\"indexing\"</code></p>"},{"location":"javascript/enums/IndexStatus/#defined-in_2","title":"Defined in","text":"<p>index.ts:712</p>"},{"location":"javascript/enums/IndexStatus/#pending","title":"Pending","text":"<p>\u2022 Pending = <code>\"pending\"</code></p>"},{"location":"javascript/enums/IndexStatus/#defined-in_3","title":"Defined in","text":"<p>index.ts:711</p>"},{"location":"javascript/enums/MetricType/","title":"MetricType","text":"<p>vectordb / Exports / MetricType</p>"},{"location":"javascript/enums/MetricType/#enumeration-metrictype","title":"Enumeration: MetricType","text":"<p>Distance metrics type.</p>"},{"location":"javascript/enums/MetricType/#table-of-contents","title":"Table of contents","text":""},{"location":"javascript/enums/MetricType/#enumeration-members","title":"Enumeration Members","text":"<ul> <li>Cosine</li> <li>Dot</li> <li>l2</li> </ul>"},{"location":"javascript/enums/MetricType/#enumeration-members_1","title":"Enumeration Members","text":""},{"location":"javascript/enums/MetricType/#cosine","title":"Cosine","text":"<p>\u2022 Cosine = <code>\"cosine\"</code></p> <p>Cosine distance</p>"},{"location":"javascript/enums/MetricType/#defined-in","title":"Defined in","text":"<p>index.ts:1381</p>"},{"location":"javascript/enums/MetricType/#dot","title":"Dot","text":"<p>\u2022 Dot = <code>\"dot\"</code></p> <p>Dot product</p>"},{"location":"javascript/enums/MetricType/#defined-in_1","title":"Defined in","text":"<p>index.ts:1386</p>"},{"location":"javascript/enums/MetricType/#l2","title":"L2","text":"<p>\u2022 L2 = <code>\"l2\"</code></p> <p>Euclidean distance</p>"},{"location":"javascript/enums/MetricType/#defined-in_2","title":"Defined in","text":"<p>index.ts:1376</p>"},{"location":"javascript/enums/WriteMode/","title":"WriteMode","text":"<p>vectordb / Exports / WriteMode</p>"},{"location":"javascript/enums/WriteMode/#enumeration-writemode","title":"Enumeration: WriteMode","text":"<p>Write mode for writing a table.</p>"},{"location":"javascript/enums/WriteMode/#table-of-contents","title":"Table of contents","text":""},{"location":"javascript/enums/WriteMode/#enumeration-members","title":"Enumeration Members","text":"<ul> <li>Append</li> <li>Create</li> <li>Overwrite</li> </ul>"},{"location":"javascript/enums/WriteMode/#enumeration-members_1","title":"Enumeration Members","text":""},{"location":"javascript/enums/WriteMode/#append","title":"Append","text":"<p>\u2022 Append = <code>\"append\"</code></p> <p>Append new data to the table.</p>"},{"location":"javascript/enums/WriteMode/#defined-in","title":"Defined in","text":"<p>index.ts:1347</p>"},{"location":"javascript/enums/WriteMode/#create","title":"Create","text":"<p>\u2022 Create = <code>\"create\"</code></p> <p>Create a new Table.</p>"},{"location":"javascript/enums/WriteMode/#defined-in_1","title":"Defined in","text":"<p>index.ts:1343</p>"},{"location":"javascript/enums/WriteMode/#overwrite","title":"Overwrite","text":"<p>\u2022 Overwrite = <code>\"overwrite\"</code></p> <p>Overwrite the existing Table if presented.</p>"},{"location":"javascript/enums/WriteMode/#defined-in_2","title":"Defined in","text":"<p>index.ts:1345</p>"},{"location":"javascript/interfaces/AwsCredentials/","title":"AwsCredentials","text":"<p>vectordb / Exports / AwsCredentials</p>"},{"location":"javascript/interfaces/AwsCredentials/#interface-awscredentials","title":"Interface: AwsCredentials","text":""},{"location":"javascript/interfaces/AwsCredentials/#table-of-contents","title":"Table of contents","text":""},{"location":"javascript/interfaces/AwsCredentials/#properties","title":"Properties","text":"<ul> <li>accessKeyId</li> <li>secretKey</li> <li>sessionToken</li> </ul>"},{"location":"javascript/interfaces/AwsCredentials/#properties_1","title":"Properties","text":""},{"location":"javascript/interfaces/AwsCredentials/#accesskeyid","title":"accessKeyId","text":"<p>\u2022 accessKeyId: <code>string</code></p>"},{"location":"javascript/interfaces/AwsCredentials/#defined-in","title":"Defined in","text":"<p>index.ts:68</p>"},{"location":"javascript/interfaces/AwsCredentials/#secretkey","title":"secretKey","text":"<p>\u2022 secretKey: <code>string</code></p>"},{"location":"javascript/interfaces/AwsCredentials/#defined-in_1","title":"Defined in","text":"<p>index.ts:70</p>"},{"location":"javascript/interfaces/AwsCredentials/#sessiontoken","title":"sessionToken","text":"<p>\u2022 <code>Optional</code> sessionToken: <code>string</code></p>"},{"location":"javascript/interfaces/AwsCredentials/#defined-in_2","title":"Defined in","text":"<p>index.ts:72</p>"},{"location":"javascript/interfaces/CleanupStats/","title":"CleanupStats","text":"<p>vectordb / Exports / CleanupStats</p>"},{"location":"javascript/interfaces/CleanupStats/#interface-cleanupstats","title":"Interface: CleanupStats","text":""},{"location":"javascript/interfaces/CleanupStats/#table-of-contents","title":"Table of contents","text":""},{"location":"javascript/interfaces/CleanupStats/#properties","title":"Properties","text":"<ul> <li>bytesRemoved</li> <li>oldVersions</li> </ul>"},{"location":"javascript/interfaces/CleanupStats/#properties_1","title":"Properties","text":""},{"location":"javascript/interfaces/CleanupStats/#bytesremoved","title":"bytesRemoved","text":"<p>\u2022 bytesRemoved: <code>number</code></p> <p>The number of bytes removed from disk.</p>"},{"location":"javascript/interfaces/CleanupStats/#defined-in","title":"Defined in","text":"<p>index.ts:1218</p>"},{"location":"javascript/interfaces/CleanupStats/#oldversions","title":"oldVersions","text":"<p>\u2022 oldVersions: <code>number</code></p> <p>The number of old table versions removed.</p>"},{"location":"javascript/interfaces/CleanupStats/#defined-in_1","title":"Defined in","text":"<p>index.ts:1222</p>"},{"location":"javascript/interfaces/ColumnAlteration/","title":"ColumnAlteration","text":"<p>vectordb / Exports / ColumnAlteration</p>"},{"location":"javascript/interfaces/ColumnAlteration/#interface-columnalteration","title":"Interface: ColumnAlteration","text":"<p>A definition of a column alteration. The alteration changes the column at <code>path</code> to have the new name <code>name</code>, to be nullable if <code>nullable</code> is true, and to have the data type <code>data_type</code>. At least one of <code>rename</code> or <code>nullable</code> must be provided.</p>"},{"location":"javascript/interfaces/ColumnAlteration/#table-of-contents","title":"Table of contents","text":""},{"location":"javascript/interfaces/ColumnAlteration/#properties","title":"Properties","text":"<ul> <li>nullable</li> <li>path</li> <li>rename</li> </ul>"},{"location":"javascript/interfaces/ColumnAlteration/#properties_1","title":"Properties","text":""},{"location":"javascript/interfaces/ColumnAlteration/#nullable","title":"nullable","text":"<p>\u2022 <code>Optional</code> nullable: <code>boolean</code></p> <p>Set the new nullability. Note that a nullable column cannot be made non-nullable.</p>"},{"location":"javascript/interfaces/ColumnAlteration/#defined-in","title":"Defined in","text":"<p>index.ts:638</p>"},{"location":"javascript/interfaces/ColumnAlteration/#path","title":"path","text":"<p>\u2022 path: <code>string</code></p> <p>The path to the column to alter. This is a dot-separated path to the column. If it is a top-level column then it is just the name of the column. If it is a nested column then it is the path to the column, e.g. \"a.b.c\" for a column <code>c</code> nested inside a column <code>b</code> nested inside a column <code>a</code>.</p>"},{"location":"javascript/interfaces/ColumnAlteration/#defined-in_1","title":"Defined in","text":"<p>index.ts:633</p>"},{"location":"javascript/interfaces/ColumnAlteration/#rename","title":"rename","text":"<p>\u2022 <code>Optional</code> rename: <code>string</code></p>"},{"location":"javascript/interfaces/ColumnAlteration/#defined-in_2","title":"Defined in","text":"<p>index.ts:634</p>"},{"location":"javascript/interfaces/CompactionMetrics/","title":"CompactionMetrics","text":"<p>vectordb / Exports / CompactionMetrics</p>"},{"location":"javascript/interfaces/CompactionMetrics/#interface-compactionmetrics","title":"Interface: CompactionMetrics","text":""},{"location":"javascript/interfaces/CompactionMetrics/#table-of-contents","title":"Table of contents","text":""},{"location":"javascript/interfaces/CompactionMetrics/#properties","title":"Properties","text":"<ul> <li>filesAdded</li> <li>filesRemoved</li> <li>fragmentsAdded</li> <li>fragmentsRemoved</li> </ul>"},{"location":"javascript/interfaces/CompactionMetrics/#properties_1","title":"Properties","text":""},{"location":"javascript/interfaces/CompactionMetrics/#filesadded","title":"filesAdded","text":"<p>\u2022 filesAdded: <code>number</code></p> <p>The number of files added. This is typically equal to the number of fragments added.</p>"},{"location":"javascript/interfaces/CompactionMetrics/#defined-in","title":"Defined in","text":"<p>index.ts:1273</p>"},{"location":"javascript/interfaces/CompactionMetrics/#filesremoved","title":"filesRemoved","text":"<p>\u2022 filesRemoved: <code>number</code></p> <p>The number of files that were removed. Each fragment may have more than one file.</p>"},{"location":"javascript/interfaces/CompactionMetrics/#defined-in_1","title":"Defined in","text":"<p>index.ts:1268</p>"},{"location":"javascript/interfaces/CompactionMetrics/#fragmentsadded","title":"fragmentsAdded","text":"<p>\u2022 fragmentsAdded: <code>number</code></p> <p>The number of new fragments that were created.</p>"},{"location":"javascript/interfaces/CompactionMetrics/#defined-in_2","title":"Defined in","text":"<p>index.ts:1263</p>"},{"location":"javascript/interfaces/CompactionMetrics/#fragmentsremoved","title":"fragmentsRemoved","text":"<p>\u2022 fragmentsRemoved: <code>number</code></p> <p>The number of fragments that were removed.</p>"},{"location":"javascript/interfaces/CompactionMetrics/#defined-in_3","title":"Defined in","text":"<p>index.ts:1259</p>"},{"location":"javascript/interfaces/CompactionOptions/","title":"CompactionOptions","text":"<p>vectordb / Exports / CompactionOptions</p>"},{"location":"javascript/interfaces/CompactionOptions/#interface-compactionoptions","title":"Interface: CompactionOptions","text":""},{"location":"javascript/interfaces/CompactionOptions/#table-of-contents","title":"Table of contents","text":""},{"location":"javascript/interfaces/CompactionOptions/#properties","title":"Properties","text":"<ul> <li>materializeDeletions</li> <li>materializeDeletionsThreshold</li> <li>maxRowsPerGroup</li> <li>numThreads</li> <li>targetRowsPerFragment</li> </ul>"},{"location":"javascript/interfaces/CompactionOptions/#properties_1","title":"Properties","text":""},{"location":"javascript/interfaces/CompactionOptions/#materializedeletions","title":"materializeDeletions","text":"<p>\u2022 <code>Optional</code> materializeDeletions: <code>boolean</code></p> <p>If true, fragments that have rows that are deleted may be compacted to remove the deleted rows. This can improve the performance of queries. Default is true.</p>"},{"location":"javascript/interfaces/CompactionOptions/#defined-in","title":"Defined in","text":"<p>index.ts:1241</p>"},{"location":"javascript/interfaces/CompactionOptions/#materializedeletionsthreshold","title":"materializeDeletionsThreshold","text":"<p>\u2022 <code>Optional</code> materializeDeletionsThreshold: <code>number</code></p> <p>A number between 0 and 1, representing the proportion of rows that must be marked deleted before a fragment is a candidate for compaction to remove the deleted rows. Default is 10%.</p>"},{"location":"javascript/interfaces/CompactionOptions/#defined-in_1","title":"Defined in","text":"<p>index.ts:1247</p>"},{"location":"javascript/interfaces/CompactionOptions/#maxrowspergroup","title":"maxRowsPerGroup","text":"<p>\u2022 <code>Optional</code> maxRowsPerGroup: <code>number</code></p> <p>The maximum number of T per group. Defaults to 1024.</p>"},{"location":"javascript/interfaces/CompactionOptions/#defined-in_2","title":"Defined in","text":"<p>index.ts:1235</p>"},{"location":"javascript/interfaces/CompactionOptions/#numthreads","title":"numThreads","text":"<p>\u2022 <code>Optional</code> numThreads: <code>number</code></p> <p>The number of threads to use for compaction. If not provided, defaults to the number of cores on the machine.</p>"},{"location":"javascript/interfaces/CompactionOptions/#defined-in_3","title":"Defined in","text":"<p>index.ts:1252</p>"},{"location":"javascript/interfaces/CompactionOptions/#targetrowsperfragment","title":"targetRowsPerFragment","text":"<p>\u2022 <code>Optional</code> targetRowsPerFragment: <code>number</code></p> <p>The number of rows per fragment to target. Fragments that have fewer rows will be compacted into adjacent fragments to produce larger fragments. Defaults to 1024 * 1024.</p>"},{"location":"javascript/interfaces/CompactionOptions/#defined-in_4","title":"Defined in","text":"<p>index.ts:1231</p>"},{"location":"javascript/interfaces/Connection/","title":"Connection","text":"<p>vectordb / Exports / Connection</p>"},{"location":"javascript/interfaces/Connection/#interface-connection","title":"Interface: Connection","text":"<p>A LanceDB Connection that allows you to open tables and create new ones.</p> <p>Connection could be local against filesystem or remote against a server.</p>"},{"location":"javascript/interfaces/Connection/#implemented-by","title":"Implemented by","text":"<ul> <li><code>LocalConnection</code></li> </ul>"},{"location":"javascript/interfaces/Connection/#table-of-contents","title":"Table of contents","text":""},{"location":"javascript/interfaces/Connection/#properties","title":"Properties","text":"<ul> <li>uri</li> </ul>"},{"location":"javascript/interfaces/Connection/#methods","title":"Methods","text":"<ul> <li>createTable</li> <li>dropTable</li> <li>openTable</li> <li>tableNames</li> <li>withMiddleware</li> </ul>"},{"location":"javascript/interfaces/Connection/#properties_1","title":"Properties","text":""},{"location":"javascript/interfaces/Connection/#uri","title":"uri","text":"<p>\u2022 uri: <code>string</code></p>"},{"location":"javascript/interfaces/Connection/#defined-in","title":"Defined in","text":"<p>index.ts:261</p>"},{"location":"javascript/interfaces/Connection/#methods_1","title":"Methods","text":""},{"location":"javascript/interfaces/Connection/#createtable","title":"createTable","text":"<p>\u25b8 createTable\\&lt;<code>T</code>&gt;(<code>\u00abdestructured\u00bb</code>): <code>Promise</code>\\&lt;<code>Table</code>\\&lt;<code>T</code>&gt;&gt;</p> <p>Creates a new Table, optionally initializing it with new data.</p>"},{"location":"javascript/interfaces/Connection/#type-parameters","title":"Type parameters","text":"Name <code>T</code>"},{"location":"javascript/interfaces/Connection/#parameters","title":"Parameters","text":"Name Type <code>\u00abdestructured\u00bb</code> <code>CreateTableOptions</code>\\&lt;<code>T</code>&gt;"},{"location":"javascript/interfaces/Connection/#returns","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>Table</code>\\&lt;<code>T</code>&gt;&gt;</p>"},{"location":"javascript/interfaces/Connection/#defined-in_1","title":"Defined in","text":"<p>index.ts:285</p> <p>\u25b8 createTable(<code>name</code>, <code>data</code>): <code>Promise</code>\\&lt;<code>Table</code>\\&lt;<code>number</code>[]&gt;&gt;</p> <p>Creates a new Table and initialize it with new data.</p>"},{"location":"javascript/interfaces/Connection/#parameters_1","title":"Parameters","text":"Name Type Description <code>name</code> <code>string</code> The name of the table. <code>data</code> <code>Table</code>\\&lt;<code>any</code>&gt; | <code>Record</code>\\&lt;<code>string</code>, <code>unknown</code>&gt;[] Non-empty Array of Records to be inserted into the table"},{"location":"javascript/interfaces/Connection/#returns_1","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>Table</code>\\&lt;<code>number</code>[]&gt;&gt;</p>"},{"location":"javascript/interfaces/Connection/#defined-in_2","title":"Defined in","text":"<p>index.ts:299</p> <p>\u25b8 createTable(<code>name</code>, <code>data</code>, <code>options</code>): <code>Promise</code>\\&lt;<code>Table</code>\\&lt;<code>number</code>[]&gt;&gt;</p> <p>Creates a new Table and initialize it with new data.</p>"},{"location":"javascript/interfaces/Connection/#parameters_2","title":"Parameters","text":"Name Type Description <code>name</code> <code>string</code> The name of the table. <code>data</code> <code>Table</code>\\&lt;<code>any</code>&gt; | <code>Record</code>\\&lt;<code>string</code>, <code>unknown</code>&gt;[] Non-empty Array of Records to be inserted into the table <code>options</code> <code>WriteOptions</code> The write options to use when creating the table."},{"location":"javascript/interfaces/Connection/#returns_2","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>Table</code>\\&lt;<code>number</code>[]&gt;&gt;</p>"},{"location":"javascript/interfaces/Connection/#defined-in_3","title":"Defined in","text":"<p>index.ts:311</p> <p>\u25b8 createTable\\&lt;<code>T</code>&gt;(<code>name</code>, <code>data</code>, <code>embeddings</code>): <code>Promise</code>\\&lt;<code>Table</code>\\&lt;<code>T</code>&gt;&gt;</p> <p>Creates a new Table and initialize it with new data.</p>"},{"location":"javascript/interfaces/Connection/#type-parameters_1","title":"Type parameters","text":"Name <code>T</code>"},{"location":"javascript/interfaces/Connection/#parameters_3","title":"Parameters","text":"Name Type Description <code>name</code> <code>string</code> The name of the table. <code>data</code> <code>Table</code>\\&lt;<code>any</code>&gt; | <code>Record</code>\\&lt;<code>string</code>, <code>unknown</code>&gt;[] Non-empty Array of Records to be inserted into the table <code>embeddings</code> <code>EmbeddingFunction</code>\\&lt;<code>T</code>&gt; An embedding function to use on this table"},{"location":"javascript/interfaces/Connection/#returns_3","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>Table</code>\\&lt;<code>T</code>&gt;&gt;</p>"},{"location":"javascript/interfaces/Connection/#defined-in_4","title":"Defined in","text":"<p>index.ts:324</p> <p>\u25b8 createTable\\&lt;<code>T</code>&gt;(<code>name</code>, <code>data</code>, <code>embeddings</code>, <code>options</code>): <code>Promise</code>\\&lt;<code>Table</code>\\&lt;<code>T</code>&gt;&gt;</p> <p>Creates a new Table and initialize it with new data.</p>"},{"location":"javascript/interfaces/Connection/#type-parameters_2","title":"Type parameters","text":"Name <code>T</code>"},{"location":"javascript/interfaces/Connection/#parameters_4","title":"Parameters","text":"Name Type Description <code>name</code> <code>string</code> The name of the table. <code>data</code> <code>Table</code>\\&lt;<code>any</code>&gt; | <code>Record</code>\\&lt;<code>string</code>, <code>unknown</code>&gt;[] Non-empty Array of Records to be inserted into the table <code>embeddings</code> <code>EmbeddingFunction</code>\\&lt;<code>T</code>&gt; An embedding function to use on this table <code>options</code> <code>WriteOptions</code> The write options to use when creating the table."},{"location":"javascript/interfaces/Connection/#returns_4","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>Table</code>\\&lt;<code>T</code>&gt;&gt;</p>"},{"location":"javascript/interfaces/Connection/#defined-in_5","title":"Defined in","text":"<p>index.ts:337</p>"},{"location":"javascript/interfaces/Connection/#droptable","title":"dropTable","text":"<p>\u25b8 dropTable(<code>name</code>): <code>Promise</code>\\&lt;<code>void</code>&gt;</p> <p>Drop an existing table.</p>"},{"location":"javascript/interfaces/Connection/#parameters_5","title":"Parameters","text":"Name Type Description <code>name</code> <code>string</code> The name of the table to drop."},{"location":"javascript/interfaces/Connection/#returns_5","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>void</code>&gt;</p>"},{"location":"javascript/interfaces/Connection/#defined-in_6","title":"Defined in","text":"<p>index.ts:348</p>"},{"location":"javascript/interfaces/Connection/#opentable","title":"openTable","text":"<p>\u25b8 openTable\\&lt;<code>T</code>&gt;(<code>name</code>, <code>embeddings?</code>): <code>Promise</code>\\&lt;<code>Table</code>\\&lt;<code>T</code>&gt;&gt;</p> <p>Open a table in the database.</p>"},{"location":"javascript/interfaces/Connection/#type-parameters_3","title":"Type parameters","text":"Name <code>T</code>"},{"location":"javascript/interfaces/Connection/#parameters_6","title":"Parameters","text":"Name Type Description <code>name</code> <code>string</code> The name of the table. <code>embeddings?</code> <code>EmbeddingFunction</code>\\&lt;<code>T</code>&gt; An embedding function to use on this table"},{"location":"javascript/interfaces/Connection/#returns_6","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>Table</code>\\&lt;<code>T</code>&gt;&gt;</p>"},{"location":"javascript/interfaces/Connection/#defined-in_7","title":"Defined in","text":"<p>index.ts:271</p>"},{"location":"javascript/interfaces/Connection/#tablenames","title":"tableNames","text":"<p>\u25b8 tableNames(): <code>Promise</code>\\&lt;<code>string</code>[]&gt;</p>"},{"location":"javascript/interfaces/Connection/#returns_7","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>string</code>[]&gt;</p>"},{"location":"javascript/interfaces/Connection/#defined-in_8","title":"Defined in","text":"<p>index.ts:263</p>"},{"location":"javascript/interfaces/Connection/#withmiddleware","title":"withMiddleware","text":"<p>\u25b8 withMiddleware(<code>middleware</code>): <code>Connection</code></p> <p>Instrument the behavior of this Connection with middleware.</p> <p>The middleware will be called in the order they are added.</p> <p>Currently this functionality is only supported for remote Connections.</p>"},{"location":"javascript/interfaces/Connection/#parameters_7","title":"Parameters","text":"Name Type <code>middleware</code> <code>HttpMiddleware</code>"},{"location":"javascript/interfaces/Connection/#returns_8","title":"Returns","text":"<p><code>Connection</code></p> <ul> <li>this Connection instrumented by the passed middleware</li> </ul>"},{"location":"javascript/interfaces/Connection/#defined-in_9","title":"Defined in","text":"<p>index.ts:360</p>"},{"location":"javascript/interfaces/ConnectionOptions/","title":"ConnectionOptions","text":"<p>vectordb / Exports / ConnectionOptions</p>"},{"location":"javascript/interfaces/ConnectionOptions/#interface-connectionoptions","title":"Interface: ConnectionOptions","text":""},{"location":"javascript/interfaces/ConnectionOptions/#table-of-contents","title":"Table of contents","text":""},{"location":"javascript/interfaces/ConnectionOptions/#properties","title":"Properties","text":"<ul> <li>apiKey</li> <li>awsCredentials</li> <li>awsRegion</li> <li>hostOverride</li> <li>readConsistencyInterval</li> <li>region</li> <li>storageOptions</li> <li>timeout</li> <li>uri</li> </ul>"},{"location":"javascript/interfaces/ConnectionOptions/#properties_1","title":"Properties","text":""},{"location":"javascript/interfaces/ConnectionOptions/#apikey","title":"apiKey","text":"<p>\u2022 <code>Optional</code> apiKey: <code>string</code></p> <p>API key for the remote connections</p> <p>Can also be passed by setting environment variable <code>LANCEDB_API_KEY</code></p>"},{"location":"javascript/interfaces/ConnectionOptions/#defined-in","title":"Defined in","text":"<p>index.ts:112</p>"},{"location":"javascript/interfaces/ConnectionOptions/#awscredentials","title":"awsCredentials","text":"<p>\u2022 <code>Optional</code> awsCredentials: <code>AwsCredentials</code></p> <p>User provided AWS crednetials.</p> <p>If not provided, LanceDB will use the default credentials provider chain.</p> <p><code>Deprecated</code></p> <p>Pass <code>aws_access_key_id</code>, <code>aws_secret_access_key</code>, and <code>aws_session_token</code> through <code>storageOptions</code> instead.</p>"},{"location":"javascript/interfaces/ConnectionOptions/#defined-in_1","title":"Defined in","text":"<p>index.ts:92</p>"},{"location":"javascript/interfaces/ConnectionOptions/#awsregion","title":"awsRegion","text":"<p>\u2022 <code>Optional</code> awsRegion: <code>string</code></p> <p>AWS region to connect to. Default is defaultAwsRegion</p> <p><code>Deprecated</code></p> <p>Pass <code>region</code> through <code>storageOptions</code> instead.</p>"},{"location":"javascript/interfaces/ConnectionOptions/#defined-in_2","title":"Defined in","text":"<p>index.ts:98</p>"},{"location":"javascript/interfaces/ConnectionOptions/#hostoverride","title":"hostOverride","text":"<p>\u2022 <code>Optional</code> hostOverride: <code>string</code></p> <p>Override the host URL for the remote connection.</p> <p>This is useful for local testing.</p>"},{"location":"javascript/interfaces/ConnectionOptions/#defined-in_3","title":"Defined in","text":"<p>index.ts:122</p>"},{"location":"javascript/interfaces/ConnectionOptions/#readconsistencyinterval","title":"readConsistencyInterval","text":"<p>\u2022 <code>Optional</code> readConsistencyInterval: <code>number</code></p> <p>(For LanceDB OSS only): The interval, in seconds, at which to check for updates to the table from other processes. If None, then consistency is not checked. For performance reasons, this is the default. For strong consistency, set this to zero seconds. Then every read will check for updates from other processes. As a compromise, you can set this to a non-zero value for eventual consistency. If more than that interval has passed since the last check, then the table will be checked for updates. Note: this consistency only applies to read operations. Write operations are always consistent.</p>"},{"location":"javascript/interfaces/ConnectionOptions/#defined-in_4","title":"Defined in","text":"<p>index.ts:140</p>"},{"location":"javascript/interfaces/ConnectionOptions/#region","title":"region","text":"<p>\u2022 <code>Optional</code> region: <code>string</code></p> <p>Region to connect. Default is 'us-east-1'</p>"},{"location":"javascript/interfaces/ConnectionOptions/#defined-in_5","title":"Defined in","text":"<p>index.ts:115</p>"},{"location":"javascript/interfaces/ConnectionOptions/#storageoptions","title":"storageOptions","text":"<p>\u2022 <code>Optional</code> storageOptions: <code>Record</code>\\&lt;<code>string</code>, <code>string</code>&gt;</p> <p>User provided options for object storage. For example, S3 credentials or request timeouts.</p> <p>The various options are described at https://lancedb.github.io/lancedb/guides/storage/</p>"},{"location":"javascript/interfaces/ConnectionOptions/#defined-in_6","title":"Defined in","text":"<p>index.ts:105</p>"},{"location":"javascript/interfaces/ConnectionOptions/#timeout","title":"timeout","text":"<p>\u2022 <code>Optional</code> timeout: <code>number</code></p> <p>Duration in milliseconds for request timeout. Default = 10,000 (10 seconds)</p>"},{"location":"javascript/interfaces/ConnectionOptions/#defined-in_7","title":"Defined in","text":"<p>index.ts:127</p>"},{"location":"javascript/interfaces/ConnectionOptions/#uri","title":"uri","text":"<p>\u2022 uri: <code>string</code></p> <p>LanceDB database URI.</p> <ul> <li><code>/path/to/database</code> - local database</li> <li><code>s3://bucket/path/to/database</code> or <code>gs://bucket/path/to/database</code> - database on cloud storage</li> <li><code>db://host:port</code> - remote database (LanceDB cloud)</li> </ul>"},{"location":"javascript/interfaces/ConnectionOptions/#defined-in_8","title":"Defined in","text":"<p>index.ts:83</p>"},{"location":"javascript/interfaces/CreateTableOptions/","title":"CreateTableOptions","text":"<p>vectordb / Exports / CreateTableOptions</p>"},{"location":"javascript/interfaces/CreateTableOptions/#interface-createtableoptionst","title":"Interface: CreateTableOptions\\&lt;T&gt;","text":""},{"location":"javascript/interfaces/CreateTableOptions/#type-parameters","title":"Type parameters","text":"Name <code>T</code>"},{"location":"javascript/interfaces/CreateTableOptions/#table-of-contents","title":"Table of contents","text":""},{"location":"javascript/interfaces/CreateTableOptions/#properties","title":"Properties","text":"<ul> <li>data</li> <li>embeddingFunction</li> <li>name</li> <li>schema</li> <li>writeOptions</li> </ul>"},{"location":"javascript/interfaces/CreateTableOptions/#properties_1","title":"Properties","text":""},{"location":"javascript/interfaces/CreateTableOptions/#data","title":"data","text":"<p>\u2022 <code>Optional</code> data: <code>Table</code>\\&lt;<code>any</code>&gt; | <code>Record</code>\\&lt;<code>string</code>, <code>unknown</code>&gt;[]</p>"},{"location":"javascript/interfaces/CreateTableOptions/#defined-in","title":"Defined in","text":"<p>index.ts:163</p>"},{"location":"javascript/interfaces/CreateTableOptions/#embeddingfunction","title":"embeddingFunction","text":"<p>\u2022 <code>Optional</code> embeddingFunction: <code>EmbeddingFunction</code>\\&lt;<code>T</code>&gt;</p>"},{"location":"javascript/interfaces/CreateTableOptions/#defined-in_1","title":"Defined in","text":"<p>index.ts:169</p>"},{"location":"javascript/interfaces/CreateTableOptions/#name","title":"name","text":"<p>\u2022 name: <code>string</code></p>"},{"location":"javascript/interfaces/CreateTableOptions/#defined-in_2","title":"Defined in","text":"<p>index.ts:160</p>"},{"location":"javascript/interfaces/CreateTableOptions/#schema","title":"schema","text":"<p>\u2022 <code>Optional</code> schema: <code>Schema</code>\\&lt;<code>any</code>&gt;</p>"},{"location":"javascript/interfaces/CreateTableOptions/#defined-in_3","title":"Defined in","text":"<p>index.ts:166</p>"},{"location":"javascript/interfaces/CreateTableOptions/#writeoptions","title":"writeOptions","text":"<p>\u2022 <code>Optional</code> writeOptions: <code>WriteOptions</code></p>"},{"location":"javascript/interfaces/CreateTableOptions/#defined-in_4","title":"Defined in","text":"<p>index.ts:172</p>"},{"location":"javascript/interfaces/EmbeddingFunction/","title":"EmbeddingFunction","text":"<p>vectordb / Exports / EmbeddingFunction</p>"},{"location":"javascript/interfaces/EmbeddingFunction/#interface-embeddingfunctiont","title":"Interface: EmbeddingFunction\\&lt;T&gt;","text":"<p>An embedding function that automatically creates vector representation for a given column.</p>"},{"location":"javascript/interfaces/EmbeddingFunction/#type-parameters","title":"Type parameters","text":"Name <code>T</code>"},{"location":"javascript/interfaces/EmbeddingFunction/#implemented-by","title":"Implemented by","text":"<ul> <li><code>OpenAIEmbeddingFunction</code></li> </ul>"},{"location":"javascript/interfaces/EmbeddingFunction/#table-of-contents","title":"Table of contents","text":""},{"location":"javascript/interfaces/EmbeddingFunction/#properties","title":"Properties","text":"<ul> <li>destColumn</li> <li>embed</li> <li>embeddingDataType</li> <li>embeddingDimension</li> <li>excludeSource</li> <li>sourceColumn</li> </ul>"},{"location":"javascript/interfaces/EmbeddingFunction/#properties_1","title":"Properties","text":""},{"location":"javascript/interfaces/EmbeddingFunction/#destcolumn","title":"destColumn","text":"<p>\u2022 <code>Optional</code> destColumn: <code>string</code></p> <p>The name of the column that will contain the embedding</p> <p>By default this is \"vector\"</p>"},{"location":"javascript/interfaces/EmbeddingFunction/#defined-in","title":"Defined in","text":"<p>embedding/embedding_function.ts:49</p>"},{"location":"javascript/interfaces/EmbeddingFunction/#embed","title":"embed","text":"<p>\u2022 embed: (<code>data</code>: <code>T</code>[]) =&gt; <code>Promise</code>\\&lt;<code>number</code>[][]&gt;</p>"},{"location":"javascript/interfaces/EmbeddingFunction/#type-declaration","title":"Type declaration","text":"<p>\u25b8 (<code>data</code>): <code>Promise</code>\\&lt;<code>number</code>[][]&gt;</p> <p>Creates a vector representation for the given values.</p>"},{"location":"javascript/interfaces/EmbeddingFunction/#parameters","title":"Parameters","text":"Name Type <code>data</code> <code>T</code>[]"},{"location":"javascript/interfaces/EmbeddingFunction/#returns","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>number</code>[][]&gt;</p>"},{"location":"javascript/interfaces/EmbeddingFunction/#defined-in_1","title":"Defined in","text":"<p>embedding/embedding_function.ts:62</p>"},{"location":"javascript/interfaces/EmbeddingFunction/#embeddingdatatype","title":"embeddingDataType","text":"<p>\u2022 <code>Optional</code> embeddingDataType: <code>Float</code>\\&lt;<code>Floats</code>&gt;</p> <p>The data type of the embedding</p> <p>The embedding function should return <code>number</code>.  This will be converted into an Arrow float array.  By default this will be Float32 but this property can be used to control the conversion.</p>"},{"location":"javascript/interfaces/EmbeddingFunction/#defined-in_2","title":"Defined in","text":"<p>embedding/embedding_function.ts:33</p>"},{"location":"javascript/interfaces/EmbeddingFunction/#embeddingdimension","title":"embeddingDimension","text":"<p>\u2022 <code>Optional</code> embeddingDimension: <code>number</code></p> <p>The dimension of the embedding</p> <p>This is optional, normally this can be determined by looking at the results of <code>embed</code>.  If this is not specified, and there is an attempt to apply the embedding to an empty table, then that process will fail.</p>"},{"location":"javascript/interfaces/EmbeddingFunction/#defined-in_3","title":"Defined in","text":"<p>embedding/embedding_function.ts:42</p>"},{"location":"javascript/interfaces/EmbeddingFunction/#excludesource","title":"excludeSource","text":"<p>\u2022 <code>Optional</code> excludeSource: <code>boolean</code></p> <p>Should the source column be excluded from the resulting table</p> <p>By default the source column is included.  Set this to true and only the embedding will be stored.</p>"},{"location":"javascript/interfaces/EmbeddingFunction/#defined-in_4","title":"Defined in","text":"<p>embedding/embedding_function.ts:57</p>"},{"location":"javascript/interfaces/EmbeddingFunction/#sourcecolumn","title":"sourceColumn","text":"<p>\u2022 sourceColumn: <code>string</code></p> <p>The name of the column that will be used as input for the Embedding Function.</p>"},{"location":"javascript/interfaces/EmbeddingFunction/#defined-in_5","title":"Defined in","text":"<p>embedding/embedding_function.ts:24</p>"},{"location":"javascript/interfaces/IndexStats/","title":"IndexStats","text":"<p>vectordb / Exports / IndexStats</p>"},{"location":"javascript/interfaces/IndexStats/#interface-indexstats","title":"Interface: IndexStats","text":""},{"location":"javascript/interfaces/IndexStats/#table-of-contents","title":"Table of contents","text":""},{"location":"javascript/interfaces/IndexStats/#properties","title":"Properties","text":"<ul> <li>distanceType</li> <li>indexType</li> <li>numIndexedRows</li> <li>numIndices</li> <li>numUnindexedRows</li> </ul>"},{"location":"javascript/interfaces/IndexStats/#properties_1","title":"Properties","text":""},{"location":"javascript/interfaces/IndexStats/#distancetype","title":"distanceType","text":"<p>\u2022 <code>Optional</code> distanceType: <code>string</code></p>"},{"location":"javascript/interfaces/IndexStats/#defined-in","title":"Defined in","text":"<p>index.ts:728</p>"},{"location":"javascript/interfaces/IndexStats/#indextype","title":"indexType","text":"<p>\u2022 indexType: <code>string</code></p>"},{"location":"javascript/interfaces/IndexStats/#defined-in_1","title":"Defined in","text":"<p>index.ts:727</p>"},{"location":"javascript/interfaces/IndexStats/#numindexedrows","title":"numIndexedRows","text":"<p>\u2022 numIndexedRows: <code>null</code> | <code>number</code></p>"},{"location":"javascript/interfaces/IndexStats/#defined-in_2","title":"Defined in","text":"<p>index.ts:725</p>"},{"location":"javascript/interfaces/IndexStats/#numindices","title":"numIndices","text":"<p>\u2022 <code>Optional</code> numIndices: <code>number</code></p>"},{"location":"javascript/interfaces/IndexStats/#defined-in_3","title":"Defined in","text":"<p>index.ts:729</p>"},{"location":"javascript/interfaces/IndexStats/#numunindexedrows","title":"numUnindexedRows","text":"<p>\u2022 numUnindexedRows: <code>null</code> | <code>number</code></p>"},{"location":"javascript/interfaces/IndexStats/#defined-in_4","title":"Defined in","text":"<p>index.ts:726</p>"},{"location":"javascript/interfaces/IvfPQIndexConfig/","title":"IvfPQIndexConfig","text":"<p>vectordb / Exports / IvfPQIndexConfig</p>"},{"location":"javascript/interfaces/IvfPQIndexConfig/#interface-ivfpqindexconfig","title":"Interface: IvfPQIndexConfig","text":""},{"location":"javascript/interfaces/IvfPQIndexConfig/#table-of-contents","title":"Table of contents","text":""},{"location":"javascript/interfaces/IvfPQIndexConfig/#properties","title":"Properties","text":"<ul> <li>column</li> <li>index_cache_size</li> <li>index_name</li> <li>max_iters</li> <li>max_opq_iters</li> <li>metric_type</li> <li>num_bits</li> <li>num_partitions</li> <li>num_sub_vectors</li> <li>replace</li> <li>type</li> <li>use_opq</li> </ul>"},{"location":"javascript/interfaces/IvfPQIndexConfig/#properties_1","title":"Properties","text":""},{"location":"javascript/interfaces/IvfPQIndexConfig/#column","title":"column","text":"<p>\u2022 <code>Optional</code> column: <code>string</code></p> <p>The column to be indexed</p>"},{"location":"javascript/interfaces/IvfPQIndexConfig/#defined-in","title":"Defined in","text":"<p>index.ts:1282</p>"},{"location":"javascript/interfaces/IvfPQIndexConfig/#index_cache_size","title":"index_cache_size","text":"<p>\u2022 <code>Optional</code> index_cache_size: <code>number</code></p> <p>Cache size of the index</p>"},{"location":"javascript/interfaces/IvfPQIndexConfig/#defined-in_1","title":"Defined in","text":"<p>index.ts:1331</p>"},{"location":"javascript/interfaces/IvfPQIndexConfig/#index_name","title":"index_name","text":"<p>\u2022 <code>Optional</code> index_name: <code>string</code></p> <p>A unique name for the index</p>"},{"location":"javascript/interfaces/IvfPQIndexConfig/#defined-in_2","title":"Defined in","text":"<p>index.ts:1287</p>"},{"location":"javascript/interfaces/IvfPQIndexConfig/#max_iters","title":"max_iters","text":"<p>\u2022 <code>Optional</code> max_iters: <code>number</code></p> <p>The max number of iterations for kmeans training.</p>"},{"location":"javascript/interfaces/IvfPQIndexConfig/#defined-in_3","title":"Defined in","text":"<p>index.ts:1302</p>"},{"location":"javascript/interfaces/IvfPQIndexConfig/#max_opq_iters","title":"max_opq_iters","text":"<p>\u2022 <code>Optional</code> max_opq_iters: <code>number</code></p> <p>Max number of iterations to train OPQ, if <code>use_opq</code> is true.</p>"},{"location":"javascript/interfaces/IvfPQIndexConfig/#defined-in_4","title":"Defined in","text":"<p>index.ts:1321</p>"},{"location":"javascript/interfaces/IvfPQIndexConfig/#metric_type","title":"metric_type","text":"<p>\u2022 <code>Optional</code> metric_type: <code>MetricType</code></p> <p>Metric type, l2 or Cosine</p>"},{"location":"javascript/interfaces/IvfPQIndexConfig/#defined-in_5","title":"Defined in","text":"<p>index.ts:1292</p>"},{"location":"javascript/interfaces/IvfPQIndexConfig/#num_bits","title":"num_bits","text":"<p>\u2022 <code>Optional</code> num_bits: <code>number</code></p> <p>The number of bits to present one PQ centroid.</p>"},{"location":"javascript/interfaces/IvfPQIndexConfig/#defined-in_6","title":"Defined in","text":"<p>index.ts:1316</p>"},{"location":"javascript/interfaces/IvfPQIndexConfig/#num_partitions","title":"num_partitions","text":"<p>\u2022 <code>Optional</code> num_partitions: <code>number</code></p> <p>The number of partitions this index</p>"},{"location":"javascript/interfaces/IvfPQIndexConfig/#defined-in_7","title":"Defined in","text":"<p>index.ts:1297</p>"},{"location":"javascript/interfaces/IvfPQIndexConfig/#num_sub_vectors","title":"num_sub_vectors","text":"<p>\u2022 <code>Optional</code> num_sub_vectors: <code>number</code></p> <p>Number of subvectors to build PQ code</p>"},{"location":"javascript/interfaces/IvfPQIndexConfig/#defined-in_8","title":"Defined in","text":"<p>index.ts:1312</p>"},{"location":"javascript/interfaces/IvfPQIndexConfig/#replace","title":"replace","text":"<p>\u2022 <code>Optional</code> replace: <code>boolean</code></p> <p>Replace an existing index with the same name if it exists.</p>"},{"location":"javascript/interfaces/IvfPQIndexConfig/#defined-in_9","title":"Defined in","text":"<p>index.ts:1326</p>"},{"location":"javascript/interfaces/IvfPQIndexConfig/#type","title":"type","text":"<p>\u2022 type: <code>\"ivf_pq\"</code></p>"},{"location":"javascript/interfaces/IvfPQIndexConfig/#defined-in_10","title":"Defined in","text":"<p>index.ts:1333</p>"},{"location":"javascript/interfaces/IvfPQIndexConfig/#use_opq","title":"use_opq","text":"<p>\u2022 <code>Optional</code> use_opq: <code>boolean</code></p> <p>Train as optimized product quantization.</p>"},{"location":"javascript/interfaces/IvfPQIndexConfig/#defined-in_11","title":"Defined in","text":"<p>index.ts:1307</p>"},{"location":"javascript/interfaces/MergeInsertArgs/","title":"MergeInsertArgs","text":"<p>vectordb / Exports / MergeInsertArgs</p>"},{"location":"javascript/interfaces/MergeInsertArgs/#interface-mergeinsertargs","title":"Interface: MergeInsertArgs","text":""},{"location":"javascript/interfaces/MergeInsertArgs/#table-of-contents","title":"Table of contents","text":""},{"location":"javascript/interfaces/MergeInsertArgs/#properties","title":"Properties","text":"<ul> <li>whenMatchedUpdateAll</li> <li>whenNotMatchedBySourceDelete</li> <li>whenNotMatchedInsertAll</li> </ul>"},{"location":"javascript/interfaces/MergeInsertArgs/#properties_1","title":"Properties","text":""},{"location":"javascript/interfaces/MergeInsertArgs/#whenmatchedupdateall","title":"whenMatchedUpdateAll","text":"<p>\u2022 <code>Optional</code> whenMatchedUpdateAll: <code>string</code> | <code>boolean</code></p> <p>If true then rows that exist in both the source table (new data) and the target table (old data) will be updated, replacing the old row with the corresponding matching row.</p> <p>If there are multiple matches then the behavior is undefined. Currently this causes multiple copies of the row to be created but that behavior is subject to change.</p> <p>Optionally, a filter can be specified.  This should be an SQL filter where fields with the prefix \"target.\" refer to fields in the target table (old data) and fields with the prefix \"source.\" refer to fields in the source table (new data).  For example, the filter \"target.lastUpdated &lt; source.lastUpdated\" will only update matched rows when the incoming <code>lastUpdated</code> value is newer.</p> <p>Rows that do not match the filter will not be updated.  Rows that do not match the filter do become \"not matched\" rows.</p>"},{"location":"javascript/interfaces/MergeInsertArgs/#defined-in","title":"Defined in","text":"<p>index.ts:690</p>"},{"location":"javascript/interfaces/MergeInsertArgs/#whennotmatchedbysourcedelete","title":"whenNotMatchedBySourceDelete","text":"<p>\u2022 <code>Optional</code> whenNotMatchedBySourceDelete: <code>string</code> | <code>boolean</code></p> <p>If true then rows that exist only in the target table (old data) will be deleted.</p> <p>If this is a string then it will be treated as an SQL filter and only rows that both do not match any row in the source table and match the given filter will be deleted.</p> <p>This can be used to replace a selection of existing data with new data.</p>"},{"location":"javascript/interfaces/MergeInsertArgs/#defined-in_1","title":"Defined in","text":"<p>index.ts:707</p>"},{"location":"javascript/interfaces/MergeInsertArgs/#whennotmatchedinsertall","title":"whenNotMatchedInsertAll","text":"<p>\u2022 <code>Optional</code> whenNotMatchedInsertAll: <code>boolean</code></p> <p>If true then rows that exist only in the source table (new data) will be inserted into the target table.</p>"},{"location":"javascript/interfaces/MergeInsertArgs/#defined-in_2","title":"Defined in","text":"<p>index.ts:695</p>"},{"location":"javascript/interfaces/Table/","title":"Table","text":"<p>vectordb / Exports / Table</p>"},{"location":"javascript/interfaces/Table/#interface-tablet","title":"Interface: Table\\&lt;T&gt;","text":"<p>A LanceDB Table is the collection of Records. Each Record has one or more vector fields.</p>"},{"location":"javascript/interfaces/Table/#type-parameters","title":"Type parameters","text":"Name Type <code>T</code> <code>number</code>[]"},{"location":"javascript/interfaces/Table/#implemented-by","title":"Implemented by","text":"<ul> <li><code>LocalTable</code></li> </ul>"},{"location":"javascript/interfaces/Table/#table-of-contents","title":"Table of contents","text":""},{"location":"javascript/interfaces/Table/#properties","title":"Properties","text":"<ul> <li>add</li> <li>countRows</li> <li>createIndex</li> <li>createScalarIndex</li> <li>delete</li> <li>indexStats</li> <li>listIndices</li> <li>mergeInsert</li> <li>name</li> <li>overwrite</li> <li>schema</li> <li>search</li> <li>update</li> </ul>"},{"location":"javascript/interfaces/Table/#methods","title":"Methods","text":"<ul> <li>addColumns</li> <li>alterColumns</li> <li>dropColumns</li> <li>filter</li> <li>withMiddleware</li> </ul>"},{"location":"javascript/interfaces/Table/#properties_1","title":"Properties","text":""},{"location":"javascript/interfaces/Table/#add","title":"add","text":"<p>\u2022 add: (<code>data</code>: <code>Table</code>\\&lt;<code>any</code>&gt; | <code>Record</code>\\&lt;<code>string</code>, <code>unknown</code>&gt;[]) =&gt; <code>Promise</code>\\&lt;<code>number</code>&gt;</p>"},{"location":"javascript/interfaces/Table/#type-declaration","title":"Type declaration","text":"<p>\u25b8 (<code>data</code>): <code>Promise</code>\\&lt;<code>number</code>&gt;</p> <p>Insert records into this Table.</p>"},{"location":"javascript/interfaces/Table/#parameters","title":"Parameters","text":"Name Type Description <code>data</code> <code>Table</code>\\&lt;<code>any</code>&gt; | <code>Record</code>\\&lt;<code>string</code>, <code>unknown</code>&gt;[] Records to be inserted into the Table"},{"location":"javascript/interfaces/Table/#returns","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>number</code>&gt;</p> <p>The number of rows added to the table</p>"},{"location":"javascript/interfaces/Table/#defined-in","title":"Defined in","text":"<p>index.ts:381</p>"},{"location":"javascript/interfaces/Table/#countrows","title":"countRows","text":"<p>\u2022 countRows: (<code>filter?</code>: <code>string</code>) =&gt; <code>Promise</code>\\&lt;<code>number</code>&gt;</p>"},{"location":"javascript/interfaces/Table/#type-declaration_1","title":"Type declaration","text":"<p>\u25b8 (<code>filter?</code>): <code>Promise</code>\\&lt;<code>number</code>&gt;</p> <p>Returns the number of rows in this table.</p>"},{"location":"javascript/interfaces/Table/#parameters_1","title":"Parameters","text":"Name Type <code>filter?</code> <code>string</code>"},{"location":"javascript/interfaces/Table/#returns_1","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>number</code>&gt;</p>"},{"location":"javascript/interfaces/Table/#defined-in_1","title":"Defined in","text":"<p>index.ts:454</p>"},{"location":"javascript/interfaces/Table/#createindex","title":"createIndex","text":"<p>\u2022 createIndex: (<code>indexParams</code>: <code>IvfPQIndexConfig</code>) =&gt; <code>Promise</code>\\&lt;<code>any</code>&gt;</p>"},{"location":"javascript/interfaces/Table/#type-declaration_2","title":"Type declaration","text":"<p>\u25b8 (<code>indexParams</code>): <code>Promise</code>\\&lt;<code>any</code>&gt;</p> <p>Create an ANN index on this Table vector index.</p>"},{"location":"javascript/interfaces/Table/#parameters_2","title":"Parameters","text":"Name Type Description <code>indexParams</code> <code>IvfPQIndexConfig</code> The parameters of this Index,"},{"location":"javascript/interfaces/Table/#returns_2","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>any</code>&gt;</p> <p><code>See</code></p> <p>VectorIndexParams.</p>"},{"location":"javascript/interfaces/Table/#defined-in_2","title":"Defined in","text":"<p>index.ts:398</p>"},{"location":"javascript/interfaces/Table/#createscalarindex","title":"createScalarIndex","text":"<p>\u2022 createScalarIndex: (<code>column</code>: <code>string</code>, <code>replace?</code>: <code>boolean</code>) =&gt; <code>Promise</code>\\&lt;<code>void</code>&gt;</p>"},{"location":"javascript/interfaces/Table/#type-declaration_3","title":"Type declaration","text":"<p>\u25b8 (<code>column</code>, <code>replace?</code>): <code>Promise</code>\\&lt;<code>void</code>&gt;</p> <p>Create a scalar index on this Table for the given column</p>"},{"location":"javascript/interfaces/Table/#parameters_3","title":"Parameters","text":"Name Type Description <code>column</code> <code>string</code> The column to index <code>replace?</code> <code>boolean</code> If false, fail if an index already exists on the column it is always set to true for remote connections Scalar indices, like vector indices, can be used to speed up scans. A scalar index can speed up scans that contain filter expressions on the indexed column. For example, the following scan will be faster if the column <code>my_col</code> has a scalar index: <code>ts const con = await lancedb.connect('./.lancedb'); const table = await con.openTable('images'); const results = await table.where('my_col = 7').execute();</code> Scalar indices can also speed up scans containing a vector search and a prefilter: <code>ts const con = await lancedb.connect('././lancedb'); const table = await con.openTable('images'); const results = await table.search([1.0, 2.0]).where('my_col != 7').prefilter(true);</code> Scalar indices can only speed up scans for basic filters using equality, comparison, range (e.g. <code>my_col BETWEEN 0 AND 100</code>), and set membership (e.g. <code>my_col IN (0, 1, 2)</code>) Scalar indices can be used if the filter contains multiple indexed columns and the filter criteria are AND'd or OR'd together (e.g. <code>my_col &lt; 0 AND other_col&gt; 100</code>) Scalar indices may be used if the filter contains non-indexed columns but, depending on the structure of the filter, they may not be usable. For example, if the column <code>not_indexed</code> does not have a scalar index then the filter <code>my_col = 0 OR not_indexed = 1</code> will not be able to use any scalar index on <code>my_col</code>."},{"location":"javascript/interfaces/Table/#returns_3","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>void</code>&gt;</p> <p><code>Examples</code></p> <pre><code>const con = await lancedb.connect('././lancedb')\nconst table = await con.openTable('images')\nawait table.createScalarIndex('my_col')\n</code></pre>"},{"location":"javascript/interfaces/Table/#defined-in_3","title":"Defined in","text":"<p>index.ts:449</p>"},{"location":"javascript/interfaces/Table/#delete","title":"delete","text":"<p>\u2022 delete: (<code>filter</code>: <code>string</code>) =&gt; <code>Promise</code>\\&lt;<code>void</code>&gt;</p>"},{"location":"javascript/interfaces/Table/#type-declaration_4","title":"Type declaration","text":"<p>\u25b8 (<code>filter</code>): <code>Promise</code>\\&lt;<code>void</code>&gt;</p> <p>Delete rows from this table.</p> <p>This can be used to delete a single row, many rows, all rows, or sometimes no rows (if your predicate matches nothing).</p>"},{"location":"javascript/interfaces/Table/#parameters_4","title":"Parameters","text":"Name Type Description <code>filter</code> <code>string</code> A filter in the same format used by a sql WHERE clause. The filter must not be empty."},{"location":"javascript/interfaces/Table/#returns_4","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>void</code>&gt;</p> <p><code>Examples</code></p> <pre><code>const con = await lancedb.connect(\"./.lancedb\")\nconst data = [\n   {id: 1, vector: [1, 2]},\n   {id: 2, vector: [3, 4]},\n   {id: 3, vector: [5, 6]},\n];\nconst tbl = await con.createTable(\"my_table\", data)\nawait tbl.delete(\"id = 2\")\nawait tbl.countRows() // Returns 2\n</code></pre> <p>If you have a list of values to delete, you can combine them into a stringified list and use the <code>IN</code> operator:</p> <pre><code>const to_remove = [1, 5];\nawait tbl.delete(`id IN (${to_remove.join(\",\")})`)\nawait tbl.countRows() // Returns 1\n</code></pre>"},{"location":"javascript/interfaces/Table/#defined-in_4","title":"Defined in","text":"<p>index.ts:488</p>"},{"location":"javascript/interfaces/Table/#indexstats","title":"indexStats","text":"<p>\u2022 indexStats: (<code>indexName</code>: <code>string</code>) =&gt; <code>Promise</code>\\&lt;<code>IndexStats</code>&gt;</p>"},{"location":"javascript/interfaces/Table/#type-declaration_5","title":"Type declaration","text":"<p>\u25b8 (<code>indexName</code>): <code>Promise</code>\\&lt;<code>IndexStats</code>&gt;</p> <p>Get statistics about an index.</p>"},{"location":"javascript/interfaces/Table/#parameters_5","title":"Parameters","text":"Name Type <code>indexName</code> <code>string</code>"},{"location":"javascript/interfaces/Table/#returns_5","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>IndexStats</code>&gt;</p>"},{"location":"javascript/interfaces/Table/#defined-in_5","title":"Defined in","text":"<p>index.ts:567</p>"},{"location":"javascript/interfaces/Table/#listindices","title":"listIndices","text":"<p>\u2022 listIndices: () =&gt; <code>Promise</code>\\&lt;<code>VectorIndex</code>[]&gt;</p>"},{"location":"javascript/interfaces/Table/#type-declaration_6","title":"Type declaration","text":"<p>\u25b8 (): <code>Promise</code>\\&lt;<code>VectorIndex</code>[]&gt;</p> <p>List the indicies on this table.</p>"},{"location":"javascript/interfaces/Table/#returns_6","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>VectorIndex</code>[]&gt;</p>"},{"location":"javascript/interfaces/Table/#defined-in_6","title":"Defined in","text":"<p>index.ts:562</p>"},{"location":"javascript/interfaces/Table/#mergeinsert","title":"mergeInsert","text":"<p>\u2022 mergeInsert: (<code>on</code>: <code>string</code>, <code>data</code>: <code>Table</code>\\&lt;<code>any</code>&gt; | <code>Record</code>\\&lt;<code>string</code>, <code>unknown</code>&gt;[], <code>args</code>: <code>MergeInsertArgs</code>) =&gt; <code>Promise</code>\\&lt;<code>void</code>&gt;</p>"},{"location":"javascript/interfaces/Table/#type-declaration_7","title":"Type declaration","text":"<p>\u25b8 (<code>on</code>, <code>data</code>, <code>args</code>): <code>Promise</code>\\&lt;<code>void</code>&gt;</p> <p>Runs a \"merge insert\" operation on the table</p> <p>This operation can add rows, update rows, and remove rows all in a single transaction. It is a very generic tool that can be used to create behaviors like \"insert if not exists\", \"update or insert (i.e. upsert)\", or even replace a portion of existing data with new data (e.g. replace all data where month=\"january\")</p> <p>The merge insert operation works by combining new data from a source table with existing data in a target table by using a join.  There are three categories of records.</p> <p>\"Matched\" records are records that exist in both the source table and the target table. \"Not matched\" records exist only in the source table (e.g. these are new data) \"Not matched by source\" records exist only in the target table (this is old data)</p> <p>The MergeInsertArgs can be used to customize what should happen for each category of data.</p> <p>Please note that the data may appear to be reordered as part of this operation.  This is because updated rows will be deleted from the dataset and then reinserted at the end with the new values.</p>"},{"location":"javascript/interfaces/Table/#parameters_6","title":"Parameters","text":"Name Type Description <code>on</code> <code>string</code> a column to join on. This is how records from the source table and target table are matched. <code>data</code> <code>Table</code>\\&lt;<code>any</code>&gt; | <code>Record</code>\\&lt;<code>string</code>, <code>unknown</code>&gt;[] the new data to insert <code>args</code> <code>MergeInsertArgs</code> parameters controlling how the operation should behave"},{"location":"javascript/interfaces/Table/#returns_7","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>void</code>&gt;</p>"},{"location":"javascript/interfaces/Table/#defined-in_7","title":"Defined in","text":"<p>index.ts:553</p>"},{"location":"javascript/interfaces/Table/#name","title":"name","text":"<p>\u2022 name: <code>string</code></p>"},{"location":"javascript/interfaces/Table/#defined-in_8","title":"Defined in","text":"<p>index.ts:367</p>"},{"location":"javascript/interfaces/Table/#overwrite","title":"overwrite","text":"<p>\u2022 overwrite: (<code>data</code>: <code>Table</code>\\&lt;<code>any</code>&gt; | <code>Record</code>\\&lt;<code>string</code>, <code>unknown</code>&gt;[]) =&gt; <code>Promise</code>\\&lt;<code>number</code>&gt;</p>"},{"location":"javascript/interfaces/Table/#type-declaration_8","title":"Type declaration","text":"<p>\u25b8 (<code>data</code>): <code>Promise</code>\\&lt;<code>number</code>&gt;</p> <p>Insert records into this Table, replacing its contents.</p>"},{"location":"javascript/interfaces/Table/#parameters_7","title":"Parameters","text":"Name Type Description <code>data</code> <code>Table</code>\\&lt;<code>any</code>&gt; | <code>Record</code>\\&lt;<code>string</code>, <code>unknown</code>&gt;[] Records to be inserted into the Table"},{"location":"javascript/interfaces/Table/#returns_8","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>number</code>&gt;</p> <p>The number of rows added to the table</p>"},{"location":"javascript/interfaces/Table/#defined-in_9","title":"Defined in","text":"<p>index.ts:389</p>"},{"location":"javascript/interfaces/Table/#schema","title":"schema","text":"<p>\u2022 schema: <code>Promise</code>\\&lt;<code>Schema</code>\\&lt;<code>any</code>&gt;&gt;</p>"},{"location":"javascript/interfaces/Table/#defined-in_10","title":"Defined in","text":"<p>index.ts:571</p>"},{"location":"javascript/interfaces/Table/#search","title":"search","text":"<p>\u2022 search: (<code>query</code>: <code>T</code>) =&gt; <code>Query</code>\\&lt;<code>T</code>&gt;</p>"},{"location":"javascript/interfaces/Table/#type-declaration_9","title":"Type declaration","text":"<p>\u25b8 (<code>query</code>): <code>Query</code>\\&lt;<code>T</code>&gt;</p> <p>Creates a search query to find the nearest neighbors of the given search term</p>"},{"location":"javascript/interfaces/Table/#parameters_8","title":"Parameters","text":"Name Type Description <code>query</code> <code>T</code> The query search term"},{"location":"javascript/interfaces/Table/#returns_9","title":"Returns","text":"<p><code>Query</code>\\&lt;<code>T</code>&gt;</p>"},{"location":"javascript/interfaces/Table/#defined-in_11","title":"Defined in","text":"<p>index.ts:373</p>"},{"location":"javascript/interfaces/Table/#update","title":"update","text":"<p>\u2022 update: (<code>args</code>: <code>UpdateArgs</code> | <code>UpdateSqlArgs</code>) =&gt; <code>Promise</code>\\&lt;<code>void</code>&gt;</p>"},{"location":"javascript/interfaces/Table/#type-declaration_10","title":"Type declaration","text":"<p>\u25b8 (<code>args</code>): <code>Promise</code>\\&lt;<code>void</code>&gt;</p> <p>Update rows in this table.</p> <p>This can be used to update a single row, many rows, all rows, or sometimes no rows (if your predicate matches nothing).</p>"},{"location":"javascript/interfaces/Table/#parameters_9","title":"Parameters","text":"Name Type Description <code>args</code> <code>UpdateArgs</code> | <code>UpdateSqlArgs</code> see UpdateArgs and UpdateSqlArgs for more details"},{"location":"javascript/interfaces/Table/#returns_10","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>void</code>&gt;</p> <p><code>Examples</code></p> <pre><code>const con = await lancedb.connect(\"./.lancedb\")\nconst data = [\n   {id: 1, vector: [3, 3], name: 'Ye'},\n   {id: 2, vector: [4, 4], name: 'Mike'},\n];\nconst tbl = await con.createTable(\"my_table\", data)\n\nawait tbl.update({\n  where: \"id = 2\",\n  values: { vector: [2, 2], name: \"Michael\" },\n})\n\nlet results = await tbl.search([1, 1]).execute();\n// Returns [\n//   {id: 2, vector: [2, 2], name: 'Michael'}\n//   {id: 1, vector: [3, 3], name: 'Ye'}\n// ]\n</code></pre>"},{"location":"javascript/interfaces/Table/#defined-in_12","title":"Defined in","text":"<p>index.ts:521</p>"},{"location":"javascript/interfaces/Table/#methods_1","title":"Methods","text":""},{"location":"javascript/interfaces/Table/#addcolumns","title":"addColumns","text":"<p>\u25b8 addColumns(<code>newColumnTransforms</code>): <code>Promise</code>\\&lt;<code>void</code>&gt;</p> <p>Add new columns with defined values.</p>"},{"location":"javascript/interfaces/Table/#parameters_10","title":"Parameters","text":"Name Type Description <code>newColumnTransforms</code> { <code>name</code>: <code>string</code> ; <code>valueSql</code>: <code>string</code>  }[] pairs of column names and the SQL expression to use to calculate the value of the new column. These expressions will be evaluated for each row in the table, and can reference existing columns in the table."},{"location":"javascript/interfaces/Table/#returns_11","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>void</code>&gt;</p>"},{"location":"javascript/interfaces/Table/#defined-in_13","title":"Defined in","text":"<p>index.ts:582</p>"},{"location":"javascript/interfaces/Table/#altercolumns","title":"alterColumns","text":"<p>\u25b8 alterColumns(<code>columnAlterations</code>): <code>Promise</code>\\&lt;<code>void</code>&gt;</p> <p>Alter the name or nullability of columns.</p>"},{"location":"javascript/interfaces/Table/#parameters_11","title":"Parameters","text":"Name Type Description <code>columnAlterations</code> <code>ColumnAlteration</code>[] One or more alterations to apply to columns."},{"location":"javascript/interfaces/Table/#returns_12","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>void</code>&gt;</p>"},{"location":"javascript/interfaces/Table/#defined-in_14","title":"Defined in","text":"<p>index.ts:591</p>"},{"location":"javascript/interfaces/Table/#dropcolumns","title":"dropColumns","text":"<p>\u25b8 dropColumns(<code>columnNames</code>): <code>Promise</code>\\&lt;<code>void</code>&gt;</p> <p>Drop one or more columns from the dataset</p> <p>This is a metadata-only operation and does not remove the data from the underlying storage. In order to remove the data, you must subsequently call <code>compact_files</code> to rewrite the data without the removed columns and then call <code>cleanup_files</code> to remove the old files.</p>"},{"location":"javascript/interfaces/Table/#parameters_12","title":"Parameters","text":"Name Type Description <code>columnNames</code> <code>string</code>[] The names of the columns to drop. These can be nested column references (e.g. \"a.b.c\") or top-level column names (e.g. \"a\")."},{"location":"javascript/interfaces/Table/#returns_13","title":"Returns","text":"<p><code>Promise</code>\\&lt;<code>void</code>&gt;</p>"},{"location":"javascript/interfaces/Table/#defined-in_15","title":"Defined in","text":"<p>index.ts:605</p>"},{"location":"javascript/interfaces/Table/#filter","title":"filter","text":"<p>\u25b8 filter(<code>value</code>): <code>Query</code>\\&lt;<code>T</code>&gt;</p>"},{"location":"javascript/interfaces/Table/#parameters_13","title":"Parameters","text":"Name Type <code>value</code> <code>string</code>"},{"location":"javascript/interfaces/Table/#returns_14","title":"Returns","text":"<p><code>Query</code>\\&lt;<code>T</code>&gt;</p>"},{"location":"javascript/interfaces/Table/#defined-in_16","title":"Defined in","text":"<p>index.ts:569</p>"},{"location":"javascript/interfaces/Table/#withmiddleware","title":"withMiddleware","text":"<p>\u25b8 withMiddleware(<code>middleware</code>): <code>Table</code>\\&lt;<code>T</code>&gt;</p> <p>Instrument the behavior of this Table with middleware.</p> <p>The middleware will be called in the order they are added.</p> <p>Currently this functionality is only supported for remote tables.</p>"},{"location":"javascript/interfaces/Table/#parameters_14","title":"Parameters","text":"Name Type <code>middleware</code> <code>HttpMiddleware</code>"},{"location":"javascript/interfaces/Table/#returns_15","title":"Returns","text":"<p><code>Table</code>\\&lt;<code>T</code>&gt;</p> <ul> <li>this Table instrumented by the passed middleware</li> </ul>"},{"location":"javascript/interfaces/Table/#defined-in_17","title":"Defined in","text":"<p>index.ts:617</p>"},{"location":"javascript/interfaces/UpdateArgs/","title":"UpdateArgs","text":"<p>vectordb / Exports / UpdateArgs</p>"},{"location":"javascript/interfaces/UpdateArgs/#interface-updateargs","title":"Interface: UpdateArgs","text":""},{"location":"javascript/interfaces/UpdateArgs/#table-of-contents","title":"Table of contents","text":""},{"location":"javascript/interfaces/UpdateArgs/#properties","title":"Properties","text":"<ul> <li>values</li> <li>where</li> </ul>"},{"location":"javascript/interfaces/UpdateArgs/#properties_1","title":"Properties","text":""},{"location":"javascript/interfaces/UpdateArgs/#values","title":"values","text":"<p>\u2022 values: <code>Record</code>\\&lt;<code>string</code>, <code>Literal</code>&gt;</p> <p>A key-value map of updates. The keys are the column names, and the values are the new values to set</p>"},{"location":"javascript/interfaces/UpdateArgs/#defined-in","title":"Defined in","text":"<p>index.ts:652</p>"},{"location":"javascript/interfaces/UpdateArgs/#where","title":"where","text":"<p>\u2022 <code>Optional</code> where: <code>string</code></p> <p>A filter in the same format used by a sql WHERE clause. The filter may be empty, in which case all rows will be updated.</p>"},{"location":"javascript/interfaces/UpdateArgs/#defined-in_1","title":"Defined in","text":"<p>index.ts:646</p>"},{"location":"javascript/interfaces/UpdateSqlArgs/","title":"UpdateSqlArgs","text":"<p>vectordb / Exports / UpdateSqlArgs</p>"},{"location":"javascript/interfaces/UpdateSqlArgs/#interface-updatesqlargs","title":"Interface: UpdateSqlArgs","text":""},{"location":"javascript/interfaces/UpdateSqlArgs/#table-of-contents","title":"Table of contents","text":""},{"location":"javascript/interfaces/UpdateSqlArgs/#properties","title":"Properties","text":"<ul> <li>valuesSql</li> <li>where</li> </ul>"},{"location":"javascript/interfaces/UpdateSqlArgs/#properties_1","title":"Properties","text":""},{"location":"javascript/interfaces/UpdateSqlArgs/#valuessql","title":"valuesSql","text":"<p>\u2022 valuesSql: <code>Record</code>\\&lt;<code>string</code>, <code>string</code>&gt;</p> <p>A key-value map of updates. The keys are the column names, and the values are the new values to set as SQL expressions.</p>"},{"location":"javascript/interfaces/UpdateSqlArgs/#defined-in","title":"Defined in","text":"<p>index.ts:666</p>"},{"location":"javascript/interfaces/UpdateSqlArgs/#where","title":"where","text":"<p>\u2022 <code>Optional</code> where: <code>string</code></p> <p>A filter in the same format used by a sql WHERE clause. The filter may be empty, in which case all rows will be updated.</p>"},{"location":"javascript/interfaces/UpdateSqlArgs/#defined-in_1","title":"Defined in","text":"<p>index.ts:660</p>"},{"location":"javascript/interfaces/VectorIndex/","title":"VectorIndex","text":"<p>vectordb / Exports / VectorIndex</p>"},{"location":"javascript/interfaces/VectorIndex/#interface-vectorindex","title":"Interface: VectorIndex","text":""},{"location":"javascript/interfaces/VectorIndex/#table-of-contents","title":"Table of contents","text":""},{"location":"javascript/interfaces/VectorIndex/#properties","title":"Properties","text":"<ul> <li>columns</li> <li>name</li> <li>status</li> <li>uuid</li> </ul>"},{"location":"javascript/interfaces/VectorIndex/#properties_1","title":"Properties","text":""},{"location":"javascript/interfaces/VectorIndex/#columns","title":"columns","text":"<p>\u2022 columns: <code>string</code>[]</p>"},{"location":"javascript/interfaces/VectorIndex/#defined-in","title":"Defined in","text":"<p>index.ts:718</p>"},{"location":"javascript/interfaces/VectorIndex/#name","title":"name","text":"<p>\u2022 name: <code>string</code></p>"},{"location":"javascript/interfaces/VectorIndex/#defined-in_1","title":"Defined in","text":"<p>index.ts:719</p>"},{"location":"javascript/interfaces/VectorIndex/#status","title":"status","text":"<p>\u2022 status: <code>IndexStatus</code></p>"},{"location":"javascript/interfaces/VectorIndex/#defined-in_2","title":"Defined in","text":"<p>index.ts:721</p>"},{"location":"javascript/interfaces/VectorIndex/#uuid","title":"uuid","text":"<p>\u2022 uuid: <code>string</code></p>"},{"location":"javascript/interfaces/VectorIndex/#defined-in_3","title":"Defined in","text":"<p>index.ts:720</p>"},{"location":"javascript/interfaces/WriteOptions/","title":"WriteOptions","text":"<p>vectordb / Exports / WriteOptions</p>"},{"location":"javascript/interfaces/WriteOptions/#interface-writeoptions","title":"Interface: WriteOptions","text":"<p>Write options when creating a Table.</p>"},{"location":"javascript/interfaces/WriteOptions/#implemented-by","title":"Implemented by","text":"<ul> <li><code>DefaultWriteOptions</code></li> </ul>"},{"location":"javascript/interfaces/WriteOptions/#table-of-contents","title":"Table of contents","text":""},{"location":"javascript/interfaces/WriteOptions/#properties","title":"Properties","text":"<ul> <li>writeMode</li> </ul>"},{"location":"javascript/interfaces/WriteOptions/#properties_1","title":"Properties","text":""},{"location":"javascript/interfaces/WriteOptions/#writemode","title":"writeMode","text":"<p>\u2022 <code>Optional</code> writeMode: <code>WriteMode</code></p> <p>A WriteMode to use on this operation</p>"},{"location":"javascript/interfaces/WriteOptions/#defined-in","title":"Defined in","text":"<p>index.ts:1355</p>"},{"location":"js/","title":"Index","text":"<p>@lancedb/lancedb \u2022 Docs</p>"},{"location":"js/#lancedb-javascript-sdk","title":"LanceDB JavaScript SDK","text":"<p>A JavaScript library for LanceDB.</p>"},{"location":"js/#installation","title":"Installation","text":"<pre><code>npm install @lancedb/lancedb\n</code></pre> <p>This will download the appropriate native library for your platform. We currently support:</p> <ul> <li>Linux (x86_64 and aarch64 on glibc and musl)</li> <li>MacOS (Intel and ARM/M1/M2)</li> <li>Windows (x86_64 and aarch64)</li> </ul>"},{"location":"js/#usage","title":"Usage","text":""},{"location":"js/#basic-example","title":"Basic Example","text":"<pre><code>import * as lancedb from \"@lancedb/lancedb\";\nconst db = await lancedb.connect(\"data/sample-lancedb\");\nconst table = await db.createTable(\"my_table\", [\n  { id: 1, vector: [0.1, 1.0], item: \"foo\", price: 10.0 },\n  { id: 2, vector: [3.9, 0.5], item: \"bar\", price: 20.0 },\n]);\nconst results = await table.vectorSearch([0.1, 0.3]).limit(20).toArray();\nconsole.log(results);\n</code></pre> <p>The quickstart contains a more complete example.</p>"},{"location":"js/#development","title":"Development","text":"<p>See CONTRIBUTING.md for information on how to contribute to LanceDB.</p>"},{"location":"js/globals/","title":"Current SDK (@lancedb/lancedb)","text":"<p>@lancedb/lancedb \u2022 Docs</p>"},{"location":"js/globals/#lancedblancedb","title":"@lancedb/lancedb","text":""},{"location":"js/globals/#namespaces","title":"Namespaces","text":"<ul> <li>embedding</li> <li>rerankers</li> </ul>"},{"location":"js/globals/#enumerations","title":"Enumerations","text":"<ul> <li>FullTextQueryType</li> </ul>"},{"location":"js/globals/#classes","title":"Classes","text":"<ul> <li>BoostQuery</li> <li>Connection</li> <li>Index</li> <li>MakeArrowTableOptions</li> <li>MatchQuery</li> <li>MergeInsertBuilder</li> <li>MultiMatchQuery</li> <li>PhraseQuery</li> <li>Query</li> <li>QueryBase</li> <li>RecordBatchIterator</li> <li>Table</li> <li>TagContents</li> <li>Tags</li> <li>VectorColumnOptions</li> <li>VectorQuery</li> </ul>"},{"location":"js/globals/#interfaces","title":"Interfaces","text":"<ul> <li>AddColumnsResult</li> <li>AddColumnsSql</li> <li>AddDataOptions</li> <li>AddResult</li> <li>AlterColumnsResult</li> <li>ClientConfig</li> <li>ColumnAlteration</li> <li>CompactionStats</li> <li>ConnectionOptions</li> <li>CreateTableOptions</li> <li>DeleteResult</li> <li>DropColumnsResult</li> <li>ExecutableQuery</li> <li>FragmentStatistics</li> <li>FragmentSummaryStats</li> <li>FtsOptions</li> <li>FullTextQuery</li> <li>FullTextSearchOptions</li> <li>HnswPqOptions</li> <li>HnswSqOptions</li> <li>IndexConfig</li> <li>IndexOptions</li> <li>IndexStatistics</li> <li>IvfFlatOptions</li> <li>IvfPqOptions</li> <li>MergeResult</li> <li>OpenTableOptions</li> <li>OptimizeOptions</li> <li>OptimizeStats</li> <li>QueryExecutionOptions</li> <li>RemovalStats</li> <li>RetryConfig</li> <li>TableNamesOptions</li> <li>TableStatistics</li> <li>TimeoutConfig</li> <li>UpdateOptions</li> <li>UpdateResult</li> <li>Version</li> <li>WriteExecutionOptions</li> </ul>"},{"location":"js/globals/#type-aliases","title":"Type Aliases","text":"<ul> <li>Data</li> <li>DataLike</li> <li>FieldLike</li> <li>IntoSql</li> <li>IntoVector</li> <li>RecordBatchLike</li> <li>SchemaLike</li> <li>TableLike</li> </ul>"},{"location":"js/globals/#functions","title":"Functions","text":"<ul> <li>connect</li> <li>makeArrowTable</li> <li>packBits</li> </ul>"},{"location":"js/_media/CONTRIBUTING/","title":"Contributing to LanceDB Typescript","text":"<p>This document outlines the process for contributing to LanceDB Typescript.</p>"},{"location":"js/_media/CONTRIBUTING/#project-layout","title":"Project layout","text":"<p>The Typescript package is a wrapper around the Rust library, <code>lancedb</code>. We use the napi-rs library to create the bindings between Rust and Typescript.</p> <ul> <li><code>src/</code>: Rust bindings source code</li> <li><code>lancedb/</code>: Typescript package source code</li> <li><code>__test__/</code>: Unit tests</li> <li><code>examples/</code>: An npm package with the examples shown in the documentation</li> </ul>"},{"location":"js/_media/CONTRIBUTING/#development-environment","title":"Development environment","text":"<p>To set up your development environment, you will need to install the following:</p> <ol> <li>Node.js 14 or later</li> <li>Rust's package manager, Cargo. Use rustup to install.</li> <li>protoc (Protocol Buffers compiler)</li> </ol> <p>Initial setup:</p> <pre><code>npm install\n</code></pre>"},{"location":"js/_media/CONTRIBUTING/#commit-hooks","title":"Commit Hooks","text":"<p>It is highly recommended to install the pre-commit hooks to ensure that your code is formatted correctly and passes basic checks before committing:</p> <pre><code>pre-commit install\n</code></pre>"},{"location":"js/_media/CONTRIBUTING/#development","title":"Development","text":"<p>Most common development commands can be run using the npm scripts.</p> <p>Build the package</p> <pre><code>npm install\nnpm run build\n</code></pre> <p>Lint:</p> <pre><code>npm run lint\n</code></pre> <p>Format and fix lints:</p> <pre><code>npm run lint-fix\n</code></pre> <p>Run tests:</p> <pre><code>npm test\n</code></pre> <p>To run a single test:</p> <pre><code># Single file: table.test.ts\nnpm test -- table.test.ts\n# Single test: 'merge insert' in table.test.ts\nnpm test -- table.test.ts --testNamePattern=merge\\ insert\n</code></pre>"},{"location":"js/classes/BoostQuery/","title":"BoostQuery","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / BoostQuery</p>"},{"location":"js/classes/BoostQuery/#class-boostquery","title":"Class: BoostQuery","text":"<p>Represents a full-text query interface. This interface defines the structure and behavior for full-text queries, including methods to retrieve the query type and convert the query to a dictionary format.</p>"},{"location":"js/classes/BoostQuery/#implements","title":"Implements","text":"<ul> <li><code>FullTextQuery</code></li> </ul>"},{"location":"js/classes/BoostQuery/#constructors","title":"Constructors","text":""},{"location":"js/classes/BoostQuery/#new-boostquery","title":"new BoostQuery()","text":"<pre><code>new BoostQuery(\n   positive,\n   negative,\n   options?): BoostQuery\n</code></pre> <p>Creates an instance of BoostQuery. The boost returns documents that match the positive query, but penalizes those that match the negative query. the penalty is controlled by the <code>negativeBoost</code> parameter.</p>"},{"location":"js/classes/BoostQuery/#parameters","title":"Parameters","text":"<ul> <li> <p>positive: <code>FullTextQuery</code>     The positive query that boosts the relevance score.</p> </li> <li> <p>negative: <code>FullTextQuery</code>     The negative query that reduces the relevance score.</p> </li> <li> <p>options?     Optional parameters for the boost query.</p> <ul> <li><code>negativeBoost</code>: The boost factor for the negative query (default is 0.0).</li> </ul> </li> <li> <p>options.negativeBoost?: <code>number</code></p> </li> </ul>"},{"location":"js/classes/BoostQuery/#returns","title":"Returns","text":"<p><code>BoostQuery</code></p>"},{"location":"js/classes/BoostQuery/#methods","title":"Methods","text":""},{"location":"js/classes/BoostQuery/#querytype","title":"queryType()","text":"<pre><code>queryType(): FullTextQueryType\n</code></pre> <p>The type of the full-text query.</p>"},{"location":"js/classes/BoostQuery/#returns_1","title":"Returns","text":"<p><code>FullTextQueryType</code></p>"},{"location":"js/classes/BoostQuery/#implementation-of","title":"Implementation of","text":"<p><code>FullTextQuery</code>.<code>queryType</code></p>"},{"location":"js/classes/Connection/","title":"Connection","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / Connection</p>"},{"location":"js/classes/Connection/#class-abstract-connection","title":"Class: <code>abstract</code> Connection","text":"<p>A LanceDB Connection that allows you to open tables and create new ones.</p> <p>Connection could be local against filesystem or remote against a server.</p> <p>A Connection is intended to be a long lived object and may hold open resources such as HTTP connection pools.  This is generally fine and a single connection should be shared if it is going to be used many times. However, if you are finished with a connection, you may call close to eagerly free these resources.  Any call to a Connection method after it has been closed will result in an error.</p> <p>Closing a connection is optional.  Connections will automatically be closed when they are garbage collected.</p> <p>Any created tables are independent and will continue to work even if the underlying connection has been closed.</p>"},{"location":"js/classes/Connection/#methods","title":"Methods","text":""},{"location":"js/classes/Connection/#close","title":"close()","text":"<pre><code>abstract close(): void\n</code></pre> <p>Close the connection, releasing any underlying resources.</p> <p>It is safe to call this method multiple times.</p> <p>Any attempt to use the connection after it is closed will result in an error.</p>"},{"location":"js/classes/Connection/#returns","title":"Returns","text":"<p><code>void</code></p>"},{"location":"js/classes/Connection/#createemptytable","title":"createEmptyTable()","text":"<pre><code>abstract createEmptyTable(\n   name,\n   schema,\n   options?): Promise&lt;Table&gt;\n</code></pre> <p>Creates a new empty Table</p>"},{"location":"js/classes/Connection/#parameters","title":"Parameters","text":"<ul> <li> <p>name: <code>string</code>     The name of the table.</p> </li> <li> <p>schema: <code>SchemaLike</code>     The schema of the table</p> </li> <li> <p>options?: <code>Partial</code>&lt;<code>CreateTableOptions</code>&gt;</p> </li> </ul>"},{"location":"js/classes/Connection/#returns_1","title":"Returns","text":"<p><code>Promise</code>&lt;<code>Table</code>&gt;</p>"},{"location":"js/classes/Connection/#createtable","title":"createTable()","text":""},{"location":"js/classes/Connection/#createtableoptions","title":"createTable(options)","text":"<pre><code>abstract createTable(options): Promise&lt;Table&gt;\n</code></pre> <p>Creates a new Table and initialize it with new data.</p>"},{"location":"js/classes/Connection/#parameters_1","title":"Parameters","text":"<ul> <li>options: <code>object</code> &amp; <code>Partial</code>&lt;<code>CreateTableOptions</code>&gt;     The options object.</li> </ul>"},{"location":"js/classes/Connection/#returns_2","title":"Returns","text":"<p><code>Promise</code>&lt;<code>Table</code>&gt;</p>"},{"location":"js/classes/Connection/#createtablename-data-options","title":"createTable(name, data, options)","text":"<pre><code>abstract createTable(\n   name,\n   data,\n   options?): Promise&lt;Table&gt;\n</code></pre> <p>Creates a new Table and initialize it with new data.</p>"},{"location":"js/classes/Connection/#parameters_2","title":"Parameters","text":"<ul> <li> <p>name: <code>string</code>     The name of the table.</p> </li> <li> <p>data: <code>TableLike</code> | <code>Record</code>&lt;<code>string</code>, <code>unknown</code>&gt;[]     Non-empty Array of Records     to be inserted into the table</p> </li> <li> <p>options?: <code>Partial</code>&lt;<code>CreateTableOptions</code>&gt;</p> </li> </ul>"},{"location":"js/classes/Connection/#returns_3","title":"Returns","text":"<p><code>Promise</code>&lt;<code>Table</code>&gt;</p>"},{"location":"js/classes/Connection/#display","title":"display()","text":"<pre><code>abstract display(): string\n</code></pre> <p>Return a brief description of the connection</p>"},{"location":"js/classes/Connection/#returns_4","title":"Returns","text":"<p><code>string</code></p>"},{"location":"js/classes/Connection/#dropalltables","title":"dropAllTables()","text":"<pre><code>abstract dropAllTables(): Promise&lt;void&gt;\n</code></pre> <p>Drop all tables in the database.</p>"},{"location":"js/classes/Connection/#returns_5","title":"Returns","text":"<p><code>Promise</code>&lt;<code>void</code>&gt;</p>"},{"location":"js/classes/Connection/#droptable","title":"dropTable()","text":"<pre><code>abstract dropTable(name): Promise&lt;void&gt;\n</code></pre> <p>Drop an existing table.</p>"},{"location":"js/classes/Connection/#parameters_3","title":"Parameters","text":"<ul> <li>name: <code>string</code>     The name of the table to drop.</li> </ul>"},{"location":"js/classes/Connection/#returns_6","title":"Returns","text":"<p><code>Promise</code>&lt;<code>void</code>&gt;</p>"},{"location":"js/classes/Connection/#isopen","title":"isOpen()","text":"<pre><code>abstract isOpen(): boolean\n</code></pre> <p>Return true if the connection has not been closed</p>"},{"location":"js/classes/Connection/#returns_7","title":"Returns","text":"<p><code>boolean</code></p>"},{"location":"js/classes/Connection/#opentable","title":"openTable()","text":"<pre><code>abstract openTable(name, options?): Promise&lt;Table&gt;\n</code></pre> <p>Open a table in the database.</p>"},{"location":"js/classes/Connection/#parameters_4","title":"Parameters","text":"<ul> <li> <p>name: <code>string</code>     The name of the table</p> </li> <li> <p>options?: <code>Partial</code>&lt;<code>OpenTableOptions</code>&gt;</p> </li> </ul>"},{"location":"js/classes/Connection/#returns_8","title":"Returns","text":"<p><code>Promise</code>&lt;<code>Table</code>&gt;</p>"},{"location":"js/classes/Connection/#tablenames","title":"tableNames()","text":"<pre><code>abstract tableNames(options?): Promise&lt;string[]&gt;\n</code></pre> <p>List all the table names in this database.</p> <p>Tables will be returned in lexicographical order.</p>"},{"location":"js/classes/Connection/#parameters_5","title":"Parameters","text":"<ul> <li>options?: <code>Partial</code>&lt;<code>TableNamesOptions</code>&gt;     options to control the     paging / start point</li> </ul>"},{"location":"js/classes/Connection/#returns_9","title":"Returns","text":"<p><code>Promise</code>&lt;<code>string</code>[]&gt;</p>"},{"location":"js/classes/Index/","title":"Index","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / Index</p>"},{"location":"js/classes/Index/#class-index","title":"Class: Index","text":""},{"location":"js/classes/Index/#methods","title":"Methods","text":""},{"location":"js/classes/Index/#bitmap","title":"bitmap()","text":"<pre><code>static bitmap(): Index\n</code></pre> <p>Create a bitmap index.</p> <p>A <code>Bitmap</code> index stores a bitmap for each distinct value in the column for every row.</p> <p>This index works best for low-cardinality columns, where the number of unique values is small (i.e., less than a few hundreds).</p>"},{"location":"js/classes/Index/#returns","title":"Returns","text":"<p><code>Index</code></p>"},{"location":"js/classes/Index/#btree","title":"btree()","text":"<pre><code>static btree(): Index\n</code></pre> <p>Create a btree index</p> <p>A btree index is an index on a scalar columns.  The index stores a copy of the column in sorted order.  A header entry is created for each block of rows (currently the block size is fixed at 4096).  These header entries are stored in a separate cacheable structure (a btree).  To search for data the header is used to determine which blocks need to be read from disk.</p> <p>For example, a btree index in a table with 1Bi rows requires sizeof(Scalar) * 256Ki bytes of memory and will generally need to read sizeof(Scalar) * 4096 bytes to find the correct row ids.</p> <p>This index is good for scalar columns with mostly distinct values and does best when the query is highly selective.</p> <p>The btree index does not currently have any parameters though parameters such as the block size may be added in the future.</p>"},{"location":"js/classes/Index/#returns_1","title":"Returns","text":"<p><code>Index</code></p>"},{"location":"js/classes/Index/#fts","title":"fts()","text":"<pre><code>static fts(options?): Index\n</code></pre> <p>Create a full text search index</p> <p>A full text search index is an index on a string column, so that you can conduct full text searches on the column.</p> <p>The results of a full text search are ordered by relevance measured by BM25.</p> <p>You can combine filters with full text search.</p>"},{"location":"js/classes/Index/#parameters","title":"Parameters","text":"<ul> <li>options?: <code>Partial</code>&lt;<code>FtsOptions</code>&gt;</li> </ul>"},{"location":"js/classes/Index/#returns_2","title":"Returns","text":"<p><code>Index</code></p>"},{"location":"js/classes/Index/#hnswpq","title":"hnswPq()","text":"<pre><code>static hnswPq(options?): Index\n</code></pre> <p>Create a hnswPq index</p> <p>HNSW-PQ stands for Hierarchical Navigable Small World - Product Quantization. It is a variant of the HNSW algorithm that uses product quantization to compress the vectors.</p>"},{"location":"js/classes/Index/#parameters_1","title":"Parameters","text":"<ul> <li>options?: <code>Partial</code>&lt;<code>HnswPqOptions</code>&gt;</li> </ul>"},{"location":"js/classes/Index/#returns_3","title":"Returns","text":"<p><code>Index</code></p>"},{"location":"js/classes/Index/#hnswsq","title":"hnswSq()","text":"<pre><code>static hnswSq(options?): Index\n</code></pre> <p>Create a hnswSq index</p> <p>HNSW-SQ stands for Hierarchical Navigable Small World - Scalar Quantization. It is a variant of the HNSW algorithm that uses scalar quantization to compress the vectors.</p>"},{"location":"js/classes/Index/#parameters_2","title":"Parameters","text":"<ul> <li>options?: <code>Partial</code>&lt;<code>HnswSqOptions</code>&gt;</li> </ul>"},{"location":"js/classes/Index/#returns_4","title":"Returns","text":"<p><code>Index</code></p>"},{"location":"js/classes/Index/#ivfflat","title":"ivfFlat()","text":"<pre><code>static ivfFlat(options?): Index\n</code></pre> <p>Create an IvfFlat index</p> <p>This index groups vectors into partitions of similar vectors.  Each partition keeps track of a centroid which is the average value of all vectors in the group.</p> <p>During a query the centroids are compared with the query vector to find the closest partitions.  The vectors in these partitions are then searched to find the closest vectors.</p> <p>The partitioning process is called IVF and the <code>num_partitions</code> parameter controls how many groups to create.</p> <p>Note that training an IVF FLAT index on a large dataset is a slow operation and currently is also a memory intensive operation.</p>"},{"location":"js/classes/Index/#parameters_3","title":"Parameters","text":"<ul> <li>options?: <code>Partial</code>&lt;<code>IvfFlatOptions</code>&gt;</li> </ul>"},{"location":"js/classes/Index/#returns_5","title":"Returns","text":"<p><code>Index</code></p>"},{"location":"js/classes/Index/#ivfpq","title":"ivfPq()","text":"<pre><code>static ivfPq(options?): Index\n</code></pre> <p>Create an IvfPq index</p> <p>This index stores a compressed (quantized) copy of every vector.  These vectors are grouped into partitions of similar vectors.  Each partition keeps track of a centroid which is the average value of all vectors in the group.</p> <p>During a query the centroids are compared with the query vector to find the closest partitions.  The compressed vectors in these partitions are then searched to find the closest vectors.</p> <p>The compression scheme is called product quantization.  Each vector is divided into subvectors and then each subvector is quantized into a small number of bits.  the parameters <code>num_bits</code> and <code>num_subvectors</code> control this process, providing a tradeoff between index size (and thus search speed) and index accuracy.</p> <p>The partitioning process is called IVF and the <code>num_partitions</code> parameter controls how many groups to create.</p> <p>Note that training an IVF PQ index on a large dataset is a slow operation and currently is also a memory intensive operation.</p>"},{"location":"js/classes/Index/#parameters_4","title":"Parameters","text":"<ul> <li>options?: <code>Partial</code>&lt;<code>IvfPqOptions</code>&gt;</li> </ul>"},{"location":"js/classes/Index/#returns_6","title":"Returns","text":"<p><code>Index</code></p>"},{"location":"js/classes/Index/#labellist","title":"labelList()","text":"<pre><code>static labelList(): Index\n</code></pre> <p>Create a label list index.</p> <p>LabelList index is a scalar index that can be used on <code>List&lt;T&gt;</code> columns to support queries with <code>array_contains_all</code> and <code>array_contains_any</code> using an underlying bitmap index.</p>"},{"location":"js/classes/Index/#returns_7","title":"Returns","text":"<p><code>Index</code></p>"},{"location":"js/classes/MakeArrowTableOptions/","title":"MakeArrowTableOptions","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / MakeArrowTableOptions</p>"},{"location":"js/classes/MakeArrowTableOptions/#class-makearrowtableoptions","title":"Class: MakeArrowTableOptions","text":"<p>Options to control the makeArrowTable call.</p>"},{"location":"js/classes/MakeArrowTableOptions/#constructors","title":"Constructors","text":""},{"location":"js/classes/MakeArrowTableOptions/#new-makearrowtableoptions","title":"new MakeArrowTableOptions()","text":"<pre><code>new MakeArrowTableOptions(values?): MakeArrowTableOptions\n</code></pre>"},{"location":"js/classes/MakeArrowTableOptions/#parameters","title":"Parameters","text":"<ul> <li>values?: <code>Partial</code>&lt;<code>MakeArrowTableOptions</code>&gt;</li> </ul>"},{"location":"js/classes/MakeArrowTableOptions/#returns","title":"Returns","text":"<p><code>MakeArrowTableOptions</code></p>"},{"location":"js/classes/MakeArrowTableOptions/#properties","title":"Properties","text":""},{"location":"js/classes/MakeArrowTableOptions/#dictionaryencodestrings","title":"dictionaryEncodeStrings","text":"<pre><code>dictionaryEncodeStrings: boolean = false;\n</code></pre> <p>If true then string columns will be encoded with dictionary encoding</p> <p>Set this to true if your string columns tend to repeat the same values often.  For more precise control use the <code>schema</code> property to specify the data type for individual columns.</p> <p>If <code>schema</code> is provided then this property is ignored.</p>"},{"location":"js/classes/MakeArrowTableOptions/#embeddingfunction","title":"embeddingFunction?","text":"<pre><code>optional embeddingFunction: EmbeddingFunctionConfig;\n</code></pre>"},{"location":"js/classes/MakeArrowTableOptions/#embeddings","title":"embeddings?","text":"<pre><code>optional embeddings: EmbeddingFunction&lt;unknown, FunctionOptions&gt;;\n</code></pre>"},{"location":"js/classes/MakeArrowTableOptions/#schema","title":"schema?","text":"<pre><code>optional schema: SchemaLike;\n</code></pre>"},{"location":"js/classes/MakeArrowTableOptions/#vectorcolumns","title":"vectorColumns","text":"<pre><code>vectorColumns: Record&lt;string, VectorColumnOptions&gt;;\n</code></pre>"},{"location":"js/classes/MatchQuery/","title":"MatchQuery","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / MatchQuery</p>"},{"location":"js/classes/MatchQuery/#class-matchquery","title":"Class: MatchQuery","text":"<p>Represents a full-text query interface. This interface defines the structure and behavior for full-text queries, including methods to retrieve the query type and convert the query to a dictionary format.</p>"},{"location":"js/classes/MatchQuery/#implements","title":"Implements","text":"<ul> <li><code>FullTextQuery</code></li> </ul>"},{"location":"js/classes/MatchQuery/#constructors","title":"Constructors","text":""},{"location":"js/classes/MatchQuery/#new-matchquery","title":"new MatchQuery()","text":"<pre><code>new MatchQuery(\n   query,\n   column,\n   options?): MatchQuery\n</code></pre> <p>Creates an instance of MatchQuery.</p>"},{"location":"js/classes/MatchQuery/#parameters","title":"Parameters","text":"<ul> <li> <p>query: <code>string</code>     The text query to search for.</p> </li> <li> <p>column: <code>string</code>     The name of the column to search within.</p> </li> <li> <p>options?     Optional parameters for the match query.</p> <ul> <li><code>boost</code>: The boost factor for the query (default is 1.0).</li> <li><code>fuzziness</code>: The fuzziness level for the query (default is 0).</li> <li><code>maxExpansions</code>: The maximum number of terms to consider for fuzzy matching (default is 50).</li> </ul> </li> <li> <p>options.boost?: <code>number</code></p> </li> <li> <p>options.fuzziness?: <code>number</code></p> </li> <li> <p>options.maxExpansions?: <code>number</code></p> </li> </ul>"},{"location":"js/classes/MatchQuery/#returns","title":"Returns","text":"<p><code>MatchQuery</code></p>"},{"location":"js/classes/MatchQuery/#methods","title":"Methods","text":""},{"location":"js/classes/MatchQuery/#querytype","title":"queryType()","text":"<pre><code>queryType(): FullTextQueryType\n</code></pre> <p>The type of the full-text query.</p>"},{"location":"js/classes/MatchQuery/#returns_1","title":"Returns","text":"<p><code>FullTextQueryType</code></p>"},{"location":"js/classes/MatchQuery/#implementation-of","title":"Implementation of","text":"<p><code>FullTextQuery</code>.<code>queryType</code></p>"},{"location":"js/classes/MergeInsertBuilder/","title":"MergeInsertBuilder","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / MergeInsertBuilder</p>"},{"location":"js/classes/MergeInsertBuilder/#class-mergeinsertbuilder","title":"Class: MergeInsertBuilder","text":"<p>A builder used to create and run a merge insert operation</p>"},{"location":"js/classes/MergeInsertBuilder/#constructors","title":"Constructors","text":""},{"location":"js/classes/MergeInsertBuilder/#new-mergeinsertbuilder","title":"new MergeInsertBuilder()","text":"<pre><code>new MergeInsertBuilder(native, schema): MergeInsertBuilder\n</code></pre> <p>Construct a MergeInsertBuilder. Internal use only.</p>"},{"location":"js/classes/MergeInsertBuilder/#parameters","title":"Parameters","text":"<ul> <li> <p>native: <code>NativeMergeInsertBuilder</code></p> </li> <li> <p>schema: <code>Schema</code>&lt;<code>any</code>&gt; | <code>Promise</code>&lt;<code>Schema</code>&lt;<code>any</code>&gt;&gt;</p> </li> </ul>"},{"location":"js/classes/MergeInsertBuilder/#returns","title":"Returns","text":"<p><code>MergeInsertBuilder</code></p>"},{"location":"js/classes/MergeInsertBuilder/#methods","title":"Methods","text":""},{"location":"js/classes/MergeInsertBuilder/#execute","title":"execute()","text":"<pre><code>execute(data, execOptions?): Promise&lt;MergeResult&gt;\n</code></pre> <p>Executes the merge insert operation</p>"},{"location":"js/classes/MergeInsertBuilder/#parameters_1","title":"Parameters","text":"<ul> <li> <p>data: <code>Data</code></p> </li> <li> <p>execOptions?: <code>Partial</code>&lt;<code>WriteExecutionOptions</code>&gt;</p> </li> </ul>"},{"location":"js/classes/MergeInsertBuilder/#returns_1","title":"Returns","text":"<p><code>Promise</code>&lt;<code>MergeResult</code>&gt;</p> <p>the merge result</p>"},{"location":"js/classes/MergeInsertBuilder/#whenmatchedupdateall","title":"whenMatchedUpdateAll()","text":"<pre><code>whenMatchedUpdateAll(options?): MergeInsertBuilder\n</code></pre> <p>Rows that exist in both the source table (new data) and the target table (old data) will be updated, replacing the old row with the corresponding matching row.</p> <p>If there are multiple matches then the behavior is undefined. Currently this causes multiple copies of the row to be created but that behavior is subject to change.</p> <p>An optional condition may be specified.  If it is, then only matched rows that satisfy the condtion will be updated.  Any rows that do not satisfy the condition will be left as they are.  Failing to satisfy the condition does not cause a \"matched row\" to become a \"not matched\" row.</p> <p>The condition should be an SQL string.  Use the prefix target. to refer to rows in the target table (old data) and the prefix source. to refer to rows in the source table (new data).</p> <p>For example, \"target.last_update &lt; source.last_update\"</p>"},{"location":"js/classes/MergeInsertBuilder/#parameters_2","title":"Parameters","text":"<ul> <li> <p>options?</p> </li> <li> <p>options.where?: <code>string</code></p> </li> </ul>"},{"location":"js/classes/MergeInsertBuilder/#returns_2","title":"Returns","text":"<p><code>MergeInsertBuilder</code></p>"},{"location":"js/classes/MergeInsertBuilder/#whennotmatchedbysourcedelete","title":"whenNotMatchedBySourceDelete()","text":"<pre><code>whenNotMatchedBySourceDelete(options?): MergeInsertBuilder\n</code></pre> <p>Rows that exist only in the target table (old data) will be deleted.  An optional condition can be provided to limit what data is deleted.</p>"},{"location":"js/classes/MergeInsertBuilder/#parameters_3","title":"Parameters","text":"<ul> <li> <p>options?</p> </li> <li> <p>options.where?: <code>string</code>     An optional condition to limit what data is deleted</p> </li> </ul>"},{"location":"js/classes/MergeInsertBuilder/#returns_3","title":"Returns","text":"<p><code>MergeInsertBuilder</code></p>"},{"location":"js/classes/MergeInsertBuilder/#whennotmatchedinsertall","title":"whenNotMatchedInsertAll()","text":"<pre><code>whenNotMatchedInsertAll(): MergeInsertBuilder\n</code></pre> <p>Rows that exist only in the source table (new data) should be inserted into the target table.</p>"},{"location":"js/classes/MergeInsertBuilder/#returns_4","title":"Returns","text":"<p><code>MergeInsertBuilder</code></p>"},{"location":"js/classes/MultiMatchQuery/","title":"MultiMatchQuery","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / MultiMatchQuery</p>"},{"location":"js/classes/MultiMatchQuery/#class-multimatchquery","title":"Class: MultiMatchQuery","text":"<p>Represents a full-text query interface. This interface defines the structure and behavior for full-text queries, including methods to retrieve the query type and convert the query to a dictionary format.</p>"},{"location":"js/classes/MultiMatchQuery/#implements","title":"Implements","text":"<ul> <li><code>FullTextQuery</code></li> </ul>"},{"location":"js/classes/MultiMatchQuery/#constructors","title":"Constructors","text":""},{"location":"js/classes/MultiMatchQuery/#new-multimatchquery","title":"new MultiMatchQuery()","text":"<pre><code>new MultiMatchQuery(\n   query,\n   columns,\n   options?): MultiMatchQuery\n</code></pre> <p>Creates an instance of MultiMatchQuery.</p>"},{"location":"js/classes/MultiMatchQuery/#parameters","title":"Parameters","text":"<ul> <li> <p>query: <code>string</code>     The text query to search for across multiple columns.</p> </li> <li> <p>columns: <code>string</code>[]     An array of column names to search within.</p> </li> <li> <p>options?     Optional parameters for the multi-match query.</p> <ul> <li><code>boosts</code>: An array of boost factors for each column (default is 1.0 for all).</li> </ul> </li> <li> <p>options.boosts?: <code>number</code>[]</p> </li> </ul>"},{"location":"js/classes/MultiMatchQuery/#returns","title":"Returns","text":"<p><code>MultiMatchQuery</code></p>"},{"location":"js/classes/MultiMatchQuery/#methods","title":"Methods","text":""},{"location":"js/classes/MultiMatchQuery/#querytype","title":"queryType()","text":"<pre><code>queryType(): FullTextQueryType\n</code></pre> <p>The type of the full-text query.</p>"},{"location":"js/classes/MultiMatchQuery/#returns_1","title":"Returns","text":"<p><code>FullTextQueryType</code></p>"},{"location":"js/classes/MultiMatchQuery/#implementation-of","title":"Implementation of","text":"<p><code>FullTextQuery</code>.<code>queryType</code></p>"},{"location":"js/classes/PhraseQuery/","title":"PhraseQuery","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / PhraseQuery</p>"},{"location":"js/classes/PhraseQuery/#class-phrasequery","title":"Class: PhraseQuery","text":"<p>Represents a full-text query interface. This interface defines the structure and behavior for full-text queries, including methods to retrieve the query type and convert the query to a dictionary format.</p>"},{"location":"js/classes/PhraseQuery/#implements","title":"Implements","text":"<ul> <li><code>FullTextQuery</code></li> </ul>"},{"location":"js/classes/PhraseQuery/#constructors","title":"Constructors","text":""},{"location":"js/classes/PhraseQuery/#new-phrasequery","title":"new PhraseQuery()","text":"<pre><code>new PhraseQuery(query, column): PhraseQuery\n</code></pre> <p>Creates an instance of <code>PhraseQuery</code>.</p>"},{"location":"js/classes/PhraseQuery/#parameters","title":"Parameters","text":"<ul> <li> <p>query: <code>string</code>     The phrase to search for in the specified column.</p> </li> <li> <p>column: <code>string</code>     The name of the column to search within.</p> </li> </ul>"},{"location":"js/classes/PhraseQuery/#returns","title":"Returns","text":"<p><code>PhraseQuery</code></p>"},{"location":"js/classes/PhraseQuery/#methods","title":"Methods","text":""},{"location":"js/classes/PhraseQuery/#querytype","title":"queryType()","text":"<pre><code>queryType(): FullTextQueryType\n</code></pre> <p>The type of the full-text query.</p>"},{"location":"js/classes/PhraseQuery/#returns_1","title":"Returns","text":"<p><code>FullTextQueryType</code></p>"},{"location":"js/classes/PhraseQuery/#implementation-of","title":"Implementation of","text":"<p><code>FullTextQuery</code>.<code>queryType</code></p>"},{"location":"js/classes/Query/","title":"Query","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / Query</p>"},{"location":"js/classes/Query/#class-query","title":"Class: Query","text":"<p>A builder for LanceDB queries.</p>"},{"location":"js/classes/Query/#see","title":"See","text":"<p>Table#query, Table#search</p>"},{"location":"js/classes/Query/#extends","title":"Extends","text":"<ul> <li><code>QueryBase</code>&lt;<code>NativeQuery</code>&gt;</li> </ul>"},{"location":"js/classes/Query/#properties","title":"Properties","text":""},{"location":"js/classes/Query/#inner","title":"inner","text":"<pre><code>protected inner: Query | Promise&lt;Query&gt;;\n</code></pre>"},{"location":"js/classes/Query/#inherited-from","title":"Inherited from","text":"<p><code>QueryBase</code>.<code>inner</code></p>"},{"location":"js/classes/Query/#methods","title":"Methods","text":""},{"location":"js/classes/Query/#analyzeplan","title":"analyzePlan()","text":"<pre><code>analyzePlan(): Promise&lt;string&gt;\n</code></pre> <p>Executes the query and returns the physical query plan annotated with runtime metrics.</p> <p>This is useful for debugging and performance analysis, as it shows how the query was executed and includes metrics such as elapsed time, rows processed, and I/O statistics.</p>"},{"location":"js/classes/Query/#returns","title":"Returns","text":"<p><code>Promise</code>&lt;<code>string</code>&gt;</p> <p>A query execution plan with runtime metrics for each step.</p>"},{"location":"js/classes/Query/#example","title":"Example","text":"<pre><code>import * as lancedb from \"@lancedb/lancedb\"\n\nconst db = await lancedb.connect(\"./.lancedb\");\nconst table = await db.createTable(\"my_table\", [\n  { vector: [1.1, 0.9], id: \"1\" },\n]);\n\nconst plan = await table.query().nearestTo([0.5, 0.2]).analyzePlan();\n\nExample output (with runtime metrics inlined):\nAnalyzeExec verbose=true, metrics=[]\n ProjectionExec: expr=[id@3 as id, vector@0 as vector, _distance@2 as _distance], metrics=[output_rows=1, elapsed_compute=3.292\u00b5s]\n  Take: columns=\"vector, _rowid, _distance, (id)\", metrics=[output_rows=1, elapsed_compute=66.001\u00b5s, batches_processed=1, bytes_read=8, iops=1, requests=1]\n   CoalesceBatchesExec: target_batch_size=1024, metrics=[output_rows=1, elapsed_compute=3.333\u00b5s]\n    GlobalLimitExec: skip=0, fetch=10, metrics=[output_rows=1, elapsed_compute=167ns]\n     FilterExec: _distance@2 IS NOT NULL, metrics=[output_rows=1, elapsed_compute=8.542\u00b5s]\n      SortExec: TopK(fetch=10), expr=[_distance@2 ASC NULLS LAST], metrics=[output_rows=1, elapsed_compute=63.25\u00b5s, row_replacements=1]\n       KNNVectorDistance: metric=l2, metrics=[output_rows=1, elapsed_compute=114.333\u00b5s, output_batches=1]\n        LanceScan: uri=/path/to/data, projection=[vector], row_id=true, row_addr=false, ordered=false, metrics=[output_rows=1, elapsed_compute=103.626\u00b5s, bytes_read=549, iops=2, requests=2]\n</code></pre>"},{"location":"js/classes/Query/#inherited-from_1","title":"Inherited from","text":"<p><code>QueryBase</code>.<code>analyzePlan</code></p>"},{"location":"js/classes/Query/#execute","title":"execute()","text":"<pre><code>protected execute(options?): RecordBatchIterator\n</code></pre> <p>Execute the query and return the results as an</p>"},{"location":"js/classes/Query/#parameters","title":"Parameters","text":"<ul> <li>options?: <code>Partial</code>&lt;<code>QueryExecutionOptions</code>&gt;</li> </ul>"},{"location":"js/classes/Query/#returns_1","title":"Returns","text":"<p><code>RecordBatchIterator</code></p>"},{"location":"js/classes/Query/#see_1","title":"See","text":"<ul> <li>AsyncIterator of</li> <li>RecordBatch.</li> </ul> <p>By default, LanceDb will use many threads to calculate results and, when the result set is large, multiple batches will be processed at one time. This readahead is limited however and backpressure will be applied if this stream is consumed slowly (this constrains the maximum memory used by a single query)</p>"},{"location":"js/classes/Query/#inherited-from_2","title":"Inherited from","text":"<p><code>QueryBase</code>.<code>execute</code></p>"},{"location":"js/classes/Query/#explainplan","title":"explainPlan()","text":"<pre><code>explainPlan(verbose): Promise&lt;string&gt;\n</code></pre> <p>Generates an explanation of the query execution plan.</p>"},{"location":"js/classes/Query/#parameters_1","title":"Parameters","text":"<ul> <li>verbose: <code>boolean</code> = <code>false</code>     If true, provides a more detailed explanation. Defaults to false.</li> </ul>"},{"location":"js/classes/Query/#returns_2","title":"Returns","text":"<p><code>Promise</code>&lt;<code>string</code>&gt;</p> <p>A Promise that resolves to a string containing the query execution plan explanation.</p>"},{"location":"js/classes/Query/#example_1","title":"Example","text":"<pre><code>import * as lancedb from \"@lancedb/lancedb\"\nconst db = await lancedb.connect(\"./.lancedb\");\nconst table = await db.createTable(\"my_table\", [\n  { vector: [1.1, 0.9], id: \"1\" },\n]);\nconst plan = await table.query().nearestTo([0.5, 0.2]).explainPlan();\n</code></pre>"},{"location":"js/classes/Query/#inherited-from_3","title":"Inherited from","text":"<p><code>QueryBase</code>.<code>explainPlan</code></p>"},{"location":"js/classes/Query/#fastsearch","title":"fastSearch()","text":"<pre><code>fastSearch(): this\n</code></pre> <p>Skip searching un-indexed data. This can make search faster, but will miss any data that is not yet indexed.</p> <p>Use Table#optimize to index all un-indexed data.</p>"},{"location":"js/classes/Query/#returns_3","title":"Returns","text":"<p><code>this</code></p>"},{"location":"js/classes/Query/#inherited-from_4","title":"Inherited from","text":"<p><code>QueryBase</code>.<code>fastSearch</code></p>"},{"location":"js/classes/Query/#filter","title":"filter()","text":"<pre><code>filter(predicate): this\n</code></pre> <p>A filter statement to be applied to this query.</p>"},{"location":"js/classes/Query/#parameters_2","title":"Parameters","text":"<ul> <li>predicate: <code>string</code></li> </ul>"},{"location":"js/classes/Query/#returns_4","title":"Returns","text":"<p><code>this</code></p>"},{"location":"js/classes/Query/#see_2","title":"See","text":"<p>where</p>"},{"location":"js/classes/Query/#deprecated","title":"Deprecated","text":"<p>Use <code>where</code> instead</p>"},{"location":"js/classes/Query/#inherited-from_5","title":"Inherited from","text":"<p><code>QueryBase</code>.<code>filter</code></p>"},{"location":"js/classes/Query/#fulltextsearch","title":"fullTextSearch()","text":"<pre><code>fullTextSearch(query, options?): this\n</code></pre>"},{"location":"js/classes/Query/#parameters_3","title":"Parameters","text":"<ul> <li> <p>query: <code>string</code> | <code>FullTextQuery</code></p> </li> <li> <p>options?: <code>Partial</code>&lt;<code>FullTextSearchOptions</code>&gt;</p> </li> </ul>"},{"location":"js/classes/Query/#returns_5","title":"Returns","text":"<p><code>this</code></p>"},{"location":"js/classes/Query/#inherited-from_6","title":"Inherited from","text":"<p><code>QueryBase</code>.<code>fullTextSearch</code></p>"},{"location":"js/classes/Query/#limit","title":"limit()","text":"<pre><code>limit(limit): this\n</code></pre> <p>Set the maximum number of results to return.</p> <p>By default, a plain search has no limit.  If this method is not called then every valid row from the table will be returned.</p>"},{"location":"js/classes/Query/#parameters_4","title":"Parameters","text":"<ul> <li>limit: <code>number</code></li> </ul>"},{"location":"js/classes/Query/#returns_6","title":"Returns","text":"<p><code>this</code></p>"},{"location":"js/classes/Query/#inherited-from_7","title":"Inherited from","text":"<p><code>QueryBase</code>.<code>limit</code></p>"},{"location":"js/classes/Query/#nearestto","title":"nearestTo()","text":"<pre><code>nearestTo(vector): VectorQuery\n</code></pre> <p>Find the nearest vectors to the given query vector.</p> <p>This converts the query from a plain query to a vector query.</p> <p>This method will attempt to convert the input to the query vector expected by the embedding model.  If the input cannot be converted then an error will be thrown.</p> <p>By default, there is no embedding model, and the input should be an array-like object of numbers (something that can be used as input to Float32Array.from)</p> <p>If there is only one vector column (a column whose data type is a fixed size list of floats) then the column does not need to be specified. If there is more than one vector column you must use</p>"},{"location":"js/classes/Query/#parameters_5","title":"Parameters","text":"<ul> <li>vector: <code>IntoVector</code></li> </ul>"},{"location":"js/classes/Query/#returns_7","title":"Returns","text":"<p><code>VectorQuery</code></p>"},{"location":"js/classes/Query/#see_3","title":"See","text":"<ul> <li>VectorQuery#column  to specify which column you would like to compare with.</li> </ul> <p>If no index has been created on the vector column then a vector query will perform a distance comparison between the query vector and every vector in the database and then sort the results.  This is sometimes called a \"flat search\"</p> <p>For small databases, with a few hundred thousand vectors or less, this can be reasonably fast.  In larger databases you should create a vector index on the column.  If there is a vector index then an \"approximate\" nearest neighbor search (frequently called an ANN search) will be performed.  This search is much faster, but the results will be approximate.</p> <p>The query can be further parameterized using the returned builder.  There are various ANN search parameters that will let you fine tune your recall accuracy vs search latency.</p> <p>Vector searches always have a <code>limit</code>.  If <code>limit</code> has not been called then a default <code>limit</code> of 10 will be used.  - Query#limit</p>"},{"location":"js/classes/Query/#nearesttotext","title":"nearestToText()","text":"<pre><code>nearestToText(query, columns?): Query\n</code></pre>"},{"location":"js/classes/Query/#parameters_6","title":"Parameters","text":"<ul> <li> <p>query: <code>string</code> | <code>FullTextQuery</code></p> </li> <li> <p>columns?: <code>string</code>[]</p> </li> </ul>"},{"location":"js/classes/Query/#returns_8","title":"Returns","text":"<p><code>Query</code></p>"},{"location":"js/classes/Query/#offset","title":"offset()","text":"<pre><code>offset(offset): this\n</code></pre>"},{"location":"js/classes/Query/#parameters_7","title":"Parameters","text":"<ul> <li>offset: <code>number</code></li> </ul>"},{"location":"js/classes/Query/#returns_9","title":"Returns","text":"<p><code>this</code></p>"},{"location":"js/classes/Query/#inherited-from_8","title":"Inherited from","text":"<p><code>QueryBase</code>.<code>offset</code></p>"},{"location":"js/classes/Query/#select","title":"select()","text":"<pre><code>select(columns): this\n</code></pre> <p>Return only the specified columns.</p> <p>By default a query will return all columns from the table.  However, this can have a very significant impact on latency.  LanceDb stores data in a columnar fashion.  This means we can finely tune our I/O to select exactly the columns we need.</p> <p>As a best practice you should always limit queries to the columns that you need.  If you pass in an array of column names then only those columns will be returned.</p> <p>You can also use this method to create new \"dynamic\" columns based on your existing columns. For example, you may not care about \"a\" or \"b\" but instead simply want \"a + b\".  This is often seen in the SELECT clause of an SQL query (e.g. <code>SELECT a+b FROM my_table</code>).</p> <p>To create dynamic columns you can pass in a Map.  A column will be returned for each entry in the map.  The key provides the name of the column.  The value is an SQL string used to specify how the column is calculated. <p>For example, an SQL query might state <code>SELECT a + b AS combined, c</code>.  The equivalent input to this method would be:</p>"},{"location":"js/classes/Query/#parameters_8","title":"Parameters","text":"<ul> <li>columns: <code>string</code> | <code>string</code>[] | <code>Record</code>&lt;<code>string</code>, <code>string</code>&gt; | <code>Map</code>&lt;<code>string</code>, <code>string</code>&gt;</li> </ul>"},{"location":"js/classes/Query/#returns_10","title":"Returns","text":"<p><code>this</code></p>"},{"location":"js/classes/Query/#example_2","title":"Example","text":"<pre><code>new Map([[\"combined\", \"a + b\"], [\"c\", \"c\"]])\n\nColumns will always be returned in the order given, even if that order is different than\nthe order used when adding the data.\n\nNote that you can pass in a `Record&lt;string, string&gt;` (e.g. an object literal). This method\nuses `Object.entries` which should preserve the insertion order of the object.  However,\nobject insertion order is easy to get wrong and `Map` is more foolproof.\n</code></pre>"},{"location":"js/classes/Query/#inherited-from_9","title":"Inherited from","text":"<p><code>QueryBase</code>.<code>select</code></p>"},{"location":"js/classes/Query/#toarray","title":"toArray()","text":"<pre><code>toArray(options?): Promise&lt;any[]&gt;\n</code></pre> <p>Collect the results as an array of objects.</p>"},{"location":"js/classes/Query/#parameters_9","title":"Parameters","text":"<ul> <li>options?: <code>Partial</code>&lt;<code>QueryExecutionOptions</code>&gt;</li> </ul>"},{"location":"js/classes/Query/#returns_11","title":"Returns","text":"<p><code>Promise</code>&lt;<code>any</code>[]&gt;</p>"},{"location":"js/classes/Query/#inherited-from_10","title":"Inherited from","text":"<p><code>QueryBase</code>.<code>toArray</code></p>"},{"location":"js/classes/Query/#toarrow","title":"toArrow()","text":"<pre><code>toArrow(options?): Promise&lt;Table&lt;any&gt;&gt;\n</code></pre> <p>Collect the results as an Arrow</p>"},{"location":"js/classes/Query/#parameters_10","title":"Parameters","text":"<ul> <li>options?: <code>Partial</code>&lt;<code>QueryExecutionOptions</code>&gt;</li> </ul>"},{"location":"js/classes/Query/#returns_12","title":"Returns","text":"<p><code>Promise</code>&lt;<code>Table</code>&lt;<code>any</code>&gt;&gt;</p>"},{"location":"js/classes/Query/#see_4","title":"See","text":"<p>ArrowTable.</p>"},{"location":"js/classes/Query/#inherited-from_11","title":"Inherited from","text":"<p><code>QueryBase</code>.<code>toArrow</code></p>"},{"location":"js/classes/Query/#where","title":"where()","text":"<pre><code>where(predicate): this\n</code></pre> <p>A filter statement to be applied to this query.</p> <p>The filter should be supplied as an SQL query string.  For example:</p>"},{"location":"js/classes/Query/#parameters_11","title":"Parameters","text":"<ul> <li>predicate: <code>string</code></li> </ul>"},{"location":"js/classes/Query/#returns_13","title":"Returns","text":"<p><code>this</code></p>"},{"location":"js/classes/Query/#example_3","title":"Example","text":"<pre><code>x &gt; 10\ny &gt; 0 AND y &lt; 100\nx &gt; 5 OR y = 'test'\n\nFiltering performance can often be improved by creating a scalar index\non the filter column(s).\n</code></pre>"},{"location":"js/classes/Query/#inherited-from_12","title":"Inherited from","text":"<p><code>QueryBase</code>.<code>where</code></p>"},{"location":"js/classes/Query/#withrowid","title":"withRowId()","text":"<pre><code>withRowId(): this\n</code></pre> <p>Whether to return the row id in the results.</p> <p>This column can be used to match results between different queries. For example, to match results from a full text search and a vector search in order to perform hybrid search.</p>"},{"location":"js/classes/Query/#returns_14","title":"Returns","text":"<p><code>this</code></p>"},{"location":"js/classes/Query/#inherited-from_13","title":"Inherited from","text":"<p><code>QueryBase</code>.<code>withRowId</code></p>"},{"location":"js/classes/QueryBase/","title":"QueryBase","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / QueryBase</p>"},{"location":"js/classes/QueryBase/#class-querybasenativequerytype","title":"Class: QueryBase&lt;NativeQueryType&gt;","text":"<p>Common methods supported by all query types</p>"},{"location":"js/classes/QueryBase/#see","title":"See","text":"<ul> <li>Query</li> <li>VectorQuery</li> </ul>"},{"location":"js/classes/QueryBase/#extended-by","title":"Extended by","text":"<ul> <li><code>Query</code></li> <li><code>VectorQuery</code></li> </ul>"},{"location":"js/classes/QueryBase/#type-parameters","title":"Type Parameters","text":"<p>\u2022 NativeQueryType extends <code>NativeQuery</code> | <code>NativeVectorQuery</code></p>"},{"location":"js/classes/QueryBase/#implements","title":"Implements","text":"<ul> <li><code>AsyncIterable</code>&lt;<code>RecordBatch</code>&gt;</li> </ul>"},{"location":"js/classes/QueryBase/#properties","title":"Properties","text":""},{"location":"js/classes/QueryBase/#inner","title":"inner","text":"<pre><code>protected inner: NativeQueryType | Promise&lt;NativeQueryType&gt;;\n</code></pre>"},{"location":"js/classes/QueryBase/#methods","title":"Methods","text":""},{"location":"js/classes/QueryBase/#analyzeplan","title":"analyzePlan()","text":"<pre><code>analyzePlan(): Promise&lt;string&gt;\n</code></pre> <p>Executes the query and returns the physical query plan annotated with runtime metrics.</p> <p>This is useful for debugging and performance analysis, as it shows how the query was executed and includes metrics such as elapsed time, rows processed, and I/O statistics.</p>"},{"location":"js/classes/QueryBase/#returns","title":"Returns","text":"<p><code>Promise</code>&lt;<code>string</code>&gt;</p> <p>A query execution plan with runtime metrics for each step.</p>"},{"location":"js/classes/QueryBase/#example","title":"Example","text":"<pre><code>import * as lancedb from \"@lancedb/lancedb\"\n\nconst db = await lancedb.connect(\"./.lancedb\");\nconst table = await db.createTable(\"my_table\", [\n  { vector: [1.1, 0.9], id: \"1\" },\n]);\n\nconst plan = await table.query().nearestTo([0.5, 0.2]).analyzePlan();\n\nExample output (with runtime metrics inlined):\nAnalyzeExec verbose=true, metrics=[]\n ProjectionExec: expr=[id@3 as id, vector@0 as vector, _distance@2 as _distance], metrics=[output_rows=1, elapsed_compute=3.292\u00b5s]\n  Take: columns=\"vector, _rowid, _distance, (id)\", metrics=[output_rows=1, elapsed_compute=66.001\u00b5s, batches_processed=1, bytes_read=8, iops=1, requests=1]\n   CoalesceBatchesExec: target_batch_size=1024, metrics=[output_rows=1, elapsed_compute=3.333\u00b5s]\n    GlobalLimitExec: skip=0, fetch=10, metrics=[output_rows=1, elapsed_compute=167ns]\n     FilterExec: _distance@2 IS NOT NULL, metrics=[output_rows=1, elapsed_compute=8.542\u00b5s]\n      SortExec: TopK(fetch=10), expr=[_distance@2 ASC NULLS LAST], metrics=[output_rows=1, elapsed_compute=63.25\u00b5s, row_replacements=1]\n       KNNVectorDistance: metric=l2, metrics=[output_rows=1, elapsed_compute=114.333\u00b5s, output_batches=1]\n        LanceScan: uri=/path/to/data, projection=[vector], row_id=true, row_addr=false, ordered=false, metrics=[output_rows=1, elapsed_compute=103.626\u00b5s, bytes_read=549, iops=2, requests=2]\n</code></pre>"},{"location":"js/classes/QueryBase/#execute","title":"execute()","text":"<pre><code>protected execute(options?): RecordBatchIterator\n</code></pre> <p>Execute the query and return the results as an</p>"},{"location":"js/classes/QueryBase/#parameters","title":"Parameters","text":"<ul> <li>options?: <code>Partial</code>&lt;<code>QueryExecutionOptions</code>&gt;</li> </ul>"},{"location":"js/classes/QueryBase/#returns_1","title":"Returns","text":"<p><code>RecordBatchIterator</code></p>"},{"location":"js/classes/QueryBase/#see_1","title":"See","text":"<ul> <li>AsyncIterator of</li> <li>RecordBatch.</li> </ul> <p>By default, LanceDb will use many threads to calculate results and, when the result set is large, multiple batches will be processed at one time. This readahead is limited however and backpressure will be applied if this stream is consumed slowly (this constrains the maximum memory used by a single query)</p>"},{"location":"js/classes/QueryBase/#explainplan","title":"explainPlan()","text":"<pre><code>explainPlan(verbose): Promise&lt;string&gt;\n</code></pre> <p>Generates an explanation of the query execution plan.</p>"},{"location":"js/classes/QueryBase/#parameters_1","title":"Parameters","text":"<ul> <li>verbose: <code>boolean</code> = <code>false</code>     If true, provides a more detailed explanation. Defaults to false.</li> </ul>"},{"location":"js/classes/QueryBase/#returns_2","title":"Returns","text":"<p><code>Promise</code>&lt;<code>string</code>&gt;</p> <p>A Promise that resolves to a string containing the query execution plan explanation.</p>"},{"location":"js/classes/QueryBase/#example_1","title":"Example","text":"<pre><code>import * as lancedb from \"@lancedb/lancedb\"\nconst db = await lancedb.connect(\"./.lancedb\");\nconst table = await db.createTable(\"my_table\", [\n  { vector: [1.1, 0.9], id: \"1\" },\n]);\nconst plan = await table.query().nearestTo([0.5, 0.2]).explainPlan();\n</code></pre>"},{"location":"js/classes/QueryBase/#fastsearch","title":"fastSearch()","text":"<pre><code>fastSearch(): this\n</code></pre> <p>Skip searching un-indexed data. This can make search faster, but will miss any data that is not yet indexed.</p> <p>Use Table#optimize to index all un-indexed data.</p>"},{"location":"js/classes/QueryBase/#returns_3","title":"Returns","text":"<p><code>this</code></p>"},{"location":"js/classes/QueryBase/#filter","title":"filter()","text":"<pre><code>filter(predicate): this\n</code></pre> <p>A filter statement to be applied to this query.</p>"},{"location":"js/classes/QueryBase/#parameters_2","title":"Parameters","text":"<ul> <li>predicate: <code>string</code></li> </ul>"},{"location":"js/classes/QueryBase/#returns_4","title":"Returns","text":"<p><code>this</code></p>"},{"location":"js/classes/QueryBase/#see_2","title":"See","text":"<p>where</p>"},{"location":"js/classes/QueryBase/#deprecated","title":"Deprecated","text":"<p>Use <code>where</code> instead</p>"},{"location":"js/classes/QueryBase/#fulltextsearch","title":"fullTextSearch()","text":"<pre><code>fullTextSearch(query, options?): this\n</code></pre>"},{"location":"js/classes/QueryBase/#parameters_3","title":"Parameters","text":"<ul> <li> <p>query: <code>string</code> | <code>FullTextQuery</code></p> </li> <li> <p>options?: <code>Partial</code>&lt;<code>FullTextSearchOptions</code>&gt;</p> </li> </ul>"},{"location":"js/classes/QueryBase/#returns_5","title":"Returns","text":"<p><code>this</code></p>"},{"location":"js/classes/QueryBase/#limit","title":"limit()","text":"<pre><code>limit(limit): this\n</code></pre> <p>Set the maximum number of results to return.</p> <p>By default, a plain search has no limit.  If this method is not called then every valid row from the table will be returned.</p>"},{"location":"js/classes/QueryBase/#parameters_4","title":"Parameters","text":"<ul> <li>limit: <code>number</code></li> </ul>"},{"location":"js/classes/QueryBase/#returns_6","title":"Returns","text":"<p><code>this</code></p>"},{"location":"js/classes/QueryBase/#offset","title":"offset()","text":"<pre><code>offset(offset): this\n</code></pre>"},{"location":"js/classes/QueryBase/#parameters_5","title":"Parameters","text":"<ul> <li>offset: <code>number</code></li> </ul>"},{"location":"js/classes/QueryBase/#returns_7","title":"Returns","text":"<p><code>this</code></p>"},{"location":"js/classes/QueryBase/#select","title":"select()","text":"<pre><code>select(columns): this\n</code></pre> <p>Return only the specified columns.</p> <p>By default a query will return all columns from the table.  However, this can have a very significant impact on latency.  LanceDb stores data in a columnar fashion.  This means we can finely tune our I/O to select exactly the columns we need.</p> <p>As a best practice you should always limit queries to the columns that you need.  If you pass in an array of column names then only those columns will be returned.</p> <p>You can also use this method to create new \"dynamic\" columns based on your existing columns. For example, you may not care about \"a\" or \"b\" but instead simply want \"a + b\".  This is often seen in the SELECT clause of an SQL query (e.g. <code>SELECT a+b FROM my_table</code>).</p> <p>To create dynamic columns you can pass in a Map.  A column will be returned for each entry in the map.  The key provides the name of the column.  The value is an SQL string used to specify how the column is calculated. <p>For example, an SQL query might state <code>SELECT a + b AS combined, c</code>.  The equivalent input to this method would be:</p>"},{"location":"js/classes/QueryBase/#parameters_6","title":"Parameters","text":"<ul> <li>columns: <code>string</code> | <code>string</code>[] | <code>Record</code>&lt;<code>string</code>, <code>string</code>&gt; | <code>Map</code>&lt;<code>string</code>, <code>string</code>&gt;</li> </ul>"},{"location":"js/classes/QueryBase/#returns_8","title":"Returns","text":"<p><code>this</code></p>"},{"location":"js/classes/QueryBase/#example_2","title":"Example","text":"<pre><code>new Map([[\"combined\", \"a + b\"], [\"c\", \"c\"]])\n\nColumns will always be returned in the order given, even if that order is different than\nthe order used when adding the data.\n\nNote that you can pass in a `Record&lt;string, string&gt;` (e.g. an object literal). This method\nuses `Object.entries` which should preserve the insertion order of the object.  However,\nobject insertion order is easy to get wrong and `Map` is more foolproof.\n</code></pre>"},{"location":"js/classes/QueryBase/#toarray","title":"toArray()","text":"<pre><code>toArray(options?): Promise&lt;any[]&gt;\n</code></pre> <p>Collect the results as an array of objects.</p>"},{"location":"js/classes/QueryBase/#parameters_7","title":"Parameters","text":"<ul> <li>options?: <code>Partial</code>&lt;<code>QueryExecutionOptions</code>&gt;</li> </ul>"},{"location":"js/classes/QueryBase/#returns_9","title":"Returns","text":"<p><code>Promise</code>&lt;<code>any</code>[]&gt;</p>"},{"location":"js/classes/QueryBase/#toarrow","title":"toArrow()","text":"<pre><code>toArrow(options?): Promise&lt;Table&lt;any&gt;&gt;\n</code></pre> <p>Collect the results as an Arrow</p>"},{"location":"js/classes/QueryBase/#parameters_8","title":"Parameters","text":"<ul> <li>options?: <code>Partial</code>&lt;<code>QueryExecutionOptions</code>&gt;</li> </ul>"},{"location":"js/classes/QueryBase/#returns_10","title":"Returns","text":"<p><code>Promise</code>&lt;<code>Table</code>&lt;<code>any</code>&gt;&gt;</p>"},{"location":"js/classes/QueryBase/#see_3","title":"See","text":"<p>ArrowTable.</p>"},{"location":"js/classes/QueryBase/#where","title":"where()","text":"<pre><code>where(predicate): this\n</code></pre> <p>A filter statement to be applied to this query.</p> <p>The filter should be supplied as an SQL query string.  For example:</p>"},{"location":"js/classes/QueryBase/#parameters_9","title":"Parameters","text":"<ul> <li>predicate: <code>string</code></li> </ul>"},{"location":"js/classes/QueryBase/#returns_11","title":"Returns","text":"<p><code>this</code></p>"},{"location":"js/classes/QueryBase/#example_3","title":"Example","text":"<pre><code>x &gt; 10\ny &gt; 0 AND y &lt; 100\nx &gt; 5 OR y = 'test'\n\nFiltering performance can often be improved by creating a scalar index\non the filter column(s).\n</code></pre>"},{"location":"js/classes/QueryBase/#withrowid","title":"withRowId()","text":"<pre><code>withRowId(): this\n</code></pre> <p>Whether to return the row id in the results.</p> <p>This column can be used to match results between different queries. For example, to match results from a full text search and a vector search in order to perform hybrid search.</p>"},{"location":"js/classes/QueryBase/#returns_12","title":"Returns","text":"<p><code>this</code></p>"},{"location":"js/classes/RecordBatchIterator/","title":"RecordBatchIterator","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / RecordBatchIterator</p>"},{"location":"js/classes/RecordBatchIterator/#class-recordbatchiterator","title":"Class: RecordBatchIterator","text":""},{"location":"js/classes/RecordBatchIterator/#implements","title":"Implements","text":"<ul> <li><code>AsyncIterator</code>&lt;<code>RecordBatch</code>&gt;</li> </ul>"},{"location":"js/classes/RecordBatchIterator/#constructors","title":"Constructors","text":""},{"location":"js/classes/RecordBatchIterator/#new-recordbatchiterator","title":"new RecordBatchIterator()","text":"<pre><code>new RecordBatchIterator(promise?): RecordBatchIterator\n</code></pre>"},{"location":"js/classes/RecordBatchIterator/#parameters","title":"Parameters","text":"<ul> <li>promise?: <code>Promise</code>&lt;<code>RecordBatchIterator</code>&gt;</li> </ul>"},{"location":"js/classes/RecordBatchIterator/#returns","title":"Returns","text":"<p><code>RecordBatchIterator</code></p>"},{"location":"js/classes/RecordBatchIterator/#methods","title":"Methods","text":""},{"location":"js/classes/RecordBatchIterator/#next","title":"next()","text":"<pre><code>next(): Promise&lt;IteratorResult&lt;RecordBatch&lt;any&gt;, any&gt;&gt;\n</code></pre>"},{"location":"js/classes/RecordBatchIterator/#returns_1","title":"Returns","text":"<p><code>Promise</code>&lt;<code>IteratorResult</code>&lt;<code>RecordBatch</code>&lt;<code>any</code>&gt;, <code>any</code>&gt;&gt;</p>"},{"location":"js/classes/RecordBatchIterator/#implementation-of","title":"Implementation of","text":"<p><code>AsyncIterator.next</code></p>"},{"location":"js/classes/Table/","title":"Table","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / Table</p>"},{"location":"js/classes/Table/#class-abstract-table","title":"Class: <code>abstract</code> Table","text":"<p>A Table is a collection of Records in a LanceDB Database.</p> <p>A Table object is expected to be long lived and reused for multiple operations. Table objects will cache a certain amount of index data in memory.  This cache will be freed when the Table is garbage collected.  To eagerly free the cache you can call the <code>close</code> method.  Once the Table is closed, it cannot be used for any further operations.</p> <p>Tables are created using the methods Connection#createTable and Connection#createEmptyTable. Existing tables are opened using Connection#openTable.</p> <p>Closing a table is optional.  It not closed, it will be closed when it is garbage collected.</p>"},{"location":"js/classes/Table/#accessors","title":"Accessors","text":""},{"location":"js/classes/Table/#name","title":"name","text":"<pre><code>get abstract name(): string\n</code></pre> <p>Returns the name of the table</p>"},{"location":"js/classes/Table/#returns","title":"Returns","text":"<p><code>string</code></p>"},{"location":"js/classes/Table/#methods","title":"Methods","text":""},{"location":"js/classes/Table/#add","title":"add()","text":"<pre><code>abstract add(data, options?): Promise&lt;AddResult&gt;\n</code></pre> <p>Insert records into this Table.</p>"},{"location":"js/classes/Table/#parameters","title":"Parameters","text":"<ul> <li> <p>data: <code>Data</code>     Records to be inserted into the Table</p> </li> <li> <p>options?: <code>Partial</code>&lt;<code>AddDataOptions</code>&gt;</p> </li> </ul>"},{"location":"js/classes/Table/#returns_1","title":"Returns","text":"<p><code>Promise</code>&lt;<code>AddResult</code>&gt;</p> <p>A promise that resolves to an object containing the new version number of the table</p>"},{"location":"js/classes/Table/#addcolumns","title":"addColumns()","text":"<pre><code>abstract addColumns(newColumnTransforms): Promise&lt;AddColumnsResult&gt;\n</code></pre> <p>Add new columns with defined values.</p>"},{"location":"js/classes/Table/#parameters_1","title":"Parameters","text":"<ul> <li>newColumnTransforms: <code>AddColumnsSql</code>[]     pairs of column names and     the SQL expression to use to calculate the value of the new column. These     expressions will be evaluated for each row in the table, and can     reference existing columns in the table.</li> </ul>"},{"location":"js/classes/Table/#returns_2","title":"Returns","text":"<p><code>Promise</code>&lt;<code>AddColumnsResult</code>&gt;</p> <p>A promise that resolves to an object containing the new version number of the table after adding the columns.</p>"},{"location":"js/classes/Table/#altercolumns","title":"alterColumns()","text":"<pre><code>abstract alterColumns(columnAlterations): Promise&lt;AlterColumnsResult&gt;\n</code></pre> <p>Alter the name or nullability of columns.</p>"},{"location":"js/classes/Table/#parameters_2","title":"Parameters","text":"<ul> <li>columnAlterations: <code>ColumnAlteration</code>[]     One or more alterations to     apply to columns.</li> </ul>"},{"location":"js/classes/Table/#returns_3","title":"Returns","text":"<p><code>Promise</code>&lt;<code>AlterColumnsResult</code>&gt;</p> <p>A promise that resolves to an object containing the new version number of the table after altering the columns.</p>"},{"location":"js/classes/Table/#checkout","title":"checkout()","text":"<pre><code>abstract checkout(version): Promise&lt;void&gt;\n</code></pre> <p>Checks out a specific version of the table This is an in-place operation.</p> <p>This allows viewing previous versions of the table. If you wish to keep writing to the dataset starting from an old version, then use the <code>restore</code> function.</p> <p>Calling this method will set the table into time-travel mode. If you wish to return to standard mode, call <code>checkoutLatest</code>.</p>"},{"location":"js/classes/Table/#parameters_3","title":"Parameters","text":"<ul> <li>version: <code>string</code> | <code>number</code>     The version to checkout, could be version number or tag</li> </ul>"},{"location":"js/classes/Table/#returns_4","title":"Returns","text":"<p><code>Promise</code>&lt;<code>void</code>&gt;</p>"},{"location":"js/classes/Table/#example","title":"Example","text":"<pre><code>import * as lancedb from \"@lancedb/lancedb\"\nconst db = await lancedb.connect(\"./.lancedb\");\nconst table = await db.createTable(\"my_table\", [\n  { vector: [1.1, 0.9], type: \"vector\" },\n]);\n\nconsole.log(await table.version()); // 1\nconsole.log(table.display());\nawait table.add([{ vector: [0.5, 0.2], type: \"vector\" }]);\nawait table.checkout(1);\nconsole.log(await table.version()); // 2\n</code></pre>"},{"location":"js/classes/Table/#checkoutlatest","title":"checkoutLatest()","text":"<pre><code>abstract checkoutLatest(): Promise&lt;void&gt;\n</code></pre> <p>Checkout the latest version of the table. This is an in-place operation.</p> <p>The table will be set back into standard mode, and will track the latest version of the table.</p>"},{"location":"js/classes/Table/#returns_5","title":"Returns","text":"<p><code>Promise</code>&lt;<code>void</code>&gt;</p>"},{"location":"js/classes/Table/#close","title":"close()","text":"<pre><code>abstract close(): void\n</code></pre> <p>Close the table, releasing any underlying resources.</p> <p>It is safe to call this method multiple times.</p> <p>Any attempt to use the table after it is closed will result in an error.</p>"},{"location":"js/classes/Table/#returns_6","title":"Returns","text":"<p><code>void</code></p>"},{"location":"js/classes/Table/#countrows","title":"countRows()","text":"<pre><code>abstract countRows(filter?): Promise&lt;number&gt;\n</code></pre> <p>Count the total number of rows in the dataset.</p>"},{"location":"js/classes/Table/#parameters_4","title":"Parameters","text":"<ul> <li>filter?: <code>string</code></li> </ul>"},{"location":"js/classes/Table/#returns_7","title":"Returns","text":"<p><code>Promise</code>&lt;<code>number</code>&gt;</p>"},{"location":"js/classes/Table/#createindex","title":"createIndex()","text":"<pre><code>abstract createIndex(column, options?): Promise&lt;void&gt;\n</code></pre> <p>Create an index to speed up queries.</p> <p>Indices can be created on vector columns or scalar columns. Indices on vector columns will speed up vector searches. Indices on scalar columns will speed up filtering (in both vector and non-vector searches)</p> <p>We currently don't support custom named indexes. The index name will always be <code>${column}_idx</code>.</p>"},{"location":"js/classes/Table/#parameters_5","title":"Parameters","text":"<ul> <li> <p>column: <code>string</code></p> </li> <li> <p>options?: <code>Partial</code>&lt;<code>IndexOptions</code>&gt;</p> </li> </ul>"},{"location":"js/classes/Table/#returns_8","title":"Returns","text":"<p><code>Promise</code>&lt;<code>void</code>&gt;</p>"},{"location":"js/classes/Table/#examples","title":"Examples","text":"<pre><code>// If the column has a vector (fixed size list) data type then\n// an IvfPq vector index will be created.\nconst table = await conn.openTable(\"my_table\");\nawait table.createIndex(\"vector\");\n</code></pre> <pre><code>// For advanced control over vector index creation you can specify\n// the index type and options.\nconst table = await conn.openTable(\"my_table\");\nawait table.createIndex(\"vector\", {\n  config: lancedb.Index.ivfPq({\n    numPartitions: 128,\n    numSubVectors: 16,\n  }),\n});\n</code></pre> <pre><code>// Or create a Scalar index\nawait table.createIndex(\"my_float_col\");\n</code></pre>"},{"location":"js/classes/Table/#delete","title":"delete()","text":"<pre><code>abstract delete(predicate): Promise&lt;DeleteResult&gt;\n</code></pre> <p>Delete the rows that satisfy the predicate.</p>"},{"location":"js/classes/Table/#parameters_6","title":"Parameters","text":"<ul> <li>predicate: <code>string</code></li> </ul>"},{"location":"js/classes/Table/#returns_9","title":"Returns","text":"<p><code>Promise</code>&lt;<code>DeleteResult</code>&gt;</p> <p>A promise that resolves to an object containing the new version number of the table</p>"},{"location":"js/classes/Table/#display","title":"display()","text":"<pre><code>abstract display(): string\n</code></pre> <p>Return a brief description of the table</p>"},{"location":"js/classes/Table/#returns_10","title":"Returns","text":"<p><code>string</code></p>"},{"location":"js/classes/Table/#dropcolumns","title":"dropColumns()","text":"<pre><code>abstract dropColumns(columnNames): Promise&lt;DropColumnsResult&gt;\n</code></pre> <p>Drop one or more columns from the dataset</p> <p>This is a metadata-only operation and does not remove the data from the underlying storage. In order to remove the data, you must subsequently call <code>compact_files</code> to rewrite the data without the removed columns and then call <code>cleanup_files</code> to remove the old files.</p>"},{"location":"js/classes/Table/#parameters_7","title":"Parameters","text":"<ul> <li>columnNames: <code>string</code>[]     The names of the columns to drop. These can     be nested column references (e.g. \"a.b.c\") or top-level column names     (e.g. \"a\").</li> </ul>"},{"location":"js/classes/Table/#returns_11","title":"Returns","text":"<p><code>Promise</code>&lt;<code>DropColumnsResult</code>&gt;</p> <p>A promise that resolves to an object containing the new version number of the table after dropping the columns.</p>"},{"location":"js/classes/Table/#dropindex","title":"dropIndex()","text":"<pre><code>abstract dropIndex(name): Promise&lt;void&gt;\n</code></pre> <p>Drop an index from the table.</p>"},{"location":"js/classes/Table/#parameters_8","title":"Parameters","text":"<ul> <li>name: <code>string</code>     The name of the index.     This does not delete the index from disk, it just removes it from the table.     To delete the index, run Table#optimize after dropping the index.     Use Table.listIndices to find the names of the indices.</li> </ul>"},{"location":"js/classes/Table/#returns_12","title":"Returns","text":"<p><code>Promise</code>&lt;<code>void</code>&gt;</p>"},{"location":"js/classes/Table/#indexstats","title":"indexStats()","text":"<pre><code>abstract indexStats(name): Promise&lt;undefined | IndexStatistics&gt;\n</code></pre> <p>List all the stats of a specified index</p>"},{"location":"js/classes/Table/#parameters_9","title":"Parameters","text":"<ul> <li>name: <code>string</code>     The name of the index.</li> </ul>"},{"location":"js/classes/Table/#returns_13","title":"Returns","text":"<p><code>Promise</code>&lt;<code>undefined</code> | <code>IndexStatistics</code>&gt;</p> <p>The stats of the index. If the index does not exist, it will return undefined</p> <p>Use Table.listIndices to find the names of the indices.</p>"},{"location":"js/classes/Table/#isopen","title":"isOpen()","text":"<pre><code>abstract isOpen(): boolean\n</code></pre> <p>Return true if the table has not been closed</p>"},{"location":"js/classes/Table/#returns_14","title":"Returns","text":"<p><code>boolean</code></p>"},{"location":"js/classes/Table/#listindices","title":"listIndices()","text":"<pre><code>abstract listIndices(): Promise&lt;IndexConfig[]&gt;\n</code></pre> <p>List all indices that have been created with Table.createIndex</p>"},{"location":"js/classes/Table/#returns_15","title":"Returns","text":"<p><code>Promise</code>&lt;<code>IndexConfig</code>[]&gt;</p>"},{"location":"js/classes/Table/#listversions","title":"listVersions()","text":"<pre><code>abstract listVersions(): Promise&lt;Version[]&gt;\n</code></pre> <p>List all the versions of the table</p>"},{"location":"js/classes/Table/#returns_16","title":"Returns","text":"<p><code>Promise</code>&lt;<code>Version</code>[]&gt;</p>"},{"location":"js/classes/Table/#mergeinsert","title":"mergeInsert()","text":"<pre><code>abstract mergeInsert(on): MergeInsertBuilder\n</code></pre>"},{"location":"js/classes/Table/#parameters_10","title":"Parameters","text":"<ul> <li>on: <code>string</code> | <code>string</code>[]</li> </ul>"},{"location":"js/classes/Table/#returns_17","title":"Returns","text":"<p><code>MergeInsertBuilder</code></p>"},{"location":"js/classes/Table/#optimize","title":"optimize()","text":"<pre><code>abstract optimize(options?): Promise&lt;OptimizeStats&gt;\n</code></pre> <p>Optimize the on-disk data and indices for better performance.</p> <p>Modeled after <code>VACUUM</code> in PostgreSQL.</p> <p>Optimization covers three operations:</p> <ul> <li>Compaction: Merges small files into larger ones</li> <li>Prune: Removes old versions of the dataset</li> <li>Index: Optimizes the indices, adding new data to existing indices</li> </ul> <p>Experimental API</p> <p>The optimization process is undergoing active development and may change.  Our goal with these changes is to improve the performance of optimization and  reduce the complexity.</p> <p>That being said, it is essential today to run optimize if you want the best  performance.  It should be stable and safe to use in production, but it our  hope that the API may be simplified (or not even need to be called) in the  future.</p> <p>The frequency an application shoudl call optimize is based on the frequency of  data modifications.  If data is frequently added, deleted, or updated then  optimize should be run frequently.  A good rule of thumb is to run optimize if  you have added or modified 100,000 or more records or run more than 20 data  modification operations.</p>"},{"location":"js/classes/Table/#parameters_11","title":"Parameters","text":"<ul> <li>options?: <code>Partial</code>&lt;<code>OptimizeOptions</code>&gt;</li> </ul>"},{"location":"js/classes/Table/#returns_18","title":"Returns","text":"<p><code>Promise</code>&lt;<code>OptimizeStats</code>&gt;</p>"},{"location":"js/classes/Table/#prewarmindex","title":"prewarmIndex()","text":"<pre><code>abstract prewarmIndex(name): Promise&lt;void&gt;\n</code></pre> <p>Prewarm an index in the table.</p>"},{"location":"js/classes/Table/#parameters_12","title":"Parameters","text":"<ul> <li>name: <code>string</code>     The name of the index.     This will load the index into memory.  This may reduce the cold-start time for     future queries.  If the index does not fit in the cache then this call may be     wasteful.</li> </ul>"},{"location":"js/classes/Table/#returns_19","title":"Returns","text":"<p><code>Promise</code>&lt;<code>void</code>&gt;</p>"},{"location":"js/classes/Table/#query","title":"query()","text":"<pre><code>abstract query(): Query\n</code></pre> <p>Create a Query Builder.</p> <p>Queries allow you to search your existing data.  By default the query will return all the data in the table in no particular order.  The builder returned by this method can be used to control the query using filtering, vector similarity, sorting, and more.</p> <p>Note: By default, all columns are returned.  For best performance, you should only fetch the columns you need.</p> <p>When appropriate, various indices and statistics based pruning will be used to accelerate the query.</p>"},{"location":"js/classes/Table/#returns_20","title":"Returns","text":"<p><code>Query</code></p> <p>A builder that can be used to parameterize the query</p>"},{"location":"js/classes/Table/#examples_1","title":"Examples","text":"<pre><code>// SQL-style filtering\n//\n// This query will return up to 1000 rows whose value in the `id` column\n// is greater than 5. LanceDb supports a broad set of filtering functions.\nfor await (const batch of table\n  .query()\n  .where(\"id &gt; 1\")\n  .select([\"id\"])\n  .limit(20)) {\n  console.log(batch);\n}\n</code></pre> <pre><code>// Vector Similarity Search\n//\n// This example will find the 10 rows whose value in the \"vector\" column are\n// closest to the query vector [1.0, 2.0, 3.0].  If an index has been created\n// on the \"vector\" column then this will perform an ANN search.\n//\n// The `refineFactor` and `nprobes` methods are used to control the recall /\n// latency tradeoff of the search.\nfor await (const batch of table\n  .query()\n  .where(\"id &gt; 1\")\n  .select([\"id\"])\n  .limit(20)) {\n  console.log(batch);\n}\n</code></pre> <pre><code>// Scan the full dataset\n//\n// This query will return everything in the table in no particular order.\nfor await (const batch of table.query()) {\n  console.log(batch);\n}\n</code></pre>"},{"location":"js/classes/Table/#restore","title":"restore()","text":"<pre><code>abstract restore(): Promise&lt;void&gt;\n</code></pre> <p>Restore the table to the currently checked out version</p> <p>This operation will fail if checkout has not been called previously</p> <p>This operation will overwrite the latest version of the table with a previous version.  Any changes made since the checked out version will no longer be visible.</p> <p>Once the operation concludes the table will no longer be in a checked out state and the read_consistency_interval, if any, will apply.</p>"},{"location":"js/classes/Table/#returns_21","title":"Returns","text":"<p><code>Promise</code>&lt;<code>void</code>&gt;</p>"},{"location":"js/classes/Table/#schema","title":"schema()","text":"<pre><code>abstract schema(): Promise&lt;Schema&lt;any&gt;&gt;\n</code></pre> <p>Get the schema of the table.</p>"},{"location":"js/classes/Table/#returns_22","title":"Returns","text":"<p><code>Promise</code>&lt;<code>Schema</code>&lt;<code>any</code>&gt;&gt;</p>"},{"location":"js/classes/Table/#search","title":"search()","text":"<pre><code>abstract search(\n   query,\n   queryType?,\n   ftsColumns?): Query | VectorQuery\n</code></pre> <p>Create a search query to find the nearest neighbors of the given query</p>"},{"location":"js/classes/Table/#parameters_13","title":"Parameters","text":"<ul> <li> <p>query: <code>string</code> | <code>IntoVector</code> | <code>FullTextQuery</code>     the query, a vector or string</p> </li> <li> <p>queryType?: <code>string</code>     the type of the query, \"vector\", \"fts\", or \"auto\"</p> </li> <li> <p>ftsColumns?: <code>string</code> | <code>string</code>[]     the columns to search in for full text search     for now, only one column can be searched at a time.     when \"auto\" is used, if the query is a string and an embedding function is defined, it will be treated as a vector query     if the query is a string and no embedding function is defined, it will be treated as a full text search query</p> </li> </ul>"},{"location":"js/classes/Table/#returns_23","title":"Returns","text":"<p><code>Query</code> | <code>VectorQuery</code></p>"},{"location":"js/classes/Table/#stats","title":"stats()","text":"<pre><code>abstract stats(): Promise&lt;TableStatistics&gt;\n</code></pre> <p>Returns table and fragment statistics</p>"},{"location":"js/classes/Table/#returns_24","title":"Returns","text":"<p><code>Promise</code>&lt;<code>TableStatistics</code>&gt;</p> <p>The table and fragment statistics</p>"},{"location":"js/classes/Table/#tags","title":"tags()","text":"<pre><code>abstract tags(): Promise&lt;Tags&gt;\n</code></pre> <p>Get a tags manager for this table.</p> <p>Tags allow you to label specific versions of a table with a human-readable name. The returned tags manager can be used to list, create, update, or delete tags.</p>"},{"location":"js/classes/Table/#returns_25","title":"Returns","text":"<p><code>Promise</code>&lt;<code>Tags</code>&gt;</p> <p>A tags manager for this table</p>"},{"location":"js/classes/Table/#example_1","title":"Example","text":"<pre><code>const tagsManager = await table.tags();\nawait tagsManager.create(\"v1\", 1);\nconst tags = await tagsManager.list();\nconsole.log(tags); // { \"v1\": { version: 1, manifestSize: ... } }\n</code></pre>"},{"location":"js/classes/Table/#toarrow","title":"toArrow()","text":"<pre><code>abstract toArrow(): Promise&lt;Table&lt;any&gt;&gt;\n</code></pre> <p>Return the table as an arrow table</p>"},{"location":"js/classes/Table/#returns_26","title":"Returns","text":"<p><code>Promise</code>&lt;<code>Table</code>&lt;<code>any</code>&gt;&gt;</p>"},{"location":"js/classes/Table/#update","title":"update()","text":""},{"location":"js/classes/Table/#updateopts","title":"update(opts)","text":"<pre><code>abstract update(opts): Promise&lt;UpdateResult&gt;\n</code></pre> <p>Update existing records in the Table</p>"},{"location":"js/classes/Table/#parameters_14","title":"Parameters","text":"<ul> <li>opts: <code>object</code> &amp; <code>Partial</code>&lt;<code>UpdateOptions</code>&gt;</li> </ul>"},{"location":"js/classes/Table/#returns_27","title":"Returns","text":"<p><code>Promise</code>&lt;<code>UpdateResult</code>&gt;</p> <p>A promise that resolves to an object containing the number of rows updated and the new version number</p>"},{"location":"js/classes/Table/#example_2","title":"Example","text":"<pre><code>table.update({where:\"x = 2\", values:{\"vector\": [10, 10]}})\n</code></pre>"},{"location":"js/classes/Table/#updateopts_1","title":"update(opts)","text":"<pre><code>abstract update(opts): Promise&lt;UpdateResult&gt;\n</code></pre> <p>Update existing records in the Table</p>"},{"location":"js/classes/Table/#parameters_15","title":"Parameters","text":"<ul> <li>opts: <code>object</code> &amp; <code>Partial</code>&lt;<code>UpdateOptions</code>&gt;</li> </ul>"},{"location":"js/classes/Table/#returns_28","title":"Returns","text":"<p><code>Promise</code>&lt;<code>UpdateResult</code>&gt;</p> <p>A promise that resolves to an object containing the number of rows updated and the new version number</p>"},{"location":"js/classes/Table/#example_3","title":"Example","text":"<pre><code>table.update({where:\"x = 2\", valuesSql:{\"x\": \"x + 1\"}})\n</code></pre>"},{"location":"js/classes/Table/#updateupdates-options","title":"update(updates, options)","text":"<pre><code>abstract update(updates, options?): Promise&lt;UpdateResult&gt;\n</code></pre> <p>Update existing records in the Table</p> <p>An update operation can be used to adjust existing values.  Use the returned builder to specify which columns to update.  The new value can be a literal value (e.g. replacing nulls with some default value) or an expression applied to the old value (e.g. incrementing a value)</p> <p>An optional condition can be specified (e.g. \"only update if the old value is 0\")</p> <p>Note: if your condition is something like \"some_id_column == 7\" and you are updating many rows (with different ids) then you will get better performance with a single [<code>merge_insert</code>] call instead of repeatedly calilng this method.</p>"},{"location":"js/classes/Table/#parameters_16","title":"Parameters","text":"<ul> <li> <p>updates: <code>Record</code>&lt;<code>string</code>, <code>string</code>&gt; | <code>Map</code>&lt;<code>string</code>, <code>string</code>&gt;     the     columns to update</p> </li> <li> <p>options?: <code>Partial</code>&lt;<code>UpdateOptions</code>&gt;     additional options to control     the update behavior</p> </li> </ul>"},{"location":"js/classes/Table/#returns_29","title":"Returns","text":"<p><code>Promise</code>&lt;<code>UpdateResult</code>&gt;</p> <p>A promise that resolves to an object containing the number of rows updated and the new version number</p> <p>Keys in the map should specify the name of the column to update. Values in the map provide the new value of the column.  These can be SQL literal strings (e.g. \"7\" or \"'foo'\") or they can be expressions based on the row being updated (e.g. \"my_col + 1\")</p>"},{"location":"js/classes/Table/#vectorsearch","title":"vectorSearch()","text":"<pre><code>abstract vectorSearch(vector): VectorQuery\n</code></pre> <p>Search the table with a given query vector.</p> <p>This is a convenience method for preparing a vector query and is the same thing as calling <code>nearestTo</code> on the builder returned by <code>query</code>.</p>"},{"location":"js/classes/Table/#parameters_17","title":"Parameters","text":"<ul> <li>vector: <code>IntoVector</code></li> </ul>"},{"location":"js/classes/Table/#returns_30","title":"Returns","text":"<p><code>VectorQuery</code></p>"},{"location":"js/classes/Table/#see","title":"See","text":"<p>Query#nearestTo for more details.</p>"},{"location":"js/classes/Table/#version","title":"version()","text":"<pre><code>abstract version(): Promise&lt;number&gt;\n</code></pre> <p>Retrieve the version of the table</p>"},{"location":"js/classes/Table/#returns_31","title":"Returns","text":"<p><code>Promise</code>&lt;<code>number</code>&gt;</p>"},{"location":"js/classes/Table/#waitforindex","title":"waitForIndex()","text":"<pre><code>abstract waitForIndex(indexNames, timeoutSeconds): Promise&lt;void&gt;\n</code></pre> <p>Waits for asynchronous indexing to complete on the table.</p>"},{"location":"js/classes/Table/#parameters_18","title":"Parameters","text":"<ul> <li> <p>indexNames: <code>string</code>[]     The name of the indices to wait for</p> </li> <li> <p>timeoutSeconds: <code>number</code>     The number of seconds to wait before timing out     This will raise an error if the indices are not created and fully indexed within the timeout.</p> </li> </ul>"},{"location":"js/classes/Table/#returns_32","title":"Returns","text":"<p><code>Promise</code>&lt;<code>void</code>&gt;</p>"},{"location":"js/classes/TagContents/","title":"TagContents","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / TagContents</p>"},{"location":"js/classes/TagContents/#class-tagcontents","title":"Class: TagContents","text":""},{"location":"js/classes/TagContents/#constructors","title":"Constructors","text":""},{"location":"js/classes/TagContents/#new-tagcontents","title":"new TagContents()","text":"<pre><code>new TagContents(): TagContents\n</code></pre>"},{"location":"js/classes/TagContents/#returns","title":"Returns","text":"<p><code>TagContents</code></p>"},{"location":"js/classes/TagContents/#properties","title":"Properties","text":""},{"location":"js/classes/TagContents/#manifestsize","title":"manifestSize","text":"<pre><code>manifestSize: number;\n</code></pre>"},{"location":"js/classes/TagContents/#version","title":"version","text":"<pre><code>version: number;\n</code></pre>"},{"location":"js/classes/Tags/","title":"Tags","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / Tags</p>"},{"location":"js/classes/Tags/#class-tags","title":"Class: Tags","text":""},{"location":"js/classes/Tags/#constructors","title":"Constructors","text":""},{"location":"js/classes/Tags/#new-tags","title":"new Tags()","text":"<pre><code>new Tags(): Tags\n</code></pre>"},{"location":"js/classes/Tags/#returns","title":"Returns","text":"<p><code>Tags</code></p>"},{"location":"js/classes/Tags/#methods","title":"Methods","text":""},{"location":"js/classes/Tags/#create","title":"create()","text":"<pre><code>create(tag, version): Promise&lt;void&gt;\n</code></pre>"},{"location":"js/classes/Tags/#parameters","title":"Parameters","text":"<ul> <li> <p>tag: <code>string</code></p> </li> <li> <p>version: <code>number</code></p> </li> </ul>"},{"location":"js/classes/Tags/#returns_1","title":"Returns","text":"<p><code>Promise</code>&lt;<code>void</code>&gt;</p>"},{"location":"js/classes/Tags/#delete","title":"delete()","text":"<pre><code>delete(tag): Promise&lt;void&gt;\n</code></pre>"},{"location":"js/classes/Tags/#parameters_1","title":"Parameters","text":"<ul> <li>tag: <code>string</code></li> </ul>"},{"location":"js/classes/Tags/#returns_2","title":"Returns","text":"<p><code>Promise</code>&lt;<code>void</code>&gt;</p>"},{"location":"js/classes/Tags/#getversion","title":"getVersion()","text":"<pre><code>getVersion(tag): Promise&lt;number&gt;\n</code></pre>"},{"location":"js/classes/Tags/#parameters_2","title":"Parameters","text":"<ul> <li>tag: <code>string</code></li> </ul>"},{"location":"js/classes/Tags/#returns_3","title":"Returns","text":"<p><code>Promise</code>&lt;<code>number</code>&gt;</p>"},{"location":"js/classes/Tags/#list","title":"list()","text":"<pre><code>list(): Promise&lt;Record&lt;string, TagContents&gt;&gt;\n</code></pre>"},{"location":"js/classes/Tags/#returns_4","title":"Returns","text":"<p><code>Promise</code>&lt;<code>Record</code>&lt;<code>string</code>, <code>TagContents</code>&gt;&gt;</p>"},{"location":"js/classes/Tags/#update","title":"update()","text":"<pre><code>update(tag, version): Promise&lt;void&gt;\n</code></pre>"},{"location":"js/classes/Tags/#parameters_3","title":"Parameters","text":"<ul> <li> <p>tag: <code>string</code></p> </li> <li> <p>version: <code>number</code></p> </li> </ul>"},{"location":"js/classes/Tags/#returns_5","title":"Returns","text":"<p><code>Promise</code>&lt;<code>void</code>&gt;</p>"},{"location":"js/classes/VectorColumnOptions/","title":"VectorColumnOptions","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / VectorColumnOptions</p>"},{"location":"js/classes/VectorColumnOptions/#class-vectorcolumnoptions","title":"Class: VectorColumnOptions","text":""},{"location":"js/classes/VectorColumnOptions/#constructors","title":"Constructors","text":""},{"location":"js/classes/VectorColumnOptions/#new-vectorcolumnoptions","title":"new VectorColumnOptions()","text":"<pre><code>new VectorColumnOptions(values?): VectorColumnOptions\n</code></pre>"},{"location":"js/classes/VectorColumnOptions/#parameters","title":"Parameters","text":"<ul> <li>values?: <code>Partial</code>&lt;<code>VectorColumnOptions</code>&gt;</li> </ul>"},{"location":"js/classes/VectorColumnOptions/#returns","title":"Returns","text":"<p><code>VectorColumnOptions</code></p>"},{"location":"js/classes/VectorColumnOptions/#properties","title":"Properties","text":""},{"location":"js/classes/VectorColumnOptions/#type","title":"type","text":"<pre><code>type: Float&lt;Floats&gt;;\n</code></pre> <p>Vector column type.</p>"},{"location":"js/classes/VectorQuery/","title":"VectorQuery","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / VectorQuery</p>"},{"location":"js/classes/VectorQuery/#class-vectorquery","title":"Class: VectorQuery","text":"<p>A builder used to construct a vector search</p> <p>This builder can be reused to execute the query many times.</p>"},{"location":"js/classes/VectorQuery/#see","title":"See","text":"<p>Query#nearestTo</p>"},{"location":"js/classes/VectorQuery/#extends","title":"Extends","text":"<ul> <li><code>QueryBase</code>&lt;<code>NativeVectorQuery</code>&gt;</li> </ul>"},{"location":"js/classes/VectorQuery/#properties","title":"Properties","text":""},{"location":"js/classes/VectorQuery/#inner","title":"inner","text":"<pre><code>protected inner: VectorQuery | Promise&lt;VectorQuery&gt;;\n</code></pre>"},{"location":"js/classes/VectorQuery/#inherited-from","title":"Inherited from","text":"<p><code>QueryBase</code>.<code>inner</code></p>"},{"location":"js/classes/VectorQuery/#methods","title":"Methods","text":""},{"location":"js/classes/VectorQuery/#addqueryvector","title":"addQueryVector()","text":"<pre><code>addQueryVector(vector): VectorQuery\n</code></pre>"},{"location":"js/classes/VectorQuery/#parameters","title":"Parameters","text":"<ul> <li>vector: <code>IntoVector</code></li> </ul>"},{"location":"js/classes/VectorQuery/#returns","title":"Returns","text":"<p><code>VectorQuery</code></p>"},{"location":"js/classes/VectorQuery/#analyzeplan","title":"analyzePlan()","text":"<pre><code>analyzePlan(): Promise&lt;string&gt;\n</code></pre> <p>Executes the query and returns the physical query plan annotated with runtime metrics.</p> <p>This is useful for debugging and performance analysis, as it shows how the query was executed and includes metrics such as elapsed time, rows processed, and I/O statistics.</p>"},{"location":"js/classes/VectorQuery/#returns_1","title":"Returns","text":"<p><code>Promise</code>&lt;<code>string</code>&gt;</p> <p>A query execution plan with runtime metrics for each step.</p>"},{"location":"js/classes/VectorQuery/#example","title":"Example","text":"<pre><code>import * as lancedb from \"@lancedb/lancedb\"\n\nconst db = await lancedb.connect(\"./.lancedb\");\nconst table = await db.createTable(\"my_table\", [\n  { vector: [1.1, 0.9], id: \"1\" },\n]);\n\nconst plan = await table.query().nearestTo([0.5, 0.2]).analyzePlan();\n\nExample output (with runtime metrics inlined):\nAnalyzeExec verbose=true, metrics=[]\n ProjectionExec: expr=[id@3 as id, vector@0 as vector, _distance@2 as _distance], metrics=[output_rows=1, elapsed_compute=3.292\u00b5s]\n  Take: columns=\"vector, _rowid, _distance, (id)\", metrics=[output_rows=1, elapsed_compute=66.001\u00b5s, batches_processed=1, bytes_read=8, iops=1, requests=1]\n   CoalesceBatchesExec: target_batch_size=1024, metrics=[output_rows=1, elapsed_compute=3.333\u00b5s]\n    GlobalLimitExec: skip=0, fetch=10, metrics=[output_rows=1, elapsed_compute=167ns]\n     FilterExec: _distance@2 IS NOT NULL, metrics=[output_rows=1, elapsed_compute=8.542\u00b5s]\n      SortExec: TopK(fetch=10), expr=[_distance@2 ASC NULLS LAST], metrics=[output_rows=1, elapsed_compute=63.25\u00b5s, row_replacements=1]\n       KNNVectorDistance: metric=l2, metrics=[output_rows=1, elapsed_compute=114.333\u00b5s, output_batches=1]\n        LanceScan: uri=/path/to/data, projection=[vector], row_id=true, row_addr=false, ordered=false, metrics=[output_rows=1, elapsed_compute=103.626\u00b5s, bytes_read=549, iops=2, requests=2]\n</code></pre>"},{"location":"js/classes/VectorQuery/#inherited-from_1","title":"Inherited from","text":"<p><code>QueryBase</code>.<code>analyzePlan</code></p>"},{"location":"js/classes/VectorQuery/#bypassvectorindex","title":"bypassVectorIndex()","text":"<pre><code>bypassVectorIndex(): VectorQuery\n</code></pre> <p>If this is called then any vector index is skipped</p> <p>An exhaustive (flat) search will be performed.  The query vector will be compared to every vector in the table.  At high scales this can be expensive.  However, this is often still useful.  For example, skipping the vector index can give you ground truth results which you can use to calculate your recall to select an appropriate value for nprobes.</p>"},{"location":"js/classes/VectorQuery/#returns_2","title":"Returns","text":"<p><code>VectorQuery</code></p>"},{"location":"js/classes/VectorQuery/#column","title":"column()","text":"<pre><code>column(column): VectorQuery\n</code></pre> <p>Set the vector column to query</p> <p>This controls which column is compared to the query vector supplied in the call to</p>"},{"location":"js/classes/VectorQuery/#parameters_1","title":"Parameters","text":"<ul> <li>column: <code>string</code></li> </ul>"},{"location":"js/classes/VectorQuery/#returns_3","title":"Returns","text":"<p><code>VectorQuery</code></p>"},{"location":"js/classes/VectorQuery/#see_1","title":"See","text":"<p>Query#nearestTo</p> <p>This parameter must be specified if the table has more than one column whose data type is a fixed-size-list of floats.</p>"},{"location":"js/classes/VectorQuery/#distancerange","title":"distanceRange()","text":"<pre><code>distanceRange(lowerBound?, upperBound?): VectorQuery\n</code></pre>"},{"location":"js/classes/VectorQuery/#parameters_2","title":"Parameters","text":"<ul> <li> <p>lowerBound?: <code>number</code></p> </li> <li> <p>upperBound?: <code>number</code></p> </li> </ul>"},{"location":"js/classes/VectorQuery/#returns_4","title":"Returns","text":"<p><code>VectorQuery</code></p>"},{"location":"js/classes/VectorQuery/#distancetype","title":"distanceType()","text":"<pre><code>distanceType(distanceType): VectorQuery\n</code></pre> <p>Set the distance metric to use</p> <p>When performing a vector search we try and find the \"nearest\" vectors according to some kind of distance metric.  This parameter controls which distance metric to use.  See</p>"},{"location":"js/classes/VectorQuery/#parameters_3","title":"Parameters","text":"<ul> <li>distanceType: <code>\"l2\"</code> | <code>\"cosine\"</code> | <code>\"dot\"</code></li> </ul>"},{"location":"js/classes/VectorQuery/#returns_5","title":"Returns","text":"<p><code>VectorQuery</code></p>"},{"location":"js/classes/VectorQuery/#see_2","title":"See","text":"<p>IvfPqOptions.distanceType for more details on the different distance metrics available.</p> <p>Note: if there is a vector index then the distance type used MUST match the distance type used to train the vector index.  If this is not done then the results will be invalid.</p> <p>By default \"l2\" is used.</p>"},{"location":"js/classes/VectorQuery/#ef","title":"ef()","text":"<pre><code>ef(ef): VectorQuery\n</code></pre> <p>Set the number of candidates to consider during the search</p> <p>This argument is only used when the vector column has an HNSW index. If there is no index then this value is ignored.</p> <p>Increasing this value will increase the recall of your query but will also increase the latency of your query. The default value is 1.5*limit.</p>"},{"location":"js/classes/VectorQuery/#parameters_4","title":"Parameters","text":"<ul> <li>ef: <code>number</code></li> </ul>"},{"location":"js/classes/VectorQuery/#returns_6","title":"Returns","text":"<p><code>VectorQuery</code></p>"},{"location":"js/classes/VectorQuery/#execute","title":"execute()","text":"<pre><code>protected execute(options?): RecordBatchIterator\n</code></pre> <p>Execute the query and return the results as an</p>"},{"location":"js/classes/VectorQuery/#parameters_5","title":"Parameters","text":"<ul> <li>options?: <code>Partial</code>&lt;<code>QueryExecutionOptions</code>&gt;</li> </ul>"},{"location":"js/classes/VectorQuery/#returns_7","title":"Returns","text":"<p><code>RecordBatchIterator</code></p>"},{"location":"js/classes/VectorQuery/#see_3","title":"See","text":"<ul> <li>AsyncIterator of</li> <li>RecordBatch.</li> </ul> <p>By default, LanceDb will use many threads to calculate results and, when the result set is large, multiple batches will be processed at one time. This readahead is limited however and backpressure will be applied if this stream is consumed slowly (this constrains the maximum memory used by a single query)</p>"},{"location":"js/classes/VectorQuery/#inherited-from_2","title":"Inherited from","text":"<p><code>QueryBase</code>.<code>execute</code></p>"},{"location":"js/classes/VectorQuery/#explainplan","title":"explainPlan()","text":"<pre><code>explainPlan(verbose): Promise&lt;string&gt;\n</code></pre> <p>Generates an explanation of the query execution plan.</p>"},{"location":"js/classes/VectorQuery/#parameters_6","title":"Parameters","text":"<ul> <li>verbose: <code>boolean</code> = <code>false</code>     If true, provides a more detailed explanation. Defaults to false.</li> </ul>"},{"location":"js/classes/VectorQuery/#returns_8","title":"Returns","text":"<p><code>Promise</code>&lt;<code>string</code>&gt;</p> <p>A Promise that resolves to a string containing the query execution plan explanation.</p>"},{"location":"js/classes/VectorQuery/#example_1","title":"Example","text":"<pre><code>import * as lancedb from \"@lancedb/lancedb\"\nconst db = await lancedb.connect(\"./.lancedb\");\nconst table = await db.createTable(\"my_table\", [\n  { vector: [1.1, 0.9], id: \"1\" },\n]);\nconst plan = await table.query().nearestTo([0.5, 0.2]).explainPlan();\n</code></pre>"},{"location":"js/classes/VectorQuery/#inherited-from_3","title":"Inherited from","text":"<p><code>QueryBase</code>.<code>explainPlan</code></p>"},{"location":"js/classes/VectorQuery/#fastsearch","title":"fastSearch()","text":"<pre><code>fastSearch(): this\n</code></pre> <p>Skip searching un-indexed data. This can make search faster, but will miss any data that is not yet indexed.</p> <p>Use Table#optimize to index all un-indexed data.</p>"},{"location":"js/classes/VectorQuery/#returns_9","title":"Returns","text":"<p><code>this</code></p>"},{"location":"js/classes/VectorQuery/#inherited-from_4","title":"Inherited from","text":"<p><code>QueryBase</code>.<code>fastSearch</code></p>"},{"location":"js/classes/VectorQuery/#filter","title":"filter()","text":"<pre><code>filter(predicate): this\n</code></pre> <p>A filter statement to be applied to this query.</p>"},{"location":"js/classes/VectorQuery/#parameters_7","title":"Parameters","text":"<ul> <li>predicate: <code>string</code></li> </ul>"},{"location":"js/classes/VectorQuery/#returns_10","title":"Returns","text":"<p><code>this</code></p>"},{"location":"js/classes/VectorQuery/#see_4","title":"See","text":"<p>where</p>"},{"location":"js/classes/VectorQuery/#deprecated","title":"Deprecated","text":"<p>Use <code>where</code> instead</p>"},{"location":"js/classes/VectorQuery/#inherited-from_5","title":"Inherited from","text":"<p><code>QueryBase</code>.<code>filter</code></p>"},{"location":"js/classes/VectorQuery/#fulltextsearch","title":"fullTextSearch()","text":"<pre><code>fullTextSearch(query, options?): this\n</code></pre>"},{"location":"js/classes/VectorQuery/#parameters_8","title":"Parameters","text":"<ul> <li> <p>query: <code>string</code> | <code>FullTextQuery</code></p> </li> <li> <p>options?: <code>Partial</code>&lt;<code>FullTextSearchOptions</code>&gt;</p> </li> </ul>"},{"location":"js/classes/VectorQuery/#returns_11","title":"Returns","text":"<p><code>this</code></p>"},{"location":"js/classes/VectorQuery/#inherited-from_6","title":"Inherited from","text":"<p><code>QueryBase</code>.<code>fullTextSearch</code></p>"},{"location":"js/classes/VectorQuery/#limit","title":"limit()","text":"<pre><code>limit(limit): this\n</code></pre> <p>Set the maximum number of results to return.</p> <p>By default, a plain search has no limit.  If this method is not called then every valid row from the table will be returned.</p>"},{"location":"js/classes/VectorQuery/#parameters_9","title":"Parameters","text":"<ul> <li>limit: <code>number</code></li> </ul>"},{"location":"js/classes/VectorQuery/#returns_12","title":"Returns","text":"<p><code>this</code></p>"},{"location":"js/classes/VectorQuery/#inherited-from_7","title":"Inherited from","text":"<p><code>QueryBase</code>.<code>limit</code></p>"},{"location":"js/classes/VectorQuery/#nprobes","title":"nprobes()","text":"<pre><code>nprobes(nprobes): VectorQuery\n</code></pre> <p>Set the number of partitions to search (probe)</p> <p>This argument is only used when the vector column has an IVF PQ index. If there is no index then this value is ignored.</p> <p>The IVF stage of IVF PQ divides the input into partitions (clusters) of related values.</p> <p>The partition whose centroids are closest to the query vector will be exhaustiely searched to find matches.  This parameter controls how many partitions should be searched.</p> <p>Increasing this value will increase the recall of your query but will also increase the latency of your query.  The default value is 20.  This default is good for many cases but the best value to use will depend on your data and the recall that you need to achieve.</p> <p>For best results we recommend tuning this parameter with a benchmark against your actual data to find the smallest possible value that will still give you the desired recall.</p>"},{"location":"js/classes/VectorQuery/#parameters_10","title":"Parameters","text":"<ul> <li>nprobes: <code>number</code></li> </ul>"},{"location":"js/classes/VectorQuery/#returns_13","title":"Returns","text":"<p><code>VectorQuery</code></p>"},{"location":"js/classes/VectorQuery/#offset","title":"offset()","text":"<pre><code>offset(offset): this\n</code></pre>"},{"location":"js/classes/VectorQuery/#parameters_11","title":"Parameters","text":"<ul> <li>offset: <code>number</code></li> </ul>"},{"location":"js/classes/VectorQuery/#returns_14","title":"Returns","text":"<p><code>this</code></p>"},{"location":"js/classes/VectorQuery/#inherited-from_8","title":"Inherited from","text":"<p><code>QueryBase</code>.<code>offset</code></p>"},{"location":"js/classes/VectorQuery/#postfilter","title":"postfilter()","text":"<pre><code>postfilter(): VectorQuery\n</code></pre> <p>If this is called then filtering will happen after the vector search instead of before.</p> <p>By default filtering will be performed before the vector search.  This is how filtering is typically understood to work.  This prefilter step does add some additional latency.  Creating a scalar index on the filter column(s) can often improve this latency.  However, sometimes a filter is too complex or scalar indices cannot be applied to the column.  In these cases postfiltering can be used instead of prefiltering to improve latency.</p> <p>Post filtering applies the filter to the results of the vector search.  This means we only run the filter on a much smaller set of data.  However, it can cause the query to return fewer than <code>limit</code> results (or even no results) if none of the nearest results match the filter.</p> <p>Post filtering happens during the \"refine stage\" (described in more detail in</p>"},{"location":"js/classes/VectorQuery/#returns_15","title":"Returns","text":"<p><code>VectorQuery</code></p>"},{"location":"js/classes/VectorQuery/#see_5","title":"See","text":"<p>VectorQuery#refineFactor).  This means that setting a higher refine factor can often help restore some of the results lost by post filtering.</p>"},{"location":"js/classes/VectorQuery/#refinefactor","title":"refineFactor()","text":"<pre><code>refineFactor(refineFactor): VectorQuery\n</code></pre> <p>A multiplier to control how many additional rows are taken during the refine step</p> <p>This argument is only used when the vector column has an IVF PQ index. If there is no index then this value is ignored.</p> <p>An IVF PQ index stores compressed (quantized) values.  They query vector is compared against these values and, since they are compressed, the comparison is inaccurate.</p> <p>This parameter can be used to refine the results.  It can improve both improve recall and correct the ordering of the nearest results.</p> <p>To refine results LanceDb will first perform an ANN search to find the nearest <code>limit</code> * <code>refine_factor</code> results.  In other words, if <code>refine_factor</code> is 3 and <code>limit</code> is the default (10) then the first 30 results will be selected.  LanceDb then fetches the full, uncompressed, values for these 30 results.  The results are then reordered by the true distance and only the nearest 10 are kept.</p> <p>Note: there is a difference between calling this method with a value of 1 and never calling this method at all.  Calling this method with any value will have an impact on your search latency.  When you call this method with a <code>refine_factor</code> of 1 then LanceDb still needs to fetch the full, uncompressed, values so that it can potentially reorder the results.</p> <p>Note: if this method is NOT called then the distances returned in the _distance column will be approximate distances based on the comparison of the quantized query vector and the quantized result vectors.  This can be considerably different than the true distance between the query vector and the actual uncompressed vector.</p>"},{"location":"js/classes/VectorQuery/#parameters_12","title":"Parameters","text":"<ul> <li>refineFactor: <code>number</code></li> </ul>"},{"location":"js/classes/VectorQuery/#returns_16","title":"Returns","text":"<p><code>VectorQuery</code></p>"},{"location":"js/classes/VectorQuery/#rerank","title":"rerank()","text":"<pre><code>rerank(reranker): VectorQuery\n</code></pre>"},{"location":"js/classes/VectorQuery/#parameters_13","title":"Parameters","text":"<ul> <li>reranker: <code>Reranker</code></li> </ul>"},{"location":"js/classes/VectorQuery/#returns_17","title":"Returns","text":"<p><code>VectorQuery</code></p>"},{"location":"js/classes/VectorQuery/#select","title":"select()","text":"<pre><code>select(columns): this\n</code></pre> <p>Return only the specified columns.</p> <p>By default a query will return all columns from the table.  However, this can have a very significant impact on latency.  LanceDb stores data in a columnar fashion.  This means we can finely tune our I/O to select exactly the columns we need.</p> <p>As a best practice you should always limit queries to the columns that you need.  If you pass in an array of column names then only those columns will be returned.</p> <p>You can also use this method to create new \"dynamic\" columns based on your existing columns. For example, you may not care about \"a\" or \"b\" but instead simply want \"a + b\".  This is often seen in the SELECT clause of an SQL query (e.g. <code>SELECT a+b FROM my_table</code>).</p> <p>To create dynamic columns you can pass in a Map.  A column will be returned for each entry in the map.  The key provides the name of the column.  The value is an SQL string used to specify how the column is calculated. <p>For example, an SQL query might state <code>SELECT a + b AS combined, c</code>.  The equivalent input to this method would be:</p>"},{"location":"js/classes/VectorQuery/#parameters_14","title":"Parameters","text":"<ul> <li>columns: <code>string</code> | <code>string</code>[] | <code>Record</code>&lt;<code>string</code>, <code>string</code>&gt; | <code>Map</code>&lt;<code>string</code>, <code>string</code>&gt;</li> </ul>"},{"location":"js/classes/VectorQuery/#returns_18","title":"Returns","text":"<p><code>this</code></p>"},{"location":"js/classes/VectorQuery/#example_2","title":"Example","text":"<pre><code>new Map([[\"combined\", \"a + b\"], [\"c\", \"c\"]])\n\nColumns will always be returned in the order given, even if that order is different than\nthe order used when adding the data.\n\nNote that you can pass in a `Record&lt;string, string&gt;` (e.g. an object literal). This method\nuses `Object.entries` which should preserve the insertion order of the object.  However,\nobject insertion order is easy to get wrong and `Map` is more foolproof.\n</code></pre>"},{"location":"js/classes/VectorQuery/#inherited-from_9","title":"Inherited from","text":"<p><code>QueryBase</code>.<code>select</code></p>"},{"location":"js/classes/VectorQuery/#toarray","title":"toArray()","text":"<pre><code>toArray(options?): Promise&lt;any[]&gt;\n</code></pre> <p>Collect the results as an array of objects.</p>"},{"location":"js/classes/VectorQuery/#parameters_15","title":"Parameters","text":"<ul> <li>options?: <code>Partial</code>&lt;<code>QueryExecutionOptions</code>&gt;</li> </ul>"},{"location":"js/classes/VectorQuery/#returns_19","title":"Returns","text":"<p><code>Promise</code>&lt;<code>any</code>[]&gt;</p>"},{"location":"js/classes/VectorQuery/#inherited-from_10","title":"Inherited from","text":"<p><code>QueryBase</code>.<code>toArray</code></p>"},{"location":"js/classes/VectorQuery/#toarrow","title":"toArrow()","text":"<pre><code>toArrow(options?): Promise&lt;Table&lt;any&gt;&gt;\n</code></pre> <p>Collect the results as an Arrow</p>"},{"location":"js/classes/VectorQuery/#parameters_16","title":"Parameters","text":"<ul> <li>options?: <code>Partial</code>&lt;<code>QueryExecutionOptions</code>&gt;</li> </ul>"},{"location":"js/classes/VectorQuery/#returns_20","title":"Returns","text":"<p><code>Promise</code>&lt;<code>Table</code>&lt;<code>any</code>&gt;&gt;</p>"},{"location":"js/classes/VectorQuery/#see_6","title":"See","text":"<p>ArrowTable.</p>"},{"location":"js/classes/VectorQuery/#inherited-from_11","title":"Inherited from","text":"<p><code>QueryBase</code>.<code>toArrow</code></p>"},{"location":"js/classes/VectorQuery/#where","title":"where()","text":"<pre><code>where(predicate): this\n</code></pre> <p>A filter statement to be applied to this query.</p> <p>The filter should be supplied as an SQL query string.  For example:</p>"},{"location":"js/classes/VectorQuery/#parameters_17","title":"Parameters","text":"<ul> <li>predicate: <code>string</code></li> </ul>"},{"location":"js/classes/VectorQuery/#returns_21","title":"Returns","text":"<p><code>this</code></p>"},{"location":"js/classes/VectorQuery/#example_3","title":"Example","text":"<pre><code>x &gt; 10\ny &gt; 0 AND y &lt; 100\nx &gt; 5 OR y = 'test'\n\nFiltering performance can often be improved by creating a scalar index\non the filter column(s).\n</code></pre>"},{"location":"js/classes/VectorQuery/#inherited-from_12","title":"Inherited from","text":"<p><code>QueryBase</code>.<code>where</code></p>"},{"location":"js/classes/VectorQuery/#withrowid","title":"withRowId()","text":"<pre><code>withRowId(): this\n</code></pre> <p>Whether to return the row id in the results.</p> <p>This column can be used to match results between different queries. For example, to match results from a full text search and a vector search in order to perform hybrid search.</p>"},{"location":"js/classes/VectorQuery/#returns_22","title":"Returns","text":"<p><code>this</code></p>"},{"location":"js/classes/VectorQuery/#inherited-from_13","title":"Inherited from","text":"<p><code>QueryBase</code>.<code>withRowId</code></p>"},{"location":"js/enumerations/FullTextQueryType/","title":"FullTextQueryType","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / FullTextQueryType</p>"},{"location":"js/enumerations/FullTextQueryType/#enumeration-fulltextquerytype","title":"Enumeration: FullTextQueryType","text":"<p>Enum representing the types of full-text queries supported.</p> <ul> <li><code>Match</code>: Performs a full-text search for terms in the query string.</li> <li><code>MatchPhrase</code>: Searches for an exact phrase match in the text.</li> <li><code>Boost</code>: Boosts the relevance score of specific terms in the query.</li> <li><code>MultiMatch</code>: Searches across multiple fields for the query terms.</li> </ul>"},{"location":"js/enumerations/FullTextQueryType/#enumeration-members","title":"Enumeration Members","text":""},{"location":"js/enumerations/FullTextQueryType/#boost","title":"Boost","text":"<pre><code>Boost: \"boost\";\n</code></pre>"},{"location":"js/enumerations/FullTextQueryType/#match","title":"Match","text":"<pre><code>Match: \"match\";\n</code></pre>"},{"location":"js/enumerations/FullTextQueryType/#matchphrase","title":"MatchPhrase","text":"<pre><code>MatchPhrase: \"match_phrase\";\n</code></pre>"},{"location":"js/enumerations/FullTextQueryType/#multimatch","title":"MultiMatch","text":"<pre><code>MultiMatch: \"multi_match\";\n</code></pre>"},{"location":"js/functions/connect/","title":"Connect","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / connect</p>"},{"location":"js/functions/connect/#function-connect","title":"Function: connect()","text":""},{"location":"js/functions/connect/#connecturi-options","title":"connect(uri, options)","text":"<pre><code>function connect(uri, options?): Promise&lt;Connection&gt;\n</code></pre> <p>Connect to a LanceDB instance at the given URI.</p> <p>Accepted formats:</p> <ul> <li><code>/path/to/database</code> - local database</li> <li><code>s3://bucket/path/to/database</code> or <code>gs://bucket/path/to/database</code> - database on cloud storage</li> <li><code>db://host:port</code> - remote database (LanceDB cloud)</li> </ul>"},{"location":"js/functions/connect/#parameters","title":"Parameters","text":"<ul> <li> <p>uri: <code>string</code>     The uri of the database. If the database uri starts     with <code>db://</code> then it connects to a remote database.</p> </li> <li> <p>options?: <code>Partial</code>&lt;<code>ConnectionOptions</code>&gt;     The options to use when connecting to the database</p> </li> </ul>"},{"location":"js/functions/connect/#returns","title":"Returns","text":"<p><code>Promise</code>&lt;<code>Connection</code>&gt;</p>"},{"location":"js/functions/connect/#see","title":"See","text":"<p>ConnectionOptions for more details on the URI format.</p>"},{"location":"js/functions/connect/#examples","title":"Examples","text":"<pre><code>const conn = await connect(\"/path/to/database\");\n</code></pre> <pre><code>const conn = await connect(\n  \"s3://bucket/path/to/database\",\n  {storageOptions: {timeout: \"60s\"}\n});\n</code></pre>"},{"location":"js/functions/connect/#connectoptions","title":"connect(options)","text":"<pre><code>function connect(options): Promise&lt;Connection&gt;\n</code></pre> <p>Connect to a LanceDB instance at the given URI.</p> <p>Accepted formats:</p> <ul> <li><code>/path/to/database</code> - local database</li> <li><code>s3://bucket/path/to/database</code> or <code>gs://bucket/path/to/database</code> - database on cloud storage</li> <li><code>db://host:port</code> - remote database (LanceDB cloud)</li> </ul>"},{"location":"js/functions/connect/#parameters_1","title":"Parameters","text":"<ul> <li>options: <code>Partial</code>&lt;<code>ConnectionOptions</code>&gt; &amp; <code>object</code>     The options to use when connecting to the database</li> </ul>"},{"location":"js/functions/connect/#returns_1","title":"Returns","text":"<p><code>Promise</code>&lt;<code>Connection</code>&gt;</p>"},{"location":"js/functions/connect/#see_1","title":"See","text":"<p>ConnectionOptions for more details on the URI format.</p>"},{"location":"js/functions/connect/#example","title":"Example","text":"<pre><code>const conn = await connect({\n  uri: \"/path/to/database\",\n  storageOptions: {timeout: \"60s\"}\n});\n</code></pre>"},{"location":"js/functions/makeArrowTable/","title":"makeArrowTable","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / makeArrowTable</p>"},{"location":"js/functions/makeArrowTable/#function-makearrowtable","title":"Function: makeArrowTable()","text":"<pre><code>function makeArrowTable(\n   data,\n   options?,\n   metadata?): ArrowTable\n</code></pre> <p>An enhanced version of the makeTable function from Apache Arrow that supports nested fields and embeddings columns.</p> <p>(typically you do not need to call this function.  It will be called automatically when creating a table or adding data to it)</p> <p>This function converts an array of Record (row-major JS objects) to an Arrow Table (a columnar structure) <p>If a schema is provided then it will be used to determine the resulting array types.  Fields will also be reordered to fit the order defined by the schema.</p> <p>If a schema is not provided then the types will be inferred and the field order will be controlled by the order of properties in the first record.  If a type is inferred it will always be nullable.</p> <p>If not all fields are found in the data, then a subset of the schema will be returned.</p> <p>If the input is empty then a schema must be provided to create an empty table.</p> <p>When a schema is not specified then data types will be inferred.  The inference rules are as follows:</p> <ul> <li>boolean =&gt; Bool</li> <li>number =&gt; Float64</li> <li>bigint =&gt; Int64</li> <li>String =&gt; Utf8</li> <li>Buffer =&gt; Binary</li> <li>Record =&gt; Struct <li>Array =&gt; List"},{"location":"js/functions/makeArrowTable/#parameters","title":"Parameters","text":"<ul> <li> <p>data: <code>Record</code>&lt;<code>string</code>, <code>unknown</code>&gt;[]</p> </li> <li> <p>options?: <code>Partial</code>&lt;<code>MakeArrowTableOptions</code>&gt;</p> </li> <li> <p>metadata?: <code>Map</code>&lt;<code>string</code>, <code>string</code>&gt;</p> </li> </ul>"},{"location":"js/functions/makeArrowTable/#returns","title":"Returns","text":"<p><code>ArrowTable</code></p>"},{"location":"js/functions/makeArrowTable/#example","title":"Example","text":"<pre><code>import { fromTableToBuffer, makeArrowTable } from \"../arrow\";\nimport { Field, FixedSizeList, Float16, Float32, Int32, Schema } from \"apache-arrow\";\n\nconst schema = new Schema([\n  new Field(\"a\", new Int32()),\n  new Field(\"b\", new Float32()),\n  new Field(\"c\", new FixedSizeList(3, new Field(\"item\", new Float16()))),\n ]);\n const table = makeArrowTable([\n   { a: 1, b: 2, c: [1, 2, 3] },\n   { a: 4, b: 5, c: [4, 5, 6] },\n   { a: 7, b: 8, c: [7, 8, 9] },\n ], { schema });\n</code></pre> <p>By default it assumes that the column named <code>vector</code> is a vector column and it will be converted into a fixed size list array of type float32. The <code>vectorColumns</code> option can be used to support other vector column names and data types.</p> <pre><code>const schema = new Schema([\n  new Field(\"a\", new Float64()),\n  new Field(\"b\", new Float64()),\n  new Field(\n    \"vector\",\n    new FixedSizeList(3, new Field(\"item\", new Float32()))\n  ),\n]);\nconst table = makeArrowTable([\n  { a: 1, b: 2, vector: [1, 2, 3] },\n  { a: 4, b: 5, vector: [4, 5, 6] },\n  { a: 7, b: 8, vector: [7, 8, 9] },\n]);\nassert.deepEqual(table.schema, schema);\n</code></pre> <p>You can specify the vector column types and names using the options as well</p> <pre><code>const schema = new Schema([\n  new Field('a', new Float64()),\n  new Field('b', new Float64()),\n  new Field('vec1', new FixedSizeList(3, new Field('item', new Float16()))),\n  new Field('vec2', new FixedSizeList(3, new Field('item', new Float16())))\n]);\nconst table = makeArrowTable([\n  { a: 1, b: 2, vec1: [1, 2, 3], vec2: [2, 4, 6] },\n  { a: 4, b: 5, vec1: [4, 5, 6], vec2: [8, 10, 12] },\n  { a: 7, b: 8, vec1: [7, 8, 9], vec2: [14, 16, 18] }\n], {\n  vectorColumns: {\n    vec1: { type: new Float16() },\n    vec2: { type: new Float16() }\n  }\n}\nassert.deepEqual(table.schema, schema)\n</code></pre>"},{"location":"js/functions/packBits/","title":"packBits","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / packBits</p>"},{"location":"js/functions/packBits/#function-packbits","title":"Function: packBits()","text":"<pre><code>function packBits(data): number[]\n</code></pre>"},{"location":"js/functions/packBits/#parameters","title":"Parameters","text":"<ul> <li>data: <code>number</code>[]</li> </ul>"},{"location":"js/functions/packBits/#returns","title":"Returns","text":"<p><code>number</code>[]</p>"},{"location":"js/interfaces/AddColumnsResult/","title":"AddColumnsResult","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / AddColumnsResult</p>"},{"location":"js/interfaces/AddColumnsResult/#interface-addcolumnsresult","title":"Interface: AddColumnsResult","text":""},{"location":"js/interfaces/AddColumnsResult/#properties","title":"Properties","text":""},{"location":"js/interfaces/AddColumnsResult/#version","title":"version","text":"<pre><code>version: number;\n</code></pre>"},{"location":"js/interfaces/AddColumnsSql/","title":"AddColumnsSql","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / AddColumnsSql</p>"},{"location":"js/interfaces/AddColumnsSql/#interface-addcolumnssql","title":"Interface: AddColumnsSql","text":"<p>A definition of a new column to add to a table.</p>"},{"location":"js/interfaces/AddColumnsSql/#properties","title":"Properties","text":""},{"location":"js/interfaces/AddColumnsSql/#name","title":"name","text":"<pre><code>name: string;\n</code></pre> <p>The name of the new column.</p>"},{"location":"js/interfaces/AddColumnsSql/#valuesql","title":"valueSql","text":"<pre><code>valueSql: string;\n</code></pre> <p>The values to populate the new column with, as a SQL expression. The expression can reference other columns in the table.</p>"},{"location":"js/interfaces/AddDataOptions/","title":"AddDataOptions","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / AddDataOptions</p>"},{"location":"js/interfaces/AddDataOptions/#interface-adddataoptions","title":"Interface: AddDataOptions","text":"<p>Options for adding data to a table.</p>"},{"location":"js/interfaces/AddDataOptions/#properties","title":"Properties","text":""},{"location":"js/interfaces/AddDataOptions/#mode","title":"mode","text":"<pre><code>mode: \"append\" | \"overwrite\";\n</code></pre> <p>If \"append\" (the default) then the new data will be added to the table</p> <p>If \"overwrite\" then the new data will replace the existing data in the table.</p>"},{"location":"js/interfaces/AddResult/","title":"AddResult","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / AddResult</p>"},{"location":"js/interfaces/AddResult/#interface-addresult","title":"Interface: AddResult","text":""},{"location":"js/interfaces/AddResult/#properties","title":"Properties","text":""},{"location":"js/interfaces/AddResult/#version","title":"version","text":"<pre><code>version: number;\n</code></pre>"},{"location":"js/interfaces/AlterColumnsResult/","title":"AlterColumnsResult","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / AlterColumnsResult</p>"},{"location":"js/interfaces/AlterColumnsResult/#interface-altercolumnsresult","title":"Interface: AlterColumnsResult","text":""},{"location":"js/interfaces/AlterColumnsResult/#properties","title":"Properties","text":""},{"location":"js/interfaces/AlterColumnsResult/#version","title":"version","text":"<pre><code>version: number;\n</code></pre>"},{"location":"js/interfaces/ClientConfig/","title":"ClientConfig","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / ClientConfig</p>"},{"location":"js/interfaces/ClientConfig/#interface-clientconfig","title":"Interface: ClientConfig","text":""},{"location":"js/interfaces/ClientConfig/#properties","title":"Properties","text":""},{"location":"js/interfaces/ClientConfig/#extraheaders","title":"extraHeaders?","text":"<pre><code>optional extraHeaders: Record&lt;string, string&gt;;\n</code></pre>"},{"location":"js/interfaces/ClientConfig/#retryconfig","title":"retryConfig?","text":"<pre><code>optional retryConfig: RetryConfig;\n</code></pre>"},{"location":"js/interfaces/ClientConfig/#timeoutconfig","title":"timeoutConfig?","text":"<pre><code>optional timeoutConfig: TimeoutConfig;\n</code></pre>"},{"location":"js/interfaces/ClientConfig/#useragent","title":"userAgent?","text":"<pre><code>optional userAgent: string;\n</code></pre>"},{"location":"js/interfaces/ColumnAlteration/","title":"ColumnAlteration","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / ColumnAlteration</p>"},{"location":"js/interfaces/ColumnAlteration/#interface-columnalteration","title":"Interface: ColumnAlteration","text":"<p>A definition of a column alteration. The alteration changes the column at <code>path</code> to have the new name <code>name</code>, to be nullable if <code>nullable</code> is true, and to have the data type <code>data_type</code>. At least one of <code>rename</code> or <code>nullable</code> must be provided.</p>"},{"location":"js/interfaces/ColumnAlteration/#properties","title":"Properties","text":""},{"location":"js/interfaces/ColumnAlteration/#datatype","title":"dataType?","text":"<pre><code>optional dataType: string | DataType&lt;Type, any&gt;;\n</code></pre> <p>A new data type for the column. If not provided then the data type will not be changed. Changing data types is limited to casting to the same general type. For example, these changes are valid: * <code>int32</code> -&gt; <code>int64</code> (integers) * <code>double</code> -&gt; <code>float</code> (floats) * <code>string</code> -&gt; <code>large_string</code> (strings) But these changes are not: * <code>int32</code> -&gt; <code>double</code> (mix integers and floats) * <code>string</code> -&gt; <code>int32</code> (mix strings and integers)</p>"},{"location":"js/interfaces/ColumnAlteration/#nullable","title":"nullable?","text":"<pre><code>optional nullable: boolean;\n</code></pre> <p>Set the new nullability. Note that a nullable column cannot be made non-nullable.</p>"},{"location":"js/interfaces/ColumnAlteration/#path","title":"path","text":"<pre><code>path: string;\n</code></pre> <p>The path to the column to alter. This is a dot-separated path to the column. If it is a top-level column then it is just the name of the column. If it is a nested column then it is the path to the column, e.g. \"a.b.c\" for a column <code>c</code> nested inside a column <code>b</code> nested inside a column <code>a</code>.</p>"},{"location":"js/interfaces/ColumnAlteration/#rename","title":"rename?","text":"<pre><code>optional rename: string;\n</code></pre> <p>The new name of the column. If not provided then the name will not be changed. This must be distinct from the names of all other columns in the table.</p>"},{"location":"js/interfaces/CompactionStats/","title":"CompactionStats","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / CompactionStats</p>"},{"location":"js/interfaces/CompactionStats/#interface-compactionstats","title":"Interface: CompactionStats","text":"<p>Statistics about a compaction operation.</p>"},{"location":"js/interfaces/CompactionStats/#properties","title":"Properties","text":""},{"location":"js/interfaces/CompactionStats/#filesadded","title":"filesAdded","text":"<pre><code>filesAdded: number;\n</code></pre> <p>The number of new, compacted data files added</p>"},{"location":"js/interfaces/CompactionStats/#filesremoved","title":"filesRemoved","text":"<pre><code>filesRemoved: number;\n</code></pre> <p>The number of data files removed</p>"},{"location":"js/interfaces/CompactionStats/#fragmentsadded","title":"fragmentsAdded","text":"<pre><code>fragmentsAdded: number;\n</code></pre> <p>The number of new, compacted fragments added</p>"},{"location":"js/interfaces/CompactionStats/#fragmentsremoved","title":"fragmentsRemoved","text":"<pre><code>fragmentsRemoved: number;\n</code></pre> <p>The number of fragments removed</p>"},{"location":"js/interfaces/ConnectionOptions/","title":"ConnectionOptions","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / ConnectionOptions</p>"},{"location":"js/interfaces/ConnectionOptions/#interface-connectionoptions","title":"Interface: ConnectionOptions","text":""},{"location":"js/interfaces/ConnectionOptions/#properties","title":"Properties","text":""},{"location":"js/interfaces/ConnectionOptions/#apikey","title":"apiKey?","text":"<pre><code>optional apiKey: string;\n</code></pre> <p>(For LanceDB cloud only): the API key to use with LanceDB Cloud.</p> <p>Can also be set via the environment variable <code>LANCEDB_API_KEY</code>.</p>"},{"location":"js/interfaces/ConnectionOptions/#clientconfig","title":"clientConfig?","text":"<pre><code>optional clientConfig: ClientConfig;\n</code></pre> <p>(For LanceDB cloud only): configuration for the remote HTTP client.</p>"},{"location":"js/interfaces/ConnectionOptions/#hostoverride","title":"hostOverride?","text":"<pre><code>optional hostOverride: string;\n</code></pre> <p>(For LanceDB cloud only): the host to use for LanceDB cloud. Used for testing purposes.</p>"},{"location":"js/interfaces/ConnectionOptions/#readconsistencyinterval","title":"readConsistencyInterval?","text":"<pre><code>optional readConsistencyInterval: number;\n</code></pre> <p>(For LanceDB OSS only): The interval, in seconds, at which to check for updates to the table from other processes. If None, then consistency is not checked. For performance reasons, this is the default. For strong consistency, set this to zero seconds. Then every read will check for updates from other processes. As a compromise, you can set this to a non-zero value for eventual consistency. If more than that interval has passed since the last check, then the table will be checked for updates. Note: this consistency only applies to read operations. Write operations are always consistent.</p>"},{"location":"js/interfaces/ConnectionOptions/#region","title":"region?","text":"<pre><code>optional region: string;\n</code></pre> <p>(For LanceDB cloud only): the region to use for LanceDB cloud. Defaults to 'us-east-1'.</p>"},{"location":"js/interfaces/ConnectionOptions/#storageoptions","title":"storageOptions?","text":"<pre><code>optional storageOptions: Record&lt;string, string&gt;;\n</code></pre> <p>(For LanceDB OSS only): configuration for object storage.</p> <p>The available options are described at https://lancedb.github.io/lancedb/guides/storage/</p>"},{"location":"js/interfaces/CreateTableOptions/","title":"CreateTableOptions","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / CreateTableOptions</p>"},{"location":"js/interfaces/CreateTableOptions/#interface-createtableoptions","title":"Interface: CreateTableOptions","text":""},{"location":"js/interfaces/CreateTableOptions/#properties","title":"Properties","text":""},{"location":"js/interfaces/CreateTableOptions/#datastorageversion","title":"dataStorageVersion?","text":"<pre><code>optional dataStorageVersion: string;\n</code></pre> <p>The version of the data storage format to use.</p> <p>The default is <code>stable</code>. Set to \"legacy\" to use the old format.</p>"},{"location":"js/interfaces/CreateTableOptions/#deprecated","title":"Deprecated","text":"<p>Pass <code>new_table_data_storage_version</code> to storageOptions instead.</p>"},{"location":"js/interfaces/CreateTableOptions/#embeddingfunction","title":"embeddingFunction?","text":"<pre><code>optional embeddingFunction: EmbeddingFunctionConfig;\n</code></pre>"},{"location":"js/interfaces/CreateTableOptions/#enablev2manifestpaths","title":"enableV2ManifestPaths?","text":"<pre><code>optional enableV2ManifestPaths: boolean;\n</code></pre> <p>Use the new V2 manifest paths. These paths provide more efficient opening of datasets with many versions on object stores.  WARNING: turning this on will make the dataset unreadable for older versions of LanceDB (prior to 0.10.0). To migrate an existing dataset, instead use the LocalTable#migrateManifestPathsV2 method.</p>"},{"location":"js/interfaces/CreateTableOptions/#deprecated_1","title":"Deprecated","text":"<p>Pass <code>new_table_enable_v2_manifest_paths</code> to storageOptions instead.</p>"},{"location":"js/interfaces/CreateTableOptions/#existok","title":"existOk","text":"<pre><code>existOk: boolean;\n</code></pre> <p>If this is true and the table already exists and the mode is \"create\" then no error will be raised.</p>"},{"location":"js/interfaces/CreateTableOptions/#mode","title":"mode","text":"<pre><code>mode: \"overwrite\" | \"create\";\n</code></pre> <p>The mode to use when creating the table.</p> <p>If this is set to \"create\" and the table already exists then either an error will be thrown or, if existOk is true, then nothing will happen.  Any provided data will be ignored.</p> <p>If this is set to \"overwrite\" then any existing table will be replaced.</p>"},{"location":"js/interfaces/CreateTableOptions/#schema","title":"schema?","text":"<pre><code>optional schema: SchemaLike;\n</code></pre>"},{"location":"js/interfaces/CreateTableOptions/#storageoptions","title":"storageOptions?","text":"<pre><code>optional storageOptions: Record&lt;string, string&gt;;\n</code></pre> <p>Configuration for object storage.</p> <p>Options already set on the connection will be inherited by the table, but can be overridden here.</p> <p>The available options are described at https://lancedb.github.io/lancedb/guides/storage/</p>"},{"location":"js/interfaces/DeleteResult/","title":"DeleteResult","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / DeleteResult</p>"},{"location":"js/interfaces/DeleteResult/#interface-deleteresult","title":"Interface: DeleteResult","text":""},{"location":"js/interfaces/DeleteResult/#properties","title":"Properties","text":""},{"location":"js/interfaces/DeleteResult/#version","title":"version","text":"<pre><code>version: number;\n</code></pre>"},{"location":"js/interfaces/DropColumnsResult/","title":"DropColumnsResult","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / DropColumnsResult</p>"},{"location":"js/interfaces/DropColumnsResult/#interface-dropcolumnsresult","title":"Interface: DropColumnsResult","text":""},{"location":"js/interfaces/DropColumnsResult/#properties","title":"Properties","text":""},{"location":"js/interfaces/DropColumnsResult/#version","title":"version","text":"<pre><code>version: number;\n</code></pre>"},{"location":"js/interfaces/ExecutableQuery/","title":"ExecutableQuery","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / ExecutableQuery</p>"},{"location":"js/interfaces/ExecutableQuery/#interface-executablequery","title":"Interface: ExecutableQuery","text":"<p>An interface for a query that can be executed</p> <p>Supported by all query types</p>"},{"location":"js/interfaces/FragmentStatistics/","title":"FragmentStatistics","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / FragmentStatistics</p>"},{"location":"js/interfaces/FragmentStatistics/#interface-fragmentstatistics","title":"Interface: FragmentStatistics","text":""},{"location":"js/interfaces/FragmentStatistics/#properties","title":"Properties","text":""},{"location":"js/interfaces/FragmentStatistics/#lengths","title":"lengths","text":"<pre><code>lengths: FragmentSummaryStats;\n</code></pre> <p>Statistics on the number of rows in the table fragments</p>"},{"location":"js/interfaces/FragmentStatistics/#numfragments","title":"numFragments","text":"<pre><code>numFragments: number;\n</code></pre> <p>The number of fragments in the table</p>"},{"location":"js/interfaces/FragmentStatistics/#numsmallfragments","title":"numSmallFragments","text":"<pre><code>numSmallFragments: number;\n</code></pre> <p>The number of uncompacted fragments in the table</p>"},{"location":"js/interfaces/FragmentSummaryStats/","title":"FragmentSummaryStats","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / FragmentSummaryStats</p>"},{"location":"js/interfaces/FragmentSummaryStats/#interface-fragmentsummarystats","title":"Interface: FragmentSummaryStats","text":""},{"location":"js/interfaces/FragmentSummaryStats/#properties","title":"Properties","text":""},{"location":"js/interfaces/FragmentSummaryStats/#max","title":"max","text":"<pre><code>max: number;\n</code></pre> <p>The number of rows in the fragment with the most rows</p>"},{"location":"js/interfaces/FragmentSummaryStats/#mean","title":"mean","text":"<pre><code>mean: number;\n</code></pre> <p>The mean number of rows in the fragments</p>"},{"location":"js/interfaces/FragmentSummaryStats/#min","title":"min","text":"<pre><code>min: number;\n</code></pre> <p>The number of rows in the fragment with the fewest rows</p>"},{"location":"js/interfaces/FragmentSummaryStats/#p25","title":"p25","text":"<pre><code>p25: number;\n</code></pre> <p>The 25th percentile of number of rows in the fragments</p>"},{"location":"js/interfaces/FragmentSummaryStats/#p50","title":"p50","text":"<pre><code>p50: number;\n</code></pre> <p>The 50th percentile of number of rows in the fragments</p>"},{"location":"js/interfaces/FragmentSummaryStats/#p75","title":"p75","text":"<pre><code>p75: number;\n</code></pre> <p>The 75th percentile of number of rows in the fragments</p>"},{"location":"js/interfaces/FragmentSummaryStats/#p99","title":"p99","text":"<pre><code>p99: number;\n</code></pre> <p>The 99th percentile of number of rows in the fragments</p>"},{"location":"js/interfaces/FtsOptions/","title":"FtsOptions","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / FtsOptions</p>"},{"location":"js/interfaces/FtsOptions/#interface-ftsoptions","title":"Interface: FtsOptions","text":"<p>Options to create a full text search index</p>"},{"location":"js/interfaces/FtsOptions/#properties","title":"Properties","text":""},{"location":"js/interfaces/FtsOptions/#asciifolding","title":"asciiFolding?","text":"<pre><code>optional asciiFolding: boolean;\n</code></pre> <p>whether to remove punctuation</p>"},{"location":"js/interfaces/FtsOptions/#basetokenizer","title":"baseTokenizer?","text":"<pre><code>optional baseTokenizer: \"raw\" | \"simple\" | \"whitespace\";\n</code></pre> <p>The tokenizer to use when building the index. The default is \"simple\".</p> <p>The following tokenizers are available:</p> <p>\"simple\" - Simple tokenizer. This tokenizer splits the text into tokens using whitespace and punctuation as a delimiter.</p> <p>\"whitespace\" - Whitespace tokenizer. This tokenizer splits the text into tokens using whitespace as a delimiter.</p> <p>\"raw\" - Raw tokenizer. This tokenizer does not split the text into tokens and indexes the entire text as a single token.</p>"},{"location":"js/interfaces/FtsOptions/#language","title":"language?","text":"<pre><code>optional language: string;\n</code></pre> <p>language for stemming and stop words this is only used when <code>stem</code> or <code>remove_stop_words</code> is true</p>"},{"location":"js/interfaces/FtsOptions/#lowercase","title":"lowercase?","text":"<pre><code>optional lowercase: boolean;\n</code></pre> <p>whether to lowercase tokens</p>"},{"location":"js/interfaces/FtsOptions/#maxtokenlength","title":"maxTokenLength?","text":"<pre><code>optional maxTokenLength: number;\n</code></pre> <p>maximum token length tokens longer than this length will be ignored</p>"},{"location":"js/interfaces/FtsOptions/#removestopwords","title":"removeStopWords?","text":"<pre><code>optional removeStopWords: boolean;\n</code></pre> <p>whether to remove stop words</p>"},{"location":"js/interfaces/FtsOptions/#stem","title":"stem?","text":"<pre><code>optional stem: boolean;\n</code></pre> <p>whether to stem tokens</p>"},{"location":"js/interfaces/FtsOptions/#withposition","title":"withPosition?","text":"<pre><code>optional withPosition: boolean;\n</code></pre> <p>Whether to build the index with positions. True by default. If set to false, the index will not store the positions of the tokens in the text, which will make the index smaller and faster to build, but will not support phrase queries.</p>"},{"location":"js/interfaces/FullTextQuery/","title":"FullTextQuery","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / FullTextQuery</p>"},{"location":"js/interfaces/FullTextQuery/#interface-fulltextquery","title":"Interface: FullTextQuery","text":"<p>Represents a full-text query interface. This interface defines the structure and behavior for full-text queries, including methods to retrieve the query type and convert the query to a dictionary format.</p>"},{"location":"js/interfaces/FullTextQuery/#methods","title":"Methods","text":""},{"location":"js/interfaces/FullTextQuery/#querytype","title":"queryType()","text":"<pre><code>queryType(): FullTextQueryType\n</code></pre> <p>The type of the full-text query.</p>"},{"location":"js/interfaces/FullTextQuery/#returns","title":"Returns","text":"<p><code>FullTextQueryType</code></p>"},{"location":"js/interfaces/FullTextSearchOptions/","title":"FullTextSearchOptions","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / FullTextSearchOptions</p>"},{"location":"js/interfaces/FullTextSearchOptions/#interface-fulltextsearchoptions","title":"Interface: FullTextSearchOptions","text":"<p>Options that control the behavior of a full text search</p>"},{"location":"js/interfaces/FullTextSearchOptions/#properties","title":"Properties","text":""},{"location":"js/interfaces/FullTextSearchOptions/#columns","title":"columns?","text":"<pre><code>optional columns: string | string[];\n</code></pre> <p>The columns to search</p> <p>If not specified, all indexed columns will be searched. For now, only one column can be searched.</p>"},{"location":"js/interfaces/HnswPqOptions/","title":"HnswPqOptions","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / HnswPqOptions</p>"},{"location":"js/interfaces/HnswPqOptions/#interface-hnswpqoptions","title":"Interface: HnswPqOptions","text":"<p>Options to create an <code>HNSW_PQ</code> index</p>"},{"location":"js/interfaces/HnswPqOptions/#properties","title":"Properties","text":""},{"location":"js/interfaces/HnswPqOptions/#distancetype","title":"distanceType?","text":"<pre><code>optional distanceType: \"l2\" | \"cosine\" | \"dot\";\n</code></pre> <p>The distance metric used to train the index.</p> <p>Default value is \"l2\".</p> <p>The following distance types are available:</p> <p>\"l2\" - Euclidean distance. This is a very common distance metric that accounts for both magnitude and direction when determining the distance between vectors. l2 distance has a range of [0, \u221e).</p> <p>\"cosine\" - Cosine distance.  Cosine distance is a distance metric calculated from the cosine similarity between two vectors. Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space. It is defined to equal the cosine of the angle between them.  Unlike l2, the cosine distance is not affected by the magnitude of the vectors.  Cosine distance has a range of [0, 2].</p> <p>\"dot\" - Dot product. Dot distance is the dot product of two vectors. Dot distance has a range of (-\u221e, \u221e). If the vectors are normalized (i.e. their l2 norm is 1), then dot distance is equivalent to the cosine distance.</p>"},{"location":"js/interfaces/HnswPqOptions/#efconstruction","title":"efConstruction?","text":"<pre><code>optional efConstruction: number;\n</code></pre> <p>The number of candidates to evaluate during the construction of the HNSW graph.</p> <p>The default value is 300.</p> <p>This value controls the tradeoff between build speed and accuracy. The higher the value the more accurate the build but the slower it will be. 150 to 300 is the typical range. 100 is a minimum for good quality search results. In most cases, there is no benefit to setting this higher than 500. This value should be set to a value that is not less than <code>ef</code> in the search phase.</p>"},{"location":"js/interfaces/HnswPqOptions/#m","title":"m?","text":"<pre><code>optional m: number;\n</code></pre> <p>The number of neighbors to select for each vector in the HNSW graph.</p> <p>The default value is 20.</p> <p>This value controls the tradeoff between search speed and accuracy. The higher the value the more accurate the search but the slower it will be.</p>"},{"location":"js/interfaces/HnswPqOptions/#maxiterations","title":"maxIterations?","text":"<pre><code>optional maxIterations: number;\n</code></pre> <p>Max iterations to train kmeans.</p> <p>The default value is 50.</p> <p>When training an IVF index we use kmeans to calculate the partitions.  This parameter controls how many iterations of kmeans to run.</p> <p>Increasing this might improve the quality of the index but in most cases the parameter is unused because kmeans will converge with fewer iterations.  The parameter is only used in cases where kmeans does not appear to converge.  In those cases it is unlikely that setting this larger will lead to the index converging anyways.</p>"},{"location":"js/interfaces/HnswPqOptions/#numpartitions","title":"numPartitions?","text":"<pre><code>optional numPartitions: number;\n</code></pre> <p>The number of IVF partitions to create.</p> <p>For HNSW, we recommend a small number of partitions. Setting this to 1 works well for most tables. For very large tables, training just one HNSW graph will require too much memory. Each partition becomes its own HNSW graph, so setting this value higher reduces the peak memory use of training.</p>"},{"location":"js/interfaces/HnswPqOptions/#numsubvectors","title":"numSubVectors?","text":"<pre><code>optional numSubVectors: number;\n</code></pre> <p>Number of sub-vectors of PQ.</p> <p>This value controls how much the vector is compressed during the quantization step. The more sub vectors there are the less the vector is compressed.  The default is the dimension of the vector divided by 16.  If the dimension is not evenly divisible by 16 we use the dimension divded by 8.</p> <p>The above two cases are highly preferred.  Having 8 or 16 values per subvector allows us to use efficient SIMD instructions.</p> <p>If the dimension is not visible by 8 then we use 1 subvector.  This is not ideal and will likely result in poor performance.</p>"},{"location":"js/interfaces/HnswPqOptions/#samplerate","title":"sampleRate?","text":"<pre><code>optional sampleRate: number;\n</code></pre> <p>The rate used to calculate the number of training vectors for kmeans.</p> <p>Default value is 256.</p> <p>When an IVF index is trained, we need to calculate partitions.  These are groups of vectors that are similar to each other.  To do this we use an algorithm called kmeans.</p> <p>Running kmeans on a large dataset can be slow.  To speed this up we run kmeans on a random sample of the data.  This parameter controls the size of the sample.  The total number of vectors used to train the index is <code>sample_rate * num_partitions</code>.</p> <p>Increasing this value might improve the quality of the index but in most cases the default should be sufficient.</p>"},{"location":"js/interfaces/HnswSqOptions/","title":"HnswSqOptions","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / HnswSqOptions</p>"},{"location":"js/interfaces/HnswSqOptions/#interface-hnswsqoptions","title":"Interface: HnswSqOptions","text":"<p>Options to create an <code>HNSW_SQ</code> index</p>"},{"location":"js/interfaces/HnswSqOptions/#properties","title":"Properties","text":""},{"location":"js/interfaces/HnswSqOptions/#distancetype","title":"distanceType?","text":"<pre><code>optional distanceType: \"l2\" | \"cosine\" | \"dot\";\n</code></pre> <p>The distance metric used to train the index.</p> <p>Default value is \"l2\".</p> <p>The following distance types are available:</p> <p>\"l2\" - Euclidean distance. This is a very common distance metric that accounts for both magnitude and direction when determining the distance between vectors. l2 distance has a range of [0, \u221e).</p> <p>\"cosine\" - Cosine distance.  Cosine distance is a distance metric calculated from the cosine similarity between two vectors. Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space. It is defined to equal the cosine of the angle between them.  Unlike l2, the cosine distance is not affected by the magnitude of the vectors.  Cosine distance has a range of [0, 2].</p> <p>\"dot\" - Dot product. Dot distance is the dot product of two vectors. Dot distance has a range of (-\u221e, \u221e). If the vectors are normalized (i.e. their l2 norm is 1), then dot distance is equivalent to the cosine distance.</p>"},{"location":"js/interfaces/HnswSqOptions/#efconstruction","title":"efConstruction?","text":"<pre><code>optional efConstruction: number;\n</code></pre> <p>The number of candidates to evaluate during the construction of the HNSW graph.</p> <p>The default value is 300.</p> <p>This value controls the tradeoff between build speed and accuracy. The higher the value the more accurate the build but the slower it will be. 150 to 300 is the typical range. 100 is a minimum for good quality search results. In most cases, there is no benefit to setting this higher than 500. This value should be set to a value that is not less than <code>ef</code> in the search phase.</p>"},{"location":"js/interfaces/HnswSqOptions/#m","title":"m?","text":"<pre><code>optional m: number;\n</code></pre> <p>The number of neighbors to select for each vector in the HNSW graph.</p> <p>The default value is 20.</p> <p>This value controls the tradeoff between search speed and accuracy. The higher the value the more accurate the search but the slower it will be.</p>"},{"location":"js/interfaces/HnswSqOptions/#maxiterations","title":"maxIterations?","text":"<pre><code>optional maxIterations: number;\n</code></pre> <p>Max iterations to train kmeans.</p> <p>The default value is 50.</p> <p>When training an IVF index we use kmeans to calculate the partitions.  This parameter controls how many iterations of kmeans to run.</p> <p>Increasing this might improve the quality of the index but in most cases the parameter is unused because kmeans will converge with fewer iterations.  The parameter is only used in cases where kmeans does not appear to converge.  In those cases it is unlikely that setting this larger will lead to the index converging anyways.</p>"},{"location":"js/interfaces/HnswSqOptions/#numpartitions","title":"numPartitions?","text":"<pre><code>optional numPartitions: number;\n</code></pre> <p>The number of IVF partitions to create.</p> <p>For HNSW, we recommend a small number of partitions. Setting this to 1 works well for most tables. For very large tables, training just one HNSW graph will require too much memory. Each partition becomes its own HNSW graph, so setting this value higher reduces the peak memory use of training.</p>"},{"location":"js/interfaces/HnswSqOptions/#samplerate","title":"sampleRate?","text":"<pre><code>optional sampleRate: number;\n</code></pre> <p>The rate used to calculate the number of training vectors for kmeans.</p> <p>Default value is 256.</p> <p>When an IVF index is trained, we need to calculate partitions.  These are groups of vectors that are similar to each other.  To do this we use an algorithm called kmeans.</p> <p>Running kmeans on a large dataset can be slow.  To speed this up we run kmeans on a random sample of the data.  This parameter controls the size of the sample.  The total number of vectors used to train the index is <code>sample_rate * num_partitions</code>.</p> <p>Increasing this value might improve the quality of the index but in most cases the default should be sufficient.</p>"},{"location":"js/interfaces/IndexConfig/","title":"IndexConfig","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / IndexConfig</p>"},{"location":"js/interfaces/IndexConfig/#interface-indexconfig","title":"Interface: IndexConfig","text":"<p>A description of an index currently configured on a column</p>"},{"location":"js/interfaces/IndexConfig/#properties","title":"Properties","text":""},{"location":"js/interfaces/IndexConfig/#columns","title":"columns","text":"<pre><code>columns: string[];\n</code></pre> <p>The columns in the index</p> <p>Currently this is always an array of size 1. In the future there may be more columns to represent composite indices.</p>"},{"location":"js/interfaces/IndexConfig/#indextype","title":"indexType","text":"<pre><code>indexType: string;\n</code></pre> <p>The type of the index</p>"},{"location":"js/interfaces/IndexConfig/#name","title":"name","text":"<pre><code>name: string;\n</code></pre> <p>The name of the index</p>"},{"location":"js/interfaces/IndexOptions/","title":"IndexOptions","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / IndexOptions</p>"},{"location":"js/interfaces/IndexOptions/#interface-indexoptions","title":"Interface: IndexOptions","text":""},{"location":"js/interfaces/IndexOptions/#properties","title":"Properties","text":""},{"location":"js/interfaces/IndexOptions/#config","title":"config?","text":"<pre><code>optional config: Index;\n</code></pre> <p>Advanced index configuration</p> <p>This option allows you to specify a specfic index to create and also allows you to pass in configuration for training the index.</p> <p>See the static methods on Index for details on the various index types.</p> <p>If this is not supplied then column data type(s) and column statistics will be used to determine the most useful kind of index to create.</p>"},{"location":"js/interfaces/IndexOptions/#replace","title":"replace?","text":"<pre><code>optional replace: boolean;\n</code></pre> <p>Whether to replace the existing index</p> <p>If this is false, and another index already exists on the same columns and the same name, then an error will be returned.  This is true even if that index is out of date.</p> <p>The default is true</p>"},{"location":"js/interfaces/IndexOptions/#waittimeoutseconds","title":"waitTimeoutSeconds?","text":"<pre><code>optional waitTimeoutSeconds: number;\n</code></pre>"},{"location":"js/interfaces/IndexStatistics/","title":"IndexStatistics","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / IndexStatistics</p>"},{"location":"js/interfaces/IndexStatistics/#interface-indexstatistics","title":"Interface: IndexStatistics","text":""},{"location":"js/interfaces/IndexStatistics/#properties","title":"Properties","text":""},{"location":"js/interfaces/IndexStatistics/#distancetype","title":"distanceType?","text":"<pre><code>optional distanceType: string;\n</code></pre> <p>The type of the distance function used by the index. This is only present for vector indices. Scalar and full text search indices do not have a distance function.</p>"},{"location":"js/interfaces/IndexStatistics/#indextype","title":"indexType","text":"<pre><code>indexType: string;\n</code></pre> <p>The type of the index</p>"},{"location":"js/interfaces/IndexStatistics/#loss","title":"loss?","text":"<pre><code>optional loss: number;\n</code></pre> <p>The KMeans loss value of the index, it is only present for vector indices.</p>"},{"location":"js/interfaces/IndexStatistics/#numindexedrows","title":"numIndexedRows","text":"<pre><code>numIndexedRows: number;\n</code></pre> <p>The number of rows indexed by the index</p>"},{"location":"js/interfaces/IndexStatistics/#numindices","title":"numIndices?","text":"<pre><code>optional numIndices: number;\n</code></pre> <p>The number of parts this index is split into.</p>"},{"location":"js/interfaces/IndexStatistics/#numunindexedrows","title":"numUnindexedRows","text":"<pre><code>numUnindexedRows: number;\n</code></pre> <p>The number of rows not indexed</p>"},{"location":"js/interfaces/IvfFlatOptions/","title":"IvfFlatOptions","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / IvfFlatOptions</p>"},{"location":"js/interfaces/IvfFlatOptions/#interface-ivfflatoptions","title":"Interface: IvfFlatOptions","text":"<p>Options to create an <code>IVF_FLAT</code> index</p>"},{"location":"js/interfaces/IvfFlatOptions/#properties","title":"Properties","text":""},{"location":"js/interfaces/IvfFlatOptions/#distancetype","title":"distanceType?","text":"<pre><code>optional distanceType: \"l2\" | \"cosine\" | \"dot\" | \"hamming\";\n</code></pre> <p>Distance type to use to build the index.</p> <p>Default value is \"l2\".</p> <p>This is used when training the index to calculate the IVF partitions (vectors are grouped in partitions with similar vectors according to this distance type).</p> <p>The distance type used to train an index MUST match the distance type used to search the index.  Failure to do so will yield inaccurate results.</p> <p>The following distance types are available:</p> <p>\"l2\" - Euclidean distance. This is a very common distance metric that accounts for both magnitude and direction when determining the distance between vectors. l2 distance has a range of [0, \u221e).</p> <p>\"cosine\" - Cosine distance.  Cosine distance is a distance metric calculated from the cosine similarity between two vectors. Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space. It is defined to equal the cosine of the angle between them.  Unlike l2, the cosine distance is not affected by the magnitude of the vectors.  Cosine distance has a range of [0, 2].</p> <p>Note: the cosine distance is undefined when one (or both) of the vectors are all zeros (there is no direction).  These vectors are invalid and may never be returned from a vector search.</p> <p>\"dot\" - Dot product. Dot distance is the dot product of two vectors. Dot distance has a range of (-\u221e, \u221e). If the vectors are normalized (i.e. their l2 norm is 1), then dot distance is equivalent to the cosine distance.</p> <p>\"hamming\" - Hamming distance. Hamming distance is a distance metric calculated from the number of bits that are different between two vectors. Hamming distance has a range of [0, dimension]. Note that the hamming distance is only valid for binary vectors.</p>"},{"location":"js/interfaces/IvfFlatOptions/#maxiterations","title":"maxIterations?","text":"<pre><code>optional maxIterations: number;\n</code></pre> <p>Max iteration to train IVF kmeans.</p> <p>When training an IVF FLAT index we use kmeans to calculate the partitions.  This parameter controls how many iterations of kmeans to run.</p> <p>Increasing this might improve the quality of the index but in most cases these extra iterations have diminishing returns.</p> <p>The default value is 50.</p>"},{"location":"js/interfaces/IvfFlatOptions/#numpartitions","title":"numPartitions?","text":"<pre><code>optional numPartitions: number;\n</code></pre> <p>The number of IVF partitions to create.</p> <p>This value should generally scale with the number of rows in the dataset. By default the number of partitions is the square root of the number of rows.</p> <p>If this value is too large then the first part of the search (picking the right partition) will be slow.  If this value is too small then the second part of the search (searching within a partition) will be slow.</p>"},{"location":"js/interfaces/IvfFlatOptions/#samplerate","title":"sampleRate?","text":"<pre><code>optional sampleRate: number;\n</code></pre> <p>The number of vectors, per partition, to sample when training IVF kmeans.</p> <p>When an IVF FLAT index is trained, we need to calculate partitions.  These are groups of vectors that are similar to each other.  To do this we use an algorithm called kmeans.</p> <p>Running kmeans on a large dataset can be slow.  To speed this up we run kmeans on a random sample of the data.  This parameter controls the size of the sample.  The total number of vectors used to train the index is <code>sample_rate * num_partitions</code>.</p> <p>Increasing this value might improve the quality of the index but in most cases the default should be sufficient.</p> <p>The default value is 256.</p>"},{"location":"js/interfaces/IvfPqOptions/","title":"IvfPqOptions","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / IvfPqOptions</p>"},{"location":"js/interfaces/IvfPqOptions/#interface-ivfpqoptions","title":"Interface: IvfPqOptions","text":"<p>Options to create an <code>IVF_PQ</code> index</p>"},{"location":"js/interfaces/IvfPqOptions/#properties","title":"Properties","text":""},{"location":"js/interfaces/IvfPqOptions/#distancetype","title":"distanceType?","text":"<pre><code>optional distanceType: \"l2\" | \"cosine\" | \"dot\";\n</code></pre> <p>Distance type to use to build the index.</p> <p>Default value is \"l2\".</p> <p>This is used when training the index to calculate the IVF partitions (vectors are grouped in partitions with similar vectors according to this distance type) and to calculate a subvector's code during quantization.</p> <p>The distance type used to train an index MUST match the distance type used to search the index.  Failure to do so will yield inaccurate results.</p> <p>The following distance types are available:</p> <p>\"l2\" - Euclidean distance. This is a very common distance metric that accounts for both magnitude and direction when determining the distance between vectors. l2 distance has a range of [0, \u221e).</p> <p>\"cosine\" - Cosine distance.  Cosine distance is a distance metric calculated from the cosine similarity between two vectors. Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space. It is defined to equal the cosine of the angle between them.  Unlike l2, the cosine distance is not affected by the magnitude of the vectors.  Cosine distance has a range of [0, 2].</p> <p>Note: the cosine distance is undefined when one (or both) of the vectors are all zeros (there is no direction).  These vectors are invalid and may never be returned from a vector search.</p> <p>\"dot\" - Dot product. Dot distance is the dot product of two vectors. Dot distance has a range of (-\u221e, \u221e). If the vectors are normalized (i.e. their l2 norm is 1), then dot distance is equivalent to the cosine distance.</p>"},{"location":"js/interfaces/IvfPqOptions/#maxiterations","title":"maxIterations?","text":"<pre><code>optional maxIterations: number;\n</code></pre> <p>Max iteration to train IVF kmeans.</p> <p>When training an IVF PQ index we use kmeans to calculate the partitions.  This parameter controls how many iterations of kmeans to run.</p> <p>Increasing this might improve the quality of the index but in most cases these extra iterations have diminishing returns.</p> <p>The default value is 50.</p>"},{"location":"js/interfaces/IvfPqOptions/#numbits","title":"numBits?","text":"<pre><code>optional numBits: number;\n</code></pre> <p>Number of bits per sub-vector.</p> <p>This value controls how much each subvector is compressed.  The more bits the more accurate the index will be but the slower search.  The default is 8 bits.</p> <p>The number of bits must be 4 or 8.</p>"},{"location":"js/interfaces/IvfPqOptions/#numpartitions","title":"numPartitions?","text":"<pre><code>optional numPartitions: number;\n</code></pre> <p>The number of IVF partitions to create.</p> <p>This value should generally scale with the number of rows in the dataset. By default the number of partitions is the square root of the number of rows.</p> <p>If this value is too large then the first part of the search (picking the right partition) will be slow.  If this value is too small then the second part of the search (searching within a partition) will be slow.</p>"},{"location":"js/interfaces/IvfPqOptions/#numsubvectors","title":"numSubVectors?","text":"<pre><code>optional numSubVectors: number;\n</code></pre> <p>Number of sub-vectors of PQ.</p> <p>This value controls how much the vector is compressed during the quantization step. The more sub vectors there are the less the vector is compressed.  The default is the dimension of the vector divided by 16.  If the dimension is not evenly divisible by 16 we use the dimension divded by 8.</p> <p>The above two cases are highly preferred.  Having 8 or 16 values per subvector allows us to use efficient SIMD instructions.</p> <p>If the dimension is not visible by 8 then we use 1 subvector.  This is not ideal and will likely result in poor performance.</p>"},{"location":"js/interfaces/IvfPqOptions/#samplerate","title":"sampleRate?","text":"<pre><code>optional sampleRate: number;\n</code></pre> <p>The number of vectors, per partition, to sample when training IVF kmeans.</p> <p>When an IVF PQ index is trained, we need to calculate partitions.  These are groups of vectors that are similar to each other.  To do this we use an algorithm called kmeans.</p> <p>Running kmeans on a large dataset can be slow.  To speed this up we run kmeans on a random sample of the data.  This parameter controls the size of the sample.  The total number of vectors used to train the index is <code>sample_rate * num_partitions</code>.</p> <p>Increasing this value might improve the quality of the index but in most cases the default should be sufficient.</p> <p>The default value is 256.</p>"},{"location":"js/interfaces/MergeResult/","title":"MergeResult","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / MergeResult</p>"},{"location":"js/interfaces/MergeResult/#interface-mergeresult","title":"Interface: MergeResult","text":""},{"location":"js/interfaces/MergeResult/#properties","title":"Properties","text":""},{"location":"js/interfaces/MergeResult/#numdeletedrows","title":"numDeletedRows","text":"<pre><code>numDeletedRows: number;\n</code></pre>"},{"location":"js/interfaces/MergeResult/#numinsertedrows","title":"numInsertedRows","text":"<pre><code>numInsertedRows: number;\n</code></pre>"},{"location":"js/interfaces/MergeResult/#numupdatedrows","title":"numUpdatedRows","text":"<pre><code>numUpdatedRows: number;\n</code></pre>"},{"location":"js/interfaces/MergeResult/#version","title":"version","text":"<pre><code>version: number;\n</code></pre>"},{"location":"js/interfaces/OpenTableOptions/","title":"OpenTableOptions","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / OpenTableOptions</p>"},{"location":"js/interfaces/OpenTableOptions/#interface-opentableoptions","title":"Interface: OpenTableOptions","text":""},{"location":"js/interfaces/OpenTableOptions/#properties","title":"Properties","text":""},{"location":"js/interfaces/OpenTableOptions/#indexcachesize","title":"indexCacheSize?","text":"<pre><code>optional indexCacheSize: number;\n</code></pre> <p>Set the size of the index cache, specified as a number of entries</p> <p>The exact meaning of an \"entry\" will depend on the type of index: - IVF: there is one entry for each IVF partition - BTREE: there is one entry for the entire index</p> <p>This cache applies to the entire opened table, across all indices. Setting this value higher will increase performance on larger datasets at the expense of more RAM</p>"},{"location":"js/interfaces/OpenTableOptions/#storageoptions","title":"storageOptions?","text":"<pre><code>optional storageOptions: Record&lt;string, string&gt;;\n</code></pre> <p>Configuration for object storage.</p> <p>Options already set on the connection will be inherited by the table, but can be overridden here.</p> <p>The available options are described at https://lancedb.github.io/lancedb/guides/storage/</p>"},{"location":"js/interfaces/OptimizeOptions/","title":"OptimizeOptions","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / OptimizeOptions</p>"},{"location":"js/interfaces/OptimizeOptions/#interface-optimizeoptions","title":"Interface: OptimizeOptions","text":""},{"location":"js/interfaces/OptimizeOptions/#properties","title":"Properties","text":""},{"location":"js/interfaces/OptimizeOptions/#cleanupolderthan","title":"cleanupOlderThan","text":"<pre><code>cleanupOlderThan: Date;\n</code></pre> <p>If set then all versions older than the given date be removed.  The current version will never be removed. The default is 7 days</p>"},{"location":"js/interfaces/OptimizeOptions/#example","title":"Example","text":"<pre><code>// Delete all versions older than 1 day\nconst olderThan = new Date();\nolderThan.setDate(olderThan.getDate() - 1));\ntbl.cleanupOlderVersions(olderThan);\n\n// Delete all versions except the current version\ntbl.cleanupOlderVersions(new Date());\n</code></pre>"},{"location":"js/interfaces/OptimizeOptions/#deleteunverified","title":"deleteUnverified","text":"<pre><code>deleteUnverified: boolean;\n</code></pre>"},{"location":"js/interfaces/OptimizeStats/","title":"OptimizeStats","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / OptimizeStats</p>"},{"location":"js/interfaces/OptimizeStats/#interface-optimizestats","title":"Interface: OptimizeStats","text":"<p>Statistics about an optimize operation</p>"},{"location":"js/interfaces/OptimizeStats/#properties","title":"Properties","text":""},{"location":"js/interfaces/OptimizeStats/#compaction","title":"compaction","text":"<pre><code>compaction: CompactionStats;\n</code></pre> <p>Statistics about the compaction operation</p>"},{"location":"js/interfaces/OptimizeStats/#prune","title":"prune","text":"<pre><code>prune: RemovalStats;\n</code></pre> <p>Statistics about the removal operation</p>"},{"location":"js/interfaces/QueryExecutionOptions/","title":"QueryExecutionOptions","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / QueryExecutionOptions</p>"},{"location":"js/interfaces/QueryExecutionOptions/#interface-queryexecutionoptions","title":"Interface: QueryExecutionOptions","text":"<p>Options that control the behavior of a particular query execution</p>"},{"location":"js/interfaces/QueryExecutionOptions/#properties","title":"Properties","text":""},{"location":"js/interfaces/QueryExecutionOptions/#maxbatchlength","title":"maxBatchLength?","text":"<pre><code>optional maxBatchLength: number;\n</code></pre> <p>The maximum number of rows to return in a single batch</p> <p>Batches may have fewer rows if the underlying data is stored in smaller chunks.</p>"},{"location":"js/interfaces/QueryExecutionOptions/#timeoutms","title":"timeoutMs?","text":"<pre><code>optional timeoutMs: number;\n</code></pre> <p>Timeout for query execution in milliseconds</p>"},{"location":"js/interfaces/RemovalStats/","title":"RemovalStats","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / RemovalStats</p>"},{"location":"js/interfaces/RemovalStats/#interface-removalstats","title":"Interface: RemovalStats","text":"<p>Statistics about a cleanup operation</p>"},{"location":"js/interfaces/RemovalStats/#properties","title":"Properties","text":""},{"location":"js/interfaces/RemovalStats/#bytesremoved","title":"bytesRemoved","text":"<pre><code>bytesRemoved: number;\n</code></pre> <p>The number of bytes removed</p>"},{"location":"js/interfaces/RemovalStats/#oldversionsremoved","title":"oldVersionsRemoved","text":"<pre><code>oldVersionsRemoved: number;\n</code></pre> <p>The number of old versions removed</p>"},{"location":"js/interfaces/RetryConfig/","title":"RetryConfig","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / RetryConfig</p>"},{"location":"js/interfaces/RetryConfig/#interface-retryconfig","title":"Interface: RetryConfig","text":"<p>Retry configuration for the remote HTTP client.</p>"},{"location":"js/interfaces/RetryConfig/#properties","title":"Properties","text":""},{"location":"js/interfaces/RetryConfig/#backofffactor","title":"backoffFactor?","text":"<pre><code>optional backoffFactor: number;\n</code></pre> <p>The backoff factor to apply between retries. Default is 0.25. Between each retry the client will wait for the amount of seconds: <code>{backoff factor} * (2 ** ({number of previous retries}))</code>. So for the default of 0.25, the first retry will wait 0.25 seconds, the second retry will wait 0.5 seconds, the third retry will wait 1 second, etc.</p> <p>You can also set this via the environment variable <code>LANCE_CLIENT_RETRY_BACKOFF_FACTOR</code>.</p>"},{"location":"js/interfaces/RetryConfig/#backoffjitter","title":"backoffJitter?","text":"<pre><code>optional backoffJitter: number;\n</code></pre> <p>The jitter to apply to the backoff factor, in seconds. Default is 0.25.</p> <p>A random value between 0 and <code>backoff_jitter</code> will be added to the backoff factor in seconds. So for the default of 0.25 seconds, between 0 and 250 milliseconds will be added to the sleep between each retry.</p> <p>You can also set this via the environment variable <code>LANCE_CLIENT_RETRY_BACKOFF_JITTER</code>.</p>"},{"location":"js/interfaces/RetryConfig/#connectretries","title":"connectRetries?","text":"<pre><code>optional connectRetries: number;\n</code></pre> <p>The maximum number of retries for connection errors. Default is 3. You can also set this via the environment variable <code>LANCE_CLIENT_CONNECT_RETRIES</code>.</p>"},{"location":"js/interfaces/RetryConfig/#readretries","title":"readRetries?","text":"<pre><code>optional readRetries: number;\n</code></pre> <p>The maximum number of retries for read errors. Default is 3. You can also set this via the environment variable <code>LANCE_CLIENT_READ_RETRIES</code>.</p>"},{"location":"js/interfaces/RetryConfig/#retries","title":"retries?","text":"<pre><code>optional retries: number;\n</code></pre> <p>The maximum number of retries for a request. Default is 3. You can also set this via the environment variable <code>LANCE_CLIENT_MAX_RETRIES</code>.</p>"},{"location":"js/interfaces/RetryConfig/#statuses","title":"statuses?","text":"<pre><code>optional statuses: number[];\n</code></pre> <p>The HTTP status codes for which to retry the request. Default is [429, 500, 502, 503].</p> <p>You can also set this via the environment variable <code>LANCE_CLIENT_RETRY_STATUSES</code>. Use a comma-separated list of integers.</p>"},{"location":"js/interfaces/TableNamesOptions/","title":"TableNamesOptions","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / TableNamesOptions</p>"},{"location":"js/interfaces/TableNamesOptions/#interface-tablenamesoptions","title":"Interface: TableNamesOptions","text":""},{"location":"js/interfaces/TableNamesOptions/#properties","title":"Properties","text":""},{"location":"js/interfaces/TableNamesOptions/#limit","title":"limit?","text":"<pre><code>optional limit: number;\n</code></pre> <p>An optional limit to the number of results to return.</p>"},{"location":"js/interfaces/TableNamesOptions/#startafter","title":"startAfter?","text":"<pre><code>optional startAfter: string;\n</code></pre> <p>If present, only return names that come lexicographically after the supplied value.</p> <p>This can be combined with limit to implement pagination by setting this to the last table name from the previous page.</p>"},{"location":"js/interfaces/TableStatistics/","title":"TableStatistics","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / TableStatistics</p>"},{"location":"js/interfaces/TableStatistics/#interface-tablestatistics","title":"Interface: TableStatistics","text":""},{"location":"js/interfaces/TableStatistics/#properties","title":"Properties","text":""},{"location":"js/interfaces/TableStatistics/#fragmentstats","title":"fragmentStats","text":"<pre><code>fragmentStats: FragmentStatistics;\n</code></pre> <p>Statistics on table fragments</p>"},{"location":"js/interfaces/TableStatistics/#numindices","title":"numIndices","text":"<pre><code>numIndices: number;\n</code></pre> <p>The number of indices in the table</p>"},{"location":"js/interfaces/TableStatistics/#numrows","title":"numRows","text":"<pre><code>numRows: number;\n</code></pre> <p>The number of rows in the table</p>"},{"location":"js/interfaces/TableStatistics/#totalbytes","title":"totalBytes","text":"<pre><code>totalBytes: number;\n</code></pre> <p>The total number of bytes in the table</p>"},{"location":"js/interfaces/TimeoutConfig/","title":"TimeoutConfig","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / TimeoutConfig</p>"},{"location":"js/interfaces/TimeoutConfig/#interface-timeoutconfig","title":"Interface: TimeoutConfig","text":"<p>Timeout configuration for remote HTTP client.</p>"},{"location":"js/interfaces/TimeoutConfig/#properties","title":"Properties","text":""},{"location":"js/interfaces/TimeoutConfig/#connecttimeout","title":"connectTimeout?","text":"<pre><code>optional connectTimeout: number;\n</code></pre> <p>The timeout for establishing a connection in seconds. Default is 120 seconds (2 minutes). This can also be set via the environment variable <code>LANCE_CLIENT_CONNECT_TIMEOUT</code>, as an integer number of seconds.</p>"},{"location":"js/interfaces/TimeoutConfig/#poolidletimeout","title":"poolIdleTimeout?","text":"<pre><code>optional poolIdleTimeout: number;\n</code></pre> <p>The timeout for keeping idle connections in the connection pool in seconds. Default is 300 seconds (5 minutes). This can also be set via the environment variable <code>LANCE_CLIENT_CONNECTION_TIMEOUT</code>, as an integer number of seconds.</p>"},{"location":"js/interfaces/TimeoutConfig/#readtimeout","title":"readTimeout?","text":"<pre><code>optional readTimeout: number;\n</code></pre> <p>The timeout for reading data from the server in seconds. Default is 300 seconds (5 minutes). This can also be set via the environment variable <code>LANCE_CLIENT_READ_TIMEOUT</code>, as an integer number of seconds.</p>"},{"location":"js/interfaces/UpdateOptions/","title":"UpdateOptions","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / UpdateOptions</p>"},{"location":"js/interfaces/UpdateOptions/#interface-updateoptions","title":"Interface: UpdateOptions","text":""},{"location":"js/interfaces/UpdateOptions/#properties","title":"Properties","text":""},{"location":"js/interfaces/UpdateOptions/#where","title":"where","text":"<pre><code>where: string;\n</code></pre> <p>A filter that limits the scope of the update.</p> <p>This should be an SQL filter expression.</p> <p>Only rows that satisfy the expression will be updated.</p> <p>For example, this could be 'my_col == 0' to replace all instances of 0 in a column with some other default value.</p>"},{"location":"js/interfaces/UpdateResult/","title":"UpdateResult","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / UpdateResult</p>"},{"location":"js/interfaces/UpdateResult/#interface-updateresult","title":"Interface: UpdateResult","text":""},{"location":"js/interfaces/UpdateResult/#properties","title":"Properties","text":""},{"location":"js/interfaces/UpdateResult/#rowsupdated","title":"rowsUpdated","text":"<pre><code>rowsUpdated: number;\n</code></pre>"},{"location":"js/interfaces/UpdateResult/#version","title":"version","text":"<pre><code>version: number;\n</code></pre>"},{"location":"js/interfaces/Version/","title":"Version","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / Version</p>"},{"location":"js/interfaces/Version/#interface-version","title":"Interface: Version","text":""},{"location":"js/interfaces/Version/#properties","title":"Properties","text":""},{"location":"js/interfaces/Version/#metadata","title":"metadata","text":"<pre><code>metadata: Record&lt;string, string&gt;;\n</code></pre>"},{"location":"js/interfaces/Version/#timestamp","title":"timestamp","text":"<pre><code>timestamp: Date;\n</code></pre>"},{"location":"js/interfaces/Version/#version","title":"version","text":"<pre><code>version: number;\n</code></pre>"},{"location":"js/interfaces/WriteExecutionOptions/","title":"WriteExecutionOptions","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / WriteExecutionOptions</p>"},{"location":"js/interfaces/WriteExecutionOptions/#interface-writeexecutionoptions","title":"Interface: WriteExecutionOptions","text":""},{"location":"js/interfaces/WriteExecutionOptions/#properties","title":"Properties","text":""},{"location":"js/interfaces/WriteExecutionOptions/#timeoutms","title":"timeoutMs?","text":"<pre><code>optional timeoutMs: number;\n</code></pre> <p>Maximum time to run the operation before cancelling it.</p> <p>By default, there is a 30-second timeout that is only enforced after the first attempt. This is to prevent spending too long retrying to resolve conflicts. For example, if a write attempt takes 20 seconds and fails, the second attempt will be cancelled after 10 seconds, hitting the 30-second timeout. However, a write that takes one hour and succeeds on the first attempt will not be cancelled.</p> <p>When this is set, the timeout is enforced on all attempts, including the first.</p>"},{"location":"js/namespaces/embedding/","title":"Index","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / embedding</p>"},{"location":"js/namespaces/embedding/#embedding","title":"embedding","text":""},{"location":"js/namespaces/embedding/#index","title":"Index","text":""},{"location":"js/namespaces/embedding/#classes","title":"Classes","text":"<ul> <li>EmbeddingFunction</li> <li>EmbeddingFunctionRegistry</li> <li>TextEmbeddingFunction</li> </ul>"},{"location":"js/namespaces/embedding/#interfaces","title":"Interfaces","text":"<ul> <li>EmbeddingFunctionConfig</li> <li>EmbeddingFunctionConstructor</li> <li>EmbeddingFunctionCreate</li> <li>FieldOptions</li> <li>FunctionOptions</li> </ul>"},{"location":"js/namespaces/embedding/#type-aliases","title":"Type Aliases","text":"<ul> <li>CreateReturnType</li> </ul>"},{"location":"js/namespaces/embedding/#functions","title":"Functions","text":"<ul> <li>LanceSchema</li> <li>getRegistry</li> <li>register</li> </ul>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/","title":"EmbeddingFunction","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / embedding / EmbeddingFunction</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#class-abstract-embeddingfunctiont-m","title":"Class: <code>abstract</code> EmbeddingFunction&lt;T, M&gt;","text":"<p>An embedding function that automatically creates vector representation for a given column.</p> <p>It's important subclasses pass the original options to the super constructor and then pass those options to <code>resolveVariables</code> to resolve any variables before using them.</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#example","title":"Example","text":"<pre><code>class MyEmbeddingFunction extends EmbeddingFunction {\n  constructor(options: {model: string, timeout: number}) {\n    super(optionsRaw);\n    const options = this.resolveVariables(optionsRaw);\n    this.model = options.model;\n    this.timeout = options.timeout;\n  }\n}\n</code></pre>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#extended-by","title":"Extended by","text":"<ul> <li><code>TextEmbeddingFunction</code></li> </ul>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#type-parameters","title":"Type Parameters","text":"<p>\u2022 T = <code>any</code></p> <p>\u2022 M extends <code>FunctionOptions</code> = <code>FunctionOptions</code></p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#constructors","title":"Constructors","text":""},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#new-embeddingfunction","title":"new EmbeddingFunction()","text":"<pre><code>new EmbeddingFunction&lt;T, M&gt;(): EmbeddingFunction&lt;T, M&gt;\n</code></pre>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#returns","title":"Returns","text":"<p><code>EmbeddingFunction</code>&lt;<code>T</code>, <code>M</code>&gt;</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#methods","title":"Methods","text":""},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#computequeryembeddings","title":"computeQueryEmbeddings()","text":"<pre><code>computeQueryEmbeddings(data): Promise&lt;number[] | Float32Array | Float64Array&gt;\n</code></pre> <p>Compute the embeddings for a single query</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#parameters","title":"Parameters","text":"<ul> <li>data: <code>T</code></li> </ul>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#returns_1","title":"Returns","text":"<p><code>Promise</code>&lt;<code>number</code>[] | <code>Float32Array</code> | <code>Float64Array</code>&gt;</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#computesourceembeddings","title":"computeSourceEmbeddings()","text":"<pre><code>abstract computeSourceEmbeddings(data): Promise&lt;number[][] | Float32Array[] | Float64Array[]&gt;\n</code></pre> <p>Creates a vector representation for the given values.</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#parameters_1","title":"Parameters","text":"<ul> <li>data: <code>T</code>[]</li> </ul>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#returns_2","title":"Returns","text":"<p><code>Promise</code>&lt;<code>number</code>[][] | <code>Float32Array</code>[] | <code>Float64Array</code>[]&gt;</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#embeddingdatatype","title":"embeddingDataType()","text":"<pre><code>abstract embeddingDataType(): Float&lt;Floats&gt;\n</code></pre> <p>The datatype of the embeddings</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#returns_3","title":"Returns","text":"<p><code>Float</code>&lt;<code>Floats</code>&gt;</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#getsensitivekeys","title":"getSensitiveKeys()","text":"<pre><code>protected getSensitiveKeys(): string[]\n</code></pre> <p>Provide a list of keys in the function options that should be treated as sensitive. If users pass raw values for these keys, they will be rejected.</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#returns_4","title":"Returns","text":"<p><code>string</code>[]</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#init","title":"init()?","text":"<pre><code>optional init(): Promise&lt;void&gt;\n</code></pre> <p>Optionally load any resources needed for the embedding function.</p> <p>This method is called after the embedding function has been initialized but before any embeddings are computed. It is useful for loading local models or other resources that are needed for the embedding function to work.</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#returns_5","title":"Returns","text":"<p><code>Promise</code>&lt;<code>void</code>&gt;</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#ndims","title":"ndims()","text":"<pre><code>ndims(): undefined | number\n</code></pre> <p>The number of dimensions of the embeddings</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#returns_6","title":"Returns","text":"<p><code>undefined</code> | <code>number</code></p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#resolvevariables","title":"resolveVariables()","text":"<pre><code>protected resolveVariables(config): Partial&lt;M&gt;\n</code></pre> <p>Apply variables to the config.</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#parameters_2","title":"Parameters","text":"<ul> <li>config: <code>Partial</code>&lt;<code>M</code>&gt;</li> </ul>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#returns_7","title":"Returns","text":"<p><code>Partial</code>&lt;<code>M</code>&gt;</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#sourcefield","title":"sourceField()","text":"<pre><code>sourceField(optionsOrDatatype): [DataType&lt;Type, any&gt;, Map&lt;string, EmbeddingFunction&lt;any, FunctionOptions&gt;&gt;]\n</code></pre> <p>sourceField is used in combination with <code>LanceSchema</code> to provide a declarative data model</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#parameters_3","title":"Parameters","text":"<ul> <li>optionsOrDatatype: <code>DataType</code>&lt;<code>Type</code>, <code>any</code>&gt; | <code>Partial</code>&lt;<code>FieldOptions</code>&lt;<code>DataType</code>&lt;<code>Type</code>, <code>any</code>&gt;&gt;&gt;     The options for the field or the datatype</li> </ul>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#returns_8","title":"Returns","text":"<p>[<code>DataType</code>&lt;<code>Type</code>, <code>any</code>&gt;, <code>Map</code>&lt;<code>string</code>, <code>EmbeddingFunction</code>&lt;<code>any</code>, <code>FunctionOptions</code>&gt;&gt;]</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#see","title":"See","text":"<p>LanceSchema</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#tojson","title":"toJSON()","text":"<pre><code>toJSON(): Record&lt;string, any&gt;\n</code></pre> <p>Get the original arguments to the constructor, to serialize them so they can be used to recreate the embedding function later.</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#returns_9","title":"Returns","text":"<p><code>Record</code>&lt;<code>string</code>, <code>any</code>&gt;</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#vectorfield","title":"vectorField()","text":"<pre><code>vectorField(optionsOrDatatype?): [DataType&lt;Type, any&gt;, Map&lt;string, EmbeddingFunction&lt;any, FunctionOptions&gt;&gt;]\n</code></pre> <p>vectorField is used in combination with <code>LanceSchema</code> to provide a declarative data model</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#parameters_4","title":"Parameters","text":"<ul> <li>optionsOrDatatype?: <code>DataType</code>&lt;<code>Type</code>, <code>any</code>&gt; | <code>Partial</code>&lt;<code>FieldOptions</code>&lt;<code>DataType</code>&lt;<code>Type</code>, <code>any</code>&gt;&gt;&gt;     The options for the field</li> </ul>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#returns_10","title":"Returns","text":"<p>[<code>DataType</code>&lt;<code>Type</code>, <code>any</code>&gt;, <code>Map</code>&lt;<code>string</code>, <code>EmbeddingFunction</code>&lt;<code>any</code>, <code>FunctionOptions</code>&gt;&gt;]</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunction/#see_1","title":"See","text":"<p>LanceSchema</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/","title":"EmbeddingFunctionRegistry","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / embedding / EmbeddingFunctionRegistry</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#class-embeddingfunctionregistry","title":"Class: EmbeddingFunctionRegistry","text":"<p>This is a singleton class used to register embedding functions and fetch them by name. It also handles serializing and deserializing. You can implement your own embedding function by subclassing EmbeddingFunction or TextEmbeddingFunction and registering it with the registry</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#constructors","title":"Constructors","text":""},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#new-embeddingfunctionregistry","title":"new EmbeddingFunctionRegistry()","text":"<pre><code>new EmbeddingFunctionRegistry(): EmbeddingFunctionRegistry\n</code></pre>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#returns","title":"Returns","text":"<p><code>EmbeddingFunctionRegistry</code></p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#methods","title":"Methods","text":""},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#functiontometadata","title":"functionToMetadata()","text":"<pre><code>functionToMetadata(conf): Record&lt;string, any&gt;\n</code></pre>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#parameters","title":"Parameters","text":"<ul> <li>conf: <code>EmbeddingFunctionConfig</code></li> </ul>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#returns_1","title":"Returns","text":"<p><code>Record</code>&lt;<code>string</code>, <code>any</code>&gt;</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#get","title":"get()","text":"<pre><code>get&lt;T&gt;(name): undefined | EmbeddingFunctionCreate&lt;T&gt;\n</code></pre> <p>Fetch an embedding function by name</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#type-parameters","title":"Type Parameters","text":"<p>\u2022 T extends <code>EmbeddingFunction</code>&lt;<code>unknown</code>, <code>FunctionOptions</code>&gt;</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#parameters_1","title":"Parameters","text":"<ul> <li>name: <code>string</code>     The name of the function</li> </ul>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#returns_2","title":"Returns","text":"<p><code>undefined</code> | <code>EmbeddingFunctionCreate</code>&lt;<code>T</code>&gt;</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#gettablemetadata","title":"getTableMetadata()","text":"<pre><code>getTableMetadata(functions): Map&lt;string, string&gt;\n</code></pre>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#parameters_2","title":"Parameters","text":"<ul> <li>functions: <code>EmbeddingFunctionConfig</code>[]</li> </ul>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#returns_3","title":"Returns","text":"<p><code>Map</code>&lt;<code>string</code>, <code>string</code>&gt;</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#getvar","title":"getVar()","text":"<pre><code>getVar(name): undefined | string\n</code></pre> <p>Get a variable.</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#parameters_3","title":"Parameters","text":"<ul> <li>name: <code>string</code></li> </ul>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#returns_4","title":"Returns","text":"<p><code>undefined</code> | <code>string</code></p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#see","title":"See","text":"<p>setVar</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#length","title":"length()","text":"<pre><code>length(): number\n</code></pre> <p>Get the number of registered functions</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#returns_5","title":"Returns","text":"<p><code>number</code></p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#register","title":"register()","text":"<pre><code>register&lt;T&gt;(this, alias?): (ctor) =&gt; any\n</code></pre> <p>Register an embedding function</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#type-parameters_1","title":"Type Parameters","text":"<p>\u2022 T extends <code>EmbeddingFunctionConstructor</code>&lt;<code>EmbeddingFunction</code>&lt;<code>any</code>, <code>FunctionOptions</code>&gt;&gt; = <code>EmbeddingFunctionConstructor</code>&lt;<code>EmbeddingFunction</code>&lt;<code>any</code>, <code>FunctionOptions</code>&gt;&gt;</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#parameters_4","title":"Parameters","text":"<ul> <li> <p>this: <code>EmbeddingFunctionRegistry</code></p> </li> <li> <p>alias?: <code>string</code></p> </li> </ul>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#returns_6","title":"Returns","text":"<p><code>Function</code></p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#parameters_5","title":"Parameters","text":"<ul> <li>ctor: <code>T</code></li> </ul>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#returns_7","title":"Returns","text":"<p><code>any</code></p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#throws","title":"Throws","text":"<p>Error if the function is already registered</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#reset","title":"reset()","text":"<pre><code>reset(this): void\n</code></pre> <p>reset the registry to the initial state</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#parameters_6","title":"Parameters","text":"<ul> <li>this: <code>EmbeddingFunctionRegistry</code></li> </ul>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#returns_8","title":"Returns","text":"<p><code>void</code></p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#setvar","title":"setVar()","text":"<pre><code>setVar(name, value): void\n</code></pre> <p>Set a variable. These can be accessed in the embedding function configuration using the syntax <code>$var:variable_name</code>. If they are not set, an error will be thrown letting you know which key is unset. If you want to supply a default value, you can add an additional part in the configuration like so: <code>$var:variable_name:default_value</code>. Default values can be used for runtime configurations that are not sensitive, such as whether to use a GPU for inference.</p> <p>The name must not contain colons. The default value can contain colons.</p>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#parameters_7","title":"Parameters","text":"<ul> <li> <p>name: <code>string</code></p> </li> <li> <p>value: <code>string</code></p> </li> </ul>"},{"location":"js/namespaces/embedding/classes/EmbeddingFunctionRegistry/#returns_9","title":"Returns","text":"<p><code>void</code></p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/","title":"TextEmbeddingFunction","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / embedding / TextEmbeddingFunction</p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#class-abstract-textembeddingfunctionm","title":"Class: <code>abstract</code> TextEmbeddingFunction&lt;M&gt;","text":"<p>an abstract class for implementing embedding functions that take text as input</p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#extends","title":"Extends","text":"<ul> <li><code>EmbeddingFunction</code>&lt;<code>string</code>, <code>M</code>&gt;</li> </ul>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#type-parameters","title":"Type Parameters","text":"<p>\u2022 M extends <code>FunctionOptions</code> = <code>FunctionOptions</code></p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#constructors","title":"Constructors","text":""},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#new-textembeddingfunction","title":"new TextEmbeddingFunction()","text":"<pre><code>new TextEmbeddingFunction&lt;M&gt;(): TextEmbeddingFunction&lt;M&gt;\n</code></pre>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#returns","title":"Returns","text":"<p><code>TextEmbeddingFunction</code>&lt;<code>M</code>&gt;</p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#inherited-from","title":"Inherited from","text":"<p><code>EmbeddingFunction</code>.<code>constructor</code></p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#methods","title":"Methods","text":""},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#computequeryembeddings","title":"computeQueryEmbeddings()","text":"<pre><code>computeQueryEmbeddings(data): Promise&lt;number[] | Float32Array | Float64Array&gt;\n</code></pre> <p>Compute the embeddings for a single query</p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#parameters","title":"Parameters","text":"<ul> <li>data: <code>string</code></li> </ul>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#returns_1","title":"Returns","text":"<p><code>Promise</code>&lt;<code>number</code>[] | <code>Float32Array</code> | <code>Float64Array</code>&gt;</p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#overrides","title":"Overrides","text":"<p><code>EmbeddingFunction</code>.<code>computeQueryEmbeddings</code></p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#computesourceembeddings","title":"computeSourceEmbeddings()","text":"<pre><code>computeSourceEmbeddings(data): Promise&lt;number[][] | Float32Array[] | Float64Array[]&gt;\n</code></pre> <p>Creates a vector representation for the given values.</p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#parameters_1","title":"Parameters","text":"<ul> <li>data: <code>string</code>[]</li> </ul>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#returns_2","title":"Returns","text":"<p><code>Promise</code>&lt;<code>number</code>[][] | <code>Float32Array</code>[] | <code>Float64Array</code>[]&gt;</p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#overrides_1","title":"Overrides","text":"<p><code>EmbeddingFunction</code>.<code>computeSourceEmbeddings</code></p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#embeddingdatatype","title":"embeddingDataType()","text":"<pre><code>embeddingDataType(): Float&lt;Floats&gt;\n</code></pre> <p>The datatype of the embeddings</p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#returns_3","title":"Returns","text":"<p><code>Float</code>&lt;<code>Floats</code>&gt;</p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#overrides_2","title":"Overrides","text":"<p><code>EmbeddingFunction</code>.<code>embeddingDataType</code></p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#generateembeddings","title":"generateEmbeddings()","text":"<pre><code>abstract generateEmbeddings(texts, ...args): Promise&lt;number[][] | Float32Array[] | Float64Array[]&gt;\n</code></pre>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#parameters_2","title":"Parameters","text":"<ul> <li> <p>texts: <code>string</code>[]</p> </li> <li> <p>...args: <code>any</code>[]</p> </li> </ul>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#returns_4","title":"Returns","text":"<p><code>Promise</code>&lt;<code>number</code>[][] | <code>Float32Array</code>[] | <code>Float64Array</code>[]&gt;</p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#getsensitivekeys","title":"getSensitiveKeys()","text":"<pre><code>protected getSensitiveKeys(): string[]\n</code></pre> <p>Provide a list of keys in the function options that should be treated as sensitive. If users pass raw values for these keys, they will be rejected.</p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#returns_5","title":"Returns","text":"<p><code>string</code>[]</p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#inherited-from_1","title":"Inherited from","text":"<p><code>EmbeddingFunction</code>.<code>getSensitiveKeys</code></p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#init","title":"init()?","text":"<pre><code>optional init(): Promise&lt;void&gt;\n</code></pre> <p>Optionally load any resources needed for the embedding function.</p> <p>This method is called after the embedding function has been initialized but before any embeddings are computed. It is useful for loading local models or other resources that are needed for the embedding function to work.</p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#returns_6","title":"Returns","text":"<p><code>Promise</code>&lt;<code>void</code>&gt;</p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#inherited-from_2","title":"Inherited from","text":"<p><code>EmbeddingFunction</code>.<code>init</code></p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#ndims","title":"ndims()","text":"<pre><code>ndims(): undefined | number\n</code></pre> <p>The number of dimensions of the embeddings</p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#returns_7","title":"Returns","text":"<p><code>undefined</code> | <code>number</code></p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#inherited-from_3","title":"Inherited from","text":"<p><code>EmbeddingFunction</code>.<code>ndims</code></p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#resolvevariables","title":"resolveVariables()","text":"<pre><code>protected resolveVariables(config): Partial&lt;M&gt;\n</code></pre> <p>Apply variables to the config.</p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#parameters_3","title":"Parameters","text":"<ul> <li>config: <code>Partial</code>&lt;<code>M</code>&gt;</li> </ul>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#returns_8","title":"Returns","text":"<p><code>Partial</code>&lt;<code>M</code>&gt;</p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#inherited-from_4","title":"Inherited from","text":"<p><code>EmbeddingFunction</code>.<code>resolveVariables</code></p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#sourcefield","title":"sourceField()","text":"<pre><code>sourceField(): [DataType&lt;Type, any&gt;, Map&lt;string, EmbeddingFunction&lt;any, FunctionOptions&gt;&gt;]\n</code></pre> <p>sourceField is used in combination with <code>LanceSchema</code> to provide a declarative data model</p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#returns_9","title":"Returns","text":"<p>[<code>DataType</code>&lt;<code>Type</code>, <code>any</code>&gt;, <code>Map</code>&lt;<code>string</code>, <code>EmbeddingFunction</code>&lt;<code>any</code>, <code>FunctionOptions</code>&gt;&gt;]</p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#see","title":"See","text":"<p>LanceSchema</p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#overrides_3","title":"Overrides","text":"<p><code>EmbeddingFunction</code>.<code>sourceField</code></p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#tojson","title":"toJSON()","text":"<pre><code>toJSON(): Record&lt;string, any&gt;\n</code></pre> <p>Get the original arguments to the constructor, to serialize them so they can be used to recreate the embedding function later.</p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#returns_10","title":"Returns","text":"<p><code>Record</code>&lt;<code>string</code>, <code>any</code>&gt;</p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#inherited-from_5","title":"Inherited from","text":"<p><code>EmbeddingFunction</code>.<code>toJSON</code></p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#vectorfield","title":"vectorField()","text":"<pre><code>vectorField(optionsOrDatatype?): [DataType&lt;Type, any&gt;, Map&lt;string, EmbeddingFunction&lt;any, FunctionOptions&gt;&gt;]\n</code></pre> <p>vectorField is used in combination with <code>LanceSchema</code> to provide a declarative data model</p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#parameters_4","title":"Parameters","text":"<ul> <li>optionsOrDatatype?: <code>DataType</code>&lt;<code>Type</code>, <code>any</code>&gt; | <code>Partial</code>&lt;<code>FieldOptions</code>&lt;<code>DataType</code>&lt;<code>Type</code>, <code>any</code>&gt;&gt;&gt;     The options for the field</li> </ul>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#returns_11","title":"Returns","text":"<p>[<code>DataType</code>&lt;<code>Type</code>, <code>any</code>&gt;, <code>Map</code>&lt;<code>string</code>, <code>EmbeddingFunction</code>&lt;<code>any</code>, <code>FunctionOptions</code>&gt;&gt;]</p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#see_1","title":"See","text":"<p>LanceSchema</p>"},{"location":"js/namespaces/embedding/classes/TextEmbeddingFunction/#inherited-from_6","title":"Inherited from","text":"<p><code>EmbeddingFunction</code>.<code>vectorField</code></p>"},{"location":"js/namespaces/embedding/functions/LanceSchema/","title":"LanceSchema","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / embedding / LanceSchema</p>"},{"location":"js/namespaces/embedding/functions/LanceSchema/#function-lanceschema","title":"Function: LanceSchema()","text":"<pre><code>function LanceSchema(fields): Schema\n</code></pre> <p>Create a schema with embedding functions.</p>"},{"location":"js/namespaces/embedding/functions/LanceSchema/#parameters","title":"Parameters","text":"<ul> <li>fields: <code>Record</code>&lt;<code>string</code>, <code>object</code> | [<code>object</code>, <code>Map</code>&lt;<code>string</code>, <code>EmbeddingFunction</code>&lt;<code>any</code>, <code>FunctionOptions</code>&gt;&gt;]&gt;</li> </ul>"},{"location":"js/namespaces/embedding/functions/LanceSchema/#returns","title":"Returns","text":"<p><code>Schema</code></p> <p>Schema</p>"},{"location":"js/namespaces/embedding/functions/LanceSchema/#example","title":"Example","text":"<pre><code>class MyEmbeddingFunction extends EmbeddingFunction {\n// ...\n}\nconst func = new MyEmbeddingFunction();\nconst schema = LanceSchema({\n  id: new Int32(),\n  text: func.sourceField(new Utf8()),\n  vector: func.vectorField(),\n  // optional: specify the datatype and/or dimensions\n  vector2: func.vectorField({ datatype: new Float32(), dims: 3}),\n});\n\nconst table = await db.createTable(\"my_table\", data, { schema });\n</code></pre>"},{"location":"js/namespaces/embedding/functions/getRegistry/","title":"getRegistry","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / embedding / getRegistry</p>"},{"location":"js/namespaces/embedding/functions/getRegistry/#function-getregistry","title":"Function: getRegistry()","text":"<pre><code>function getRegistry(): EmbeddingFunctionRegistry\n</code></pre> <p>Utility function to get the global instance of the registry</p>"},{"location":"js/namespaces/embedding/functions/getRegistry/#returns","title":"Returns","text":"<p><code>EmbeddingFunctionRegistry</code></p> <p><code>EmbeddingFunctionRegistry</code> The global instance of the registry</p>"},{"location":"js/namespaces/embedding/functions/getRegistry/#example","title":"Example","text":"<p>```ts const registry = getRegistry(); const openai = registry.get(\"openai\").create();</p>"},{"location":"js/namespaces/embedding/functions/register/","title":"Register","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / embedding / register</p>"},{"location":"js/namespaces/embedding/functions/register/#function-register","title":"Function: register()","text":"<pre><code>function register(name?): (ctor) =&gt; any\n</code></pre>"},{"location":"js/namespaces/embedding/functions/register/#parameters","title":"Parameters","text":"<ul> <li>name?: <code>string</code></li> </ul>"},{"location":"js/namespaces/embedding/functions/register/#returns","title":"Returns","text":"<p><code>Function</code></p>"},{"location":"js/namespaces/embedding/functions/register/#parameters_1","title":"Parameters","text":"<ul> <li>ctor: <code>EmbeddingFunctionConstructor</code>&lt;<code>EmbeddingFunction</code>&lt;<code>any</code>, <code>FunctionOptions</code>&gt;&gt;</li> </ul>"},{"location":"js/namespaces/embedding/functions/register/#returns_1","title":"Returns","text":"<p><code>any</code></p>"},{"location":"js/namespaces/embedding/interfaces/EmbeddingFunctionConfig/","title":"EmbeddingFunctionConfig","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / embedding / EmbeddingFunctionConfig</p>"},{"location":"js/namespaces/embedding/interfaces/EmbeddingFunctionConfig/#interface-embeddingfunctionconfig","title":"Interface: EmbeddingFunctionConfig","text":""},{"location":"js/namespaces/embedding/interfaces/EmbeddingFunctionConfig/#properties","title":"Properties","text":""},{"location":"js/namespaces/embedding/interfaces/EmbeddingFunctionConfig/#function","title":"function","text":"<pre><code>function: EmbeddingFunction&lt;any, FunctionOptions&gt;;\n</code></pre>"},{"location":"js/namespaces/embedding/interfaces/EmbeddingFunctionConfig/#sourcecolumn","title":"sourceColumn","text":"<pre><code>sourceColumn: string;\n</code></pre>"},{"location":"js/namespaces/embedding/interfaces/EmbeddingFunctionConfig/#vectorcolumn","title":"vectorColumn?","text":"<pre><code>optional vectorColumn: string;\n</code></pre>"},{"location":"js/namespaces/embedding/interfaces/EmbeddingFunctionConstructor/","title":"EmbeddingFunctionConstructor","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / embedding / EmbeddingFunctionConstructor</p>"},{"location":"js/namespaces/embedding/interfaces/EmbeddingFunctionConstructor/#interface-embeddingfunctionconstructort","title":"Interface: EmbeddingFunctionConstructor&lt;T&gt;","text":""},{"location":"js/namespaces/embedding/interfaces/EmbeddingFunctionConstructor/#type-parameters","title":"Type Parameters","text":"<p>\u2022 T extends <code>EmbeddingFunction</code> = <code>EmbeddingFunction</code></p>"},{"location":"js/namespaces/embedding/interfaces/EmbeddingFunctionConstructor/#constructors","title":"Constructors","text":""},{"location":"js/namespaces/embedding/interfaces/EmbeddingFunctionConstructor/#new-embeddingfunctionconstructor","title":"new EmbeddingFunctionConstructor()","text":"<pre><code>new EmbeddingFunctionConstructor(modelOptions?): T\n</code></pre>"},{"location":"js/namespaces/embedding/interfaces/EmbeddingFunctionConstructor/#parameters","title":"Parameters","text":"<ul> <li>modelOptions?: <code>T</code>[<code>\"TOptions\"</code>]</li> </ul>"},{"location":"js/namespaces/embedding/interfaces/EmbeddingFunctionConstructor/#returns","title":"Returns","text":"<p><code>T</code></p>"},{"location":"js/namespaces/embedding/interfaces/EmbeddingFunctionCreate/","title":"EmbeddingFunctionCreate","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / embedding / EmbeddingFunctionCreate</p>"},{"location":"js/namespaces/embedding/interfaces/EmbeddingFunctionCreate/#interface-embeddingfunctioncreatet","title":"Interface: EmbeddingFunctionCreate&lt;T&gt;","text":""},{"location":"js/namespaces/embedding/interfaces/EmbeddingFunctionCreate/#type-parameters","title":"Type Parameters","text":"<p>\u2022 T extends <code>EmbeddingFunction</code></p>"},{"location":"js/namespaces/embedding/interfaces/EmbeddingFunctionCreate/#methods","title":"Methods","text":""},{"location":"js/namespaces/embedding/interfaces/EmbeddingFunctionCreate/#create","title":"create()","text":"<pre><code>create(options?): CreateReturnType&lt;T&gt;\n</code></pre>"},{"location":"js/namespaces/embedding/interfaces/EmbeddingFunctionCreate/#parameters","title":"Parameters","text":"<ul> <li>options?: <code>T</code>[<code>\"TOptions\"</code>]</li> </ul>"},{"location":"js/namespaces/embedding/interfaces/EmbeddingFunctionCreate/#returns","title":"Returns","text":"<p><code>CreateReturnType</code>&lt;<code>T</code>&gt;</p>"},{"location":"js/namespaces/embedding/interfaces/FieldOptions/","title":"FieldOptions","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / embedding / FieldOptions</p>"},{"location":"js/namespaces/embedding/interfaces/FieldOptions/#interface-fieldoptionst","title":"Interface: FieldOptions&lt;T&gt;","text":""},{"location":"js/namespaces/embedding/interfaces/FieldOptions/#type-parameters","title":"Type Parameters","text":"<p>\u2022 T extends <code>DataType</code> = <code>DataType</code></p>"},{"location":"js/namespaces/embedding/interfaces/FieldOptions/#properties","title":"Properties","text":""},{"location":"js/namespaces/embedding/interfaces/FieldOptions/#datatype","title":"datatype","text":"<pre><code>datatype: T;\n</code></pre>"},{"location":"js/namespaces/embedding/interfaces/FieldOptions/#dims","title":"dims?","text":"<pre><code>optional dims: number;\n</code></pre>"},{"location":"js/namespaces/embedding/interfaces/FunctionOptions/","title":"FunctionOptions","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / embedding / FunctionOptions</p>"},{"location":"js/namespaces/embedding/interfaces/FunctionOptions/#interface-functionoptions","title":"Interface: FunctionOptions","text":"<p>Options for a given embedding function</p>"},{"location":"js/namespaces/embedding/interfaces/FunctionOptions/#indexable","title":"Indexable","text":"<p>[<code>key</code>: <code>string</code>]: <code>any</code></p>"},{"location":"js/namespaces/embedding/type-aliases/CreateReturnType/","title":"CreateReturnType","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / embedding / CreateReturnType</p>"},{"location":"js/namespaces/embedding/type-aliases/CreateReturnType/#type-alias-createreturntypet","title":"Type Alias: CreateReturnType&lt;T&gt;","text":"<pre><code>type CreateReturnType&lt;T&gt;: T extends object ? Promise&lt;T&gt; : T;\n</code></pre>"},{"location":"js/namespaces/embedding/type-aliases/CreateReturnType/#type-parameters","title":"Type Parameters","text":"<p>\u2022 T</p>"},{"location":"js/namespaces/rerankers/","title":"Index","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / rerankers</p>"},{"location":"js/namespaces/rerankers/#rerankers","title":"rerankers","text":""},{"location":"js/namespaces/rerankers/#index","title":"Index","text":""},{"location":"js/namespaces/rerankers/#classes","title":"Classes","text":"<ul> <li>RRFReranker</li> </ul>"},{"location":"js/namespaces/rerankers/#interfaces","title":"Interfaces","text":"<ul> <li>Reranker</li> </ul>"},{"location":"js/namespaces/rerankers/classes/RRFReranker/","title":"RRFReranker","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / rerankers / RRFReranker</p>"},{"location":"js/namespaces/rerankers/classes/RRFReranker/#class-rrfreranker","title":"Class: RRFReranker","text":"<p>Reranks the results using the Reciprocal Rank Fusion (RRF) algorithm.</p>"},{"location":"js/namespaces/rerankers/classes/RRFReranker/#methods","title":"Methods","text":""},{"location":"js/namespaces/rerankers/classes/RRFReranker/#rerankhybrid","title":"rerankHybrid()","text":"<pre><code>rerankHybrid(\n   query,\n   vecResults,\n   ftsResults): Promise&lt;RecordBatch&lt;any&gt;&gt;\n</code></pre>"},{"location":"js/namespaces/rerankers/classes/RRFReranker/#parameters","title":"Parameters","text":"<ul> <li> <p>query: <code>string</code></p> </li> <li> <p>vecResults: <code>RecordBatch</code>&lt;<code>any</code>&gt;</p> </li> <li> <p>ftsResults: <code>RecordBatch</code>&lt;<code>any</code>&gt;</p> </li> </ul>"},{"location":"js/namespaces/rerankers/classes/RRFReranker/#returns","title":"Returns","text":"<p><code>Promise</code>&lt;<code>RecordBatch</code>&lt;<code>any</code>&gt;&gt;</p>"},{"location":"js/namespaces/rerankers/classes/RRFReranker/#create","title":"create()","text":"<pre><code>static create(k): Promise&lt;RRFReranker&gt;\n</code></pre>"},{"location":"js/namespaces/rerankers/classes/RRFReranker/#parameters_1","title":"Parameters","text":"<ul> <li>k: <code>number</code> = <code>60</code></li> </ul>"},{"location":"js/namespaces/rerankers/classes/RRFReranker/#returns_1","title":"Returns","text":"<p><code>Promise</code>&lt;<code>RRFReranker</code>&gt;</p>"},{"location":"js/namespaces/rerankers/interfaces/Reranker/","title":"Reranker","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / rerankers / Reranker</p>"},{"location":"js/namespaces/rerankers/interfaces/Reranker/#interface-reranker","title":"Interface: Reranker","text":""},{"location":"js/namespaces/rerankers/interfaces/Reranker/#methods","title":"Methods","text":""},{"location":"js/namespaces/rerankers/interfaces/Reranker/#rerankhybrid","title":"rerankHybrid()","text":"<pre><code>rerankHybrid(\n   query,\n   vecResults,\n   ftsResults): Promise&lt;RecordBatch&lt;any&gt;&gt;\n</code></pre>"},{"location":"js/namespaces/rerankers/interfaces/Reranker/#parameters","title":"Parameters","text":"<ul> <li> <p>query: <code>string</code></p> </li> <li> <p>vecResults: <code>RecordBatch</code>&lt;<code>any</code>&gt;</p> </li> <li> <p>ftsResults: <code>RecordBatch</code>&lt;<code>any</code>&gt;</p> </li> </ul>"},{"location":"js/namespaces/rerankers/interfaces/Reranker/#returns","title":"Returns","text":"<p><code>Promise</code>&lt;<code>RecordBatch</code>&lt;<code>any</code>&gt;&gt;</p>"},{"location":"js/type-aliases/Data/","title":"Data","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / Data</p>"},{"location":"js/type-aliases/Data/#type-alias-data","title":"Type Alias: Data","text":"<pre><code>type Data: Record&lt;string, unknown&gt;[] | TableLike;\n</code></pre> <p>Data type accepted by NodeJS SDK</p>"},{"location":"js/type-aliases/DataLike/","title":"DataLike","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / DataLike</p>"},{"location":"js/type-aliases/DataLike/#type-alias-datalike","title":"Type Alias: DataLike","text":"<pre><code>type DataLike: Data | object;\n</code></pre>"},{"location":"js/type-aliases/FieldLike/","title":"FieldLike","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / FieldLike</p>"},{"location":"js/type-aliases/FieldLike/#type-alias-fieldlike","title":"Type Alias: FieldLike","text":"<pre><code>type FieldLike: Field | object;\n</code></pre>"},{"location":"js/type-aliases/IntoSql/","title":"IntoSql","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / IntoSql</p>"},{"location":"js/type-aliases/IntoSql/#type-alias-intosql","title":"Type Alias: IntoSql","text":"<pre><code>type IntoSql:\n  | string\n  | number\n  | boolean\n  | null\n  | Date\n  | ArrayBufferLike\n  | Buffer\n  | IntoSql[];\n</code></pre>"},{"location":"js/type-aliases/IntoVector/","title":"IntoVector","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / IntoVector</p>"},{"location":"js/type-aliases/IntoVector/#type-alias-intovector","title":"Type Alias: IntoVector","text":"<pre><code>type IntoVector: Float32Array | Float64Array | number[] | Promise&lt;Float32Array | Float64Array | number[]&gt;;\n</code></pre>"},{"location":"js/type-aliases/RecordBatchLike/","title":"RecordBatchLike","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / RecordBatchLike</p>"},{"location":"js/type-aliases/RecordBatchLike/#type-alias-recordbatchlike","title":"Type Alias: RecordBatchLike","text":"<pre><code>type RecordBatchLike: RecordBatch | object;\n</code></pre>"},{"location":"js/type-aliases/SchemaLike/","title":"SchemaLike","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / SchemaLike</p>"},{"location":"js/type-aliases/SchemaLike/#type-alias-schemalike","title":"Type Alias: SchemaLike","text":"<pre><code>type SchemaLike: Schema | object;\n</code></pre>"},{"location":"js/type-aliases/TableLike/","title":"TableLike","text":"<p>@lancedb/lancedb \u2022 Docs</p> <p>@lancedb/lancedb / TableLike</p>"},{"location":"js/type-aliases/TableLike/#type-alias-tablelike","title":"Type Alias: TableLike","text":"<pre><code>type TableLike: ArrowTable | object;\n</code></pre>"},{"location":"notebooks/DisappearingEmbeddingFunction/","title":"Example - MultiModal CLIP Embeddings","text":"In\u00a0[1]: Copied! <pre>import lancedb\n</pre> import lancedb <p>First, download the dataset from https://www.robots.ox.ac.uk/~vgg/data/pets/ Specifically, download the images.tar.gz</p> <p>This notebook assumes you've downloaded it into your ~/Downloads directory. When you extract the tarball, it will create an <code>images</code> directory.</p> In\u00a0[7]: Copied! <pre>from lancedb.embeddings import EmbeddingFunctionRegistry\n\nregistry = EmbeddingFunctionRegistry.get_instance()\nclip = registry.get(\"open-clip\").create()\n</pre> from lancedb.embeddings import EmbeddingFunctionRegistry  registry = EmbeddingFunctionRegistry.get_instance() clip = registry.get(\"open-clip\").create() <pre>/home/saksham/Documents/lancedb/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nDownloading (\u2026)ip_pytorch_model.bin: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 605M/605M [00:41&lt;00:00, 14.6MB/s] \n</pre> In\u00a0[6]: Copied! <pre>!pip install open_clip_torch\n</pre> !pip install open_clip_torch <pre>Collecting open_clip_torch\n  Downloading open_clip_torch-2.20.0-py3-none-any.whl (1.5 MB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.5 MB 771 kB/s eta 0:00:01\nRequirement already satisfied: regex in /home/saksham/Documents/lancedb/env/lib/python3.8/site-packages (from open_clip_torch) (2023.10.3)\nRequirement already satisfied: tqdm in /home/saksham/Documents/lancedb/env/lib/python3.8/site-packages (from open_clip_torch) (4.66.1)\nCollecting torchvision\n  Downloading torchvision-0.16.0-cp38-cp38-manylinux1_x86_64.whl (6.9 MB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.9 MB 21.0 MB/s eta 0:00:01\nCollecting huggingface-hub\n  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 295 kB 43.1 MB/s eta 0:00:01\nCollecting protobuf&lt;4\n  Using cached protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\nCollecting timm\n  Downloading timm-0.9.7-py3-none-any.whl (2.2 MB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.2 MB 28.3 MB/s eta 0:00:01\nCollecting sentencepiece\n  Downloading sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.3 MB 39.9 MB/s eta 0:00:01\nCollecting torch&gt;=1.9.0\n  Downloading torch-2.1.0-cp38-cp38-manylinux1_x86_64.whl (670.2 MB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 670.2 MB 47 kB/s s eta 0:00:01\nCollecting ftfy\n  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 53 kB 2.3 MB/s  eta 0:00:01\nCollecting pillow!=8.3.*,&gt;=5.3.0\n  Using cached Pillow-10.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5 MB)\nRequirement already satisfied: requests in /home/saksham/Documents/lancedb/env/lib/python3.8/site-packages (from torchvision-&gt;open_clip_torch) (2.31.0)\nRequirement already satisfied: numpy in /home/saksham/Documents/lancedb/env/lib/python3.8/site-packages (from torchvision-&gt;open_clip_torch) (1.24.4)\nRequirement already satisfied: packaging&gt;=20.9 in /home/saksham/Documents/lancedb/env/lib/python3.8/site-packages (from huggingface-hub-&gt;open_clip_torch) (23.2)\nCollecting fsspec\n  Downloading fsspec-2023.9.2-py3-none-any.whl (173 kB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 173 kB 22.0 MB/s eta 0:00:01\nCollecting filelock\n  Using cached filelock-3.12.4-py3-none-any.whl (11 kB)\nRequirement already satisfied: pyyaml&gt;=5.1 in /home/saksham/Documents/lancedb/env/lib/python3.8/site-packages (from huggingface-hub-&gt;open_clip_torch) (6.0.1)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /home/saksham/Documents/lancedb/env/lib/python3.8/site-packages (from huggingface-hub-&gt;open_clip_torch) (4.8.0)\nCollecting safetensors\n  Downloading safetensors-0.3.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.3 MB 22.8 MB/s eta 0:00:01\nCollecting networkx\n  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.1 MB 16.6 MB/s eta 0:00:01\nCollecting triton==2.1.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n  Downloading triton-2.1.0-0-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 89.2 MB 31.6 MB/s eta 0:00:01\nCollecting nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 56.5 MB 15.9 MB/s eta 0:00:01\nCollecting nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 99 kB 9.4 MB/s  eta 0:00:01\nCollecting sympy\n  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.7 MB 16.4 MB/s eta 0:00:01\nCollecting nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 196.0 MB 78 kB/s  eta 0:00:011\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 23.7 MB 619 kB/s eta 0:00:011\nCollecting nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 121.6 MB 93 kB/s s eta 0:00:01\nCollecting nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14.1 MB 19.5 MB/s eta 0:00:01\nRequirement already satisfied: jinja2 in /home/saksham/Documents/lancedb/env/lib/python3.8/site-packages (from torch&gt;=1.9.0-&gt;open_clip_torch) (3.1.2)\nCollecting nvidia-nccl-cu12==2.18.1; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 209.8 MB 5.2 kB/s  eta 0:00:01     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 208.2 MB 17.0 MB/s eta 0:00:01\nCollecting nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 731.7 MB 22 kB/s  eta 0:00:011\nCollecting nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 410.6 MB 9.2 kB/s eta 0:00:012\nCollecting nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 823 kB 18.5 MB/s eta 0:00:01\nCollecting nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 124.2 MB 43 kB/s s eta 0:00:01ta 0:00:02\nRequirement already satisfied: wcwidth&gt;=0.2.5 in /home/saksham/Documents/lancedb/env/lib/python3.8/site-packages (from ftfy-&gt;open_clip_torch) (0.2.8)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /home/saksham/Documents/lancedb/env/lib/python3.8/site-packages (from requests-&gt;torchvision-&gt;open_clip_torch) (2023.7.22)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /home/saksham/Documents/lancedb/env/lib/python3.8/site-packages (from requests-&gt;torchvision-&gt;open_clip_torch) (2.0.6)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /home/saksham/Documents/lancedb/env/lib/python3.8/site-packages (from requests-&gt;torchvision-&gt;open_clip_torch) (3.4)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /home/saksham/Documents/lancedb/env/lib/python3.8/site-packages (from requests-&gt;torchvision-&gt;open_clip_torch) (3.3.0)\nCollecting mpmath&gt;=0.19\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 536 kB 14.2 MB/s eta 0:00:01\nCollecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.2.140-py3-none-manylinux1_x86_64.whl (20.2 MB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20.2 MB 14.3 MB/s eta 0:00:01\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /home/saksham/Documents/lancedb/env/lib/python3.8/site-packages (from jinja2-&gt;torch&gt;=1.9.0-&gt;open_clip_torch) (2.1.3)\nInstalling collected packages: pillow, networkx, filelock, triton, nvidia-curand-cu12, nvidia-nvtx-cu12, mpmath, sympy, nvidia-nvjitlink-cu12, nvidia-cusparse-cu12, fsspec, nvidia-cuda-nvrtc-cu12, nvidia-cufft-cu12, nvidia-cuda-cupti-cu12, nvidia-nccl-cu12, nvidia-cublas-cu12, nvidia-cudnn-cu12, nvidia-cuda-runtime-cu12, nvidia-cusolver-cu12, torch, torchvision, huggingface-hub, protobuf, safetensors, timm, sentencepiece, ftfy, open-clip-torch\nSuccessfully installed filelock-3.12.4 fsspec-2023.9.2 ftfy-6.1.1 huggingface-hub-0.17.3 mpmath-1.3.0 networkx-3.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.2.140 nvidia-nvtx-cu12-12.1.105 open-clip-torch-2.20.0 pillow-10.0.1 protobuf-3.20.3 safetensors-0.3.3 sentencepiece-0.1.99 sympy-1.12 timm-0.9.7 torch-2.1.0 torchvision-0.16.0 triton-2.1.0\n</pre> In\u00a0[8]: Copied! <pre>clip\n</pre> clip Out[8]: <pre>OpenClipEmbeddings(name='ViT-B-32', pretrained='laion2b_s34b_b79k', device='cpu', batch_size=64, normalize=True)</pre> In\u00a0[\u00a0]: Copied! <pre>from PIL import Image\nfrom lancedb.pydantic import LanceModel, Vector\n\nclass Pets(LanceModel):\n    vector: Vector(clip.ndims()) = clip.VectorField()\n    image_uri: str = clip.SourceField()\n\n    @property\n    def image(self):\n        return Image.open(self.image_uri)\n</pre> from PIL import Image from lancedb.pydantic import LanceModel, Vector  class Pets(LanceModel):     vector: Vector(clip.ndims()) = clip.VectorField()     image_uri: str = clip.SourceField()      @property     def image(self):         return Image.open(self.image_uri) <p>First we connect to a local lancedb directory</p> In\u00a0[\u00a0]: Copied! <pre>db = lancedb.connect(\"~/.lancedb\")\n</pre> db = lancedb.connect(\"~/.lancedb\") <p>Next we get all of the paths for the images we downloaded and create a table. Notice that we didn't have to worry about generating the image embeddings ourselves.</p> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom pathlib import Path\nfrom random import sample\n\nif \"pets\" in db:\n    table = db[\"pets\"]\nelse:\n    table = db.create_table(\"pets\", schema=Pets)\n    # use a sampling of 1000 images\n    p = Path(\"~/Downloads/images\").expanduser()\n    uris = [str(f) for f in p.glob(\"*.jpg\")]\n    uris = sample(uris, 1000)\n    table.add(pd.DataFrame({\"image_uri\": uris}))\n</pre> import pandas as pd from pathlib import Path from random import sample  if \"pets\" in db:     table = db[\"pets\"] else:     table = db.create_table(\"pets\", schema=Pets)     # use a sampling of 1000 images     p = Path(\"~/Downloads/images\").expanduser()     uris = [str(f) for f in p.glob(\"*.jpg\")]     uris = sample(uris, 1000)     table.add(pd.DataFrame({\"image_uri\": uris})) In\u00a0[\u00a0]: Copied! <pre>table.head().to_pandas()\n</pre> table.head().to_pandas() Out[\u00a0]: vector image_uri 0 [0.018789755, 0.11621179, -0.09760579, -0.0268... /Users/changshe/Downloads/images/leonberger_14... 1 [0.021960497, 0.06073219, -0.1625527, 0.021481... /Users/changshe/Downloads/images/havanese_63.jpg 2 [0.0074375155, 0.084355146, -0.027461205, -0.0... /Users/changshe/Downloads/images/english_cocke... 3 [-0.01220356, 0.020815236, -0.08587208, -0.027... /Users/changshe/Downloads/images/shiba_inu_143... 4 [-0.010112503, 0.14021927, -0.14588796, -0.046... /Users/changshe/Downloads/images/saint_bernard... In\u00a0[\u00a0]: Copied! <pre>rs = table.search(\"dog\").limit(3).to_pydantic(Pets)\nrs[0].image\n</pre> rs = table.search(\"dog\").limit(3).to_pydantic(Pets) rs[0].image Out[\u00a0]: <p>Create a query image using PIL</p> In\u00a0[\u00a0]: Copied! <pre>from PIL import Image\np = Path(\"~/Downloads/images/samoyed_100.jpg\").expanduser()\nquery_image = Image.open(p)\nquery_image\n</pre> from PIL import Image p = Path(\"~/Downloads/images/samoyed_100.jpg\").expanduser() query_image = Image.open(p) query_image Out[\u00a0]: <p>Pass in the query_image to the search API</p> In\u00a0[\u00a0]: Copied! <pre>rs = table.search(query_image).limit(3).to_pydantic(Pets)\nrs[2].image\n</pre> rs = table.search(query_image).limit(3).to_pydantic(Pets) rs[2].image Out[\u00a0]: <p>For example we can recreate the database connection and table object</p> In\u00a0[\u00a0]: Copied! <pre>db = lancedb.connect(\"~/.lancedb\")\ntable = db[\"pets\"]\n</pre> db = lancedb.connect(\"~/.lancedb\") table = db[\"pets\"] <p>We can observe that it's read out as table metadata</p> In\u00a0[\u00a0]: Copied! <pre>import json\n\njson.loads(table.schema.metadata[b\"embedding_functions\"])[0]\n</pre> import json  json.loads(table.schema.metadata[b\"embedding_functions\"])[0] Out[\u00a0]: <pre>{'name': 'open-clip',\n 'model': {'name': 'ViT-B-32',\n  'pretrained': 'laion2b_s34b_b79k',\n  'device': 'cpu',\n  'batch_size': 64,\n  'normalize': True},\n 'source_column': 'image_uri',\n 'vector_column': 'vector'}</pre> <p>And we can also run queries as before without having to reinstantiate the embedding function explicitly</p> In\u00a0[\u00a0]: Copied! <pre>rs = table.search(\"big dog\").limit(3).to_pydantic(Pets)\nrs[0].image\n</pre> rs = table.search(\"big dog\").limit(3).to_pydantic(Pets) rs[0].image Out[\u00a0]: <ul> <li>LanceDB's new embedding functions feature makes it easy for builders of LLM apps</li> <li>You no longer need to manually encode the data yourself</li> <li>You no longer need to figure out how many dimensions is your vector</li> <li>You no longer need to manually encode the query</li> <li>And with the right embedding model, you can search way more than just text</li> </ul> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/DisappearingEmbeddingFunction/#example-multimodal-clip-embeddings","title":"Example - MultiModal CLIP Embeddings\u00b6","text":""},{"location":"notebooks/DisappearingEmbeddingFunction/#the-disappearing-embedding-function","title":"The Disappearing Embedding Function\u00b6","text":"<p>Previously, to use vector databases, you had to do the embedding process yourself and interact with the system using vectors directly. With this new release of LanceDB, we make it much more convenient so you don't need to worry about that at all.</p> <ol> <li>We present you with sentence-transformer, openai, and openclip embedding functions that can be saved directly as table metadata</li> <li>You no longer have to generate the vectors directly either during query time or ingestion time</li> <li>The embedding function interface is extensible so you can create your own</li> <li>The function is persisted as table metadata so you can use it across sessions</li> </ol>"},{"location":"notebooks/DisappearingEmbeddingFunction/#multi-modal-search-made-easy","title":"Multi-modal search made easy\u00b6","text":"<p>In this example we'll go over multi-modal image search using:</p> <ul> <li>Oxford Pet dataset</li> <li>OpenClip model</li> <li>LanceDB</li> </ul>"},{"location":"notebooks/DisappearingEmbeddingFunction/#data","title":"Data\u00b6","text":""},{"location":"notebooks/DisappearingEmbeddingFunction/#define-embedding-function","title":"Define embedding function\u00b6","text":"<p>We'll use the OpenClipEmbeddingFunction here for multi-modal image search.</p>"},{"location":"notebooks/DisappearingEmbeddingFunction/#the-data-model","title":"The data model\u00b6","text":"<p>We'll declare a new model that subclasses LanceModel (special pydantic model) to represent the table. This table has two columns, one for the image_uri and one for the vector generated from those images. The embedding function defines the number of dimensions in its vectors so you don't need to look it up.</p> <p>We use the <code>VectorField</code> method from the embedding function to annotate the model so that LanceDB knows to use the open-clip embedding function to generate query embeddings that correspond to the <code>vector</code> column.</p> <p>We also use the <code>SourceField</code> so that when adding data, LanceDB knows to automatically use open-clip to encode the input images.</p> <p>Finally, because we're working with images, we add a convenience property <code>image</code> to open the image and return a PIL Image so it can be visualized in Jupyter Notebook</p>"},{"location":"notebooks/DisappearingEmbeddingFunction/#create-the-table","title":"Create the table\u00b6","text":""},{"location":"notebooks/DisappearingEmbeddingFunction/#querying-via-text","title":"Querying via text\u00b6","text":"<p>We also don't need to generate the embeddings when querying either. LanceDB does that automatically so you can query directly using text input.</p> <p>The pydantic model we declared for the table schema also makes it really easy for us to work with the search results</p>"},{"location":"notebooks/DisappearingEmbeddingFunction/#querying-via-images","title":"Querying via images\u00b6","text":"<p>The great thing about CLIP is that it's multi-modal. So you can search using not just text but images as well.</p>"},{"location":"notebooks/DisappearingEmbeddingFunction/#persistence","title":"Persistence\u00b6","text":"<p>Embedding functions are persisted as table metadata so it's much easier to use across sessions.</p>"},{"location":"notebooks/DisappearingEmbeddingFunction/#lancedb-makes-multimodal-ai-easy","title":"LanceDB makes multimodal AI easy\u00b6","text":""},{"location":"notebooks/LlamaIndex_example/","title":"LanceDB Vector Store","text":"<p>If you're opening this Notebook on colab, you will probably need to install LlamaIndex \ud83e\udd99.</p> In\u00a0[\u00a0]: Copied! <pre>%pip install llama-index llama-index-vector-stores-lancedb\n</pre> %pip install llama-index llama-index-vector-stores-lancedb In\u00a0[\u00a0]: Copied! <pre>%pip install lancedb==0.6.13 #Only required if the above cell installs an older version of lancedb (pypi package may not be released yet)\n</pre> %pip install lancedb==0.6.13 #Only required if the above cell installs an older version of lancedb (pypi package may not be released yet) In\u00a0[\u00a0]: Copied! <pre># Refresh vector store URI if restarting or re-using the same notebook\n! rm -rf ./lancedb\n</pre> # Refresh vector store URI if restarting or re-using the same notebook ! rm -rf ./lancedb In\u00a0[\u00a0]: Copied! <pre>import logging\nimport sys\n\n# Uncomment to see debug logs\n# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\nfrom llama_index.core import SimpleDirectoryReader, Document, StorageContext\nfrom llama_index.core import VectorStoreIndex\nfrom llama_index.vector_stores.lancedb import LanceDBVectorStore\nimport textwrap\n</pre> import logging import sys  # Uncomment to see debug logs # logging.basicConfig(stream=sys.stdout, level=logging.DEBUG) # logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))   from llama_index.core import SimpleDirectoryReader, Document, StorageContext from llama_index.core import VectorStoreIndex from llama_index.vector_stores.lancedb import LanceDBVectorStore import textwrap In\u00a0[\u00a0]: Copied! <pre>import openai\n\nopenai.api_key = \"sk-\"\n</pre> import openai  openai.api_key = \"sk-\" <p>Download Data</p> In\u00a0[\u00a0]: Copied! <pre>!mkdir -p 'data/paul_graham/'\n!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'\n</pre> !mkdir -p 'data/paul_graham/' !wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt' <pre>--2024-06-11 16:42:37--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 75042 (73K) [text/plain]\nSaving to: \u2018data/paul_graham/paul_graham_essay.txt\u2019\n\ndata/paul_graham/pa 100%[===================&gt;]  73.28K  --.-KB/s    in 0.02s   \n\n2024-06-11 16:42:37 (3.97 MB/s) - \u2018data/paul_graham/paul_graham_essay.txt\u2019 saved [75042/75042]\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\nprint(\"Document ID:\", documents[0].doc_id, \"Document Hash:\", documents[0].hash)\n</pre> documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data() print(\"Document ID:\", documents[0].doc_id, \"Document Hash:\", documents[0].hash) <pre>Document ID: cac1ba78-5007-4cf8-89ba-280264790115 Document Hash: fe2d4d3ef3a860780f6c2599808caa587c8be6516fe0ba4ca53cf117044ba953\n</pre> In\u00a0[\u00a0]: Copied! <pre>vector_store = LanceDBVectorStore(\n    uri=\"./lancedb\", mode=\"overwrite\", query_type=\"hybrid\"\n)\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\nindex = VectorStoreIndex.from_documents(\n    documents, storage_context=storage_context\n)\n</pre> vector_store = LanceDBVectorStore(     uri=\"./lancedb\", mode=\"overwrite\", query_type=\"hybrid\" ) storage_context = StorageContext.from_defaults(vector_store=vector_store)  index = VectorStoreIndex.from_documents(     documents, storage_context=storage_context ) In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.vector_stores import (\n    MetadataFilters,\n    FilterOperator,\n    FilterCondition,\n    MetadataFilter,\n)\n\nfrom datetime import datetime\n\n\nquery_filters = MetadataFilters(\n    filters=[\n        MetadataFilter(\n            key=\"creation_date\",\n            operator=FilterOperator.EQ,\n            value=datetime.now().strftime(\"%Y-%m-%d\"),\n        ),\n        MetadataFilter(\n            key=\"file_size\", value=75040, operator=FilterOperator.GT\n        ),\n    ],\n    condition=FilterCondition.AND,\n)\n</pre> from llama_index.core.vector_stores import (     MetadataFilters,     FilterOperator,     FilterCondition,     MetadataFilter, )  from datetime import datetime   query_filters = MetadataFilters(     filters=[         MetadataFilter(             key=\"creation_date\",             operator=FilterOperator.EQ,             value=datetime.now().strftime(\"%Y-%m-%d\"),         ),         MetadataFilter(             key=\"file_size\", value=75040, operator=FilterOperator.GT         ),     ],     condition=FilterCondition.AND, ) In\u00a0[\u00a0]: Copied! <pre>! pip install -U torch transformers tantivy@git+https://github.com/quickwit-oss/tantivy-py#164adc87e1a033117001cf70e38c82a53014d985\n</pre> ! pip install -U torch transformers tantivy@git+https://github.com/quickwit-oss/tantivy-py#164adc87e1a033117001cf70e38c82a53014d985 <p>if you want to add a reranker at vector store initialization, you can pass it in the arguments like below :</p> <pre><code>from lancedb.rerankers import ColbertReranker\nreranker = ColbertReranker()\nvector_store = LanceDBVectorStore(uri=\"./lancedb\", reranker=reranker, mode=\"overwrite\")\n</code></pre> In\u00a0[\u00a0]: Copied! <pre>import lancedb\n</pre> import lancedb In\u00a0[\u00a0]: Copied! <pre>from lancedb.rerankers import ColbertReranker\n\nreranker = ColbertReranker()\nvector_store._add_reranker(reranker)\n\nquery_engine = index.as_query_engine(\n    filters=query_filters,\n    # vector_store_kwargs={\n    #     \"query_type\": \"fts\",\n    # },\n)\n\nresponse = query_engine.query(\"How much did Viaweb charge per month?\")\n</pre> from lancedb.rerankers import ColbertReranker  reranker = ColbertReranker() vector_store._add_reranker(reranker)  query_engine = index.as_query_engine(     filters=query_filters,     # vector_store_kwargs={     #     \"query_type\": \"fts\",     # }, )  response = query_engine.query(\"How much did Viaweb charge per month?\") In\u00a0[\u00a0]: Copied! <pre>print(response)\nprint(\"metadata -\", response.metadata)\n</pre> print(response) print(\"metadata -\", response.metadata) <pre>Viaweb charged $100 a month for a small store and $300 a month for a big one.\nmetadata - {'65ed5f07-5b8a-4143-a939-e8764884828e': {'file_path': '/Users/raghavdixit/Desktop/open_source/llama_index_lance/docs/docs/examples/vector_stores/data/paul_graham/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2024-06-11', 'last_modified_date': '2024-06-11'}, 'be231827-20b8-4988-ac75-94fa79b3c22e': {'file_path': '/Users/raghavdixit/Desktop/open_source/llama_index_lance/docs/docs/examples/vector_stores/data/paul_graham/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2024-06-11', 'last_modified_date': '2024-06-11'}}\n</pre> In\u00a0[\u00a0]: Copied! <pre>lance_filter = \"metadata.file_name = 'paul_graham_essay.txt' \"\nretriever = index.as_retriever(vector_store_kwargs={\"where\": lance_filter})\nresponse = retriever.retrieve(\"What did the author do growing up?\")\n</pre> lance_filter = \"metadata.file_name = 'paul_graham_essay.txt' \" retriever = index.as_retriever(vector_store_kwargs={\"where\": lance_filter}) response = retriever.retrieve(\"What did the author do growing up?\") In\u00a0[\u00a0]: Copied! <pre>print(response[0].get_content())\nprint(\"metadata -\", response[0].metadata)\n</pre> print(response[0].get_content()) print(\"metadata -\", response[0].metadata) <pre>What I Worked On\n\nFebruary 2021\n\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines \u2014 CPU, disk drives, printer, card reader \u2014 sitting up on a raised floor under bright fluorescent lights.\n\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.\n\nI was puzzled by the 1401. I couldn't figure out what to do with it. And in retrospect there's not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn't have any data stored on punched cards. The only other option was to do things that didn't rely on any input, like calculate approximations of pi, but I didn't know enough math to do anything interesting of that type. So I'm not surprised I can't remember any programs I wrote, because they can't have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn't. On a machine without time-sharing, this was a social as well as a technical error, as the data center manager's expression made clear.\n\nWith microcomputers, everything changed. Now you could have a computer sitting right in front of you, on a desk, that could respond to your keystrokes as it was running instead of just churning through a stack of punch cards and then stopping. [1]\n\nThe first of my friends to get a microcomputer built it himself. It was sold as a kit by Heathkit. I remember vividly how impressed and envious I felt watching him sitting in front of it, typing programs right into the computer.\n\nComputers were expensive in those days and it took me years of nagging before I convinced my father to buy one, a TRS-80, in about 1980. The gold standard then was the Apple II, but a TRS-80 was good enough. This was when I really started programming. I wrote simple games, a program to predict how high my model rockets would fly, and a word processor that my father used to write at least one book. There was only room in memory for about 2 pages of text, so he'd write 2 pages at a time and then print them out, but it was a lot better than a typewriter.\n\nThough I liked programming, I didn't plan to study it in college. In college I was going to study philosophy, which sounded much more powerful. It seemed, to my naive high school self, to be the study of the ultimate truths, compared to which the things studied in other fields would be mere domain knowledge. What I discovered when I got to college was that the other fields took up so much of the space of ideas that there wasn't much left for these supposed ultimate truths. All that seemed left for philosophy were edge cases that people in other fields felt could safely be ignored.\n\nI couldn't have put this into words when I was 18. All I knew at the time was that I kept taking philosophy courses and they kept being boring. So I decided to switch to AI.\n\nAI was in the air in the mid 1980s, but there were two things especially that made me want to work on it: a novel by Heinlein called The Moon is a Harsh Mistress, which featured an intelligent computer called Mike, and a PBS documentary that showed Terry Winograd using SHRDLU. I haven't tried rereading The Moon is a Harsh Mistress, so I don't know how well it has aged, but when I read it I was drawn entirely into its world.\nmetadata - {'file_path': '/Users/raghavdixit/Desktop/open_source/llama_index_lance/docs/docs/examples/vector_stores/data/paul_graham/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2024-06-11', 'last_modified_date': '2024-06-11'}\n</pre> In\u00a0[\u00a0]: Copied! <pre>nodes = [node.node for node in response]\n</pre> nodes = [node.node for node in response] In\u00a0[\u00a0]: Copied! <pre>del index\n\nindex = VectorStoreIndex.from_documents(\n    [Document(text=\"The sky is purple in Portland, Maine\")],\n    uri=\"/tmp/new_dataset\",\n)\n</pre> del index  index = VectorStoreIndex.from_documents(     [Document(text=\"The sky is purple in Portland, Maine\")],     uri=\"/tmp/new_dataset\", ) In\u00a0[\u00a0]: Copied! <pre>index.insert_nodes(nodes)\n</pre> index.insert_nodes(nodes) In\u00a0[\u00a0]: Copied! <pre>query_engine = index.as_query_engine()\nresponse = query_engine.query(\"Where is the sky purple?\")\nprint(textwrap.fill(str(response), 100))\n</pre> query_engine = index.as_query_engine() response = query_engine.query(\"Where is the sky purple?\") print(textwrap.fill(str(response), 100)) <pre>Portland, Maine\n</pre> <p>You can also create an index from an existing table</p> In\u00a0[\u00a0]: Copied! <pre>del index\n\nvec_store = LanceDBVectorStore.from_table(vector_store._table)\nindex = VectorStoreIndex.from_vector_store(vec_store)\n</pre> del index  vec_store = LanceDBVectorStore.from_table(vector_store._table) index = VectorStoreIndex.from_vector_store(vec_store) In\u00a0[\u00a0]: Copied! <pre>query_engine = index.as_query_engine()\nresponse = query_engine.query(\"What companies did the author start?\")\nprint(textwrap.fill(str(response), 100))\n</pre> query_engine = index.as_query_engine() response = query_engine.query(\"What companies did the author start?\") print(textwrap.fill(str(response), 100)) <pre>The author started Viaweb and Aspra.\n</pre>"},{"location":"notebooks/LlamaIndex_example/#lancedb-vector-store","title":"LanceDB Vector Store\u00b6","text":"<p>In this notebook we are going to show how to use LanceDB to perform vector searches in LlamaIndex</p>"},{"location":"notebooks/LlamaIndex_example/#setup-openai","title":"Setup OpenAI\u00b6","text":"<p>The first step is to configure the openai key. It will be used to created embeddings for the documents loaded into the index</p>"},{"location":"notebooks/LlamaIndex_example/#loading-documents","title":"Loading documents\u00b6","text":"<p>Load the documents stored in the <code>data/paul_graham/</code> using the SimpleDirectoryReader</p>"},{"location":"notebooks/LlamaIndex_example/#create-the-index","title":"Create the index\u00b6","text":"<p>Here we create an index backed by LanceDB using the documents loaded previously. LanceDBVectorStore takes a few arguments.</p> <ul> <li><p>uri (str, required): Location where LanceDB will store its files.</p> </li> <li><p>table_name (str, optional): The table name where the embeddings will be stored. Defaults to \"vectors\".</p> </li> <li><p>nprobes (int, optional): The number of probes used. A higher number makes search more accurate but also slower. Defaults to 20.</p> </li> <li><p>refine_factor: (int, optional): Refine the results by reading extra elements and re-ranking them in memory. Defaults to None</p> </li> <li><p>More details can be found at LanceDB docs</p> </li> </ul>"},{"location":"notebooks/LlamaIndex_example/#for-lancedb-cloud","title":"For LanceDB cloud :\u00b6","text":"<pre>vector_store = LanceDBVectorStore( \n    uri=\"db://db_name\", # your remote DB URI\n    api_key=\"sk_..\", # lancedb cloud api key\n    region=\"your-region\" # the region you configured\n    ...\n)\n</pre>"},{"location":"notebooks/LlamaIndex_example/#query-the-index","title":"Query the index\u00b6","text":"<p>We can now ask questions using our index. We can use filtering via <code>MetadataFilters</code> or use native lance <code>where</code> clause.</p>"},{"location":"notebooks/LlamaIndex_example/#hybrid-search","title":"Hybrid Search\u00b6","text":"<p>LanceDB offers hybrid search with reranking capabilities. For complete documentation, refer here.</p> <p>This example uses the <code>colbert</code> reranker. The following cell installs the necessary dependencies for <code>colbert</code>. If you choose a different reranker, make sure to adjust the dependencies accordingly.</p>"},{"location":"notebooks/LlamaIndex_example/#lance-filterssql-like-directly-via-the-where-clause","title":"lance filters(SQL like) directly via the <code>where</code> clause :\u00b6","text":""},{"location":"notebooks/LlamaIndex_example/#appending-data","title":"Appending data\u00b6","text":"<p>You can also add data to an existing index</p>"},{"location":"notebooks/Multivector_on_LanceDB/","title":"Multivector Search: Efficient Document Retrieval with ColPali and LanceDB","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install lancedb colpali-engine datasets tqdm\n</pre> !pip install lancedb colpali-engine datasets tqdm In\u00a0[\u00a0]: Copied! <pre>from math import sqrt\n\nimport pyarrow as pa\nfrom tqdm import tqdm\nimport lancedb\nfrom datasets import load_dataset\nfrom colpali_engine.models import ColPali, ColPaliProcessor\nimport torch\n\ndataset = load_dataset(\"davanstrien/ufo-ColPali\", split=\"train\")\ndataset\ndataset[333][\"image\"]\n</pre> from math import sqrt  import pyarrow as pa from tqdm import tqdm import lancedb from datasets import load_dataset from colpali_engine.models import ColPali, ColPaliProcessor import torch  dataset = load_dataset(\"davanstrien/ufo-ColPali\", split=\"train\") dataset dataset[333][\"image\"] <pre>/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \nThe secret `HF_TOKEN` does not exist in your Colab secrets.\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\nYou will be able to reuse this secret in all of your notebooks.\nPlease note that authentication is recommended but still optional to access public models or datasets.\n  warnings.warn(\n</pre> <pre>README.md:   0%|          | 0.00/1.20k [00:00&lt;?, ?B/s]</pre> <pre>train-00000-of-00001.parquet:   0%|          | 0.00/293M [00:00&lt;?, ?B/s]</pre> <pre>Generating train split:   0%|          | 0/2243 [00:00&lt;?, ? examples/s]</pre> Out[\u00a0]: In\u00a0[\u00a0]: Copied! <pre># load the model\ncolpali_model = ColPali.from_pretrained(\n    \"davanstrien/finetune_colpali_v1_2-ufo-4bit\",\n    torch_dtype=torch.bfloat16,\n    device_map=\"cuda\",  # change to cuda if you have a Nvidia GPU, or cpu if you don't have any GPU\n)\ncolpali_processor = ColPaliProcessor.from_pretrained(\n    \"vidore/colpaligemma-3b-pt-448-base\"\n)\n</pre> # load the model colpali_model = ColPali.from_pretrained(     \"davanstrien/finetune_colpali_v1_2-ufo-4bit\",     torch_dtype=torch.bfloat16,     device_map=\"cuda\",  # change to cuda if you have a Nvidia GPU, or cpu if you don't have any GPU ) colpali_processor = ColPaliProcessor.from_pretrained(     \"vidore/colpaligemma-3b-pt-448-base\" ) <pre>Loading checkpoint shards:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>adapter_model.safetensors:   0%|          | 0.00/157M [00:00&lt;?, ?B/s]</pre> <pre>preprocessor_config.json:   0%|          | 0.00/425 [00:00&lt;?, ?B/s]</pre> <pre>tokenizer_config.json:   0%|          | 0.00/243k [00:00&lt;?, ?B/s]</pre> <pre>tokenizer.json:   0%|          | 0.00/17.8M [00:00&lt;?, ?B/s]</pre> <pre>special_tokens_map.json:   0%|          | 0.00/733 [00:00&lt;?, ?B/s]</pre> In\u00a0[\u00a0]: Copied! <pre># create the lancedb table\nmultivector_type = pa.list_(pa.list_(pa.float32(), 128))\nschema = pa.schema(\n    [\n        pa.field(\"id\", pa.int64()),\n        pa.field(\"vector\", multivector_type),\n    ]\n)\ndb = lancedb.connect(\"my_database\")\ntable = db.create_table(\"ufo\", schema=schema, exist_ok=True)\n</pre> # create the lancedb table multivector_type = pa.list_(pa.list_(pa.float32(), 128)) schema = pa.schema(     [         pa.field(\"id\", pa.int64()),         pa.field(\"vector\", multivector_type),     ] ) db = lancedb.connect(\"my_database\") table = db.create_table(\"ufo\", schema=schema, exist_ok=True) <p>\u26a0\ufe0f WARNING: LONG EMBEDDING &amp; INGESTION STEP \u2757 Skip this cell unless you want to re-run the full embedding process.</p> <p>Why? Embedding the UFO dataset and ingesting it into LanceDB takes ~2 hours on a T4 GPU. To save time:</p> <ul> <li>**Use the pre-prepared table with index created ** (provided below) to proceed directly to step7: search.</li> <li>Step 5a contains the full ingestion code for reference (run it only if necessary).</li> <li>Step 6 contains the details on creating the index on the multivector column</li> </ul> In\u00a0[\u00a0]: Copied! <pre>!wget http://vectordb-recipes.s3.us-west-2.amazonaws.com/multivector_example.zip\n!unzip multivector_example.zip\n!rm multivector_example.zip\n</pre> !wget http://vectordb-recipes.s3.us-west-2.amazonaws.com/multivector_example.zip !unzip multivector_example.zip !rm multivector_example.zip In\u00a0[\u00a0]: Copied! <pre>db = lancedb.connect(\"multivector_example\")\ntable_name = \"ufo\"\ntable = db.open_table(\"ufo\")\n</pre> db = lancedb.connect(\"multivector_example\") table_name = \"ufo\" table = db.open_table(\"ufo\") In\u00a0[\u00a0]: Copied! <pre># search the table\nqueries = [\"alien\", \"crop circles\", \"unidentified\"]\nimage_results = []\nfor query_text in queries:\n    # encode the query\n    query = colpali_processor.process_queries([query_text]).to(colpali_model.device)\n    query = colpali_model(**query)[0].cpu().float().numpy()\n    print(f\"query shape: {query.shape}\")\n\n    # search the table\n    results = table.search(query).select([\"id\"]).limit(5).to_arrow()\n    id = results[\"id\"][0].as_py()\n    image = dataset[id][\"image\"]\n    image_results.append(image)\n</pre> # search the table queries = [\"alien\", \"crop circles\", \"unidentified\"] image_results = [] for query_text in queries:     # encode the query     query = colpali_processor.process_queries([query_text]).to(colpali_model.device)     query = colpali_model(**query)[0].cpu().float().numpy()     print(f\"query shape: {query.shape}\")      # search the table     results = table.search(query).select([\"id\"]).limit(5).to_arrow()     id = results[\"id\"][0].as_py()     image = dataset[id][\"image\"]     image_results.append(image) <pre>query shape: (15, 128)\nquery shape: (16, 128)\nquery shape: (15, 128)\n</pre> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nfor image in image_results:\n    plt.figure()\n    plt.imshow(image)\n</pre> import matplotlib.pyplot as plt  for image in image_results:     plt.figure()     plt.imshow(image) In\u00a0[\u00a0]: Copied! <pre># this would take 40 mins for first run on Apple M3 Max, may be longer if you are using CPU\nbatch_size = 4 # low it if you have a low memory GPU\nwith tqdm(total=len(dataset), desc=\"ingesting\") as pbar:\n    for i in range(0, len(dataset), batch_size):\n        batch = dataset[i : i + batch_size]\n        images = batch[\"image\"]\n\n        # encode the images\n        with torch.no_grad():\n            batch_images = colpali_processor.process_images(images).to(\n                colpali_model.device\n            )\n            image_embeddings = colpali_model(**batch_images)\n\n        real_size = len(images)\n        multivector = image_embeddings.cpu().float().numpy()\n        multivector = pa.array(multivector.tolist(), type=multivector_type)\n        data = pa.Table.from_pydict(\n            {\n                \"id\": list(range(i, i + real_size)),\n                \"vector\": multivector,\n            }\n        )\n        table.add(data)\n        pbar.update(real_size)\n</pre> # this would take 40 mins for first run on Apple M3 Max, may be longer if you are using CPU batch_size = 4 # low it if you have a low memory GPU with tqdm(total=len(dataset), desc=\"ingesting\") as pbar:     for i in range(0, len(dataset), batch_size):         batch = dataset[i : i + batch_size]         images = batch[\"image\"]          # encode the images         with torch.no_grad():             batch_images = colpali_processor.process_images(images).to(                 colpali_model.device             )             image_embeddings = colpali_model(**batch_images)          real_size = len(images)         multivector = image_embeddings.cpu().float().numpy()         multivector = pa.array(multivector.tolist(), type=multivector_type)         data = pa.Table.from_pydict(             {                 \"id\": list(range(i, i + real_size)),                 \"vector\": multivector,             }         )         table.add(data)         pbar.update(real_size) In\u00a0[\u00a0]: Copied! <pre>num_rows = table.count_rows()\ntable.create_index(\n    metric=\"cosine\",  # for now only cosine is supported for multivector\n    num_partitions=int(\n        sqrt(num_rows * 1030) # 1030 is number of embeddings per document\n    ),  # it's recommended to set sqrt of the number of embeddings as the number of partitions\n    num_sub_vectors=32,  # higher for accuracy, lower for speed\n    index_type=\"IVF_PQ\",\n)\n</pre> num_rows = table.count_rows() table.create_index(     metric=\"cosine\",  # for now only cosine is supported for multivector     num_partitions=int(         sqrt(num_rows * 1030) # 1030 is number of embeddings per document     ),  # it's recommended to set sqrt of the number of embeddings as the number of partitions     num_sub_vectors=32,  # higher for accuracy, lower for speed     index_type=\"IVF_PQ\", )"},{"location":"notebooks/Multivector_on_LanceDB/#multivector-search-efficient-document-retrieval-with-colpali-and-lancedb","title":"Multivector Search: Efficient Document Retrieval with ColPali and LanceDB\u00b6","text":"<p>Modern documents\u2014PDFs, scans, forms, invoices, or scientific diagrams\u2014rely heavily on visual elements like tables, figures, and spatial layouts to convey meaning. Retrieving context from these documents poses unique challenges:</p> <ul> <li>\ud83d\uddbc\ufe0f Loss of Context: Plain-text extraction destroys critical visual relationships (e.g., a table's structure or a diagram's annotations).</li> <li>\ud83e\udde9 Multi-Modal Complexity: Layouts combine text, images, and structured elements that require joint understanding.</li> <li>\ud83d\udccf Scale vs. Precision: Balancing pixel-perfect accuracy with efficient search across millions of documents.</li> </ul>"},{"location":"notebooks/Multivector_on_LanceDB/#why-traditional-methods-fail","title":"Why Traditional Methods Fail\u00b6","text":"<p>The traditional method is a brittle, multi-stage pipeline where visual context is eroded at every step. Retrieval becomes a \"best guess\" based on partial text. Usually, it will involve the following steps:</p> <ol> <li>OCR Text Extraction - extract raw text from scanned PDFs/images.</li> <li>Layout Detection - use models like LayoutLM or rule-based tools to segment pages into regions (titles, tables, figures).</li> <li>Structure Reconstruction - use heuristic rules or ML models try to infer reading order and hierarchy.</li> <li>Optional: Image/Table Captioning - apply vision-language models (e.g., GPT-4V) to describe figures/tables in natural language.</li> <li>Text Chunking - split text into fixed-size chunks or \"semantic\" passages (e.g., by paragraphs).</li> <li>Embedding &amp; Indexing- use text-based embeddings (e.g., BERT) and store in a vector DB (e.g., LanceDB).</li> </ol>"},{"location":"notebooks/Multivector_on_LanceDB/#our-approach-colpali-with-xtr-for-performant-retrieval","title":"Our Approach: ColPali with XTR for performant retrieval\u00b6","text":"<p>ColPali (Contextualized Late Interaction Over PaliGemma) enhances document retrieval by combining a vision-language model (VLM) with a multi-vector late interaction framework inspired by ColBERT. In this framework, documents and queries are encoded as collections of contextualized vectors\u2014precomputed for documents and indexed for queries. Unlike traditional methods, late interaction defers complex similarity computations between query and document vectors until the final retrieval stage, enabling nuanced semantic matching while maintaining efficiency.</p> <p>To further accelerate retrieval, we integrate XTR (ConteXtualized Token Retriever), which prioritizes critical document tokens during initial retrieval stage and removes the gathering stage to significantly improve the performance. By focusing on the most semantically salient tokens early in the process, XTR reduces computational complexity with improved recall, ensuring rapid identification of candidate documents.</p> <p>We used the UFO dataset, a dataset with rich tables, images and text, to demonstrate how to efficiently retrieve documents with ColPali and LanceDB.</p>"},{"location":"notebooks/Multivector_on_LanceDB/#step-1-install-required-libraries","title":"Step 1: Install Required Libraries\u00b6","text":""},{"location":"notebooks/Multivector_on_LanceDB/#step-2-load-the-ufo-dataset","title":"Step 2: Load the UFO dataset\u00b6","text":"<p>The UFO dataset has 2243 rows in total with an embedding of 128 dimension each. We show an example of the document to show how complicated it is with text and images blended in the document.</p>"},{"location":"notebooks/Multivector_on_LanceDB/#step-3-load-the-colpali-model","title":"Step 3: Load the ColPali model\u00b6","text":"<p>Note: select \"cuda\" if you are using a Nvidia GPU or \"cpu\" if there is no GPU available. Mac users, please use \"mps\". This step can take a few minutes.</p>"},{"location":"notebooks/Multivector_on_LanceDB/#step-4-connect-to-lancedb","title":"Step 4: Connect to LanceDB\u00b6","text":""},{"location":"notebooks/Multivector_on_LanceDB/#step-5-open-the-lancedb-table","title":"Step 5: Open the LanceDB table\u00b6","text":""},{"location":"notebooks/Multivector_on_LanceDB/#step-7-retrieve-documents-from-a-query","title":"Step 7: Retrieve documents from a query\u00b6","text":""},{"location":"notebooks/Multivector_on_LanceDB/#lets-see-the-retrieved-documents","title":"Let's see the retrieved documents!\u00b6","text":""},{"location":"notebooks/Multivector_on_LanceDB/#step-5a-embed-the-ufo-dataset-and-ingest-data-into-lancedb","title":"Step 5a: Embed the UFO dataset and ingest data into LanceDB\u00b6","text":"<p>Note: This step will take up to 2h when running with T4 GPU with a <code>batch_size=4</code>. You can increase the <code>batch_size</code> to accelerate the process if there is more memory available, e.g. <code>batch_size=32</code> requires 60GB of memory.</p>"},{"location":"notebooks/Multivector_on_LanceDB/#step-6-create-an-index-on-the-multivector-column","title":"Step 6: Create an index on the multivector column\u00b6","text":"<p>Note: LanceDB Cloud automatically infers the multivector column directly from the schema. If your dataset contains only one column with a list of vectors, no manual specification is required when building the vector index\u2014the system handles this implicitly.</p>"},{"location":"notebooks/embedding_tuner/","title":"Improve retrieval performance by Fine-tuning embedding model","text":"In\u00a0[24]: Copied! <pre>%pip install llama-index-llms-openai llama-index-embeddings-openai llama-index-finetuning llama-index-readers-file scikit-learn llama-index-embeddings-huggingface llama-index-vector-stores-lancedb pyarrow==12.0.1 -qq\n</pre> %pip install llama-index-llms-openai llama-index-embeddings-openai llama-index-finetuning llama-index-readers-file scikit-learn llama-index-embeddings-huggingface llama-index-vector-stores-lancedb pyarrow==12.0.1 -qq <pre>ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf-cu12 24.4.1 requires pyarrow&lt;15.0.0a0,&gt;=14.0.1, but you have pyarrow 12.0.1 which is incompatible.\ndatasets 2.20.0 requires pyarrow&gt;=15.0.0, but you have pyarrow 12.0.1 which is incompatible.\n</pre> In\u00a0[22]: Copied! <pre># For eval utils\n!git clone https://github.com/lancedb/ragged.git\n!cd ragged &amp;&amp; pip install .\n</pre> # For eval utils !git clone https://github.com/lancedb/ragged.git !cd ragged &amp;&amp; pip install .  <pre>Cloning into 'ragged'...\nremote: Enumerating objects: 160, done.\nremote: Counting objects: 100% (160/160), done.\nremote: Compressing objects: 100% (103/103), done.\nremote: Total 160 (delta 70), reused 125 (delta 41), pack-reused 0\nReceiving objects: 100% (160/160), 38.15 KiB | 9.54 MiB/s, done.\nResolving deltas: 100% (70/70), done.\nProcessing /content/ragged\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nCollecting datasets (from ragged==0.1.dev0)\n  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 547.8/547.8 kB 13.8 MB/s eta 0:00:00\nRequirement already satisfied: lancedb in /usr/local/lib/python3.10/dist-packages (from ragged==0.1.dev0) (0.9.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ragged==0.1.dev0) (2.0.3)\nCollecting streamlit (from ragged==0.1.dev0)\n  Downloading streamlit-1.36.0-py2.py3-none-any.whl (8.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 8.6/8.6 MB 54.1 MB/s eta 0:00:00\nRequirement already satisfied: tantivy in /usr/local/lib/python3.10/dist-packages (from ragged==0.1.dev0) (0.22.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets-&gt;ragged==0.1.dev0) (3.15.4)\nRequirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets-&gt;ragged==0.1.dev0) (1.25.2)\nCollecting pyarrow&gt;=15.0.0 (from datasets-&gt;ragged==0.1.dev0)\n  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 40.8/40.8 MB 14.0 MB/s eta 0:00:00\nRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets-&gt;ragged==0.1.dev0) (0.6)\nCollecting dill&lt;0.3.9,&gt;=0.3.0 (from datasets-&gt;ragged==0.1.dev0)\n  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 116.3/116.3 kB 20.6 MB/s eta 0:00:00\nCollecting requests&gt;=2.32.2 (from datasets-&gt;ragged==0.1.dev0)\n  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 64.9/64.9 kB 11.1 MB/s eta 0:00:00\nRequirement already satisfied: tqdm&gt;=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets-&gt;ragged==0.1.dev0) (4.66.4)\nCollecting xxhash (from datasets-&gt;ragged==0.1.dev0)\n  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 194.1/194.1 kB 29.8 MB/s eta 0:00:00\nCollecting multiprocess (from datasets-&gt;ragged==0.1.dev0)\n  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.8/134.8 kB 24.7 MB/s eta 0:00:00\nRequirement already satisfied: fsspec[http]&lt;=2024.5.0,&gt;=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets-&gt;ragged==0.1.dev0) (2023.6.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets-&gt;ragged==0.1.dev0) (3.9.5)\nRequirement already satisfied: huggingface-hub&gt;=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets-&gt;ragged==0.1.dev0) (0.23.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets-&gt;ragged==0.1.dev0) (24.1)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets-&gt;ragged==0.1.dev0) (6.0.1)\nRequirement already satisfied: deprecation in /usr/local/lib/python3.10/dist-packages (from lancedb-&gt;ragged==0.1.dev0) (2.1.0)\nRequirement already satisfied: pylance==0.13.0 in /usr/local/lib/python3.10/dist-packages (from lancedb-&gt;ragged==0.1.dev0) (0.13.0)\nRequirement already satisfied: ratelimiter~=1.0 in /usr/local/lib/python3.10/dist-packages (from lancedb-&gt;ragged==0.1.dev0) (1.2.0.post0)\nRequirement already satisfied: retry&gt;=0.9.2 in /usr/local/lib/python3.10/dist-packages (from lancedb-&gt;ragged==0.1.dev0) (0.9.2)\nRequirement already satisfied: pydantic&gt;=1.10 in /usr/local/lib/python3.10/dist-packages (from lancedb-&gt;ragged==0.1.dev0) (2.8.0)\nRequirement already satisfied: attrs&gt;=21.3.0 in /usr/local/lib/python3.10/dist-packages (from lancedb-&gt;ragged==0.1.dev0) (23.2.0)\nRequirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from lancedb-&gt;ragged==0.1.dev0) (5.3.3)\nRequirement already satisfied: overrides&gt;=0.7 in /usr/local/lib/python3.10/dist-packages (from lancedb-&gt;ragged==0.1.dev0) (7.7.0)\nCollecting pyarrow&gt;=15.0.0 (from datasets-&gt;ragged==0.1.dev0)\n  Downloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 38.3/38.3 MB 12.3 MB/s eta 0:00:00\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;ragged==0.1.dev0) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;ragged==0.1.dev0) (2023.4)\nRequirement already satisfied: tzdata&gt;=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;ragged==0.1.dev0) (2024.1)\nRequirement already satisfied: altair&lt;6,&gt;=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit-&gt;ragged==0.1.dev0) (4.2.2)\nRequirement already satisfied: blinker&lt;2,&gt;=1.0.0 in /usr/lib/python3/dist-packages (from streamlit-&gt;ragged==0.1.dev0) (1.4)\nRequirement already satisfied: click&lt;9,&gt;=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit-&gt;ragged==0.1.dev0) (8.1.7)\nRequirement already satisfied: pillow&lt;11,&gt;=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit-&gt;ragged==0.1.dev0) (9.4.0)\nRequirement already satisfied: protobuf&lt;6,&gt;=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit-&gt;ragged==0.1.dev0) (3.20.3)\nRequirement already satisfied: rich&lt;14,&gt;=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit-&gt;ragged==0.1.dev0) (13.7.1)\nRequirement already satisfied: tenacity&lt;9,&gt;=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit-&gt;ragged==0.1.dev0) (8.3.0)\nRequirement already satisfied: toml&lt;2,&gt;=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit-&gt;ragged==0.1.dev0) (0.10.2)\nRequirement already satisfied: typing-extensions&lt;5,&gt;=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit-&gt;ragged==0.1.dev0) (4.12.2)\nCollecting gitpython!=3.1.19,&lt;4,&gt;=3.0.7 (from streamlit-&gt;ragged==0.1.dev0)\n  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 207.3/207.3 kB 22.6 MB/s eta 0:00:00\nCollecting pydeck&lt;1,&gt;=0.8.0b4 (from streamlit-&gt;ragged==0.1.dev0)\n  Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.9/6.9 MB 63.4 MB/s eta 0:00:00\nRequirement already satisfied: tornado&lt;7,&gt;=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit-&gt;ragged==0.1.dev0) (6.3.3)\nCollecting watchdog&lt;5,&gt;=2.1.5 (from streamlit-&gt;ragged==0.1.dev0)\n  Downloading watchdog-4.0.1-py3-none-manylinux2014_x86_64.whl (83 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 83.0/83.0 kB 11.0 MB/s eta 0:00:00\nRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair&lt;6,&gt;=4.0-&gt;streamlit-&gt;ragged==0.1.dev0) (0.4)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair&lt;6,&gt;=4.0-&gt;streamlit-&gt;ragged==0.1.dev0) (3.1.4)\nRequirement already satisfied: jsonschema&gt;=3.0 in /usr/local/lib/python3.10/dist-packages (from altair&lt;6,&gt;=4.0-&gt;streamlit-&gt;ragged==0.1.dev0) (4.19.2)\nRequirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair&lt;6,&gt;=4.0-&gt;streamlit-&gt;ragged==0.1.dev0) (0.12.1)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets-&gt;ragged==0.1.dev0) (1.3.1)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets-&gt;ragged==0.1.dev0) (1.4.1)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets-&gt;ragged==0.1.dev0) (6.0.5)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets-&gt;ragged==0.1.dev0) (1.9.4)\nRequirement already satisfied: async-timeout&lt;5.0,&gt;=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets-&gt;ragged==0.1.dev0) (4.0.3)\nCollecting gitdb&lt;5,&gt;=4.0.1 (from gitpython!=3.1.19,&lt;4,&gt;=3.0.7-&gt;streamlit-&gt;ragged==0.1.dev0)\n  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 62.7/62.7 kB 10.6 MB/s eta 0:00:00\nRequirement already satisfied: annotated-types&gt;=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic&gt;=1.10-&gt;lancedb-&gt;ragged==0.1.dev0) (0.7.0)\nRequirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic&gt;=1.10-&gt;lancedb-&gt;ragged==0.1.dev0) (2.20.0)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;ragged==0.1.dev0) (1.16.0)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.32.2-&gt;datasets-&gt;ragged==0.1.dev0) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.32.2-&gt;datasets-&gt;ragged==0.1.dev0) (3.7)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.32.2-&gt;datasets-&gt;ragged==0.1.dev0) (2.0.7)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.32.2-&gt;datasets-&gt;ragged==0.1.dev0) (2024.6.2)\nRequirement already satisfied: decorator&gt;=3.4.2 in /usr/local/lib/python3.10/dist-packages (from retry&gt;=0.9.2-&gt;lancedb-&gt;ragged==0.1.dev0) (4.4.2)\nRequirement already satisfied: py&lt;2.0.0,&gt;=1.4.26 in /usr/local/lib/python3.10/dist-packages (from retry&gt;=0.9.2-&gt;lancedb-&gt;ragged==0.1.dev0) (1.11.0)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich&lt;14,&gt;=10.14.0-&gt;streamlit-&gt;ragged==0.1.dev0) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich&lt;14,&gt;=10.14.0-&gt;streamlit-&gt;ragged==0.1.dev0) (2.16.1)\nCollecting smmap&lt;6,&gt;=3.0.1 (from gitdb&lt;5,&gt;=4.0.1-&gt;gitpython!=3.1.19,&lt;4,&gt;=3.0.7-&gt;streamlit-&gt;ragged==0.1.dev0)\n  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;altair&lt;6,&gt;=4.0-&gt;streamlit-&gt;ragged==0.1.dev0) (2.1.5)\nRequirement already satisfied: jsonschema-specifications&gt;=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema&gt;=3.0-&gt;altair&lt;6,&gt;=4.0-&gt;streamlit-&gt;ragged==0.1.dev0) (2023.12.1)\nRequirement already satisfied: referencing&gt;=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema&gt;=3.0-&gt;altair&lt;6,&gt;=4.0-&gt;streamlit-&gt;ragged==0.1.dev0) (0.35.1)\nRequirement already satisfied: rpds-py&gt;=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema&gt;=3.0-&gt;altair&lt;6,&gt;=4.0-&gt;streamlit-&gt;ragged==0.1.dev0) (0.18.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich&lt;14,&gt;=10.14.0-&gt;streamlit-&gt;ragged==0.1.dev0) (0.1.2)\nBuilding wheels for collected packages: ragged\n  Building wheel for ragged (pyproject.toml) ... done\n  Created wheel for ragged: filename=ragged-0.1.dev0-py3-none-any.whl size=24662 sha256=d086741b289188a92153223fdb65db69f9297a523c7874746fd1669f7d3f9c07\n  Stored in directory: /tmp/pip-ephem-wheel-cache-q327t6y_/wheels/aa/3f/b0/d70e6f86074491db9b0bc7431c11f0138f2ed2359151509cf7\nSuccessfully built ragged\nInstalling collected packages: xxhash, watchdog, smmap, requests, pyarrow, dill, pydeck, multiprocess, gitdb, gitpython, datasets, streamlit, ragged\n  Attempting uninstall: requests\n    Found existing installation: requests 2.31.0\n    Uninstalling requests-2.31.0:\n      Successfully uninstalled requests-2.31.0\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 12.0.1\n    Uninstalling pyarrow-12.0.1:\n      Successfully uninstalled pyarrow-12.0.1\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf-cu12 24.4.1 requires pyarrow&lt;15.0.0a0,&gt;=14.0.1, but you have pyarrow 15.0.0 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\nSuccessfully installed datasets-2.20.0 dill-0.3.8 gitdb-4.0.11 gitpython-3.1.43 multiprocess-0.70.16 pyarrow-15.0.0 pydeck-0.9.1 ragged-0.1.dev0 requests-2.32.3 smmap-5.0.1 streamlit-1.36.0 watchdog-4.0.1 xxhash-3.4.1\n</pre> In\u00a0[8]: Copied! <pre>!wget https://raw.githubusercontent.com/AyushExel/assets/main/data_qa.csv\n</pre> !wget https://raw.githubusercontent.com/AyushExel/assets/main/data_qa.csv <pre>--2024-07-09 20:37:46--  https://raw.githubusercontent.com/AyushExel/assets/main/data_qa.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 680439 (664K) [text/plain]\nSaving to: \u2018data_qa.csv\u2019\n\ndata_qa.csv         100%[===================&gt;] 664.49K  --.-KB/s    in 0.006s  \n\n2024-07-09 20:37:47 (100 MB/s) - \u2018data_qa.csv\u2019 saved [680439/680439]\n\n</pre> In\u00a0[9]: Copied! <pre>import pandas as pd\n\ndata = pd.read_csv(\"data_qa.csv\")\n</pre> import pandas as pd  data = pd.read_csv(\"data_qa.csv\") In\u00a0[10]: Copied! <pre>from pathlib import Path\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.readers.file import PagedCSVReader\n\ndef load_corpus(file, verbose=False):\n    if verbose:\n        print(f\"Loading files {file}...\")\n\n    loader = PagedCSVReader(encoding=\"utf-8\")\n    docs = loader.load_data(file=Path(file))\n\n    if verbose:\n        print(f\"Loaded {len(docs)} docs\")\n\n    parser = SentenceSplitter()\n    nodes = parser.get_nodes_from_documents(docs, show_progress=verbose)\n\n    if verbose:\n        print(f\"Parsed {len(nodes)} nodes\")\n\n    return nodes\n</pre> from pathlib import Path from llama_index.core.node_parser import SentenceSplitter from llama_index.readers.file import PagedCSVReader  def load_corpus(file, verbose=False):     if verbose:         print(f\"Loading files {file}...\")      loader = PagedCSVReader(encoding=\"utf-8\")     docs = loader.load_data(file=Path(file))      if verbose:         print(f\"Loaded {len(docs)} docs\")      parser = SentenceSplitter()     nodes = parser.get_nodes_from_documents(docs, show_progress=verbose)      if verbose:         print(f\"Parsed {len(nodes)} nodes\")      return nodes In\u00a0[11]: Copied! <pre>import pandas as pd\n\ndf = pd.read_csv(\"data_qa.csv\", index_col=0)\n</pre> import pandas as pd  df = pd.read_csv(\"data_qa.csv\", index_col=0) In\u00a0[12]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"INSERT KEY\"\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"INSERT KEY\" <p>Split into train and validation sets. We'll use the original df for val as that has different queries generated via a different prompt.</p> In\u00a0[13]: Copied! <pre>from sklearn.model_selection import train_test_split\n\n# Randomly shuffle df.\n#df = df.sample(frac=1, random_state=42)\n\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n\ntrain_df.to_csv(\"train_data_qa.csv\", index=False)\nval_df.to_csv(\"val_data_qa.csv\", index=False)\n</pre> from sklearn.model_selection import train_test_split  # Randomly shuffle df. #df = df.sample(frac=1, random_state=42)  train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)  train_df.to_csv(\"train_data_qa.csv\", index=False) val_df.to_csv(\"val_data_qa.csv\", index=False) In\u00a0[14]: Copied! <pre>train_nodes = load_corpus(\"train_data_qa.csv\", verbose=True)\nval_nodes = load_corpus(\"val_data_qa.csv\", verbose=True)\n</pre> train_nodes = load_corpus(\"train_data_qa.csv\", verbose=True) val_nodes = load_corpus(\"val_data_qa.csv\", verbose=True) <pre>Loading files train_data_qa.csv...\nLoaded 176 docs\n</pre> <pre>Parsing nodes:   0%|          | 0/176 [00:00&lt;?, ?it/s]</pre> <pre>Parsed 221 nodes\nLoading files val_data_qa.csv...\nLoaded 44 docs\n</pre> <pre>Parsing nodes:   0%|          | 0/44 [00:00&lt;?, ?it/s]</pre> <pre>Parsed 59 nodes\n</pre> In\u00a0[15]: Copied! <pre>from llama_index.finetuning import generate_qa_embedding_pairs\nfrom llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n</pre> from llama_index.finetuning import generate_qa_embedding_pairs from llama_index.core.evaluation import EmbeddingQAFinetuneDataset In\u00a0[16]: Copied! <pre>from llama_index.llms.openai import OpenAI\n\n\ntrain_dataset = generate_qa_embedding_pairs(\n    llm=OpenAI(model=\"gpt-3.5-turbo\"), nodes=train_nodes, verbose=False\n)\nval_dataset = generate_qa_embedding_pairs(\n    llm=OpenAI(model=\"gpt-3.5-turbo\"), nodes=val_nodes, verbose=False\n)\n\ntrain_dataset.save_json(\"train_dataset.json\")\nval_dataset.save_json(\"val_dataset.json\")\n</pre> from llama_index.llms.openai import OpenAI   train_dataset = generate_qa_embedding_pairs(     llm=OpenAI(model=\"gpt-3.5-turbo\"), nodes=train_nodes, verbose=False ) val_dataset = generate_qa_embedding_pairs(     llm=OpenAI(model=\"gpt-3.5-turbo\"), nodes=val_nodes, verbose=False )  train_dataset.save_json(\"train_dataset.json\") val_dataset.save_json(\"val_dataset.json\") <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 221/221 [05:29&lt;00:00,  1.49s/it]\n221it [00:00, ?it/s]\n</pre> In\u00a0[17]: Copied! <pre># Load again\ntrain_dataset = EmbeddingQAFinetuneDataset.from_json(\"train_dataset.json\")\n\nval_dataset = EmbeddingQAFinetuneDataset.from_json(\"val_dataset.json\")\n</pre> # Load again train_dataset = EmbeddingQAFinetuneDataset.from_json(\"train_dataset.json\")  val_dataset = EmbeddingQAFinetuneDataset.from_json(\"val_dataset.json\") In\u00a0[18]: Copied! <pre>import torch\nfrom llama_index.finetuning import SentenceTransformersFinetuneEngine\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nfinetune_engine = SentenceTransformersFinetuneEngine(\n    train_dataset,\n    model_id=\"BAAI/bge-small-en-v1.5\",\n    model_output_path=\"tuned_model\",\n    val_dataset=val_dataset,\n    device=device\n)\n</pre> import torch from llama_index.finetuning import SentenceTransformersFinetuneEngine  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  finetune_engine = SentenceTransformersFinetuneEngine(     train_dataset,     model_id=\"BAAI/bge-small-en-v1.5\",     model_output_path=\"tuned_model\",     val_dataset=val_dataset,     device=device ) <pre>/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \nThe secret `HF_TOKEN` does not exist in your Colab secrets.\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\nYou will be able to reuse this secret in all of your notebooks.\nPlease note that authentication is recommended but still optional to access public models or datasets.\n  warnings.warn(\n</pre> <pre>modules.json:   0%|          | 0.00/349 [00:00&lt;?, ?B/s]</pre> <pre>config_sentence_transformers.json:   0%|          | 0.00/124 [00:00&lt;?, ?B/s]</pre> <pre>README.md:   0%|          | 0.00/94.8k [00:00&lt;?, ?B/s]</pre> <pre>sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00&lt;?, ?B/s]</pre> <pre>/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n</pre> <pre>config.json:   0%|          | 0.00/743 [00:00&lt;?, ?B/s]</pre> <pre>model.safetensors:   0%|          | 0.00/133M [00:00&lt;?, ?B/s]</pre> <pre>tokenizer_config.json:   0%|          | 0.00/366 [00:00&lt;?, ?B/s]</pre> <pre>vocab.txt:   0%|          | 0.00/232k [00:00&lt;?, ?B/s]</pre> <pre>tokenizer.json:   0%|          | 0.00/711k [00:00&lt;?, ?B/s]</pre> <pre>special_tokens_map.json:   0%|          | 0.00/125 [00:00&lt;?, ?B/s]</pre> <pre>1_Pooling/config.json:   0%|          | 0.00/190 [00:00&lt;?, ?B/s]</pre> In\u00a0[19]: Copied! <pre>finetune_engine.finetune( )\n</pre> finetune_engine.finetune( )  <pre>Epoch:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>Iteration:   0%|          | 0/45 [00:00&lt;?, ?it/s]</pre> <pre>Iteration:   0%|          | 0/45 [00:00&lt;?, ?it/s]</pre> In\u00a0[20]: Copied! <pre>embed_model = finetune_engine.get_finetuned_model()\n</pre> embed_model = finetune_engine.get_finetuned_model()  In\u00a0[25]: Copied! <pre>from ragged.dataset import CSVDataset, SquadDataset\nfrom ragged.rag import llamaIndexRAG\nfrom ragged.metrics.retriever.hit_rate import HitRate\nfrom ragged.search_utils import QueryType\n\n\ndef evaluate_vector(\n    dataset,\n    embed_model_name_or_path,\n    top_k=5,\n):\n  dataset = CSVDataset(dataset)\n\n  hit_rate = HitRate(dataset, embed_model_kwarg={\"name\": embed_model_name_or_path})\n\n  print(hit_rate.evaluate(top_k, query_type=QueryType.VECTOR))\n\n\ndef evaluate_all(\n    dataset,\n    embed_model_name_or_path,\n    reranker,\n    top_k=5,\n):\n  dataset = CSVDataset(dataset)\n  hit_rate = HitRate(dataset, embed_model_kwarg={\"name\": embed_model_name_or_path}, reranker=reranker)\n\n  print(hit_rate.evaluate(top_k, query_type=QueryType.ALL))\n</pre> from ragged.dataset import CSVDataset, SquadDataset from ragged.rag import llamaIndexRAG from ragged.metrics.retriever.hit_rate import HitRate from ragged.search_utils import QueryType   def evaluate_vector(     dataset,     embed_model_name_or_path,     top_k=5, ):   dataset = CSVDataset(dataset)    hit_rate = HitRate(dataset, embed_model_kwarg={\"name\": embed_model_name_or_path})    print(hit_rate.evaluate(top_k, query_type=QueryType.VECTOR))   def evaluate_all(     dataset,     embed_model_name_or_path,     reranker,     top_k=5, ):   dataset = CSVDataset(dataset)   hit_rate = HitRate(dataset, embed_model_kwarg={\"name\": embed_model_name_or_path}, reranker=reranker)    print(hit_rate.evaluate(top_k, query_type=QueryType.ALL))  In\u00a0[28]: Copied! <pre>from lancedb.rerankers import CohereReranker, LinearCombinationReranker\n\n\n#linear_combination_reranker = LinearCombinationReranker()\ncohere_reranker = CohereReranker(api_key=\"INSERT KEY\")\n\n#evaluate_all(\"data_qa.csv\", \"BAAI/bge-small-en-v1.5\", linear_combination_reranker)\nhit_rate_bge_cohere = evaluate_all(\"data_qa.csv\", \"BAAI/bge-small-en-v1.5\", cohere_reranker)\n</pre> from lancedb.rerankers import CohereReranker, LinearCombinationReranker   #linear_combination_reranker = LinearCombinationReranker() cohere_reranker = CohereReranker(api_key=\"INSERT KEY\")  #evaluate_all(\"data_qa.csv\", \"BAAI/bge-small-en-v1.5\", linear_combination_reranker) hit_rate_bge_cohere = evaluate_all(\"data_qa.csv\", \"BAAI/bge-small-en-v1.5\", cohere_reranker)  <pre>INFO:lancedb:Adding 110 documents to LanceDB, in 1 batches of size 110\n</pre> <pre>Adding 110 documents to LanceDB, in 1 batches of size 110\n</pre> <pre>Adding batch to LanceDB: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 110/110 [00:00&lt;00:00, 165663.71it/s]\nINFO:lancedb:Adding batch 0 to LanceDB\n</pre> <pre>Adding batch 0 to LanceDB\n</pre> <pre>INFO:lancedb:created table with length 110\n</pre> <pre>created table with length 110\n</pre> <pre>INFO:lancedb:Evaluating query type: vector\n</pre> <pre>Evaluating query type: vector\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 220/220 [00:10&lt;00:00, 20.61it/s]\nINFO:lancedb:Hit rate for vector: 0.6409090909090909\n</pre> <pre>Hit rate for vector: 0.6409090909090909\n</pre> <pre>INFO:lancedb:Evaluating query type: fts\n</pre> <pre>Evaluating query type: fts\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 220/220 [00:00&lt;00:00, 361.50it/s]\nINFO:lancedb:Hit rate for fts: 0.5954545454545455\n</pre> <pre>Hit rate for fts: 0.5954545454545455\n</pre> <pre>INFO:lancedb:Evaluating query type: rerank_vector\n</pre> <pre>Evaluating query type: rerank_vector\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 220/220 [01:32&lt;00:00,  2.38it/s]\nINFO:lancedb:Hit rate for rerank_vector: 0.6772727272727272\n</pre> <pre>Hit rate for rerank_vector: 0.6772727272727272\n</pre> <pre>INFO:lancedb:Evaluating query type: rerank_fts\n</pre> <pre>Evaluating query type: rerank_fts\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 220/220 [01:23&lt;00:00,  2.63it/s]\nINFO:lancedb:Hit rate for rerank_fts: 0.6727272727272727\n</pre> <pre>Hit rate for rerank_fts: 0.6727272727272727\n</pre> <pre>INFO:lancedb:Evaluating query type: hybrid\n</pre> <pre>Evaluating query type: hybrid\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 220/220 [01:28&lt;00:00,  2.47it/s]\nINFO:lancedb:Hit rate for hybrid: 0.759090909090909\n</pre> <pre>Hit rate for hybrid: 0.759090909090909\nvector=0.6409090909090909 fts=0.5954545454545455 rerank_vector=0.6772727272727272 rerank_fts=0.6727272727272727 hybrid=0.759090909090909\n</pre> In\u00a0[29]: Copied! <pre>#evaluate_all(\"data_qa.csv\", \"tuned_model/\", linear_combination_reranker)\nevaluate_all(\"data_qa.csv\", \"tuned_model/\", cohere_reranker)\n</pre> #evaluate_all(\"data_qa.csv\", \"tuned_model/\", linear_combination_reranker) evaluate_all(\"data_qa.csv\", \"tuned_model/\", cohere_reranker)   <pre>INFO:lancedb:Adding 110 documents to LanceDB, in 1 batches of size 110\n</pre> <pre>Adding 110 documents to LanceDB, in 1 batches of size 110\n</pre> <pre>Adding batch to LanceDB: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 110/110 [00:00&lt;00:00, 91234.61it/s]\nINFO:lancedb:Adding batch 0 to LanceDB\n</pre> <pre>Adding batch 0 to LanceDB\n</pre> <pre>INFO:lancedb:created table with length 110\n</pre> <pre>created table with length 110\n</pre> <pre>INFO:lancedb:Evaluating query type: vector\n</pre> <pre>Evaluating query type: vector\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 220/220 [00:09&lt;00:00, 22.17it/s]\nINFO:lancedb:Hit rate for vector: 0.6727272727272727\n</pre> <pre>Hit rate for vector: 0.6727272727272727\n</pre> <pre>INFO:lancedb:Evaluating query type: fts\n</pre> <pre>Evaluating query type: fts\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 220/220 [00:00&lt;00:00, 285.43it/s]\nINFO:lancedb:Hit rate for fts: 0.5954545454545455\n</pre> <pre>Hit rate for fts: 0.5954545454545455\n</pre> <pre>INFO:lancedb:Evaluating query type: rerank_vector\n</pre> <pre>Evaluating query type: rerank_vector\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 220/220 [01:29&lt;00:00,  2.45it/s]\nINFO:lancedb:Hit rate for rerank_vector: 0.7545454545454545\n</pre> <pre>Hit rate for rerank_vector: 0.7545454545454545\n</pre> <pre>INFO:lancedb:Evaluating query type: rerank_fts\n</pre> <pre>Evaluating query type: rerank_fts\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 220/220 [01:22&lt;00:00,  2.66it/s]\nINFO:lancedb:Hit rate for rerank_fts: 0.6727272727272727\n</pre> <pre>Hit rate for rerank_fts: 0.6727272727272727\n</pre> <pre>INFO:lancedb:Evaluating query type: hybrid\n</pre> <pre>Evaluating query type: hybrid\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 220/220 [01:28&lt;00:00,  2.48it/s]\nINFO:lancedb:Hit rate for hybrid: 0.7681818181818182\n</pre> <pre>Hit rate for hybrid: 0.7681818181818182\nvector=0.6727272727272727 fts=0.5954545454545455 rerank_vector=0.7545454545454545 rerank_fts=0.6727272727272727 hybrid=0.7681818181818182\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/embedding_tuner/#improve-retrieval-performance-by-fine-tuning-embedding-model","title":"Improve retrieval performance by Fine-tuning embedding model\u00b6","text":"<p>Another way to improve retriever performance is to fine-tune the embedding model itself. Fine-tuning the embedding model can help in learning better representations for the documents and queries in the dataset. This can be particularly useful when the dataset is very different from the pre-trained data used to train the embedding model.</p>"},{"location":"notebooks/embedding_tuner/#the-dataset","title":"The dataset\u00b6","text":"<p>The dataset we'll use is a synthetic QA dataset generated from LLama2 review paper. The paper was divided into chunks, with each chunk being a unique context. An LLM was prompted to ask questions relevant to the context for testing a retriever. The exact code and other utility functions for this can be found in this repo</p>"},{"location":"notebooks/embedding_tuner/#pre-processing","title":"Pre-processing\u00b6","text":"<p>Now we need to parse the context(corpus) of the dataset as llama-index text nodes.</p>"},{"location":"notebooks/embedding_tuner/#generate-the-query-from-context-from-training","title":"Generate the query from context from training\u00b6","text":""},{"location":"notebooks/embedding_tuner/#fine-tune-the-embedding-model","title":"Fine-tune the embedding model\u00b6","text":""},{"location":"notebooks/embedding_tuner/#evaluate-on-hit-rate","title":"Evaluate on Hit-rate\u00b6","text":""},{"location":"notebooks/hybrid_search/","title":"Example - Airbnb financial data search","text":"In\u00a0[\u00a0]: Copied! <pre># Setup\n!pip install lancedb pandas langchain langchain_openai langchain-community pypdf openai cohere tiktoken sentence_transformers tantivy==0.20.1\n</pre> # Setup !pip install lancedb pandas langchain langchain_openai langchain-community pypdf openai cohere tiktoken sentence_transformers tantivy==0.20.1 In\u00a0[2]: Copied! <pre>import os\nimport getpass\n\n# Set your OpenAI API key\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n</pre> import os import getpass  # Set your OpenAI API key os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()  <pre>\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n</pre> In\u00a0[3]: Copied! <pre>def pretty_print(docs):\n    for doc in docs:\n        print(doc + \"\\n\\n\")\n</pre> def pretty_print(docs):     for doc in docs:         print(doc + \"\\n\\n\") In\u00a0[\u00a0]: Copied! <pre>from langchain_community.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# Load $ABNB's financial report. This may take 1-2 minutes since the PDF is large\nsec_filing_pdf = \"https://d18rn0p25nwr6d.cloudfront.net/CIK-0001559720/8a9ebed0-815a-469a-87eb-1767d21d8cec.pdf\"\n\n# Create your PDF loader\nloader = PyPDFLoader(sec_filing_pdf)\n\n# Load the PDF document\ndocuments = loader.load()\n\n# Chunk the financial report\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=0)\ndocs = text_splitter.split_documents(documents)\n</pre> from langchain_community.document_loaders import PyPDFLoader from langchain.text_splitter import RecursiveCharacterTextSplitter  # Load $ABNB's financial report. This may take 1-2 minutes since the PDF is large sec_filing_pdf = \"https://d18rn0p25nwr6d.cloudfront.net/CIK-0001559720/8a9ebed0-815a-469a-87eb-1767d21d8cec.pdf\"  # Create your PDF loader loader = PyPDFLoader(sec_filing_pdf)  # Load the PDF document documents = loader.load()  # Chunk the financial report text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=0) docs = text_splitter.split_documents(documents) In\u00a0[\u00a0]: Copied! <pre>from langchain_community.vectorstores import LanceDB\nfrom langchain_openai import OpenAIEmbeddings\nimport lancedb\n\n\nembedding_function = OpenAIEmbeddings()\n\ndb = lancedb.connect(\"~/langchain\")\n\n# Load the document into LanceDB\ndb = LanceDB.from_documents(docs, embedding_function, connection=db)\n</pre> from langchain_community.vectorstores import LanceDB from langchain_openai import OpenAIEmbeddings import lancedb   embedding_function = OpenAIEmbeddings()  db = lancedb.connect(\"~/langchain\")  # Load the document into LanceDB db = LanceDB.from_documents(docs, embedding_function, connection=db) In\u00a0[\u00a0]: Copied! <pre>table = db._table\ntable.create_fts_index(\"text\")\n</pre> table = db._table table.create_fts_index(\"text\") In\u00a0[19]: Copied! <pre>table.to_pandas().head()\n</pre> table.to_pandas().head() Out[19]: vector id text metadata 0 [-0.0016961554, -0.03531899, 0.011809787, -0.0... 5c66d086-0fed-4270-a91b-c2b67b3ed052 Table of Contents\\nUNITED STATES\\nSECURITIES A... {'page': 0, 'source': 'https://d18rn0p25nwr6d.... 1 [-0.021446472, -0.021045355, 0.010823516, -0.0... ddcfa6b1-3de8-4933-a187-6aa7b7ae87b4 Class A common stock, par value $0.0001 per sh... {'page': 0, 'source': 'https://d18rn0p25nwr6d.... 2 [-0.020018686, -0.014233166, -0.010991167, -0.... c391b1e1-6f66-41f2-82ff-18db5a218303 this chapter) during the preceding 12 months (... {'page': 0, 'source': 'https://d18rn0p25nwr6d.... 3 [-0.019061018, -0.0022632438, -0.011158161, -0... 3e896a62-8631-4a54-86bd-ee2f69f3b373 Indicate by check mark whether the registrant ... {'page': 0, 'source': 'https://d18rn0p25nwr6d.... 4 [-0.015733723, -0.012287037, -0.004055117, -0.... 47f5dd55-b3e7-4879-afba-5ca9eea7341b As of June 30, 2022, the aggregate market valu... {'page': 1, 'source': 'https://d18rn0p25nwr6d.... In\u00a0[30]: Copied! <pre>str_query = \"What are the specific factors contributing to Airbnb's increased operational expenses in the last fiscal year?\"\nquery = embedding_function.embed_query(str_query)\ndocs = table.search(query, query_type=\"vector\").limit(5).to_pandas()[\"text\"].to_list()\n</pre> str_query = \"What are the specific factors contributing to Airbnb's increased operational expenses in the last fiscal year?\" query = embedding_function.embed_query(str_query) docs = table.search(query, query_type=\"vector\").limit(5).to_pandas()[\"text\"].to_list() In\u00a0[31]: Copied! <pre>pretty_print(docs)\n</pre> pretty_print(docs) <pre>In addition, the number of listings on Airbnb may decline as a result of a number of other factors affecting Hosts, including: the COVID-19 pandemic; enforcement or threatenedenforcement of laws and regulations, including short-term occupancy and tax laws; private groups, such as homeowners, landlords, and condominium and neighborhood\nassociations, adopting and enforcing contracts that prohibit or restrict home sharing; leases, mortgages, and other agreements, or regulations that purport to ban or otherwise restrict\nhome sharing; Hosts opting for long-term rentals on other third-party platforms as an alternative to listing on our platform; economic, social, and political factors; perceptions of trust\nand safety on and off our platform; negative experiences with guests, including guests who damage Host property, throw unauthorized parties, or engage in violent and unlawful\n\n\nMade Possible by Hosts, Strangers, AirCover, Categories, and OMG marketing campaigns and launches, a $67.9 million increase in our search engine marketing and advertising\nspend, a $25.1 million increase in payroll-related expenses due to growth in headcount and increase in compensation costs, a $22.0 million increase in third-party service provider\nexpenses, and a $11.1 million increase in coupon expense in line with increase in revenue and launch of AirCover for guests, partially offset by a decrease of $22.9 million related to\nthe changes in the fair value of contingent consideration related to a 2019 acquisition.\nGeneral and Administrative\n2021 2022 % Change\n(in millions, except percentages)\nGeneral and administrative $ 836 $ 950 14 %\nPercentage of revenue 14 % 11 %\nGeneral and administrative expense increased $114.0 million, or 14%, in 2022 compared to 2021, primarily due to an increase in other business and operational taxes of $41.3\n\n\nOur success depends significantly on existing guests continuing to book and attracting new guests to book on our platform. Our ability to attract and retain guests could be materially\nadversely affected by a number of factors discussed elsewhere in these \u201cRisk Factors,\u201d including:\n\u2022 events beyond our control such as the ongoing COVID-19 pandemic, other pandemics and health concerns, restrictions on travel, immigration, trade disputes, economic\ndownturns, and the impact of climate change on travel including the availability of preferred destinations and the increase in the frequency and severity of weather-relatedevents, including fires, floods, droughts, extreme temperatures and ambient temperature increases, severe weather and other natural disasters, and the impact of other\nclimate change on seasonal destinations;\n\u2022 political, social, or economic instability;\n\n\n\u2022 Hosts failing to meet guests\u2019 expectations, including increased expectations for cleanliness in light of the COVID-19 pandemic;\u2022 increased competition and use of our competitors\u2019 platforms and services;\n\u2022 Hosts failing to provide differentiated, high-quality, and an adequate supply of stays or experiences at competitive prices;\n\u2022 guests not receiving timely and adequate community support from us;\n\u2022 our failure to provide new or enhanced offerings, tiers, or features that guests value;\n\u2022 declines or inefficiencies in our marketing efforts;\u2022 negative associations with, or reduced awareness of, our brand;\n\u2022 actual or perceived discrimination by Hosts in deciding whether to accept a requested reservation;\n\u2022 negative perceptions of the trust and safety on our platform; and\n\u2022 macroeconomic and other conditions outside of our control affecting travel and hospitality industries generally.\n\n\nTable of Contents\nAirbnb, Inc.\nConsolidated Statements of Operations\n(in millions, except per share amounts)\nYear Ended December 31,\n2020 2021 2022\nRevenue $ 3,378 $ 5,992 $ 8,399 \nCosts and expenses:\nCost of revenue 876 1,156 1,499 \nOperations and support 878 847 1,041 \nProduct development 2,753 1,425 1,502 \nSales and marketing 1,175 1,186 1,516 \nGeneral and administrative 1,135 836 950 \nRestructuring charges 151 113 89 \nTotal costs and expenses 6,968 5,563 6,597 \nIncome (loss) from operations (3,590) 429 1,802 \nInterest income 27 13 186 \nInterest expense (172) (438) (24)\nOther income (expense), net (947) (304) 25 \nIncome (loss) before income taxes (4,682) (300) 1,989 \nProvision for (benefit from) income taxes (97) 52 96 \nNet income (loss) $ (4,585)$ (352)$ 1,893 \nNet income (loss) per share attributable to Class A and Class B common stockholders:\nBasic $ (16.12)$ (0.57)$ 2.97 \nDiluted $ (16.12)$ (0.57)$ 2.79\n\n\n</pre> In\u00a0[32]: Copied! <pre>docs = table.search(query_type=\"hybrid\").vector(query).text(str_query).limit(5).to_pandas()[\"text\"].to_list()\n</pre> docs = table.search(query_type=\"hybrid\").vector(query).text(str_query).limit(5).to_pandas()[\"text\"].to_list() In\u00a0[33]: Copied! <pre>pretty_print(docs)\n</pre> pretty_print(docs) <pre>In addition, the number of listings on Airbnb may decline as a result of a number of other factors affecting Hosts, including: the COVID-19 pandemic; enforcement or threatenedenforcement of laws and regulations, including short-term occupancy and tax laws; private groups, such as homeowners, landlords, and condominium and neighborhood\nassociations, adopting and enforcing contracts that prohibit or restrict home sharing; leases, mortgages, and other agreements, or regulations that purport to ban or otherwise restrict\nhome sharing; Hosts opting for long-term rentals on other third-party platforms as an alternative to listing on our platform; economic, social, and political factors; perceptions of trust\nand safety on and off our platform; negative experiences with guests, including guests who damage Host property, throw unauthorized parties, or engage in violent and unlawful\n\n\n\u201cInitial Delivery Date\u201d); provided that the Pricing Certificate for any fiscal year may be delivered on any date following the Initial DeliveryDate that is prior to the date that is 365 days following the last day of the preceding fiscal year, so long as such Pricing Certificate includes acertification that delivery of such Pricing Certificate on or before the Initial Delivery Date was not possible because (i) the informationrequired to calculate the KPI Metrics for such preceding fiscal year was not available at such time or (ii) the report of the KPI Metrics Auditor,if relevant, was not available at such time (the date of the Administrative Agent\u2019s receipt thereof, each a \u201cPricing Certificate Date\u201d). Upondelivery of a Pricing Certificate in respect of a fiscal year, (i) the Applicable Rate for the Loans incurred by the Borrower shall be increased ordecreased (or neither increased nor decreased), as applicable, pursuant to the Sustainability Margin Adjustment as set forth in the KPI MetricsCertificate\n\n\nMade Possible by Hosts, Strangers, AirCover, Categories, and OMG marketing campaigns and launches, a $67.9 million increase in our search engine marketing and advertising\nspend, a $25.1 million increase in payroll-related expenses due to growth in headcount and increase in compensation costs, a $22.0 million increase in third-party service provider\nexpenses, and a $11.1 million increase in coupon expense in line with increase in revenue and launch of AirCover for guests, partially offset by a decrease of $22.9 million related to\nthe changes in the fair value of contingent consideration related to a 2019 acquisition.\nGeneral and Administrative\n2021 2022 % Change\n(in millions, except percentages)\nGeneral and administrative $ 836 $ 950 14 %\nPercentage of revenue 14 % 11 %\nGeneral and administrative expense increased $114.0 million, or 14%, in 2022 compared to 2021, primarily due to an increase in other business and operational taxes of $41.3\n\n\n(c) If, for any fiscal year, either (i) no Pricing Certificate shall have been delivered for such fiscal year or (ii) the PricingCertificate delivered for such fiscal year shall fail to include the Diverse Supplier Spend Percentage or GHG Emissions Intensity for suchfiscal year, then the Sustainability Margin Adjustment will be positive 0.050% and/or the Sustainability Fee Adjustment will be positive0.010%, as applicable, in each case commencing on the last day such Pricing Certificate could have been delivered in accordance with theterms of clause (a) above (it being understood that, in the case of the foregoing clause (ii), the Sustainability Margin Adjustment or theSustainability Fee Adjustment will be determined in accordance with such Pricing Certificate to the extent the (A) Sustainability MarginAdjustment or the Sustainability Fee Adjustment is included in such Pricing Certificate and (B) the Administrative Agent has separatelyreceived the Diverse Supplier Spend Percentage and/or GHG Emissions\n\n\nOur success depends significantly on existing guests continuing to book and attracting new guests to book on our platform. Our ability to attract and retain guests could be materially\nadversely affected by a number of factors discussed elsewhere in these \u201cRisk Factors,\u201d including:\n\u2022 events beyond our control such as the ongoing COVID-19 pandemic, other pandemics and health concerns, restrictions on travel, immigration, trade disputes, economic\ndownturns, and the impact of climate change on travel including the availability of preferred destinations and the increase in the frequency and severity of weather-relatedevents, including fires, floods, droughts, extreme temperatures and ambient temperature increases, severe weather and other natural disasters, and the impact of other\nclimate change on seasonal destinations;\n\u2022 political, social, or economic instability;\n\n\n</pre> In\u00a0[34]: Copied! <pre># Free API key\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n</pre> # Free API key os.environ[\"COHERE_API_KEY\"] = getpass.getpass() <pre>\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n</pre> In\u00a0[35]: Copied! <pre>from lancedb.rerankers import CohereReranker\n\nreranker = CohereReranker()\ndocs = table.search(query_type=\"hybrid\").vector(query).text(str_query).limit(5).rerank(reranker).to_pandas()[\"text\"].to_list()\n</pre> from lancedb.rerankers import CohereReranker  reranker = CohereReranker() docs = table.search(query_type=\"hybrid\").vector(query).text(str_query).limit(5).rerank(reranker).to_pandas()[\"text\"].to_list() In\u00a0[36]: Copied! <pre>pretty_print(docs)\n</pre> pretty_print(docs) <pre>Increased operating expenses, decreased revenue, negative publicity, negative reaction from our Hosts and guests and other stakeholders, or other adverse impacts from any of the\nabove factors or other risks related to our international operations could materially adversely affect our brand, reputation, business, results of operations, and financial condition.\nIn addition, we will continue to incur significant expenses to operate our outbound business in China, and we may never achieve profitability in that market. These factors, combined\nwith sentiment of the workforce in China, and China\u2019s policy towards foreign direct investment may particularly impact our operations in China. In addition, we need to ensure that\nour business practices in China are compliant with local laws and regulations, which may be interpreted and enforced in ways that are different from our interpretation, and/or create\n\n\nMade Possible by Hosts, Strangers, AirCover, Categories, and OMG marketing campaigns and launches, a $67.9 million increase in our search engine marketing and advertising\nspend, a $25.1 million increase in payroll-related expenses due to growth in headcount and increase in compensation costs, a $22.0 million increase in third-party service provider\nexpenses, and a $11.1 million increase in coupon expense in line with increase in revenue and launch of AirCover for guests, partially offset by a decrease of $22.9 million related to\nthe changes in the fair value of contingent consideration related to a 2019 acquisition.\nGeneral and Administrative\n2021 2022 % Change\n(in millions, except percentages)\nGeneral and administrative $ 836 $ 950 14 %\nPercentage of revenue 14 % 11 %\nGeneral and administrative expense increased $114.0 million, or 14%, in 2022 compared to 2021, primarily due to an increase in other business and operational taxes of $41.3\n\n\n\u2022 Hosts failing to meet guests\u2019 expectations, including increased expectations for cleanliness in light of the COVID-19 pandemic;\u2022 increased competition and use of our competitors\u2019 platforms and services;\n\u2022 Hosts failing to provide differentiated, high-quality, and an adequate supply of stays or experiences at competitive prices;\n\u2022 guests not receiving timely and adequate community support from us;\n\u2022 our failure to provide new or enhanced offerings, tiers, or features that guests value;\n\u2022 declines or inefficiencies in our marketing efforts;\u2022 negative associations with, or reduced awareness of, our brand;\n\u2022 actual or perceived discrimination by Hosts in deciding whether to accept a requested reservation;\n\u2022 negative perceptions of the trust and safety on our platform; and\n\u2022 macroeconomic and other conditions outside of our control affecting travel and hospitality industries generally.\n\n\nTable of Contents\nAirbnb, Inc.\nConsolidated Statements of Operations\n(in millions, except per share amounts)\nYear Ended December 31,\n2020 2021 2022\nRevenue $ 3,378 $ 5,992 $ 8,399 \nCosts and expenses:\nCost of revenue 876 1,156 1,499 \nOperations and support 878 847 1,041 \nProduct development 2,753 1,425 1,502 \nSales and marketing 1,175 1,186 1,516 \nGeneral and administrative 1,135 836 950 \nRestructuring charges 151 113 89 \nTotal costs and expenses 6,968 5,563 6,597 \nIncome (loss) from operations (3,590) 429 1,802 \nInterest income 27 13 186 \nInterest expense (172) (438) (24)\nOther income (expense), net (947) (304) 25 \nIncome (loss) before income taxes (4,682) (300) 1,989 \nProvision for (benefit from) income taxes (97) 52 96 \nNet income (loss) $ (4,585)$ (352)$ 1,893 \nNet income (loss) per share attributable to Class A and Class B common stockholders:\nBasic $ (16.12)$ (0.57)$ 2.97 \nDiluted $ (16.12)$ (0.57)$ 2.79\n\n\nOur success depends significantly on existing guests continuing to book and attracting new guests to book on our platform. Our ability to attract and retain guests could be materially\nadversely affected by a number of factors discussed elsewhere in these \u201cRisk Factors,\u201d including:\n\u2022 events beyond our control such as the ongoing COVID-19 pandemic, other pandemics and health concerns, restrictions on travel, immigration, trade disputes, economic\ndownturns, and the impact of climate change on travel including the availability of preferred destinations and the increase in the frequency and severity of weather-relatedevents, including fires, floods, droughts, extreme temperatures and ambient temperature increases, severe weather and other natural disasters, and the impact of other\nclimate change on seasonal destinations;\n\u2022 political, social, or economic instability;\n\n\n</pre> <p>Relevance score is returned by Cohere API and is independent of individual FTS and vector search scores.</p> In\u00a0[38]: Copied! <pre>table.search(query_type=\"hybrid\").vector(query).text(str_query).limit(5).rerank(reranker).to_pandas()\n</pre> table.search(query_type=\"hybrid\").vector(query).text(str_query).limit(5).rerank(reranker).to_pandas() Out[38]: vector id text metadata _relevance_score 0 [0.0034929817, -0.024774546, 0.012623285, -0.0... 18d4a926-99d9-447f-8b57-264d7a148bd7 Increased operating expenses, decreased revenu... {'page': 18, 'source': 'https://d18rn0p25nwr6d... 0.985328 1 [-0.0042489874, -0.005382498, 0.007190078, -0.... a91b3506-39a2-4b19-8409-08333d83a1c6 Made Possible by Hosts, Strangers, AirCover, C... {'page': 62, 'source': 'https://d18rn0p25nwr6d... 0.979036 2 [0.0076079983, -0.013340506, 0.018701892, -0.0... fcc532b9-347b-4e36-8ae8-5a2a726bf574 \u2022 Hosts failing to meet guests\u2019 expectations, ... {'page': 11, 'source': 'https://d18rn0p25nwr6d... 0.961606 3 [-0.008694107, -0.01993283, 0.014201017, -0.02... 72b844e2-cc93-4495-bb67-c2c1a1fd6532 Table of Contents\\nAirbnb, Inc.\\nConsolidated ... {'page': 72, 'source': 'https://d18rn0p25nwr6d... 0.696578 4 [0.005813433, -0.028278675, 0.018041687, -0.02... 1694d5a5-7ece-40b8-8022-dc3fa9aaa05a Our success depends significantly on existing ... {'page': 11, 'source': 'https://d18rn0p25nwr6d... 0.500779 In\u00a0[48]: Copied! <pre>!pip install rerankers\n</pre> !pip install rerankers <pre>Collecting rerankers\n  Downloading rerankers-0.6.0-py3-none-any.whl.metadata (28 kB)\nRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from rerankers) (2.9.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from rerankers) (4.66.6)\nRequirement already satisfied: annotated-types&gt;=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-&gt;rerankers) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic-&gt;rerankers) (2.23.4)\nRequirement already satisfied: typing-extensions&gt;=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic-&gt;rerankers) (4.12.2)\nDownloading rerankers-0.6.0-py3-none-any.whl (41 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0.0/41.1 kB ? eta -:--:--\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 41.1/41.1 kB 2.7 MB/s eta 0:00:00\nInstalling collected packages: rerankers\nSuccessfully installed rerankers-0.6.0\n</pre> In\u00a0[49]: Copied! <pre>from lancedb.rerankers import ColbertReranker\n\nreranker = ColbertReranker()\ndocs = table.search(query_type=\"hybrid\").vector(query).text(str_query).limit(5).rerank(reranker).to_pandas()[\"text\"].to_list()\n</pre> from lancedb.rerankers import ColbertReranker  reranker = ColbertReranker() docs = table.search(query_type=\"hybrid\").vector(query).text(str_query).limit(5).rerank(reranker).to_pandas()[\"text\"].to_list() <pre>Loading ColBERTRanker model colbert-ir/colbertv2.0 (this message can be suppressed by setting verbose=0)\nNo device set\nUsing device cpu\nNo dtype set\nUsing dtype torch.float32\nLoading model colbert-ir/colbertv2.0, this might take a while...\n</pre> <pre>tokenizer_config.json:   0%|          | 0.00/405 [00:00&lt;?, ?B/s]</pre> <pre>vocab.txt:   0%|          | 0.00/232k [00:00&lt;?, ?B/s]</pre> <pre>tokenizer.json:   0%|          | 0.00/466k [00:00&lt;?, ?B/s]</pre> <pre>special_tokens_map.json:   0%|          | 0.00/112 [00:00&lt;?, ?B/s]</pre> <pre>config.json:   0%|          | 0.00/743 [00:00&lt;?, ?B/s]</pre> <pre>model.safetensors:   0%|          | 0.00/438M [00:00&lt;?, ?B/s]</pre> <pre>Linear Dim set to: 128 for downcasting\n</pre> In\u00a0[50]: Copied! <pre>pretty_print(docs)\n</pre> pretty_print(docs) <pre>Table of Contents\nAirbnb, Inc.\nConsolidated Statements of Operations\n(in millions, except per share amounts)\nYear Ended December 31,\n2020 2021 2022\nRevenue $ 3,378 $ 5,992 $ 8,399 \nCosts and expenses:\nCost of revenue 876 1,156 1,499 \nOperations and support 878 847 1,041 \nProduct development 2,753 1,425 1,502 \nSales and marketing 1,175 1,186 1,516 \nGeneral and administrative 1,135 836 950 \nRestructuring charges 151 113 89 \nTotal costs and expenses 6,968 5,563 6,597 \nIncome (loss) from operations (3,590) 429 1,802 \nInterest income 27 13 186 \nInterest expense (172) (438) (24)\nOther income (expense), net (947) (304) 25 \nIncome (loss) before income taxes (4,682) (300) 1,989 \nProvision for (benefit from) income taxes (97) 52 96 \nNet income (loss) $ (4,585)$ (352)$ 1,893 \nNet income (loss) per share attributable to Class A and Class B common stockholders:\nBasic $ (16.12)$ (0.57)$ 2.97 \nDiluted $ (16.12)$ (0.57)$ 2.79\n\n\nIn addition, the number of listings on Airbnb may decline as a result of a number of other factors affecting Hosts, including: the COVID-19 pandemic; enforcement or threatenedenforcement of laws and regulations, including short-term occupancy and tax laws; private groups, such as homeowners, landlords, and condominium and neighborhood\nassociations, adopting and enforcing contracts that prohibit or restrict home sharing; leases, mortgages, and other agreements, or regulations that purport to ban or otherwise restrict\nhome sharing; Hosts opting for long-term rentals on other third-party platforms as an alternative to listing on our platform; economic, social, and political factors; perceptions of trust\nand safety on and off our platform; negative experiences with guests, including guests who damage Host property, throw unauthorized parties, or engage in violent and unlawful\n\n\nIncreased operating expenses, decreased revenue, negative publicity, negative reaction from our Hosts and guests and other stakeholders, or other adverse impacts from any of the\nabove factors or other risks related to our international operations could materially adversely affect our brand, reputation, business, results of operations, and financial condition.\nIn addition, we will continue to incur significant expenses to operate our outbound business in China, and we may never achieve profitability in that market. These factors, combined\nwith sentiment of the workforce in China, and China\u2019s policy towards foreign direct investment may particularly impact our operations in China. In addition, we need to ensure that\nour business practices in China are compliant with local laws and regulations, which may be interpreted and enforced in ways that are different from our interpretation, and/or create\n\n\nMade Possible by Hosts, Strangers, AirCover, Categories, and OMG marketing campaigns and launches, a $67.9 million increase in our search engine marketing and advertising\nspend, a $25.1 million increase in payroll-related expenses due to growth in headcount and increase in compensation costs, a $22.0 million increase in third-party service provider\nexpenses, and a $11.1 million increase in coupon expense in line with increase in revenue and launch of AirCover for guests, partially offset by a decrease of $22.9 million related to\nthe changes in the fair value of contingent consideration related to a 2019 acquisition.\nGeneral and Administrative\n2021 2022 % Change\n(in millions, except percentages)\nGeneral and administrative $ 836 $ 950 14 %\nPercentage of revenue 14 % 11 %\nGeneral and administrative expense increased $114.0 million, or 14%, in 2022 compared to 2021, primarily due to an increase in other business and operational taxes of $41.3\n\n\nOur success depends significantly on existing guests continuing to book and attracting new guests to book on our platform. Our ability to attract and retain guests could be materially\nadversely affected by a number of factors discussed elsewhere in these \u201cRisk Factors,\u201d including:\n\u2022 events beyond our control such as the ongoing COVID-19 pandemic, other pandemics and health concerns, restrictions on travel, immigration, trade disputes, economic\ndownturns, and the impact of climate change on travel including the availability of preferred destinations and the increase in the frequency and severity of weather-relatedevents, including fires, floods, droughts, extreme temperatures and ambient temperature increases, severe weather and other natural disasters, and the impact of other\nclimate change on seasonal destinations;\n\u2022 political, social, or economic instability;\n\n\n</pre> In\u00a0[41]: Copied! <pre>from lancedb.rerankers import CrossEncoderReranker\n\nreranker=CrossEncoderReranker()\ndocs = table.search(query_type=\"hybrid\").vector(query).text(str_query).limit(5).rerank(reranker).to_pandas()[\"text\"].to_list()\n</pre> from lancedb.rerankers import CrossEncoderReranker  reranker=CrossEncoderReranker() docs = table.search(query_type=\"hybrid\").vector(query).text(str_query).limit(5).rerank(reranker).to_pandas()[\"text\"].to_list() <pre>/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \nThe secret `HF_TOKEN` does not exist in your Colab secrets.\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\nYou will be able to reuse this secret in all of your notebooks.\nPlease note that authentication is recommended but still optional to access public models or datasets.\n  warnings.warn(\n</pre> <pre>config.json:   0%|          | 0.00/612 [00:00&lt;?, ?B/s]</pre> <pre>pytorch_model.bin:   0%|          | 0.00/268M [00:00&lt;?, ?B/s]</pre> <pre>tokenizer_config.json:   0%|          | 0.00/541 [00:00&lt;?, ?B/s]</pre> <pre>vocab.txt:   0%|          | 0.00/232k [00:00&lt;?, ?B/s]</pre> <pre>special_tokens_map.json:   0%|          | 0.00/112 [00:00&lt;?, ?B/s]</pre> <pre>model.safetensors:   0%|          | 0.00/268M [00:00&lt;?, ?B/s]</pre> In\u00a0[42]: Copied! <pre>pretty_print(docs)\n</pre> pretty_print(docs) <pre>Table of Contents\nAirbnb, Inc.\nConsolidated Statements of Operations\n(in millions, except per share amounts)\nYear Ended December 31,\n2020 2021 2022\nRevenue $ 3,378 $ 5,992 $ 8,399 \nCosts and expenses:\nCost of revenue 876 1,156 1,499 \nOperations and support 878 847 1,041 \nProduct development 2,753 1,425 1,502 \nSales and marketing 1,175 1,186 1,516 \nGeneral and administrative 1,135 836 950 \nRestructuring charges 151 113 89 \nTotal costs and expenses 6,968 5,563 6,597 \nIncome (loss) from operations (3,590) 429 1,802 \nInterest income 27 13 186 \nInterest expense (172) (438) (24)\nOther income (expense), net (947) (304) 25 \nIncome (loss) before income taxes (4,682) (300) 1,989 \nProvision for (benefit from) income taxes (97) 52 96 \nNet income (loss) $ (4,585)$ (352)$ 1,893 \nNet income (loss) per share attributable to Class A and Class B common stockholders:\nBasic $ (16.12)$ (0.57)$ 2.97 \nDiluted $ (16.12)$ (0.57)$ 2.79\n\n\nMade Possible by Hosts, Strangers, AirCover, Categories, and OMG marketing campaigns and launches, a $67.9 million increase in our search engine marketing and advertising\nspend, a $25.1 million increase in payroll-related expenses due to growth in headcount and increase in compensation costs, a $22.0 million increase in third-party service provider\nexpenses, and a $11.1 million increase in coupon expense in line with increase in revenue and launch of AirCover for guests, partially offset by a decrease of $22.9 million related to\nthe changes in the fair value of contingent consideration related to a 2019 acquisition.\nGeneral and Administrative\n2021 2022 % Change\n(in millions, except percentages)\nGeneral and administrative $ 836 $ 950 14 %\nPercentage of revenue 14 % 11 %\nGeneral and administrative expense increased $114.0 million, or 14%, in 2022 compared to 2021, primarily due to an increase in other business and operational taxes of $41.3\n\n\nIncreased operating expenses, decreased revenue, negative publicity, negative reaction from our Hosts and guests and other stakeholders, or other adverse impacts from any of the\nabove factors or other risks related to our international operations could materially adversely affect our brand, reputation, business, results of operations, and financial condition.\nIn addition, we will continue to incur significant expenses to operate our outbound business in China, and we may never achieve profitability in that market. These factors, combined\nwith sentiment of the workforce in China, and China\u2019s policy towards foreign direct investment may particularly impact our operations in China. In addition, we need to ensure that\nour business practices in China are compliant with local laws and regulations, which may be interpreted and enforced in ways that are different from our interpretation, and/or create\n\n\nIn addition, the number of listings on Airbnb may decline as a result of a number of other factors affecting Hosts, including: the COVID-19 pandemic; enforcement or threatenedenforcement of laws and regulations, including short-term occupancy and tax laws; private groups, such as homeowners, landlords, and condominium and neighborhood\nassociations, adopting and enforcing contracts that prohibit or restrict home sharing; leases, mortgages, and other agreements, or regulations that purport to ban or otherwise restrict\nhome sharing; Hosts opting for long-term rentals on other third-party platforms as an alternative to listing on our platform; economic, social, and political factors; perceptions of trust\nand safety on and off our platform; negative experiences with guests, including guests who damage Host property, throw unauthorized parties, or engage in violent and unlawful\n\n\nOur success depends significantly on existing guests continuing to book and attracting new guests to book on our platform. Our ability to attract and retain guests could be materially\nadversely affected by a number of factors discussed elsewhere in these \u201cRisk Factors,\u201d including:\n\u2022 events beyond our control such as the ongoing COVID-19 pandemic, other pandemics and health concerns, restrictions on travel, immigration, trade disputes, economic\ndownturns, and the impact of climate change on travel including the availability of preferred destinations and the increase in the frequency and severity of weather-relatedevents, including fires, floods, droughts, extreme temperatures and ambient temperature increases, severe weather and other natural disasters, and the impact of other\nclimate change on seasonal destinations;\n\u2022 political, social, or economic instability;\n\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>from lancedb.rerankers import OpenaiReranker\n\nreranker=OpenaiReranker(model_name=\"gpt-4-turbo-preview\")\ndocs = table.search(query_type=\"hybrid\").vector(query).text(str_query).limit(5).rerank(reranker).to_pandas()[\"text\"].to_list()\n</pre> from lancedb.rerankers import OpenaiReranker  reranker=OpenaiReranker(model_name=\"gpt-4-turbo-preview\") docs = table.search(query_type=\"hybrid\").vector(query).text(str_query).limit(5).rerank(reranker).to_pandas()[\"text\"].to_list() In\u00a0[\u00a0]: Copied! <pre>pretty_print(docs)\n</pre> pretty_print(docs) In\u00a0[44]: Copied! <pre>from lancedb.rerankers import Reranker\nimport pyarrow as pa\n\nclass MyCustomReranker(Reranker):\n    def rerank_hybrid(self, query: str, vector_results: pa.Table, fts_results: pa.Table)-&gt; pa.Table:\n        combined_results = self.merge(vector_results, fts_results) # Or custom merge algo\n        # Custom Reranking logic here\n\n        return combined_results\n</pre> from lancedb.rerankers import Reranker import pyarrow as pa  class MyCustomReranker(Reranker):     def rerank_hybrid(self, query: str, vector_results: pa.Table, fts_results: pa.Table)-&gt; pa.Table:         combined_results = self.merge(vector_results, fts_results) # Or custom merge algo         # Custom Reranking logic here          return combined_results In\u00a0[45]: Copied! <pre>from typing import List, Union\nimport pandas as pd\nfrom lancedb.rerankers import CohereReranker\n\nclass MofidifiedCohereReranker(CohereReranker):\n    def __init__(self, filters: Union[str, List[str]], **kwargs):\n        super().__init__(**kwargs)\n        filters = filters if isinstance(filters, list) else [filters]\n        self.filters = filters\n\n    def rerank_hybrid(self, query: str, vector_results: pa.Table, fts_results: pa.Table)-&gt; pa.Table:\n        combined_result = super().rerank_hybrid(query, vector_results, fts_results)\n        df = combined_result.to_pandas()\n        for filter in self.filters:\n            df = df.query(\"not text.str.contains(@filter)\")\n\n        return pa.Table.from_pandas(df)\n\nreranker = MofidifiedCohereReranker(filters=\"Table of Contents\")\n</pre> from typing import List, Union import pandas as pd from lancedb.rerankers import CohereReranker  class MofidifiedCohereReranker(CohereReranker):     def __init__(self, filters: Union[str, List[str]], **kwargs):         super().__init__(**kwargs)         filters = filters if isinstance(filters, list) else [filters]         self.filters = filters      def rerank_hybrid(self, query: str, vector_results: pa.Table, fts_results: pa.Table)-&gt; pa.Table:         combined_result = super().rerank_hybrid(query, vector_results, fts_results)         df = combined_result.to_pandas()         for filter in self.filters:             df = df.query(\"not text.str.contains(@filter)\")          return pa.Table.from_pandas(df)  reranker = MofidifiedCohereReranker(filters=\"Table of Contents\") In\u00a0[46]: Copied! <pre>docs = table.search(query_type=\"hybrid\").vector(query).text(str_query).limit(5).rerank(reranker).to_pandas()[\"text\"].to_list()\n</pre> docs = table.search(query_type=\"hybrid\").vector(query).text(str_query).limit(5).rerank(reranker).to_pandas()[\"text\"].to_list() In\u00a0[47]: Copied! <pre>pretty_print(docs)\n</pre> pretty_print(docs) <pre>Increased operating expenses, decreased revenue, negative publicity, negative reaction from our Hosts and guests and other stakeholders, or other adverse impacts from any of the\nabove factors or other risks related to our international operations could materially adversely affect our brand, reputation, business, results of operations, and financial condition.\nIn addition, we will continue to incur significant expenses to operate our outbound business in China, and we may never achieve profitability in that market. These factors, combined\nwith sentiment of the workforce in China, and China\u2019s policy towards foreign direct investment may particularly impact our operations in China. In addition, we need to ensure that\nour business practices in China are compliant with local laws and regulations, which may be interpreted and enforced in ways that are different from our interpretation, and/or create\n\n\nMade Possible by Hosts, Strangers, AirCover, Categories, and OMG marketing campaigns and launches, a $67.9 million increase in our search engine marketing and advertising\nspend, a $25.1 million increase in payroll-related expenses due to growth in headcount and increase in compensation costs, a $22.0 million increase in third-party service provider\nexpenses, and a $11.1 million increase in coupon expense in line with increase in revenue and launch of AirCover for guests, partially offset by a decrease of $22.9 million related to\nthe changes in the fair value of contingent consideration related to a 2019 acquisition.\nGeneral and Administrative\n2021 2022 % Change\n(in millions, except percentages)\nGeneral and administrative $ 836 $ 950 14 %\nPercentage of revenue 14 % 11 %\nGeneral and administrative expense increased $114.0 million, or 14%, in 2022 compared to 2021, primarily due to an increase in other business and operational taxes of $41.3\n\n\n\u2022 Hosts failing to meet guests\u2019 expectations, including increased expectations for cleanliness in light of the COVID-19 pandemic;\u2022 increased competition and use of our competitors\u2019 platforms and services;\n\u2022 Hosts failing to provide differentiated, high-quality, and an adequate supply of stays or experiences at competitive prices;\n\u2022 guests not receiving timely and adequate community support from us;\n\u2022 our failure to provide new or enhanced offerings, tiers, or features that guests value;\n\u2022 declines or inefficiencies in our marketing efforts;\u2022 negative associations with, or reduced awareness of, our brand;\n\u2022 actual or perceived discrimination by Hosts in deciding whether to accept a requested reservation;\n\u2022 negative perceptions of the trust and safety on our platform; and\n\u2022 macroeconomic and other conditions outside of our control affecting travel and hospitality industries generally.\n\n\nOur success depends significantly on existing guests continuing to book and attracting new guests to book on our platform. Our ability to attract and retain guests could be materially\nadversely affected by a number of factors discussed elsewhere in these \u201cRisk Factors,\u201d including:\n\u2022 events beyond our control such as the ongoing COVID-19 pandemic, other pandemics and health concerns, restrictions on travel, immigration, trade disputes, economic\ndownturns, and the impact of climate change on travel including the availability of preferred destinations and the increase in the frequency and severity of weather-relatedevents, including fires, floods, droughts, extreme temperatures and ambient temperature increases, severe weather and other natural disasters, and the impact of other\nclimate change on seasonal destinations;\n\u2022 political, social, or economic instability;\n\n\nIn addition, the number of listings on Airbnb may decline as a result of a number of other factors affecting Hosts, including: the COVID-19 pandemic; enforcement or threatenedenforcement of laws and regulations, including short-term occupancy and tax laws; private groups, such as homeowners, landlords, and condominium and neighborhood\nassociations, adopting and enforcing contracts that prohibit or restrict home sharing; leases, mortgages, and other agreements, or regulations that purport to ban or otherwise restrict\nhome sharing; Hosts opting for long-term rentals on other third-party platforms as an alternative to listing on our platform; economic, social, and political factors; perceptions of trust\nand safety on and off our platform; negative experiences with guests, including guests who damage Host property, throw unauthorized parties, or engage in violent and unlawful\n\n\n</pre> <p>As you can see, the document containing the table of contents no longer shows up.</p>"},{"location":"notebooks/hybrid_search/#example-airbnb-financial-data-search","title":"Example - Airbnb financial data search\u00b6","text":"<p>The code below is an example of hybrid search, a search algorithm that combines FTS and vector search in LanceDB.</p> <p>Let's get stared with an example. In this notebook we'll use Airbnb financial data documents to search for \"the specific reasons for higher operating costs\" in a particular year.</p>"},{"location":"notebooks/hybrid_search/#vector-search","title":"Vector Search\u00b6","text":"<p>Average latency: <code>3.48 ms \u00b1 71.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)</code></p>"},{"location":"notebooks/hybrid_search/#hybrid-search","title":"Hybrid Search\u00b6","text":"<p>LanceDB support hybrid search with custom Rerankers. Here's the summary of latency numbers of some of the Reranking methods available </p> <p>Let us now perform hybrid search by combining vector and FTS search results. First, we'll cover the default Reranker.</p>"},{"location":"notebooks/hybrid_search/#linear-combination-reranker","title":"Linear Combination Reranker\u00b6","text":"<p><code>LinearCombinationReranker(weight=0.7)</code> is used as the default reranker for reranking the hybrid search results if the reranker isn't specified explicitly. The <code>weight</code> param controls the weightage provided to vector search score. The weight of <code>1-weight</code> is applied to FTS scores when reranking.</p> <p>Latency: <code>71 ms \u00b1 25.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)</code></p>"},{"location":"notebooks/hybrid_search/#cohere-reranker","title":"Cohere Reranker\u00b6","text":"<p>This uses Cohere's Reranking API to re-rank  the results. It accepts the reranking model name as a parameter. By default it uses the english-v3 model but you can easily switch to a multi-lingual model.</p> <p>Latency: <code>605 ms \u00b1 78.1 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)</code></p>"},{"location":"notebooks/hybrid_search/#colbert-reranker","title":"ColBERT Reranker\u00b6","text":"<p>Colbert Reranker is powered by ColBERT model. It runs locally using the huggingface implementation.</p> <p>Latency - <code>950 ms \u00b1 5.78 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)</code></p> <p>Note: First query might be slow. It is recommended to reuse the <code>Reranker</code> objects as the models are cached. Subsequent runs will be faster on reusing the same reranker object</p>"},{"location":"notebooks/hybrid_search/#cross-encoder-reranker","title":"Cross Encoder Reranker\u00b6","text":"<p>Uses cross encoder models are rerankers. Uses sentence transformer implementation locally</p> <p>Latency: <code>1.38 s \u00b1 64.6 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)</code></p>"},{"location":"notebooks/hybrid_search/#experimental-openai-reranker","title":"(Experimental) OpenAI Reranker\u00b6","text":"<p>This prompts a chat model to rerank results and is not a dedicated reranker model. This should be treated as experimental. You might exceed the token limit so set the search limits based on your token limit. NOTE: It is recommended to use <code>gpt-4-turbo-preview</code> as older models might lead to bad behaviour</p> <p>Latency: <code>Can take 10s of seconds if using GPT-4 model</code></p>"},{"location":"notebooks/hybrid_search/#use-your-custom-reranker","title":"Use your custom Reranker\u00b6","text":"<p>Hybrid search in LanceDB is designed to be very flexible. You can easily plug in your own Re-reranking logic. To do so, you simply need to implement the base Reranker class:</p>"},{"location":"notebooks/hybrid_search/#custom-reranker-based-on-coherereranker","title":"Custom Reranker based on CohereReranker\u00b6","text":"<p>For the sake of simplicity let's build a custom reranker that enhances the Cohere Reranker by accepting a filter query, and accepts other CohereReranker params as kwargs.</p> <p>For this toy example let's say we want to get rid of docs that represent a table of contents or appendix, as these are semantically close to representing costs but don't represent the specific reasons why operating costs were high.</p>"},{"location":"notebooks/lancedb-python-1/","title":"Lancedb python 1","text":"In\u00a0[\u00a0]: Copied!"},{"location":"notebooks/lancedb_reranking/","title":"Example - Improve Retrievers using Rerankers &amp; Hybrid search","text":"In\u00a0[16]: Copied! <pre>!pip install lancedb sentence-transformers cohere tantivy pyarrow==13.0.0 -q\n</pre> !pip install lancedb sentence-transformers cohere tantivy pyarrow==13.0.0 -q <pre>     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 177.4/177.4 kB 4.7 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 139.2/139.2 kB 6.2 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.1/3.1 MB 16.4 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 75.6/75.6 kB 10.2 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12.4/12.4 MB 51.0 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 82.7/82.7 kB 12.2 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 77.9/77.9 kB 11.8 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 58.3/58.3 kB 7.6 MB/s eta 0:00:00\n</pre> In\u00a0[2]: Copied! <pre>!wget https://raw.githubusercontent.com/AyushExel/assets/main/data_qa.csv\n</pre> !wget https://raw.githubusercontent.com/AyushExel/assets/main/data_qa.csv <pre>--2024-07-24 14:22:47--  https://raw.githubusercontent.com/AyushExel/assets/main/data_qa.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 680439 (664K) [text/plain]\nSaving to: \u2018data_qa.csv\u2019\n\ndata_qa.csv         100%[===================&gt;] 664.49K  --.-KB/s    in 0.03s   \n\n2024-07-24 14:22:48 (19.9 MB/s) - \u2018data_qa.csv\u2019 saved [680439/680439]\n\n</pre> In\u00a0[3]: Copied! <pre>import pandas as pd\n\ndata = pd.read_csv(\"data_qa.csv\")\n</pre> import pandas as pd  data = pd.read_csv(\"data_qa.csv\") In\u00a0[4]: Copied! <pre>data\n</pre> data Out[4]: Unnamed: 0 query context answer 0 0 How does the performance of Llama 2-Chat model... Llama 2 : Open Foundation and Fine-Tuned Chat ... Llama 2-Chat models have shown to exceed the p... 1 1 What benefits does the enhancement and safety ... Llama 2 : Open Foundation and Fine-Tuned Chat ... The safety and enhancement measures implemente... 2 2 How does one ensure the reliability and robust... Contents\\n1 Introduction 3\\n2 Pretraining 5\\n2... In the initial steps of model development, the... 3 3 What methodologies are employed to align machi... Contents\\n1 Introduction 3\\n2 Pretraining 5\\n2... Machine learning models can be aligned with de... 4 4 What are some of the primary insights gained f... . . . . . . . . 23\\n4.3 Red Teaming . . . . . ... The key insights gained from evaluating platfo... ... ... ... ... ... 215 215 How are the terms 'clean', 'not clean', 'dirty... Giventhe\\nembarrassinglyparallelnatureofthetas... In the discussed dataset analysis, samples are... 216 216 How does the size of the model influence the a... Dataset Model Subset Type Avg. Contam. % n \u00afX ... The size of the model significantly influences... 217 217 What impact does the model contamination have ... Dataset Model Subset Type Avg. Contam. % n \u00afX ... Model contamination affects various contaminat... 218 218 What are the different sizes and types availab... A.7 Model Card\\nTable 52 presents a model card... Llama 2 is available in three distinct paramet... 219 219 Could you discuss the sustainability measures ... A.7 Model Card\\nTable 52 presents a model card... Throughout the training of Llama 2, which invo... <p>220 rows \u00d7 4 columns</p> In\u00a0[5]: Copied! <pre># Define schema using Pydantic. We're using Embedding API to automatically vectorize dataset and queries\nimport torch\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.embeddings import get_registry\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nembed_model = get_registry().get(\"huggingface\").create(name=\"BAAI/bge-small-en-v1.5\", device=device)\n\nclass Schema(LanceModel):\n    text: str = embed_model.SourceField()\n    vector: Vector(embed_model.ndims()) = embed_model.VectorField()\n</pre> # Define schema using Pydantic. We're using Embedding API to automatically vectorize dataset and queries import torch from lancedb.pydantic import LanceModel, Vector from lancedb.embeddings import get_registry  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") embed_model = get_registry().get(\"huggingface\").create(name=\"BAAI/bge-small-en-v1.5\", device=device)  class Schema(LanceModel):     text: str = embed_model.SourceField()     vector: Vector(embed_model.ndims()) = embed_model.VectorField()  <pre>/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \nThe secret `HF_TOKEN` does not exist in your Colab secrets.\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\nYou will be able to reuse this secret in all of your notebooks.\nPlease note that authentication is recommended but still optional to access public models or datasets.\n  warnings.warn(\n</pre> <pre>tokenizer_config.json:   0%|          | 0.00/366 [00:00&lt;?, ?B/s]</pre> <pre>vocab.txt:   0%|          | 0.00/232k [00:00&lt;?, ?B/s]</pre> <pre>tokenizer.json:   0%|          | 0.00/711k [00:00&lt;?, ?B/s]</pre> <pre>special_tokens_map.json:   0%|          | 0.00/125 [00:00&lt;?, ?B/s]</pre> <pre>config.json:   0%|          | 0.00/743 [00:00&lt;?, ?B/s]</pre> <pre>model.safetensors:   0%|          | 0.00/133M [00:00&lt;?, ?B/s]</pre> In\u00a0[6]: Copied! <pre># Create a local lancedb connection\nimport lancedb\n\ndb = lancedb.connect(\"~/lancedb/\")\ntbl = db.create_table(\"qa_data\", schema=Schema, mode=\"overwrite\")\n</pre> # Create a local lancedb connection import lancedb  db = lancedb.connect(\"~/lancedb/\") tbl = db.create_table(\"qa_data\", schema=Schema, mode=\"overwrite\") In\u00a0[7]: Copied! <pre>contexts = [\n    {\"text\": context} for context in data[\"context\"].unique()\n]\nprint(contexts[0:5])\ntbl.add(contexts)\n</pre> contexts = [     {\"text\": context} for context in data[\"context\"].unique() ] print(contexts[0:5]) tbl.add(contexts) <pre>[{'text': 'Llama 2 : Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron\u2217Louis Martin\u2020Kevin Stone\u2020\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\\nPunit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\\nYinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\\nIgor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\\nAlan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\u2217\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-\\nsource models. We provide a detailed description of our approach to fine-tuning and safety\\nimprovements of Llama 2-Chat in order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.\\n\u2217Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\\n\u2020Second author\\nContributions for all the authors can be found in Section A.1.arXiv:2307.09288v2  [cs.CL]  19 Jul 2023'}, {'text': 'Contents\\n1 Introduction 3\\n2 Pretraining 5\\n2.1 Pretraining Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2 Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.3 Llama 2 Pretrained Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n3 Fine-tuning 8\\n3.1 Supervised Fine-Tuning (SFT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n3.2 Reinforcement Learning with Human Feedback (RLHF) . . . . . . . . . . . . . . . . . . . . . 9\\n3.3 System Message for Multi-Turn Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n3.4 RLHF Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n4 Safety 20\\n4.1 Safety in Pretraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n4.2 Safety Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n4.3 Red Teaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n4.4 Safety Evaluation of Llama 2-Chat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'}, {'text': '. . . . . . . . 23\\n4.3 Red Teaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n4.4 Safety Evaluation of Llama 2-Chat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n5 Discussion 32\\n5.1 Learnings and Observations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n5.2 Limitations and Ethical Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\\n5.3 Responsible Release Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n6 Related Work 35\\n7 Conclusion 36\\nA Appendix 46\\nA.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\nA.2 Additional Details for Pretraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\nA.3 Additional Details for Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\\nA.4 Additional Details for Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\\nA.5 Data Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\\nA.6 Dataset Contamination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'}, {'text': '. . . . . . 58\\nA.5 Data Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\\nA.6 Dataset Contamination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\\nA.7 Model Card . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\\n2'}, {'text': 'Figure 1: Helpfulness human evaluation results for Llama\\n2-Chatcomparedtootheropen-sourceandclosed-source\\nmodels. Human raters compared model generations on ~4k\\npromptsconsistingofbothsingleandmulti-turnprompts.\\nThe95%confidenceintervalsforthisevaluationarebetween\\n1%and2%. MoredetailsinSection3.4.2. Whilereviewing\\nthese results, it is important to note that human evaluations\\ncanbenoisyduetolimitationsofthepromptset,subjectivity\\nof the review guidelines, subjectivity of individual raters,\\nand the inherent difficulty of comparing generations.\\nFigure 2: Win-rate % for helpfulness and\\nsafety between commercial-licensed base-\\nlines and Llama 2-Chat , according to GPT-\\n4. Tocomplementthehumanevaluation,we\\nused a more capable model, not subject to\\nourownguidance. Greenareaindicatesour\\nmodelisbetteraccordingtoGPT-4. Toremove\\nties, we used win/ (win+loss). The orders in\\nwhichthemodelresponsesarepresentedto\\nGPT-4arerandomlyswappedtoalleviatebias.\\n1 Introduction\\nLarge Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in\\ncomplex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized\\ndomains such as programming and creative writing. They enable interaction with humans through intuitive\\nchat interfaces, which has led to rapid and widespread adoption among the general public.\\nThecapabilitiesofLLMsareremarkableconsideringtheseeminglystraightforwardnatureofthetraining\\nmethodology. Auto-regressivetransformersarepretrainedonanextensivecorpusofself-superviseddata,\\nfollowed by alignment with human preferences via techniques such as Reinforcement Learning with Human\\nFeedback(RLHF).Althoughthetrainingmethodologyissimple,highcomputationalrequirementshave\\nlimited the development of LLMs to a few players. There have been public releases of pretrained LLMs\\n(such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that\\nmatch the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla\\n(Hoffmann et al., 2022), but none of these models are suitable substitutes for closed \u201cproduct\u201d LLMs, such\\nasChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyfine-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require significant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and\\nLlama 2-Chat , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nLlama 2-Chat models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\\nFigures1and3). Wehavetakenmeasurestoincreasethesafetyofthesemodels,usingsafety-specificdata\\nannotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,\\nthispapercontributesathoroughdescriptionofourfine-tuningmethodologyandapproachtoimproving\\nLLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and\\ncontinue to improve the safety of those models, paving the way for more responsible development of LLMs.\\nWealsosharenovelobservationswemadeduringthedevelopmentof Llama 2 andLlama 2-Chat ,suchas\\nthe emergence of tool usage and temporal organization of knowledge.\\n3'}]\n</pre> In\u00a0[8]: Copied! <pre>\"\"\"\nUtil for searching lancedb table with different query types and rerankers. In case of Vector and FTS only reranking, we'll overfetch the results\nby a factor of 2 and get top K after reranking. Without overfetching, vector only and fts only search results won't have any effect on hit-rate metric\n\"\"\"\nfrom lancedb.rerankers import Reranker\n\nVALID_QUERY_TYPES = [\"vector\", \"fts\", \"hybrid\", \"rerank_vector\", \"rerank_fts\"]\n\ndef search_table(table: lancedb.table, reranker:Reranker, query_type: str, query_string: str, top_k:int=5, overfetch_factor:int=2):\n    if query_type not in VALID_QUERY_TYPES:\n        raise ValueError(f\"Invalid query type: {query_type}\")\n    if query_type in [\"hybrid\", \"rerank_vector\", \"rerank_fts\"] and reranker is None:\n        raise ValueError(f\"Reranker must be provided for query type: {query_type}\")\n\n    if query_type in [\"vector\", \"fts\"]:\n        rs = table.search(query_string, query_type=query_type).limit(top_k).to_pandas()\n    elif query_type == [\"rerank_vector\", \"rerank_fts\"]:\n        rs = table.search(query_string, query_type=query_type).rerank(reranker=reranker).limit(overfetch_factor*top_k).to_pandas()\n    elif query_type == \"hybrid\":\n        rs = table.search(query_string, query_type=query_type).rerank(reranker=reranker).limit(top_k).to_pandas()\n\n    return rs\n</pre> \"\"\" Util for searching lancedb table with different query types and rerankers. In case of Vector and FTS only reranking, we'll overfetch the results by a factor of 2 and get top K after reranking. Without overfetching, vector only and fts only search results won't have any effect on hit-rate metric \"\"\" from lancedb.rerankers import Reranker  VALID_QUERY_TYPES = [\"vector\", \"fts\", \"hybrid\", \"rerank_vector\", \"rerank_fts\"]  def search_table(table: lancedb.table, reranker:Reranker, query_type: str, query_string: str, top_k:int=5, overfetch_factor:int=2):     if query_type not in VALID_QUERY_TYPES:         raise ValueError(f\"Invalid query type: {query_type}\")     if query_type in [\"hybrid\", \"rerank_vector\", \"rerank_fts\"] and reranker is None:         raise ValueError(f\"Reranker must be provided for query type: {query_type}\")      if query_type in [\"vector\", \"fts\"]:         rs = table.search(query_string, query_type=query_type).limit(top_k).to_pandas()     elif query_type == [\"rerank_vector\", \"rerank_fts\"]:         rs = table.search(query_string, query_type=query_type).rerank(reranker=reranker).limit(overfetch_factor*top_k).to_pandas()     elif query_type == \"hybrid\":         rs = table.search(query_string, query_type=query_type).rerank(reranker=reranker).limit(top_k).to_pandas()      return rs In\u00a0[9]: Copied! <pre>import tqdm\n\ndef hit_rate(ds, table, query_type:str, top_k:int = 5, reranker:Reranker = None) -&gt; float:\n    eval_results = []\n    for idx in tqdm.tqdm(range(len(ds))):\n        query = ds[\"query\"][idx]\n        reference_context = ds[\"context\"][idx]\n        if not reference_context:\n            print(\"reference_context is None for query: {idx}. \\\n                            Skipping this query. Please check your dataset.\")\n            continue\n        try:\n            rs = search_table(table, reranker, query_type, query, top_k)\n        except Exception as e:\n            print(f'Error with query: {idx} {e}')\n            eval_results.append({\n                'is_hit': False,\n                'retrieved': [],\n                'expected': reference_context,\n                'query': query,\n            })\n            continue\n        retrieved_texts = rs['text'].tolist()[:top_k]\n        expected_text = reference_context[0] if isinstance(reference_context, list) else reference_context\n        is_hit = False\n\n        # HACK: to handle new line characters added my llamaindex doc reader\n        if expected_text in retrieved_texts or expected_text+'\\n' in retrieved_texts:\n            is_hit = True\n        eval_result = {\n            'is_hit': is_hit,\n            'retrieved': retrieved_texts,\n            'expected': expected_text,\n            'query': query,\n        }\n        eval_results.append(eval_result)\n\n    result = pd.DataFrame(eval_results)\n    hit_rate = result['is_hit'].mean()\n    return hit_rate\n</pre> import tqdm  def hit_rate(ds, table, query_type:str, top_k:int = 5, reranker:Reranker = None) -&gt; float:     eval_results = []     for idx in tqdm.tqdm(range(len(ds))):         query = ds[\"query\"][idx]         reference_context = ds[\"context\"][idx]         if not reference_context:             print(\"reference_context is None for query: {idx}. \\                             Skipping this query. Please check your dataset.\")             continue         try:             rs = search_table(table, reranker, query_type, query, top_k)         except Exception as e:             print(f'Error with query: {idx} {e}')             eval_results.append({                 'is_hit': False,                 'retrieved': [],                 'expected': reference_context,                 'query': query,             })             continue         retrieved_texts = rs['text'].tolist()[:top_k]         expected_text = reference_context[0] if isinstance(reference_context, list) else reference_context         is_hit = False          # HACK: to handle new line characters added my llamaindex doc reader         if expected_text in retrieved_texts or expected_text+'\\n' in retrieved_texts:             is_hit = True         eval_result = {             'is_hit': is_hit,             'retrieved': retrieved_texts,             'expected': expected_text,             'query': query,         }         eval_results.append(eval_result)      result = pd.DataFrame(eval_results)     hit_rate = result['is_hit'].mean()     return hit_rate In\u00a0[10]: Copied! <pre>tbl.create_fts_index(\"text\", replace=True)\nhit_rate_vector = hit_rate(data, tbl, \"vector\")\nhit_rate_fts = hit_rate(data, tbl, \"fts\")\nprint(f\"\\n Vector Search Hit Rate: {hit_rate_vector}\")\nprint(f\"FTS Search Hit Rate: {hit_rate_fts}\")\n</pre> tbl.create_fts_index(\"text\", replace=True) hit_rate_vector = hit_rate(data, tbl, \"vector\") hit_rate_fts = hit_rate(data, tbl, \"fts\") print(f\"\\n Vector Search Hit Rate: {hit_rate_vector}\") print(f\"FTS Search Hit Rate: {hit_rate_fts}\") <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 220/220 [00:10&lt;00:00, 21.62it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 220/220 [00:00&lt;00:00, 358.03it/s]</pre> <pre>\n Vector Search Hit Rate: 0.6409090909090909\nFTS Search Hit Rate: 0.5954545454545455\n</pre> <pre>\n</pre> <ol> <li>Reranked vector search</li> </ol> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[11]: Copied! <pre>from lancedb.rerankers import LinearCombinationReranker # LanceDB hybrid search uses LinearCombinationReranker by default\n\nreranker = LinearCombinationReranker(weight=0.7)\nhit_rate_hybrid = hit_rate(data, tbl, \"hybrid\", reranker=reranker)\n\nprint(f\"\\n Hybrid Search with LinearCombinationReranker Hit Rate: {hit_rate_hybrid}\")\n</pre> from lancedb.rerankers import LinearCombinationReranker # LanceDB hybrid search uses LinearCombinationReranker by default  reranker = LinearCombinationReranker(weight=0.7) hit_rate_hybrid = hit_rate(data, tbl, \"hybrid\", reranker=reranker)  print(f\"\\n Hybrid Search with LinearCombinationReranker Hit Rate: {hit_rate_hybrid}\") <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 220/220 [00:10&lt;00:00, 20.60it/s]</pre> <pre>\n Hybrid Search with LinearCombinationReranker Hit Rate: 0.6454545454545455\n</pre> <pre>\n</pre> In\u00a0[12]: Copied! <pre>#WARNING:  This cell takes a long time without CUDA\nfrom lancedb.rerankers import JinaReranker, CrossEncoderReranker, CohereReranker\n\nreranker = CrossEncoderReranker()\nhit_rate_hybrid = hit_rate(data, tbl, \"hybrid\", reranker=reranker)\nprint(f\" \\n Hybrid Search with CrossEncoderReranker Hit Rate: {hit_rate_hybrid}\")\n</pre> #WARNING:  This cell takes a long time without CUDA from lancedb.rerankers import JinaReranker, CrossEncoderReranker, CohereReranker  reranker = CrossEncoderReranker() hit_rate_hybrid = hit_rate(data, tbl, \"hybrid\", reranker=reranker) print(f\" \\n Hybrid Search with CrossEncoderReranker Hit Rate: {hit_rate_hybrid}\") <pre>\r  0%|          | 0/220 [00:00&lt;?, ?it/s]</pre> <pre>config.json:   0%|          | 0.00/612 [00:00&lt;?, ?B/s]</pre> <pre>pytorch_model.bin:   0%|          | 0.00/268M [00:00&lt;?, ?B/s]</pre> <pre>tokenizer_config.json:   0%|          | 0.00/541 [00:00&lt;?, ?B/s]</pre> <pre>vocab.txt:   0%|          | 0.00/232k [00:00&lt;?, ?B/s]</pre> <pre>special_tokens_map.json:   0%|          | 0.00/112 [00:00&lt;?, ?B/s]</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 220/220 [01:03&lt;00:00,  3.44it/s]</pre> <pre> \n Hybrid Search with CrossEncoderReranker Hit Rate: 0.6772727272727272\n</pre> <pre>\n</pre> <ol> <li>Jina AI Reranker</li> </ol> In\u00a0[14]: Copied! <pre># Jina AI Reranker\nimport os\nfrom lancedb.rerankers import JinaReranker\n\n# Colab secret setup\nfrom google.colab import userdata\nos.environ[\"JINA_API_KEY\"] = userdata.get('JINA_API_KEY')\n\nreranker = JinaReranker(model_name=\"jina-reranker-v2-base-multilingual\")\nhit_rate_hybrid = hit_rate(data, tbl, \"hybrid\", reranker=reranker)\nprint(f\" \\n Hybrid Search with JinaReranker Hit Rate: {hit_rate_hybrid}\")\n</pre> # Jina AI Reranker import os from lancedb.rerankers import JinaReranker  # Colab secret setup from google.colab import userdata os.environ[\"JINA_API_KEY\"] = userdata.get('JINA_API_KEY')  reranker = JinaReranker(model_name=\"jina-reranker-v2-base-multilingual\") hit_rate_hybrid = hit_rate(data, tbl, \"hybrid\", reranker=reranker) print(f\" \\n Hybrid Search with JinaReranker Hit Rate: {hit_rate_hybrid}\") <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 220/220 [01:24&lt;00:00,  2.60it/s]</pre> <pre> \n Hybrid Search with JinaReranker Hit Rate: 0.7681818181818182\n</pre> <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>os.environ[\"COHERE_API_KEY\"] = userdata.get('COHERE_API_KEY')\n\nreranker = CohereReranker()\nhit_rate_hybrid = hit_rate(data, tbl, \"hybrid\", reranker=reranker)\nprint(f\" \\n Hybrid Search with CohereReranker Hit Rate: {hit_rate_hybrid}\")\n</pre> os.environ[\"COHERE_API_KEY\"] = userdata.get('COHERE_API_KEY')  reranker = CohereReranker() hit_rate_hybrid = hit_rate(data, tbl, \"hybrid\", reranker=reranker) print(f\" \\n Hybrid Search with CohereReranker Hit Rate: {hit_rate_hybrid}\")"},{"location":"notebooks/lancedb_reranking/#example-improve-retrievers-using-rerankers-hybrid-search","title":"Example - Improve Retrievers using Rerankers &amp; Hybrid search\u00b6","text":""},{"location":"notebooks/lancedb_reranking/#optimizing-rag-retrieval-performance-using-hybrid-search-reranking","title":"Optimizing RAG retrieval performance using hybrid search &amp; reranking\u00b6","text":""},{"location":"notebooks/lancedb_reranking/#what-is-a-retriever","title":"What is a retriever\u00b6","text":"<p>VectorDBs are used as retrievers in recommender or chatbot-based systems for retrieving relevant data based on user queries. For example, retriever is a critical component of Retrieval Augmented Generation (RAG) acrhitectures. In this section, we will discuss how to improve the performance of retrievers.</p> <p></p> <p>source</p>"},{"location":"notebooks/lancedb_reranking/#how-do-you-go-about-improving-retreival-performance","title":"How do you go about improving retreival performance\u00b6","text":"<p>Some of the common techniques are:</p> <ul> <li>Using different search types - vector/semantic, FTS (BM25)</li> <li>Hybrid search</li> <li>Reranking</li> <li>Fine-tuning the embedding models</li> <li>Using different embedding models</li> </ul> <p>Obviously, the above list is not exhaustive. There are other subtler ways that can improve retrieval performance like alternative chunking algorithms, using different distance/similarity metrics, and more. For brevity, we'll only cover high level and more impactful techniques here.</p>"},{"location":"notebooks/lancedb_reranking/#lancedb","title":"LanceDB\u00b6","text":"<ul> <li>Multimodal DB for AI</li> <li>Powered by an innovative &amp; open-source in-house file format</li> <li>Zero setup</li> <li>Scales up on disk storage</li> <li>Native support for vector, full-text(BM25) and hybrid search</li> </ul>"},{"location":"notebooks/lancedb_reranking/#the-dataset","title":"The dataset\u00b6","text":"<p>The dataset we'll use is a synthetic QA dataset generated from LLama2 review paper. The paper was divided into chunks, with each chunk being a unique context. An LLM was prompted to ask questions relevant to the context for testing a retriever. The exact code and other utility functions for this can be found in this repo.</p>"},{"location":"notebooks/lancedb_reranking/#ingestion","title":"Ingestion\u00b6","text":"<p>Let us now ingest the contexts in LanceDB. The steps will be:</p> <ul> <li>Create a schema (Pydantic or Pyarrow)</li> <li>Select an embedding model from LanceDB Embedding API (to allow automatic vectorization of data)</li> <li>Ingest the contexts</li> </ul>"},{"location":"notebooks/lancedb_reranking/#different-query-types-in-lancedb","title":"Different Query types in LanceDB\u00b6","text":"<p>LanceDB allows switching query types with by setting <code>query_type</code> argument, which defaults to <code>vector</code> when using Embedding API. In this example we'll use <code>JinaReranker</code> which is one of many rerankers supported by LanceDB.</p>"},{"location":"notebooks/lancedb_reranking/#vector-search","title":"Vector search:\u00b6","text":"<p>Vector search</p> <pre><code>table.search(query, query_type=\"vector\")` or `table.search(query)\n</code></pre> <p>Vector search with Reranking</p> <pre><code>reranker = JinaReranker()\ntable.search(query).rerank(reranker=reranker)\n</code></pre>"},{"location":"notebooks/lancedb_reranking/#full-text-search","title":"Full-text search:\u00b6","text":"<p>FTS</p> <pre><code>table.search(query, query_type=\"fts\")\n</code></pre>"},{"location":"notebooks/lancedb_reranking/#fts-with-reranking","title":"FTS with Reranking\u00b6","text":"<pre><code>table.search(query, query_type=\"fts\").rerank(reranker=reranker)\n</code></pre>"},{"location":"notebooks/lancedb_reranking/#hybrid-search","title":"Hybrid search\u00b6","text":"<pre><code>table.search(query, query_type=\"hybrid\").rerank(reranker=reranker)\n</code></pre>"},{"location":"notebooks/lancedb_reranking/#hit-rate-eval-metric","title":"Hit-rate eval metric\u00b6","text":"<p>We'll be using a simple metric called <code>\"hit-rate\"</code> for evaluating the performance of the retriever across this guide.</p> <p>Hit-rate is the percentage of queries for which the retriever returned the correct answer in the top-k results.</p> <p>For example, if the retriever returned the correct answer in the top-3 results for 70% of the queries, then the hit-rate@3 is 0.7.</p>"},{"location":"notebooks/lancedb_reranking/#hybrid-search","title":"Hybrid Search\u00b6","text":""},{"location":"notebooks/lancedb_reranking/#trying-out-different-rerankers","title":"Trying out different rerankers\u00b6","text":""},{"location":"notebooks/lancedb_reranking/#1-cross-encoder-reranker","title":"1. Cross Encoder Reranker\u00b6","text":"<p>Bi-Encoders produce for a given sentence a sentence embedding. We pass to a BERT independently the sentences A and B, which result in the sentence embeddings u and v. These sentence embedding can then be compared using cosine similarity.</p> <p>In contrast, for a Cross-Encoder, we pass both sentences simultaneously to the Transformer network. It produces then an output value between 0 and 1 indicating the similarity of the input sentence pair:</p> <p>A Cross-Encoder does not produce a sentence embedding. Also, we are not able to pass individual sentences to a Cross-Encoder.</p>"},{"location":"notebooks/lancedb_reranking/#all-results","title":"All results:\u00b6","text":"Query Type Hit-rate@5 Vector 0.640 FTS 0.595 Reranked vector (Cohere Reranker) 0.677 Reranked fts (Cohere Reranker) 0.672 Hybrid (Cohere Reranker) 0.759 Hybrid (Jina Reranker) 0.768"},{"location":"notebooks/lancedb_reranking/#results-on-other-datasets","title":"Results on other datasets\u00b6","text":""},{"location":"notebooks/lancedb_reranking/#squad-dataset","title":"SQuAD Dataset\u00b6","text":"<p>[TODO]</p>"},{"location":"notebooks/lancedb_reranking/#uber10k-sec-filing-dataset","title":"Uber10K sec filing Dataset\u00b6","text":"Query Type Hit-rate@5 Vector 0.608 FTS 0.824 Reranked vector 0.671 Reranked fts 0.843 Hybrid 0.849"},{"location":"notebooks/lancedb_reranking/#full-text-search-is-generally-a-good-baseline","title":"Full text search is generally a good baseline!\u00b6","text":""},{"location":"notebooks/lancedb_reranking/#implementing-custom-rerankers-with-lancedb","title":"Implementing Custom <code>Rerankers</code> with LanceDB\u00b6","text":"<p>LanceDB</p> <pre><code>from lancedb.rerankers import Reranker\nimport pyarrow as pa\n\nclass MyReranker(Reranker):\n    def __init__(self, param1, param2, ..., return_score=\"relevance\"):\n\n\n    def rerank_hybrid(self, query: str, vector_results: pa.Table, fts_results: pa.Table):\n        # Use the built-in merging function\n        combined_result = self.merge_results(vector_results, fts_results)\n\n        # Do something with the combined results\n        return combined_result\n\n    def rerank_vector(self, query: str, vector_results: pa.Table):\n        # Do something with the vector results\n        return vector_results\n\n    def rerank_fts(self, query: str, fts_results: pa.Table):\n        # Do something with the FTS results\n        return fts_results\n\n</code></pre>"},{"location":"notebooks/lancedb_reranking/#takeaways-tradeoffs","title":"Takeaways &amp; Tradeoffs\u00b6","text":"<ul> <li><p>Rerankers significantly improve accuracy at little cost. Using Hybrid search and/or rerankers can significantly improve retrieval performance without spending any additional time or effort on tuning embedding models, generators, or dissecting the dataset.</p> </li> <li><p>Reranking is an expensive operation. Depending on the type of reranker you choose, they can incur significant latecy to query times. Although some API-based rerankers can be significantly faster.</p> </li> <li><p>Pre-warmed GPU environments reduce latency.  When using models locally, having a warmed-up GPU environment will significantly reduce latency. This is especially useful if the application doesn't need to be strictly realtime. Pre-warming comes at the expense of GPU resources.</p> </li> </ul>"},{"location":"notebooks/lancedb_reranking/#applications","title":"Applications\u00b6","text":"<ul> <li><p>Not all recommendation problems are strictly real-time. When considering problem statements involving chatbots, search recommendations, auto-complete etc. low latency is a hard requirement.</p> </li> <li><p>But there another category of applications where retrieval accurate information need not be real-time. For example:</p> <ol> <li><p>Personalized music or movie recommendation: These systems generally start off by recommending close to random / or some generally accurate recommendations. They keep improving recommendations async with the user interation data. </p> </li> <li><p>Social media personalised timeline</p> </li> <li><p>Recommend blogs, videos, etc. via push notifications</p> </li> </ol> <p>\"YouTube now gives notifications for \"recommended\", non-subscribed channels\" - https://www.reddit.com/r/assholedesign/comments/807zpe/youtube_now_gives_notifications_for_recommended/</p> <p></p> </li> </ul>"},{"location":"notebooks/langchain_example/","title":"LanceDB","text":"In\u00a0[\u00a0]: Copied! <pre>! pip install tantivy\n</pre> ! pip install tantivy In\u00a0[\u00a0]: Copied! <pre>! pip install -U langchain-openai langchain-community\n</pre> ! pip install -U langchain-openai langchain-community In\u00a0[\u00a0]: Copied! <pre>! pip install lancedb\n</pre> ! pip install lancedb <p>We want to use OpenAIEmbeddings so we have to get the OpenAI API Key.</p> In\u00a0[1]: Copied! <pre>import getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n</pre> import getpass import os  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\") In\u00a0[2]: Copied! <pre>! rm -rf /tmp/lancedb\n</pre> ! rm -rf /tmp/lancedb In\u00a0[3]: Copied! <pre>from langchain_community.document_loaders import TextLoader\nfrom langchain_community.vectorstores import LanceDB\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_text_splitters import CharacterTextSplitter\n\nloader = TextLoader(\"../../how_to/state_of_the_union.txt\")\ndocuments = loader.load()\n\ndocuments = CharacterTextSplitter().split_documents(documents)\nembeddings = OpenAIEmbeddings()\n</pre> from langchain_community.document_loaders import TextLoader from langchain_community.vectorstores import LanceDB from langchain_openai import OpenAIEmbeddings from langchain_text_splitters import CharacterTextSplitter  loader = TextLoader(\"../../how_to/state_of_the_union.txt\") documents = loader.load()  documents = CharacterTextSplitter().split_documents(documents) embeddings = OpenAIEmbeddings() In\u00a0[4]: Copied! <pre>from lancedb.rerankers import LinearCombinationReranker\n\nreranker = LinearCombinationReranker(weight=0.3)\n\ndocsearch = LanceDB.from_documents(documents, embeddings, reranker=reranker)\nquery = \"What did the president say about Ketanji Brown Jackson\"\n</pre> from lancedb.rerankers import LinearCombinationReranker  reranker = LinearCombinationReranker(weight=0.3)  docsearch = LanceDB.from_documents(documents, embeddings, reranker=reranker) query = \"What did the president say about Ketanji Brown Jackson\" In\u00a0[31]: Copied! <pre>docs = docsearch.similarity_search_with_relevance_scores(query)\nprint(\"relevance score - \", docs[0][1])\nprint(\"text- \", docs[0][0].page_content[:1000])\n</pre> docs = docsearch.similarity_search_with_relevance_scores(query) print(\"relevance score - \", docs[0][1]) print(\"text- \", docs[0][0].page_content[:1000]) <pre>relevance score -  0.7066475030191711\ntext-  They were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \n\nOfficer Mora was 27 years old. \n\nOfficer Rivera was 22. \n\nBoth Dominican Americans who\u2019d grown up on the same streets they later chose to patrol as police officers. \n\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves. \n\nI\u2019ve worked on these issues a long time. \n\nI know what works: Investing in crime prevention and community police officers who\u2019ll walk the beat, who\u2019ll know the neighborhood, and who can restore trust and safety. \n\nSo let\u2019s not abandon our streets. Or choose between safety and equal justice. \n\nLet\u2019s come together to protect our communities, restore trust, and hold law enforcement accountable. \n\nThat\u2019s why the Justice Department required body cameras, banned chokeholds, and restricted no-knock warrants for its officers. \n\nThat\u2019s why the American Rescue \n</pre> In\u00a0[33]: Copied! <pre>docs = docsearch.similarity_search_with_score(query=\"Headaches\", query_type=\"hybrid\")\nprint(\"distance - \", docs[0][1])\nprint(\"text- \", docs[0][0].page_content[:1000])\n</pre> docs = docsearch.similarity_search_with_score(query=\"Headaches\", query_type=\"hybrid\") print(\"distance - \", docs[0][1]) print(\"text- \", docs[0][0].page_content[:1000]) <pre>distance -  0.30000001192092896\ntext-  My administration is providing assistance with job training and housing, and now helping lower-income veterans get VA care debt-free.  \n\nOur troops in Iraq and Afghanistan faced many dangers. \n\nOne was stationed at bases and breathing in toxic smoke from \u201cburn pits\u201d that incinerated wastes of war\u2014medical and hazard material, jet fuel, and more. \n\nWhen they came home, many of the world\u2019s fittest and best trained warriors were never the same. \n\nHeadaches. Numbness. Dizziness. \n\nA cancer that would put them in a flag-draped coffin. \n\nI know. \n\nOne of those soldiers was my son Major Beau Biden. \n\nWe don\u2019t know for sure if a burn pit was the cause of his brain cancer, or the diseases of so many of our troops. \n\nBut I\u2019m committed to finding out everything we can. \n\nCommitted to military families like Danielle Robinson from Ohio. \n\nThe widow of Sergeant First Class Heath Robinson.  \n\nHe was born a soldier. Army National Guard. Combat medic in Kosovo and Iraq. \n\nStationed near Baghdad, just ya\n</pre> In\u00a0[8]: Copied! <pre>print(\"reranker : \", docsearch._reranker)\n</pre> print(\"reranker : \", docsearch._reranker) <pre>reranker :  &lt;lancedb.rerankers.linear_combination.LinearCombinationReranker object at 0x107ef1130&gt;\n</pre> <p>Additionaly, to explore the table you can load it into a df or save it in a csv file:</p> <pre>tbl = docsearch.get_table()\nprint(\"tbl:\", tbl)\npd_df = tbl.to_pandas()\n# pd_df.to_csv(\"docsearch.csv\", index=False)\n\n# you can also create a new vector store object using an older connection object:\nvector_store = LanceDB(connection=tbl, embedding=embeddings)\n</pre> In\u00a0[15]: Copied! <pre>docs = docsearch.similarity_search(\n    query=query, filter={\"metadata.source\": \"../../how_to/state_of_the_union.txt\"}\n)\n\nprint(\"metadata :\", docs[0].metadata)\n\n# or you can directly supply SQL string filters :\n\nprint(\"\\nSQL filtering :\\n\")\ndocs = docsearch.similarity_search(query=query, filter=\"text LIKE '%Officer Rivera%'\")\nprint(docs[0].page_content)\n</pre> docs = docsearch.similarity_search(     query=query, filter={\"metadata.source\": \"../../how_to/state_of_the_union.txt\"} )  print(\"metadata :\", docs[0].metadata)  # or you can directly supply SQL string filters :  print(\"\\nSQL filtering :\\n\") docs = docsearch.similarity_search(query=query, filter=\"text LIKE '%Officer Rivera%'\") print(docs[0].page_content) <pre>metadata : {'source': '../../how_to/state_of_the_union.txt'}\n\nSQL filtering :\n\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \n\nOfficer Mora was 27 years old. \n\nOfficer Rivera was 22. \n\nBoth Dominican Americans who\u2019d grown up on the same streets they later chose to patrol as police officers. \n\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves. \n\nI\u2019ve worked on these issues a long time. \n\nI know what works: Investing in crime prevention and community police officers who\u2019ll walk the beat, who\u2019ll know the neighborhood, and who can restore trust and safety. \n\nSo let\u2019s not abandon our streets. Or choose between safety and equal justice. \n\nLet\u2019s come together to protect our communities, restore trust, and hold law enforcement accountable. \n\nThat\u2019s why the Justice Department required body cameras, banned chokeholds, and restricted no-knock warrants for its officers. \n\nThat\u2019s why the American Rescue Plan provided $350 Billion that cities, states, and counties can use to hire more police and invest in proven strategies like community violence interruption\u2014trusted messengers breaking the cycle of violence and trauma and giving young people hope.  \n\nWe should all agree: The answer is not to Defund the police. The answer is to FUND the police with the resources and training they need to protect our communities. \n\nI ask Democrats and Republicans alike: Pass my budget and keep our neighborhoods safe.  \n\nAnd I will keep doing everything in my power to crack down on gun trafficking and ghost guns you can buy online and make at home\u2014they have no serial numbers and can\u2019t be traced. \n\nAnd I ask Congress to pass proven measures to reduce gun violence. Pass universal background checks. Why should anyone on a terrorist list be able to purchase a weapon? \n\nBan assault weapons and high-capacity magazines. \n\nRepeal the liability shield that makes gun manufacturers the only industry in America that can\u2019t be sued. \n\nThese laws don\u2019t infringe on the Second Amendment. They save lives. \n\nThe most fundamental right in America is the right to vote \u2013 and to have it counted. And it\u2019s under assault. \n\nIn state after state, new laws have been passed, not only to suppress the vote, but to subvert entire elections. \n\nWe cannot let this happen. \n\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\u2019re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence. \n\nA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she\u2019s been nominated, she\u2019s received a broad range of support\u2014from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \n\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \n\nWe can do both. At our border, we\u2019ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \n\nWe\u2019ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \n\nWe\u2019re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster.\n</pre> In\u00a0[\u00a0]: Copied! <pre>! pip install -U langchain-experimental\n</pre> ! pip install -U langchain-experimental In\u00a0[\u00a0]: Copied! <pre>! pip install open_clip_torch torch\n</pre> ! pip install open_clip_torch torch In\u00a0[16]: Copied! <pre>! rm -rf '/tmp/multimmodal_lance'\n</pre> ! rm -rf '/tmp/multimmodal_lance' In\u00a0[17]: Copied! <pre>from langchain_experimental.open_clip import OpenCLIPEmbeddings\n</pre> from langchain_experimental.open_clip import OpenCLIPEmbeddings In\u00a0[18]: Copied! <pre>import os\n\nimport requests\n\n# List of image URLs to download\nimage_urls = [\n    \"https://github.com/raghavdixit99/assets/assets/34462078/abf47cc4-d979-4aaa-83be-53a2115bf318\",\n    \"https://github.com/raghavdixit99/assets/assets/34462078/93be928e-522b-4e37-889d-d4efd54b2112\",\n]\n\ntexts = [\"bird\", \"dragon\"]\n\n# Directory to save images\ndir_name = \"./photos/\"\n\n# Create directory if it doesn't exist\nos.makedirs(dir_name, exist_ok=True)\n\nimage_uris = []\n# Download and save each image\nfor i, url in enumerate(image_urls, start=1):\n    response = requests.get(url)\n    path = os.path.join(dir_name, f\"image{i}.jpg\")\n    image_uris.append(path)\n    with open(path, \"wb\") as f:\n        f.write(response.content)\n</pre> import os  import requests  # List of image URLs to download image_urls = [     \"https://github.com/raghavdixit99/assets/assets/34462078/abf47cc4-d979-4aaa-83be-53a2115bf318\",     \"https://github.com/raghavdixit99/assets/assets/34462078/93be928e-522b-4e37-889d-d4efd54b2112\", ]  texts = [\"bird\", \"dragon\"]  # Directory to save images dir_name = \"./photos/\"  # Create directory if it doesn't exist os.makedirs(dir_name, exist_ok=True)  image_uris = [] # Download and save each image for i, url in enumerate(image_urls, start=1):     response = requests.get(url)     path = os.path.join(dir_name, f\"image{i}.jpg\")     image_uris.append(path)     with open(path, \"wb\") as f:         f.write(response.content) In\u00a0[21]: Copied! <pre>from langchain_community.vectorstores import LanceDB\n\nvec_store = LanceDB(\n    table_name=\"multimodal_test\",\n    embedding=OpenCLIPEmbeddings(),\n)\n</pre> from langchain_community.vectorstores import LanceDB  vec_store = LanceDB(     table_name=\"multimodal_test\",     embedding=OpenCLIPEmbeddings(), ) In\u00a0[22]: Copied! <pre>vec_store.add_images(uris=image_uris)\n</pre> vec_store.add_images(uris=image_uris) Out[22]: <pre>['b673620b-01f0-42ca-a92e-d033bb92c0a6',\n '99c3a5b0-b577-417a-8177-92f4a655dbfb']</pre> In\u00a0[23]: Copied! <pre>vec_store.add_texts(texts)\n</pre> vec_store.add_texts(texts) Out[23]: <pre>['f7adde5d-a4a3-402b-9e73-088b230722c3',\n 'cbed59da-0aec-4bff-8820-9e59d81a2140']</pre> In\u00a0[24]: Copied! <pre>img_embed = vec_store._embedding.embed_query(\"bird\")\n</pre> img_embed = vec_store._embedding.embed_query(\"bird\") In\u00a0[25]: Copied! <pre>vec_store.similarity_search_by_vector(img_embed)[0]\n</pre> vec_store.similarity_search_by_vector(img_embed)[0] Out[25]: <pre>Document(page_content='bird', metadata={'id': 'f7adde5d-a4a3-402b-9e73-088b230722c3'})</pre> In\u00a0[26]: Copied! <pre>vec_store._table\n</pre> vec_store._table Out[26]: <pre>LanceTable(connection=LanceDBConnection(/tmp/lancedb), name=\"multimodal_test\")</pre>"},{"location":"notebooks/langchain_example/#lancedb","title":"LanceDB\u00b6","text":"<p>LanceDB is an open-source database for vector-search built with persistent storage, which greatly simplifies retrevial, filtering and management of embeddings. Fully open source.</p> <p>This notebook shows how to use functionality related to the <code>LanceDB</code> vector database based on the Lance data format.</p>"},{"location":"notebooks/langchain_example/#for-lancedb-cloud-you-can-invoke-the-vector-store-as-follows","title":"For LanceDB cloud, you can invoke the vector store as follows :\u00b6","text":"<pre>db_url = \"db://lang_test\" # url of db you created\napi_key = \"xxxxx\" # your API key\nregion=\"us-east-1-dev\"  # your selected region\n\nvector_store = LanceDB(\n    uri=db_url,\n    api_key=api_key,\n    region=region,\n    embedding=embeddings,\n    table_name='langchain_test'\n    )\n</pre> <p>You can also add <code>region</code>, <code>api_key</code>, <code>uri</code> to <code>from_documents()</code> classmethod</p>"},{"location":"notebooks/langchain_example/#adding-images","title":"Adding images\u00b6","text":""},{"location":"notebooks/multi_lingual_example/","title":"Example - Multi-lingual semantic search","text":"In\u00a0[1]: Copied! <pre>!pip install -qU datasets cohere openai lancedb\n</pre> !pip install -qU datasets cohere openai lancedb  <pre>\n[notice] A new release of pip is available: 23.2.1 -&gt; 23.3\n[notice] To update, run: pip install --upgrade pip\n</pre> In\u00a0[11]: Copied! <pre>from datasets import load_dataset\n\nen = dataset = load_dataset(\"wikipedia\", \"20220301.en\", streaming=True,)\nfr = load_dataset(\"wikipedia\", \"20220301.fr\", streaming=True)\n\ndatasets = {\"english\": iter(en['train']), \"french\": iter(fr['train'])}\n</pre> from datasets import load_dataset  en = dataset = load_dataset(\"wikipedia\", \"20220301.en\", streaming=True,) fr = load_dataset(\"wikipedia\", \"20220301.fr\", streaming=True)  datasets = {\"english\": iter(en['train']), \"french\": iter(fr['train'])} <pre>/Users/ayush/vectordb-recipes/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> <p>Let's take a look at the dataset format</p> In\u00a0[38]: Copied! <pre>next(iter(en['train']))\n</pre> next(iter(en['train'])) Out[38]: <pre>{'id': '12',\n 'url': 'https://en.wikipedia.org/wiki/Anarchism',\n 'title': 'Anarchism',\n 'text': 'Anarchism is a political philosophy and movement that is sceptical of authority and rejects all involuntary, coercive forms of hierarchy. Anarchism calls for the abolition of the state, which it holds to be unnecessary, undesirable, and harmful. As a historically left-wing movement, placed on the farthest left of the political spectrum, it is usually described alongside communalism and libertarian Marxism as the libertarian wing (libertarian socialism) of the socialist movement, and has a strong historical association with anti-capitalism and socialism.\\n\\nHumans lived in societies without formal hierarchies long before the establishment of formal states, realms, or empires. With the rise of organised hierarchical bodies, scepticism toward authority also rose. Although traces of anarchist thought are found throughout history, modern anarchism emerged from the Enlightenment. During the latter half of the 19th and the first decades of the 20th century, the anarchist movement flourished in most parts of the world and had a significant role in workers\\' struggles for emancipation. Various anarchist schools of thought formed during this period. Anarchists have taken part in several revolutions, most notably in the Paris Commune, the Russian Civil War and the Spanish Civil War, whose end marked the end of the classical era of anarchism. In the last decades of the 20th and into the 21st century, the anarchist movement has been resurgent once more.\\n\\nAnarchism employs a diversity of tactics in order to meet its ideal ends which can be broadly separated into revolutionary and evolutionary tactics; there is significant overlap between the two, which are merely descriptive. Revolutionary tactics aim to bring down authority and state, having taken a violent turn in the past, while evolutionary tactics aim to prefigure what an anarchist society would be like. Anarchist thought, criticism, and praxis have played a part in diverse areas of human society. Criticism of anarchism include claims that it is internally inconsistent, violent, or utopian.\\n\\nEtymology, terminology, and definition \\n\\nThe etymological origin of anarchism is from the Ancient Greek anarkhia, meaning \"without a ruler\", composed of the prefix an- (\"without\") and the word arkhos (\"leader\" or \"ruler\"). The suffix -ism denotes the ideological current that favours anarchy. Anarchism appears in English from 1642 as anarchisme and anarchy from 1539; early English usages emphasised a sense of disorder. Various factions within the French Revolution labelled their opponents as anarchists, although few such accused shared many views with later anarchists. Many revolutionaries of the 19th century such as William Godwin (1756\u20131836) and Wilhelm Weitling (1808\u20131871) would contribute to the anarchist doctrines of the next generation but did not use anarchist or anarchism in describing themselves or their beliefs.\\n\\nThe first political philosopher to call himself an anarchist () was Pierre-Joseph Proudhon (1809\u20131865), marking the formal birth of anarchism in the mid-19th century. Since the 1890s and beginning in France, libertarianism has often been used as a synonym for anarchism and its use as a synonym is still common outside the United States. Some usages of libertarianism refer to individualistic free-market philosophy only, and free-market anarchism in particular is termed libertarian anarchism.\\n\\nWhile the term libertarian has been largely synonymous with anarchism, its meaning has more recently diluted with wider adoption from ideologically disparate groups, including both the New Left and libertarian Marxists, who do not associate themselves with authoritarian socialists or a vanguard party, and extreme cultural liberals, who are primarily concerned with civil liberties. Additionally, some anarchists use libertarian socialist to avoid anarchism\\'s negative connotations and emphasise its connections with socialism. Anarchism is broadly used to describe the anti-authoritarian wing of the socialist movement. Anarchism is contrasted to socialist forms which are state-oriented or from above. Scholars of anarchism generally highlight anarchism\\'s socialist credentials and criticise attempts at creating dichotomies between the two. Some scholars describe anarchism as having many influences from liberalism, and being both liberals and socialists but more so, while most scholars reject anarcho-capitalism as a misunderstanding of anarchist principles.\\n\\nWhile opposition to the state is central to anarchist thought, defining anarchism is not an easy task for scholars, as there is a lot of discussion among scholars and anarchists on the matter, and various currents perceive anarchism slightly differently. Major definitional elements include the will for a non-coercive society, the rejection of the state apparatus, the belief that human nature allows humans to exist in or progress toward such a non-coercive society, and a suggestion on how to act to pursue the ideal of anarchy.\\n\\nHistory\\n\\nPre-modern era \\n\\nBefore the establishment of towns and cities, an established authority did not exist. It was after the creation of institutions of authority that anarchistic ideas espoused as a reaction. The most notable precursors to anarchism in the ancient world were in China and Greece. In China, philosophical anarchism (the discussion on the legitimacy of the state) was delineated by Taoist philosophers Zhuang Zhou and Laozi. Alongside Stoicism, Taoism has been said to have had \"significant anticipations\" of anarchism.\\n \\nAnarchic attitudes were also articulated by tragedians and philosophers in Greece. Aeschylus and Sophocles used the myth of Antigone to illustrate the conflict between rules set by the state and personal autonomy. Socrates questioned Athenian authorities constantly and insisted on the right of individual freedom of conscience. Cynics dismissed human law (nomos) and associated authorities while trying to live according to nature (physis). Stoics were supportive of a society based on unofficial and friendly relations among its citizens without the presence of a state.\\n\\nIn medieval Europe, there was no anarchistic activity except some ascetic religious movements. These, and other Muslim movements, later gave birth to religious anarchism. In the Sasanian Empire, Mazdak called for an egalitarian society and the abolition of monarchy, only to be soon executed by Emperor Kavad I.\\n\\nIn Basra, religious sects preached against the state. In Europe, various sects developed anti-state and libertarian tendencies. Renewed interest in antiquity during the Renaissance and in private judgment during the Reformation restored elements of anti-authoritarian secularism, particularly in France. Enlightenment challenges to intellectual authority (secular and religious) and the revolutions of the 1790s and 1848 all spurred the ideological development of what became the era of classical anarchism.\\n\\nModern era \\nDuring the French Revolution, partisan groups such as the Enrag\u00e9s and the  saw a turning point in the fermentation of anti-state and federalist sentiments. The first anarchist currents developed throughout the 18th century as William Godwin espoused philosophical anarchism in England, morally delegitimising the state, Max Stirner\\'s thinking paved the way to individualism and Pierre-Joseph Proudhon\\'s theory of mutualism found fertile soil in France. By the late 1870s, various anarchist schools of thought had become well-defined and a wave of then unprecedented globalisation occurred from 1880 to 1914. This era of classical anarchism lasted until the end of the Spanish Civil War and is considered the golden age of anarchism.\\n\\nDrawing from mutualism, Mikhail Bakunin founded collectivist anarchism and entered the International Workingmen\\'s Association, a class worker union later known as the First International that formed in 1864 to unite diverse revolutionary currents. The International became a significant political force, with Karl Marx being a leading figure and a member of its General Council. Bakunin\\'s faction (the Jura Federation) and Proudhon\\'s followers (the mutualists) opposed state socialism, advocating political abstentionism and small property holdings. After bitter disputes, the Bakuninists were expelled from the International by the Marxists at the 1872 Hague Congress. Anarchists were treated similarly in the Second International, being ultimately expelled in 1896. Bakunin famously predicted that if revolutionaries gained power by Marx\\'s terms, they would end up the new tyrants of workers. In response to their expulsion from the First International, anarchists formed the St. Imier International. Under the influence of Peter Kropotkin, a Russian philosopher and scientist, anarcho-communism overlapped with collectivism. Anarcho-communists, who drew inspiration from the 1871 Paris Commune, advocated for free federation and for the distribution of goods according to one\\'s needs.\\n\\nAt the turn of the century, anarchism had spread all over the world. It was a notable feature of the international syndicalism movement. In China, small groups of students imported the humanistic pro-science version of anarcho-communism. Tokyo was a hotspot for rebellious youth from countries of the far east, travelling to the Japanese capital to study. In Latin America, Argentina was a stronghold for anarcho-syndicalism, where it became the most prominent left-wing ideology. During this time, a minority of anarchists adopted tactics of revolutionary political violence. This strategy became known as propaganda of the deed. The dismemberment of the French socialist movement into many groups and the execution and exile of many Communards to penal colonies following the suppression of the Paris Commune favoured individualist political expression and acts. Even though many anarchists distanced themselves from these terrorist acts, infamy came upon the movement and attempts were made to exclude them from American immigration, including the Immigration Act of 1903, also called the Anarchist Exclusion Act. Illegalism was another strategy which some anarchists adopted during this period.\\n\\nDespite concerns, anarchists enthusiastically participated in the Russian Revolution in opposition to the White movement; however, they met harsh suppression after the Bolshevik government was stabilised. Several anarchists from Petrograd and Moscow fled to Ukraine, notably leading to the Kronstadt rebellion and Nestor Makhno\\'s struggle in the Free Territory. With the anarchists being crushed in Russia, two new antithetical currents emerged, namely platformism and synthesis anarchism. The former sought to create a coherent group that would push for revolution while the latter were against anything that would resemble a political party. Seeing the victories of the Bolsheviks in the October Revolution and the resulting Russian Civil War, many workers and activists turned to communist parties which grew at the expense of anarchism and other socialist movements. In France and the United States, members of major syndicalist movements such as the General Confederation of Labour and the Industrial Workers of the World left their organisations and joined the Communist International.\\n\\nIn the Spanish Civil War of 1936, anarchists and syndicalists (CNT and FAI) once again allied themselves with various currents of leftists. A long tradition of Spanish anarchism led to anarchists playing a pivotal role in the war. In response to the army rebellion, an anarchist-inspired movement of peasants and workers, supported by armed militias, took control of Barcelona and of large areas of rural Spain, where they collectivised the land. The Soviet Union provided some limited assistance at the beginning of the war, but the result was a bitter fight among communists and anarchists at a series of events named May Days as Joseph Stalin tried to seize control of the Republicans.\\n\\nPost-war era \\n\\nAt the end of World War II, the anarchist movement was severely weakened. The 1960s witnessed a revival of anarchism, likely caused by a perceived failure of Marxism\u2013Leninism and tensions built by the Cold War. During this time, anarchism found a presence in other movements critical towards both capitalism and the state such as the anti-nuclear, environmental, and peace movements, the counterculture of the 1960s, and the New Left. It also saw a transition from its previous revolutionary nature to provocative anti-capitalist reformism. Anarchism became associated with punk subculture as exemplified by bands such as Crass and the Sex Pistols. The established feminist tendencies of anarcha-feminism returned with vigour during the second wave of feminism. Black anarchism began to take form at this time and influenced anarchism\\'s move from a Eurocentric demographic. This coincided with its failure to gain traction in Northern Europe and its unprecedented height in Latin America.\\n\\nAround the turn of the 21st century, anarchism grew in popularity and influence within anti-capitalist, anti-war and anti-globalisation movements. Anarchists became known for their involvement in protests against the World Trade Organization (WTO), the Group of Eight and the World Economic Forum. During the protests, ad hoc leaderless anonymous cadres known as black blocs engaged in rioting, property destruction and violent confrontations with the police. Other organisational tactics pioneered in this time include affinity groups, security culture and the use of decentralised technologies such as the Internet. A significant event of this period was the confrontations at the 1999 Seattle WTO conference. Anarchist ideas have been influential in the development of the Zapatistas in Mexico and the Democratic Federation of Northern Syria, more commonly known as Rojava, a de facto autonomous region in northern Syria.\\n\\nThought \\n\\nAnarchist schools of thought have been generally grouped into two main historical traditions, social anarchism and individualist anarchism, owing to their different origins, values and evolution. The individualist current emphasises negative liberty in opposing restraints upon the free individual, while the social current emphasises positive liberty in aiming to achieve the free potential of society through equality and social ownership. In a chronological sense, anarchism can be segmented by the classical currents of the late 19th century and the post-classical currents (anarcha-feminism, green anarchism, and post-anarchism) developed thereafter.\\n\\nBeyond the specific factions of anarchist movements which constitute political anarchism lies philosophical anarchism which holds that the state lacks moral legitimacy, without necessarily accepting the imperative of revolution to eliminate it. A component especially of individualist anarchism, philosophical anarchism may tolerate the existence of a minimal state but claims that citizens have no moral obligation to obey government when it conflicts with individual autonomy. Anarchism pays significant attention to moral arguments since ethics have a central role in anarchist philosophy. Anarchism\\'s emphasis on anti-capitalism, egalitarianism, and for the extension of community and individuality sets it apart from anarcho-capitalism and other types of economic libertarianism.\\n\\nAnarchism is usually placed on the far-left of the political spectrum. Much of its economics and legal philosophy reflect anti-authoritarian, anti-statist, libertarian, and radical interpretations of left-wing and socialist politics such as collectivism, communism, individualism, mutualism, and syndicalism, among other libertarian socialist economic theories. As anarchism does not offer a fixed body of doctrine from a single particular worldview, many anarchist types and traditions exist and varieties of anarchy diverge widely. One reaction against sectarianism within the anarchist milieu was anarchism without adjectives, a call for toleration and unity among anarchists first adopted by Fernando Tarrida del M\u00e1rmol in 1889 in response to the bitter debates of anarchist theory at the time. Belief in political nihilism has been espoused by anarchists. Despite separation, the various anarchist schools of thought are not seen as distinct entities but rather as tendencies that intermingle and are connected through a set of uniform principles such as individual and local autonomy, mutual aid, network organisation, communal democracy, justified authority and decentralisation.\\n\\nClassical \\n\\nInceptive currents among classical anarchist currents were mutualism and individualism. They were followed by the major currents of social anarchism (collectivist, communist and syndicalist). They differ on organisational and economic aspects of their ideal society.\\n\\nMutualism is an 18th-century economic theory that was developed into anarchist theory by Pierre-Joseph Proudhon. Its aims include reciprocity, free association, voluntary contract, federation and monetary reform of both credit and currency that would be regulated by a bank of the people. Mutualism has been retrospectively characterised as ideologically situated between individualist and collectivist forms of anarchism. In What Is Property? (1840), Proudhon first characterised his goal as a \"third form of society, the synthesis of communism and property.\" Collectivist anarchism is a revolutionary socialist form of anarchism commonly associated with Mikhail Bakunin. Collectivist anarchists advocate collective ownership of the means of production which is theorised to be achieved through violent revolution and that workers be paid according to time worked, rather than goods being distributed according to need as in communism. Collectivist anarchism arose alongside Marxism but rejected the dictatorship of the proletariat despite the stated Marxist goal of a collectivist stateless society.\\n\\nAnarcho-communism is a theory of anarchism that advocates a communist society with common ownership of the means of production, direct democracy and a horizontal network of voluntary associations, workers\\' councils and worker cooperatives, with production and consumption based on the guiding principle \"From each according to his ability, to each according to his need.\" Anarcho-communism developed from radical socialist currents after the French Revolution but was first formulated as such in the Italian section of the First International. It was later expanded upon in the theoretical work of Peter Kropotkin, whose specific style would go onto become the dominating view of anarchists by the late 19th century. Anarcho-syndicalism is a branch of anarchism that views labour syndicates as a potential force for revolutionary social change, replacing capitalism and the state with a new society democratically self-managed by workers. The basic principles of anarcho-syndicalism are direct action, workers\\' solidarity and workers\\' self-management.\\n\\nIndividualist anarchism is a set of several traditions of thought within the anarchist movement that emphasise the individual and their will over any kinds of external determinants. Early influences on individualist forms of anarchism include William Godwin, Max Stirner, and Henry David Thoreau. Through many countries, individualist anarchism attracted a small yet diverse following of Bohemian artists and intellectuals as well as young anarchist outlaws in what became known as illegalism and individual reclamation.\\n\\nPost-classical and contemporary \\n\\nAnarchist principles undergird contemporary radical social movements of the left. Interest in the anarchist movement developed alongside momentum in the anti-globalisation movement, whose leading activist networks were anarchist in orientation. As the movement shaped 21st century radicalism, wider embrace of anarchist principles signaled a revival of interest. Anarchism has continued to generate many philosophies and movements, at times eclectic, drawing upon various sources and combining disparate concepts to create new philosophical approaches. The anti-capitalist tradition of classical anarchism has remained prominent within contemporary currents.\\n\\nContemporary news coverage which emphasizes black bloc demonstrations has reinforced anarchism\\'s historical association with chaos and violence. Its publicity has also led more scholars in fields such as anthropology and history to engage with the anarchist movement, although contemporary anarchism favours actions over academic theory. Various anarchist groups, tendencies, and schools of thought exist today, making it difficult to describe the contemporary anarchist movement. While theorists and activists have established \"relatively stable constellations of anarchist principles\", there is no consensus on which principles are core and commentators describe multiple anarchisms, rather than a singular anarchism, in which common principles are shared between schools of anarchism while each group prioritizes those principles differently. Gender equality can be a common principle, although it ranks as a higher priority to anarcha-feminists than anarcho-communists.\\n\\nAnarchists are generally committed against coercive authority in all forms, namely \"all centralized and hierarchical forms of government (e.g., monarchy, representative democracy, state socialism, etc.), economic class systems (e.g., capitalism, Bolshevism, feudalism, slavery, etc.), autocratic religions (e.g., fundamentalist Islam, Roman Catholicism, etc.), patriarchy, heterosexism, white supremacy, and imperialism.\" Anarchist schools disagree on the methods by which these forms should be opposed. The principle of equal liberty is closer to anarchist political ethics in that it transcends both the liberal and socialist traditions. This entails that liberty and equality cannot be implemented within the state, resulting in the questioning of all forms of domination and hierarchy.\\n\\nTactics \\nAnarchists\\' tactics take various forms but in general serve two major goals, namely to first oppose the Establishment and secondly to promote anarchist ethics and reflect an anarchist vision of society, illustrating the unity of means and ends. A broad categorisation can be made between aims to destroy oppressive states and institutions by revolutionary means on one hand and aims to change society through evolutionary means on the other. Evolutionary tactics embrace nonviolence, reject violence and take a gradual approach to anarchist aims, although there is significant overlap between the two.\\n\\nAnarchist tactics have shifted during the course of the last century. Anarchists during the early 20th century focused more on strikes and militancy while contemporary anarchists use a broader array of approaches.\\n\\nClassical era tactics \\n\\nDuring the classical era, anarchists had a militant tendency. Not only did they confront state armed forces, as in Spain and Ukraine, but some of them also employed terrorism as propaganda of the deed. Assassination attempts were carried out against heads of state, some of which were successful. Anarchists also took part in revolutions. Many anarchists, especially the Galleanists, believed that these attempts would be the impetus for a revolution against capitalism and the state. Many of these attacks were done by individual assailants and the majority took place in the late 1870s, the early 1880s and the 1890s, with some still occurring in the early 1900s. Their decrease in prevalence was the result of further judicial power and targeting and cataloging by state institutions.\\n\\nAnarchist perspectives towards violence have always been controversial. Anarcho-pacifists advocate for non-violence means to achieve their stateless, nonviolent ends. Other anarchist groups advocate direct action, a tactic which can include acts of sabotage or terrorism. This attitude was quite prominent a century ago when seeing the state as a tyrant and some anarchists believing that they had every right to oppose its oppression by any means possible. Emma Goldman and Errico Malatesta, who were proponents of limited use of violence, stated that violence is merely a reaction to state violence as a necessary evil.\\n\\nAnarchists took an active role in strike actions, although they tended to be antipathetic to formal syndicalism, seeing it as reformist. They saw it as a part of the movement which sought to overthrow the state and capitalism. Anarchists also reinforced their propaganda within the arts, some of whom practiced naturism and nudism. Those anarchists also built communities which were based on friendship and were involved in the news media.\\n\\nRevolutionary tactics \\n\\nIn the current era, Italian anarchist Alfredo Bonanno, a proponent of insurrectionary anarchism, has reinstated the debate on violence by rejecting the nonviolence tactic adopted since the late 19th century by Kropotkin and other prominent anarchists afterwards. Both Bonanno and the French group The Invisible Committee advocate for small, informal affiliation groups, where each member is responsible for their own actions but works together to bring down oppression utilizing sabotage and other violent means against state, capitalism, and other enemies. Members of The Invisible Committee were arrested in 2008 on various charges, terrorism included.\\n\\nOverall, contemporary anarchists are much less violent and militant than their ideological ancestors. They mostly engage in confronting the police during demonstrations and riots, especially in countries such as Canada, Greece, and Mexico. Militant black bloc protest groups are known for clashing with the police; however, anarchists not only clash with state operators, they also engage in the struggle against fascists and racists, taking anti-fascist action and mobilizing to prevent hate rallies from happening.\\n\\nEvolutionary tactics \\nAnarchists commonly employ direct action. This can take the form of disrupting and protesting against unjust hierarchy, or the form of self-managing their lives through the creation of counter-institutions such as communes and non-hierarchical collectives. Decision-making is often handled in an anti-authoritarian way, with everyone having equal say in each decision, an approach known as horizontalism. Contemporary-era anarchists have been engaging with various grassroots movements that are more or less based on horizontalism, although not explicitly anarchist, respecting personal autonomy and participating in mass activism such as strikes and demonstrations. In contrast with the big-A anarchism of the classical era, the newly coined term small-a anarchism signals their tendency not to base their thoughts and actions on classical-era anarchism or to refer to classical anarchists such as Peter Kropotkin and Pierre-Joseph Proudhon to justify their opinions. Those anarchists would rather base their thought and praxis on their own experience which they will later theorize.\\n\\nThe decision-making process of small anarchist affinity groups plays a significant tactical role. Anarchists have employed various methods in order to build a rough consensus among members of their group without the need of a leader or a leading group. One way is for an individual from the group to play the role of facilitator to help achieve a consensus without taking part in the discussion themselves or promoting a specific point. Minorities usually accept rough consensus, except when they feel the proposal contradicts anarchist ethics, goals and values. Anarchists usually form small groups (5\u201320 individuals) to enhance autonomy and friendships among their members. These kinds of groups more often than not interconnect with each other, forming larger networks. Anarchists still support and participate in strikes, especially wildcat strikes as these are leaderless strikes not organised centrally by a syndicate.\\n\\nAs in the past, newspapers and journals are used, and anarchists have gone online in the World Wide Web to spread their message. Anarchists have found it easier to create websites because of distributional and other difficulties, hosting electronic libraries and other portals. Anarchists were also involved in developing various software that are available for free. The way these hacktivists work to develop and distribute resembles the anarchist ideals, especially when it comes to preserving users\\' privacy from state surveillance.\\n\\nAnarchists organize themselves to squat and reclaim public spaces. During important events such as protests and when spaces are being occupied, they are often called Temporary Autonomous Zones (TAZ), spaces where art, poetry, and surrealism are blended to display the anarchist ideal. As seen by anarchists, squatting is a way to regain urban space from the capitalist market, serving pragmatical needs and also being an exemplary direct action. Acquiring space enables anarchists to experiment with their ideas and build social bonds. Adding up these tactics while having in mind that not all anarchists share the same attitudes towards them, along with various forms of protesting at highly symbolic events, make up a carnivalesque atmosphere that is part of contemporary anarchist vividity.\\n\\nKey issues \\n\\nAs anarchism is a philosophy that embodies many diverse attitudes, tendencies, and schools of thought; disagreement over questions of values, ideology, and tactics is common. Its diversity has led to widely different uses of identical terms among different anarchist traditions which has created a number of definitional concerns in anarchist theory. The compatibility of capitalism, nationalism, and religion with anarchism is widely disputed, and anarchism enjoys complex relationships with ideologies such as communism, collectivism, Marxism, and trade unionism. Anarchists may be motivated by humanism, divine authority, enlightened self-interest, veganism, or any number of alternative ethical doctrines. Phenomena such as civilisation, technology (e.g. within anarcho-primitivism), and the democratic process may be sharply criticised within some anarchist tendencies and simultaneously lauded in others.\\n\\nGender, sexuality, and free love \\n\\nAs gender and sexuality carry along them dynamics of hierarchy, many anarchists address, analyse, and oppose the suppression of one\\'s autonomy imposed by gender roles.\\n\\nSexuality was not often discussed by classical anarchists but the few that did felt that an anarchist society would lead to sexuality naturally developing. Sexual violence was a concern for anarchists such as Benjamin Tucker, who opposed age of consent laws, believing they would benefit predatory men. A historical current that arose and flourished during 1890 and 1920 within anarchism was free love. In contemporary anarchism, this current survives as a tendency to support polyamory and queer anarchism. Free love advocates were against marriage, which they saw as a way of men imposing authority over women, largely because marriage law greatly favoured the power of men. The notion of free love was much broader and included a critique of the established order that limited women\\'s sexual freedom and pleasure. Those free love movements contributed to the establishment of communal houses, where large groups of travelers, anarchists and other activists slept in beds together. Free love had roots both in Europe and the United States; however, some anarchists struggled with the jealousy that arose from free love. Anarchist feminists were advocates of free love, against marriage, and pro-choice (utilising a contemporary term), and had a similar agenda. Anarchist and non-anarchist feminists differed on suffrage but were supportive of one another.\\n\\nDuring the second half of the 20th century, anarchism intermingled with the second wave of feminism, radicalising some currents of the feminist movement and being influenced as well. By the latest decades of the 20th century, anarchists and feminists were advocating for the rights and autonomy of women, gays, queers and other marginalised groups, with some feminist thinkers suggesting a fusion of the two currents. With the third wave of feminism, sexual identity and compulsory heterosexuality became a subject of study for anarchists, yielding a post-structuralist critique of sexual normality. Some anarchists distanced themselves from this line of thinking, suggesting that it leaned towards an individualism that was dropping the cause of social liberation.\\n\\nAnarchism and education \\n\\nThe interest of anarchists in education stretches back to the first emergence of classical anarchism. Anarchists consider proper education, one which sets the foundations of the future autonomy of the individual and the society, to be an act of mutual aid. Anarchist writers such as William Godwin (Political Justice) and Max Stirner (\"The False Principle of Our Education\") attacked both state education and private education as another means by which the ruling class replicate their privileges.\\n\\nIn 1901, Catalan anarchist and free thinker Francisco Ferrer established the Escuela Moderna in Barcelona as an opposition to the established education system which was dictated largely by the Catholic Church. Ferrer\\'s approach was secular, rejecting both state and church involvement in the educational process whilst giving pupils large amounts of autonomy in planning their work and attendance. Ferrer aimed to educate the working class and explicitly sought to foster class consciousness among students. The school closed after constant harassment by the state and Ferrer was later arrested. Nonetheless, his ideas formed the inspiration for a series of modern schools around the world. Christian anarchist Leo Tolstoy, who published the essay Education and Culture, also established a similar school with its founding principle being that \"for education to be effective it had to be free.\" In a similar token, A. S. Neill founded what became the Summerhill School in 1921, also declaring being free from coercion.\\n\\nAnarchist education is based largely on the idea that a child\\'s right to develop freely and without manipulation ought to be respected and that rationality would lead children to morally good conclusions; however, there has been little consensus among anarchist figures as to what constitutes manipulation. Ferrer believed that moral indoctrination was necessary and explicitly taught pupils that equality, liberty and social justice were not possible under capitalism, along with other critiques of government and nationalism.\\n\\nLate 20th century and contemporary anarchist writers (Paul Goodman, Herbert Read, and Colin Ward) intensified and expanded the anarchist critique of state education, largely focusing on the need for a system that focuses on children\\'s creativity rather than on their ability to attain a career or participate in consumerism as part of a consumer society. Contemporary anarchists such as Ward claim that state education serves to perpetuate socioeconomic inequality.\\n\\nWhile few anarchist education institutions have survived to the modern-day, major tenets of anarchist schools, among them respect for child autonomy and relying on reasoning rather than indoctrination as a teaching method, have spread among mainstream educational institutions. Judith Suissa names three schools as explicitly anarchists schools, namely the Free Skool Santa Cruz in the United States which is part of a wider American-Canadian network of schools, the Self-Managed Learning College in Brighton, England, and the Paideia School in Spain.\\n\\nAnarchism and the state \\n\\nObjection to the state and its institutions is a sine qua non of anarchism. Anarchists consider the state as a tool of domination and believe it to be illegitimate regardless of its political tendencies. Instead of people being able to control the aspects of their life, major decisions are taken by a small elite. Authority ultimately rests solely on power, regardless of whether that power is open or transparent, as it still has the ability to coerce people. Another anarchist argument against states is that the people constituting a government, even the most altruistic among officials, will unavoidably seek to gain more power, leading to corruption. Anarchists consider the idea that the state is the collective will of the people to be an unachievable fiction due to the fact that the ruling class is distinct from the rest of society.\\n\\nSpecific anarchist attitudes towards the state vary. Robert Paul Wolff believed that the tension between authority and autonomy would mean the state could never be legitimate. Bakunin saw the state as meaning \"coercion, domination by means of coercion, camouflaged if possible but unceremonious and overt if need be.\" A. John Simmons and Leslie Green, who leaned toward philosophical anarchism, believed that the state could be legitimate if it is governed by consensus, although they saw this as highly unlikely. Beliefs on how to abolish the state also differ.\\n\\nAnarchism and the arts \\n\\nThe connection between anarchism and art was quite profound during the classical era of anarchism, especially among artistic currents that were developing during that era such as futurists, surrealists and others. In literature, anarchism was mostly associated with the New Apocalyptics and the neo-romanticism movement. In music, anarchism has been associated with music scenes such as punk. Anarchists such as Leo Tolstoy and Herbert Read stated that the border between the artist and the non-artist, what separates art from a daily act, is a construct produced by the alienation caused by capitalism and it prevents humans from living a joyful life.\\n\\nOther anarchists advocated for or used art as a means to achieve anarchist ends. In his book Breaking the Spell: A History of Anarchist Filmmakers, Videotape Guerrillas, and Digital Ninjas, Chris Rob\u00e9 claims that \"anarchist-inflected practices have increasingly structured movement-based video activism.\" Throughout the 20th century, many prominent anarchists (Peter Kropotkin, Emma Goldman, Gustav Landauer and Camillo Berneri) and publications such as Anarchy wrote about matters pertaining to the arts.\\n\\nThree overlapping properties made art useful to anarchists. It could depict a critique of existing society and hierarchies, serve as a prefigurative tool to reflect the anarchist ideal society and even turn into a means of direct action such as in protests. As it appeals to both emotion and reason, art could appeal to the whole human and have a powerful effect. The 19th-century neo-impressionist movement had an ecological aesthetic and offered an example of an anarchist perception of the road towards socialism. In Les chataigniers a Osny by anarchist painter Camille Pissarro, the blending of aesthetic and social harmony is prefiguring an ideal anarchistic agrarian community.\\n\\nAnalysis \\nThe most common critique of anarchism is that humans cannot self-govern and so a state is necessary for human survival. Philosopher Bertrand Russell supported this critique, stating that \"[p]eace and war, tariffs, regulations of sanitary conditions and the sale of noxious drugs, the preservation of a just system of distribution: these, among others, are functions which could hardly be performed in a community in which there was no central government.\" Another common criticism of anarchism is that it fits a world of isolation in which only the small enough entities can be self-governing; a response would be that major anarchist thinkers advocated anarchist federalism.\\n\\nPhilosophy lecturer Andrew G. Fiala composed a list of common arguments against anarchism which includes critiques such as that anarchism is innately related to violence and destruction, not only in the pragmatic world, such as at protests, but in the world of ethics as well. Secondly, anarchism is evaluated as unfeasible or utopian since the state cannot be defeated practically. This line of arguments most often calls for political action within the system to reform it. The third argument is that anarchism is self-contradictory. While it advocates for no-one to archiei, if accepted by the many, then anarchism would turn into the ruling political theory. In this line of criticism also comes the self-contradiction that anarchism calls for collective action whilst endorsing the autonomy of the individual, hence no collective action can be taken. Lastly, Fiala mentions a critique towards philosophical anarchism of being ineffective (all talk and thoughts) and in the meantime capitalism and bourgeois class remains strong.\\n\\nPhilosophical anarchism has met the criticism of members of academia following the release of pro-anarchist books such as A. John Simmons\\' Moral Principles and Political Obligations. Law professor William A. Edmundson authored an essay to argue against three major philosophical anarchist principles which he finds fallacious. Edmundson says that while the individual does not owe the state a duty of obedience, this does not imply that anarchism is the inevitable conclusion and the state is still morally legitimate. In The Problem of Political Authority, Michael Huemer defends philosophical anarchism, claiming that \"political authority is a moral illusion.\"\\n\\nOne of the earliest criticisms is that anarchism defies and fails to understand the biological inclination to authority. Joseph Raz states that the acceptance of authority implies the belief that following their instructions will afford more success. Raz believes that this argument is true in following both authorities\\' successful and mistaken instruction. Anarchists reject this criticism because challenging or disobeying authority does not entail the disappearance of its advantages by acknowledging authority such as doctors or lawyers as reliable, nor does it involve a complete surrender of independent judgment. Anarchist perception of human nature, rejection of the state, and commitment to social revolution has been criticised by academics as naive, overly simplistic, and unrealistic, respectively. Classical anarchism has been criticised for relying too heavily on the belief that the abolition of the state will lead to human cooperation prospering.\\n\\nFriedrich Engels, considered to be one of the principal founders of Marxism, criticised anarchism\\'s anti-authoritarianism as inherently counter-revolutionary because in his view a revolution is by itself authoritarian. Academic John Molyneux writes in his book Anarchism: A Marxist Criticism that \"anarchism cannot win\", believing that it lacks the ability to properly implement its ideas. The Marxist criticism of anarchism is that it has a utopian character because all individuals should have anarchist views and values. According to the Marxist view, that a social idea would follow directly from this human ideal and out of the free will of every individual formed its essence. Marxists state that this contradiction was responsible for their inability to act. In the anarchist vision, the conflict between liberty and equality was resolved through coexistence and intertwining.\\n\\nSee also \\n\\n Anarchism by country\\n Governance without government\\n List of anarchist political ideologies\\n List of books about anarchism\\n\\nReferences\\n\\nCitations\\n\\nNotes\\n\\nSources\\n\\nPrimary sources\\n\\nSecondary sources\\n\\nTertiary sources\\n\\nFurther reading \\n \\n  Criticism of philosophical anarchism.\\n \\n  A defence of philosophical anarchism, stating that \"both kinds of \\'anarchism\\' [i.e. philosophical and political anarchism] are philosophical and political claims.\" (p.\\xa0137)\\n  Anarchistic popular fiction novel.\\n \\n \\n \\n  An argument for philosophical anarchism.\\n\\nExternal links \\n Anarchy Archives. Anarchy Archives is an online research center on the history and theory of anarchism.\\n\\n \\nAnti-capitalism\\nAnti-fascism\\nEconomic ideologies\\nLeft-wing politics\\nLibertarian socialism\\nLibertarianism\\nPolitical culture\\nPolitical movements\\nPolitical ideologies\\nSocial theories\\nSocialism\\nFar-left politics'}</pre> In\u00a0[39]: Copied! <pre>next(iter(fr['train']))\n</pre> next(iter(fr['train'])) Out[39]: <pre>{'id': '3',\n 'url': 'https://fr.wikipedia.org/wiki/Antoine%20Meillet',\n 'title': 'Antoine Meillet',\n 'text': \"Paul Jules Antoine Meillet, n\u00e9 le  \u00e0 Moulins (Allier) et mort le  \u00e0 Ch\u00e2teaumeillant (Cher), est le principal linguiste fran\u00e7ais des premi\u00e8res d\u00e9cennies du . Il est aussi philologue.\\n\\nBiographie \\nD'origine bourbonnaise, fils d'un notaire de Ch\u00e2teaumeillant (Cher), Antoine Meillet fait ses \u00e9tudes secondaires au lyc\u00e9e de Moulins.\\n\\n\u00c9tudiant \u00e0 la facult\u00e9 des lettres de Paris \u00e0 partir de 1885 o\u00f9 il suit notamment les cours de Louis Havet, il assiste \u00e9galement \u00e0 ceux de Michel Br\u00e9al au Coll\u00e8ge de France et de Ferdinand de Saussure \u00e0 l'\u00c9cole pratique des hautes \u00e9tudes.\\n\\nEn 1889, il est major de l'agr\u00e9gation de grammaire.\\n\\nIl assure \u00e0 la suite de Saussure le cours de grammaire compar\u00e9e, qu'il compl\u00e8te \u00e0 partir de 1894 par une conf\u00e9rence sur les langues persanes.\\n\\nEn 1897, il soutient sa th\u00e8se pour le doctorat \u00e8s lettres (Recherches sur l'emploi du g\u00e9nitif-accusatif en vieux-slave). En 1905, il occupe la chaire de grammaire compar\u00e9e au Coll\u00e8ge de France, o\u00f9 il consacre ses cours \u00e0 l'histoire et \u00e0 la structure des langues indo-europ\u00e9ennes. Il succ\u00e9da au linguiste Auguste Carri\u00e8re \u00e0 la t\u00eate de la chaire d'arm\u00e9nien \u00e0 l'\u00c9cole des langues orientales.\\n\\nSecr\u00e9taire de la Soci\u00e9t\u00e9 de linguistique de Paris, il est \u00e9lu \u00e0 l'Acad\u00e9mie des inscriptions et belles-lettres en 1924. Il pr\u00e9side \u00e9galement l'Institut d'\u00c9tudes Slaves de 1921 \u00e0 sa mort.\\n\\nIl a form\u00e9 toute une g\u00e9n\u00e9ration de linguistes fran\u00e7ais, parmi lesquels \u00c9mile Benveniste, Marcel Cohen, Georges Dum\u00e9zil, Andr\u00e9 Martinet, Aur\u00e9lien Sauvageot, Lucien Tesni\u00e8re, Joseph Vendryes, ainsi que le japonisant Charles Haguenauer. Antoine Meillet devait diriger la th\u00e8se de Jean Paulhan sur la s\u00e9mantique du proverbe et c'est lui qui d\u00e9couvrit Gustave Guillaume.\\n\\nIl a influenc\u00e9 aussi un certain nombre de linguistes \u00e9trangers. Il a \u00e9galement \u00e9t\u00e9 le premier \u00e0 identifier le ph\u00e9nom\u00e8ne de la grammaticalisation.\\n\\nSelon le linguiste allemand Walter Porzig, Meillet est un \u00ab grand pr\u00e9curseur \u00bb. Il montre, par exemple, que, dans les dialectes indo-europ\u00e9ens, les groupes indo-europ\u00e9ens sont le r\u00e9sultat historique d'une variation diatopique.\\n\\nL\u2019acte de naissance de la sociolinguistique est sign\u00e9 par Antoine Meillet fondateur de la sociolinguistique qui s\u2019est oppos\u00e9 au Cours de linguistique g\u00e9n\u00e9rale de Ferdinand de Saussure d\u00e8s son apparition en 1916 en le critiquant sur plusieurs plans.\\n\\n\u00c9tudes arm\u00e9niennes \\n 1890 : une mission de trois mois dans le Caucase lui permet d'apprendre l'arm\u00e9nien moderne.\\n 1902 : il obtient la chaire d'arm\u00e9nien de l'\u00c9cole des langues orientales.\\n 1903 : nouvelle mission en Arm\u00e9nie russe, il publie son Esquisse d'une grammaire compar\u00e9e de l'arm\u00e9nien classique, qui demeure une r\u00e9f\u00e9rence en linguistique arm\u00e9nienne et indo-europ\u00e9enne jusqu'\u00e0 ce jour. L'un de ses \u00e9tudiants, Hratchia Adjarian, devient le fondateur de la dialectologie arm\u00e9nienne. C'est \u00e9galement sous les encouragements de Meillet qu'\u00c9mile Benveniste \u00e9tudie la langue arm\u00e9nienne.\\n 1919 : il est cofondateur de la Soci\u00e9t\u00e9 des \u00e9tudes arm\u00e9niennes avec Victor B\u00e9rard, Charles Diehl, Andr\u00e9-Ferdinand H\u00e9rold, H. Lacroix, Fr\u00e9d\u00e9ric Macler, Gabriel Millet, Gustave Schlumberger.\\n 1920 : le , il cr\u00e9e la Revue des \u00e9tudes arm\u00e9niennes avec Fr\u00e9d\u00e9ric Macler.\\n\\n\u00c9tudes hom\u00e9riques \\n\u00c0 la Sorbonne, Meillet supervise le travail de Milman Parry. Meillet offre \u00e0 son \u00e9tudiant l'opinion, nouvelle \u00e0 cette \u00e9poque, que la structure formula\u00efque de l'Iliade serait une cons\u00e9quence directe de sa transmission orale. Ainsi, il le dirige vers l'\u00e9tude de l'oralit\u00e9 dans son cadre natif et lui sugg\u00e8re d'observer les m\u00e9canismes d'une tradition orale vivante \u00e0 c\u00f4t\u00e9 du texte classique (l'Iliade) qui est cens\u00e9 r\u00e9sulter d'une telle tradition. En cons\u00e9quence, Meillet pr\u00e9sente Parry \u00e0 Matija Murko, savant originaire de Slov\u00e9nie qui avait longuement \u00e9crit sur la tradition h\u00e9ro\u00efque \u00e9pique dans les Balkans, surtout en Bosnie-Herz\u00e9govine. Par leurs recherches, dont les r\u00e9sultats sont \u00e0 pr\u00e9sent h\u00e9berg\u00e9s par l'universit\u00e9 de Harvard, Parry et son \u00e9l\u00e8ve, Albert Lord, ont profond\u00e9ment renouvel\u00e9 les \u00e9tudes hom\u00e9riques.\\n\\nPrincipaux ouvrages \\n \u00c9tudes sur l'\u00e9tymologie et le vocabulaire du vieux slave. Paris, Bouillon, 1902-05.\\n Esquisse d'une grammaire compar\u00e9e de l'arm\u00e9nien classique, 1903.\\n Introduction \u00e0 l'\u00e9tude comparative des langues indo-europ\u00e9ennes, 1903 ( \u00e9d.), Hachette, Paris, 1912 ( \u00e9d.).\\n Les dialectes indo-europ\u00e9ens, 1908.\\n Aper\u00e7u d'une histoire de la langue grecque, 1913.\\n Altarmenisches Elementarbuch, 1913. Heidelberg (en fran\u00e7ais : Manuel \u00e9l\u00e9mentaire d'Arm\u00e9nien classique, traduction de Gabriel K\u00e9p\u00e9klian, Limoges, Lambert-Lucas, 2017 )\\n Caract\u00e8res g\u00e9n\u00e9raux des langues germaniques, 1917, rev. edn. 1949.\\n Linguistique historique et linguistique g\u00e9n\u00e9rale, 1921 (le tome II est paru en 1936 ; les deux tomes ont \u00e9t\u00e9 r\u00e9unis chez Lambert-Lucas, Limoges, 2015).\\n Les origines indo-europ\u00e9ennes des m\u00e8tres grecs, 1923.\\n Trait\u00e9 de grammaire compar\u00e9e des langues classiques, 1924 (avec Joseph Vendry\u00e9s).\\n La m\u00e9thode comparative en linguistique historique, 1925, Oslo, Instituttet for Sammenlignende Kulturforskning (r\u00e9impr. Paris, Champion, 1954).\\n .\\n Dictionnaire \u00e9tymologique de la langue latine, 1932 (en collab. Avec Alfred Ernout (1879-1973), \u00e9d. augment\u00e9e, par Jacques Andr\u00e9 (1910-1994), Paris : Klincksieck, 2001,  \\n Meillet en Arm\u00e9nie, 1891, 1903, Journaux et lettres publi\u00e9s par Francis Gandon, Limoges, Lambert-Lucas, 2014, .\\n\\nNotes et r\u00e9f\u00e9rences\\n\\nVoir aussi\\n\\nBibliographie \\n Marc D\u00e9cimo, Sciences et pataphysique, t. 2 : Comment la linguistique vint \u00e0 Paris ?, De Michel Br\u00e9al \u00e0 Ferdinand de Saussure, Dijon, Les Presses du r\u00e9el, coll. Les H\u00e9t\u00e9roclites, 2014 .\\n\\nArticles connexes \\n Franz Bopp\\n Johann Kaspar Zeuss\\n\\nLiens externes \\n \\n \\n \\n\\nCommandeur de la L\u00e9gion d'honneur\\nAcad\u00e9mie des inscriptions et belles-lettres\\nAgr\u00e9g\u00e9 de grammaire\\nLinguiste fran\u00e7ais\\nPhilologue fran\u00e7ais\\nSlaviste\\nPersonnalit\u00e9 li\u00e9e \u00e0 la langue kurde\\nInstitut national des langues et civilisations orientales\\nArm\u00e9nologue fran\u00e7ais\\nIndo-europ\u00e9aniste\\n\u00c9tudiant de l'universit\u00e9 de Paris\\nNaissance en novembre 1866\\nNaissance \u00e0 Moulins (Allier)\\nD\u00e9c\u00e8s en septembre 1936\\nD\u00e9c\u00e8s \u00e0 69 ans\\nD\u00e9c\u00e8s dans le Cher\\nPersonnalit\u00e9 inhum\u00e9e \u00e0 Moulins\"}</pre> In\u00a0[\u00a0]: Copied! <pre>import os\nimport lancedb\nimport getpass\nfrom lancedb.embeddings import EmbeddingFunctionRegistry\nfrom lancedb.pydantic import LanceModel, Vector\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ['OPENAI_API_KEY'] = getpass.getpass(\"Enter your OpenAI API key: \")\n    \nregistry = EmbeddingFunctionRegistry().get_instance()\nopenai = registry.get(\"openai\").create() # uses multi-lingual model by default (768 dim)\n\nclass Schema(LanceModel):\n    vector: Vector(openai.ndims()) = openai.VectorField()\n    text: str = openai.SourceField()\n    url: str\n    title: str\n    id: str\n    lang: str\n\ndb = lancedb.connect(\"~/lancedb\")\ntbl_openai = db.create_table(\"wikipedia-openai\", schema=Schema, mode=\"overwrite\")\n</pre> import os import lancedb import getpass from lancedb.embeddings import EmbeddingFunctionRegistry from lancedb.pydantic import LanceModel, Vector  if \"OPENAI_API_KEY\" not in os.environ:     os.environ['OPENAI_API_KEY'] = getpass.getpass(\"Enter your OpenAI API key: \")      registry = EmbeddingFunctionRegistry().get_instance() openai = registry.get(\"openai\").create() # uses multi-lingual model by default (768 dim)  class Schema(LanceModel):     vector: Vector(openai.ndims()) = openai.VectorField()     text: str = openai.SourceField()     url: str     title: str     id: str     lang: str  db = lancedb.connect(\"~/lancedb\") tbl_openai = db.create_table(\"wikipedia-openai\", schema=Schema, mode=\"overwrite\") In\u00a0[29]: Copied! <pre>import os\nimport lancedb\nimport getpass\nfrom lancedb.embeddings import EmbeddingFunctionRegistry\nfrom lancedb.pydantic import LanceModel, Vector\n\nif \"COHERE_API_KEY\" not in os.environ:\n    os.environ['COHERE_API_KEY'] = getpass.getpass(\"Enter your Cohere API key: \")\n    \nregistry = EmbeddingFunctionRegistry().get_instance()\ncohere = registry.get(\"cohere\").create() # uses multi-lingual model by default (768 dim)\n\nclass Schema(LanceModel):\n    vector: Vector(cohere.ndims()) = cohere.VectorField()\n    text: str = cohere.SourceField()\n    url: str\n    title: str\n    id: str\n    lang: str\n\ndb = lancedb.connect(\"~/lancedb\")\ntbl_cohere = db.create_table(\"wikipedia-cohere\", schema=Schema, mode=\"overwrite\")\n</pre> import os import lancedb import getpass from lancedb.embeddings import EmbeddingFunctionRegistry from lancedb.pydantic import LanceModel, Vector  if \"COHERE_API_KEY\" not in os.environ:     os.environ['COHERE_API_KEY'] = getpass.getpass(\"Enter your Cohere API key: \")      registry = EmbeddingFunctionRegistry().get_instance() cohere = registry.get(\"cohere\").create() # uses multi-lingual model by default (768 dim)  class Schema(LanceModel):     vector: Vector(cohere.ndims()) = cohere.VectorField()     text: str = cohere.SourceField()     url: str     title: str     id: str     lang: str  db = lancedb.connect(\"~/lancedb\") tbl_cohere = db.create_table(\"wikipedia-cohere\", schema=Schema, mode=\"overwrite\") In\u00a0[30]: Copied! <pre>from tqdm.auto import tqdm\nimport time\n# let's use cohere embeddings. Use can also set it to openai version of the table\ntbl = tbl_cohere\nbatch_size = 1000\nnum_records = 10000\ndata = []\n\nfor i in tqdm(range(0, num_records, batch_size)):\n\n    for lang, dataset in datasets.items():\n        \n        batch = [next(dataset) for _ in range(batch_size)]\n        \n        texts = [x['text'] for x in batch]\n        ids = [f\"{x['id']}-{lang}\" for x in batch]\n        data.extend({\n           'text': x['text'], 'title': x['title'], 'url': x['url'], 'lang': lang, 'id': f\"{lang}-{x['id']}\"\n        } for x in batch)\n\n    # add in batches to avoid token limit\n    tbl.add(data)\n    data = []\n    time.sleep(20) # wait for 20 seconds to avoid rate limit\n</pre> from tqdm.auto import tqdm import time # let's use cohere embeddings. Use can also set it to openai version of the table tbl = tbl_cohere batch_size = 1000 num_records = 10000 data = []  for i in tqdm(range(0, num_records, batch_size)):      for lang, dataset in datasets.items():                  batch = [next(dataset) for _ in range(batch_size)]                  texts = [x['text'] for x in batch]         ids = [f\"{x['id']}-{lang}\" for x in batch]         data.extend({            'text': x['text'], 'title': x['title'], 'url': x['url'], 'lang': lang, 'id': f\"{lang}-{x['id']}\"         } for x in batch)      # add in batches to avoid token limit     tbl.add(data)     data = []     time.sleep(20) # wait for 20 seconds to avoid rate limit <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [06:10&lt;00:00, 37.08s/it]\n</pre> In\u00a0[12]: Copied! <pre>it = iter(fr['train'])\nfor i in range(5):\n    next(it)\nquery = next(it)\nquery\n</pre> it = iter(fr['train']) for i in range(5):     next(it) query = next(it) query Out[12]: <pre>{'id': '12',\n 'url': 'https://fr.wikipedia.org/wiki/Arm%C3%A9e%20r%C3%A9publicaine%20irlandaise',\n 'title': 'Arm\u00e9e r\u00e9publicaine irlandaise',\n 'text': \"L'Arm\u00e9e r\u00e9publicaine irlandaise (, IRA ; ) est le nom port\u00e9, depuis le d\u00e9but du , par plusieurs organisations paramilitaires luttant par les armes contre la pr\u00e9sence britannique en Irlande du Nord. Les diff\u00e9rents groupes se r\u00e9f\u00e9rent \u00e0 eux comme \u00d3glaigh na h\u00c9ireann (\u00ab volontaires d'Irlande \u00bb).\\n\\n L' appel\u00e9e aussi Old IRA, issue de l'union en 1916 entre l' (proche du Parti travailliste irlandais) et les Irish Volunteers (alors g\u00e9n\u00e9ralement proches de l'IRB), est active entre  et , pendant la guerre d'ind\u00e9pendance irlandaise. Si ceux qui ont accept\u00e9 le trait\u00e9 anglo-irlandais forment les Forces de D\u00e9fense irlandaises, une partie de l'organisation, refusant cet accord, se constitue en une nouvelle Irish Republican Army, ill\u00e9gale.\\n L'Irish Republican Army anti-trait\u00e9 appara\u00eet entre avril et  du fait du refus du trait\u00e9 anglo-irlandais par une partie de l'Old IRA. Elle participe ainsi \u00e0 la guerre civile irlandaise de  \u00e0 . Elle maintient son activit\u00e9 dans les deux Irlandes (\u00c9tat libre d'Irlande, ind\u00e9pendant, et Irlande du Nord, britannique), mais concentre son action sur les int\u00e9r\u00eats britanniques, surtout en Irlande du Nord. En 1969 l'organisation se divise, donnant naissance \u00e0 lOfficial Irish Republican Army et \u00e0 la Provisional Irish Republican Army, minoritaire, moins socialiste et plus activiste.\\n LOfficial Irish Republican Army, proche de l'''Official Sinn F\u00e9in, plus socialiste et moins nationaliste que la Provisional Irish Republican Army, m\u00e8ne des campagnes d'attentats principalement entre 1969 et 1972 durant le conflit nord-irlandais, avant de d\u00e9cr\u00e9ter un cessez-le-feu.\\n La Provisional Irish Republican Army, minoritaire apr\u00e8s la scission de 1969 (d'o\u00f9 son nom de provisional, \u00ab\\xa0provisoire\\xa0\u00bb) devient rapidement gr\u00e2ce \u00e0 son militantisme la principale organisation arm\u00e9e r\u00e9publicaine du conflit nord-irlandais. Le terme de provisional est d'ailleurs abandonn\u00e9 vers la fin des ann\u00e9es 1970. Elle fut active de 1969 \u00e0 1997 (date du cessez-le-feu d\u00e9finitif), puis d\u00e9posa d\u00e9finitivement les armes en 2005. Refusant le processus de paix, deux organisations scissionn\u00e8rent d'avec la PIRA : la Real Irish Republican Army et la Continuity Irish Republican Army.\\n La Continuity Irish Republican Army est issue d'une scission d'avec la Provisional Irish Republican Army d\u00e8s 1986. Oppos\u00e9e \u00e0 l'accord du Vendredi saint de 1997, elle continue son action arm\u00e9e jusqu'\u00e0 aujourd'hui.\\n La Real Irish Republican Army est une scission oppos\u00e9e au processus de paix de la Provisional Irish Republican Army, apparue en 1997 et encore active aujourd'hui.\\n LIrish Republican Liberation Army na\u00eet en 2006 d'une scission de la Continuity Irish Republican Army''.\\n\\nG\u00e9n\u00e9alogie de l'Irish Republican Army\"}</pre> <p>Let's take the first line from the above text body:</p> <pre><code>L'Arm\u00e9e r\u00e9publicaine irlandaise (, IRA ; ) est le nom port\u00e9, depuis le d\u00e9but du , par plusieurs organisations paramilitaires luttant par les armes contre la pr\u00e9sence britannique en Irlande du Nord.\n</code></pre> <p>This translates to the following in english</p> <pre><code>The Irish Republican Army (, IRA; ) is the name worn, since the beginning of the 19th century, by several paramilitary organizations fighting with arms against the British presence in Northern Ireland.\n</code></pre> <p>Let us now see what at the results that are semantically closer to this in our dataset.</p> In\u00a0[21]: Copied! <pre>import os\nimport getpass\nimport lancedb\n\nif \"COHERE_API_KEY\" not in os.environ:\n    os.environ['COHERE_API_KEY'] = getpass.getpass(\"Enter your Cohere API key: \")\n</pre> import os import getpass import lancedb  if \"COHERE_API_KEY\" not in os.environ:     os.environ['COHERE_API_KEY'] = getpass.getpass(\"Enter your Cohere API key: \")  <p>You can now load the table even in a different session and anything ingest or search will be automatically vectorized. Let us now run the query.</p> In\u00a0[23]: Copied! <pre>%%timeit\n\ndb = lancedb.connect(\"~/lancedb\")\ntbl = db.open_table(\"wikipedia-cohere\") # We just open the existing\nrs = tbl.search(query[\"text\"]).limit(3).to_list()\n</pre> %%timeit  db = lancedb.connect(\"~/lancedb\") tbl = db.open_table(\"wikipedia-cohere\") # We just open the existing rs = tbl.search(query[\"text\"]).limit(3).to_list() <pre>469 ms \u00b1 39.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> In\u00a0[24]: Copied! <pre>for r in rs:\n    print(f\" **TEXT id-{r['id']}** \\n {r['text']} \\n\")\n#\n</pre> for r in rs:     print(f\" **TEXT id-{r['id']}** \\n {r['text']} \\n\") # <pre> **TEXT id-french-12** \n L'Arm\u00e9e r\u00e9publicaine irlandaise (, IRA ; ) est le nom port\u00e9, depuis le d\u00e9but du , par plusieurs organisations paramilitaires luttant par les armes contre la pr\u00e9sence britannique en Irlande du Nord. Les diff\u00e9rents groupes se r\u00e9f\u00e9rent \u00e0 eux comme \u00d3glaigh na h\u00c9ireann (\u00ab volontaires d'Irlande \u00bb).\n\n L' appel\u00e9e aussi Old IRA, issue de l'union en 1916 entre l' (proche du Parti travailliste irlandais) et les Irish Volunteers (alors g\u00e9n\u00e9ralement proches de l'IRB), est active entre  et , pendant la guerre d'ind\u00e9pendance irlandaise. Si ceux qui ont accept\u00e9 le trait\u00e9 anglo-irlandais forment les Forces de D\u00e9fense irlandaises, une partie de l'organisation, refusant cet accord, se constitue en une nouvelle Irish Republican Army, ill\u00e9gale.\n L'Irish Republican Army anti-trait\u00e9 appara\u00eet entre avril et  du fait du refus du trait\u00e9 anglo-irlandais par une partie de l'Old IRA. Elle participe ainsi \u00e0 la guerre civile irlandaise de  \u00e0 . Elle maintient son activit\u00e9 dans les deux Irlandes (\u00c9tat libre d'Irlande, ind\u00e9pendant, et Irlande du Nord, britannique), mais concentre son action sur les int\u00e9r\u00eats britanniques, surtout en Irlande du Nord. En 1969 l'organisation se divise, donnant naissance \u00e0 lOfficial Irish Republican Army et \u00e0 la Provisional Irish Republican Army, minoritaire, moins socialiste et plus activiste.\n LOfficial Irish Republican Army, proche de l'''Official Sinn F\u00e9in, plus socialiste et moins nationaliste que la Provisional Irish Republican Army, m\u00e8ne des campagnes d'attentats principalement entre 1969 et 1972 durant le conflit nord-irlandais, avant de d\u00e9cr\u00e9ter un cessez-le-feu.\n La Provisional Irish Republican Army, minoritaire apr\u00e8s la scission de 1969 (d'o\u00f9 son nom de provisional, \u00ab\u00a0provisoire\u00a0\u00bb) devient rapidement gr\u00e2ce \u00e0 son militantisme la principale organisation arm\u00e9e r\u00e9publicaine du conflit nord-irlandais. Le terme de provisional est d'ailleurs abandonn\u00e9 vers la fin des ann\u00e9es 1970. Elle fut active de 1969 \u00e0 1997 (date du cessez-le-feu d\u00e9finitif), puis d\u00e9posa d\u00e9finitivement les armes en 2005. Refusant le processus de paix, deux organisations scissionn\u00e8rent d'avec la PIRA : la Real Irish Republican Army et la Continuity Irish Republican Army.\n La Continuity Irish Republican Army est issue d'une scission d'avec la Provisional Irish Republican Army d\u00e8s 1986. Oppos\u00e9e \u00e0 l'accord du Vendredi saint de 1997, elle continue son action arm\u00e9e jusqu'\u00e0 aujourd'hui.\n La Real Irish Republican Army est une scission oppos\u00e9e au processus de paix de la Provisional Irish Republican Army, apparue en 1997 et encore active aujourd'hui.\n LIrish Republican Liberation Army na\u00eet en 2006 d'une scission de la Continuity Irish Republican Army''.\n\nG\u00e9n\u00e9alogie de l'Irish Republican Army \n\n **TEXT id-english-14732** \n The Irish Republican Army (IRA; ) was an Irish republican revolutionary paramilitary organisation. The ancestor of many groups also known as the Irish Republican Army, and distinguished from them as the \"Old IRA\", it was descended from the Irish Volunteers, an organisation established on 25 November 1913 that staged the Easter Rising in April 1916. In 1919, the Irish Republic that had been proclaimed during the Easter Rising was formally established by an elected assembly (D\u00e1il \u00c9ireann), and the Irish Volunteers were recognised by D\u00e1il \u00c9ireann as its legitimate army. Thereafter, the IRA waged a guerrilla campaign against the British occupation of Ireland in the 1919\u20131921 Irish War of Independence.\n\nFollowing the signing in 1921 of the Anglo-Irish Treaty, which ended the War of Independence, a split occurred within the IRA. Members who supported the treaty formed the nucleus of the Irish National Army. However, the majority of the IRA was opposed to the treaty. The anti-treaty IRA fought a civil war against the Free State Army in 1922\u201323, with the intention of creating a fully independent all-Ireland republic. Having lost the civil war, this group remained in existence, with the intention of overthrowing the governments of both the Irish Free State and Northern Ireland and achieving the Irish Republic proclaimed in 1916.\n\nOrigins\n\nThe Irish Volunteers, founded in 1913, staged the Easter Rising, which aimed at ending British rule in Ireland, in 1916. Following the suppression of the Rising, thousands of Volunteers were imprisoned or interned, leading to the break-up of the organisation. It was reorganised in 1917 following the release of first the internees and then the prisoners. At the army convention held in Dublin in October 1917, \u00c9amon de Valera was elected president, Michael Collins Director for Organisation and Cathal Brugha Chairman of the Resident Executive, which in effect made him Chief of Staff.\n\nFollowing the success of Sinn F\u00e9in in the general election of 1918 and the setting up of the First D\u00e1il (the legislature of the Irish Republic), Volunteers commenced military action against the Royal Irish Constabulary (RIC), the paramilitary police force in Ireland, and subsequently against the British Army. It began with the Soloheadbeg Ambush, when members of the Third Tipperary Brigade led by S\u00e9umas Robinson, Se\u00e1n Treacy, Dan Breen and Se\u00e1n Hogan, seized a quantity of gelignite, killing two RIC constables in the process.\n\nThe D\u00e1il leadership worried that the Volunteers would not accept its authority, given that, under their own constitution, they were bound to obey their own executive and no other body. In August 1919, Brugha proposed to the D\u00e1il that the Volunteers be asked to swear allegiance to the D\u00e1il, but one commentator states that another year passed before the movement took an oath of allegiance to the Irish Republic and its government in \"August 1920\". In sharp contrast, a contemporary in the struggle for Irish independence notes that by late 1919, the term \"Irish Republican Army (IRA)\" was replacing \"Volunteers\" in everyday usage. This change is attributed to the Volunteers, having accepted the authority of the D\u00e1il, being referred to as the \"army of the Irish Republic\", popularly known as the \"Irish Republican Army\".\n\nA power struggle continued between Brugha and Collins, both cabinet ministers, over who had the greater influence. Brugha was nominally the superior as Minister for Defence, but Collins's power base came from his position as Director of Organisation of the IRA and from his membership on the Supreme Council of the Irish Republican Brotherhood (IRB). De Valera resented Collins's clear power and influence, which he saw as coming more from the secretive IRB than from his position as a Teachta D\u00e1la (TD) and minister in the Aireacht. Brugha and de Valera both urged the IRA to undertake larger, more conventional military actions for the propaganda effect but were ignored by Collins and Mulcahy. Brugha at one stage proposed the assassination of the entire British cabinet. This was also discounted due to its presumed negative effect on British public opinion. Moreover, many members of the D\u00e1il, notably Arthur Griffith, did not approve of IRA violence and would have preferred a campaign of passive resistance to the British rule. The D\u00e1il belatedly accepted responsibility for IRA actions in April 1921, just three months before the end of the Irish War of Independence.\n\nIn practice, the IRA was commanded by Collins, with Richard Mulcahy as second in command. These men were able to issue orders and directives to IRA guerrilla units around the country and at times to send arms and organisers to specific areas. However, because of the localised and irregular character of the war, they were only able to exert limited control over local IRA commanders such as Tom Barry, Liam Lynch in Cork and Se\u00e1n Mac Eoin in Longford.\n\nThe IRA claimed a total strength of 70,000, but only about 3,000 were actively engaged in fighting against the Crown. The IRA distrusted those Irishmen who had fought in the British Army during the First World War as potential informers, but there were a number of exceptions such as Emmet Dalton, Tom Barry and Martin Doyle. The IRA divided its members into three classes, namely \"unreliable\", \"reliable\" and \"active\". The \"unreliable\" members were those who were nominally IRA members but did not do very much for the struggle, \"reliable\" members played a supporting role in the war while occasionally fighting and the \"active\" men those who were engaged in full-time fighting. Of the IRA brigades only about one to two-thirds were considered to be \"reliable\" while those considered \"active\" were even smaller. A disproportionate number of the \"active\" IRA men were teachers, medical students, shoemakers and bootmakers; those engaged in building trades like painters, carpenters and bricklayers; draper's assistants and creamery workers. The Canadian historian Peter Hart wrote \"...the guerrillas were disproportionately skilled, trained and urban\". Farmers and fishermen tended to be underrepresented in the IRA. Those Irishmen engaged in white-collar trades or working as skilled labourers were much more likely to be involved in cultural nationalist groups like the Gaelic League than farmers or fishermen, and thus to have a stronger sense of Irish nationalism. Furthermore, the authority of the Crown tended to be stronger in towns and cities than in the countryside. Thus, those engaged in Irish nationalist activities in urban areas were much more likely to come into conflict with the Crown, leading to a greater chance of radicalisation. Finally, the British tactic of blowing up the homes of IRA members had the effect of discouraging many farmers from joining the struggle as the destruction of the family farm could easily reduce a farmer and his family to destitution. Of the \"active\" IRA members, three-quarters were in their late teens or early 20s and only 5% of the \"active\" men were in the age range of 40 or older. The \"active\" members were overwhelmingly single men with only 4% being married or engaged in a relationship. The life of an \"active\" IRA man with its stress of living on the run and constantly being in hiding tended to attract single men who could adjust to this lifestyle far more easily than a man in a relationship. Furthermore, the IRA preferred to recruit single men as it was found that singles could devote themselves more wholeheartedly to the struggle.\n\nWomen were active in the republican movement, but almost no women fought with the IRA whose \"active\" members were almost entirely male. The IRA was not a sectarian group and went out of its way to proclaim it was open to all Irishmen, but its membership was largely Catholic with virtually no Protestants serving as \"active\" IRA men. Hart wrote that in his study of the IRA membership that he found only three Protestants serving as \"active\" IRA men between 1919 and 1921. Of the 917 IRA men convicted by British courts under the Defence of the Realm Act in 1919, only one was a Protestant. The majority of those serving in the IRA were practising Catholics, but there was a large minority of \"pagans\" as atheists or non-practising Catholics were known in Ireland. The majority of the IRA men serving in metropolitan Britain were permanent residents with very few sent over from Ireland. The majority of the IRA men operating in Britain were Irish-born, but there a substantial minority who were British-born, something that made them especially insistent on asserting their Irish identity.\n\nIrish War of Independence\n\nIRA campaign and organisation\n\nThe IRA fought a guerrilla war against the Crown forces in Ireland from 1919 to July 1921. The most intense period of the war was from November 1920 onwards. The IRA campaign can broadly be split into three phases. The first, in 1919, involved the re-organisation of the Irish Volunteers as a guerrilla army and only sporadic attacks. Organisers such as Ernie O'Malley were sent around the country to set up viable guerrilla units. On paper, there were 100,000 or so Volunteers enrolled after the conscription crisis of 1918. However, only about 15,000 of these participated in the guerrilla war. In 1919, Collins, the IRA's Director of Intelligence, organised the \"Squad\"\u2014an assassination unit based in Dublin which killed police involved in intelligence work (the Irish playwright Brendan Behan's father Stephen Behan was a member of the Squad). Typical of Collins's sardonic sense of humour, the Squad was often referred to as his \"Twelve Apostles\". In addition, there were some arms raids on RIC barracks. By the end of 1919, four Dublin Metropolitan Police and 11 RIC men had been killed. The RIC abandoned most of their smaller rural barracks in late 1919. Around 400 of these were burned in a co-ordinated IRA operation around the country in April 1920.\n\nThe second phase of the IRA campaign, roughly from January to July 1920, involved attacks on the fortified police barracks located in the towns. Between January and June 1920, 16 of these were destroyed and 29 badly damaged. Several events of late 1920 greatly escalated the conflict. Firstly, the British declared martial law in parts of the country\u2014allowing for internment and executions of IRA men. Secondly they deployed paramilitary forces, the Black and Tans and Auxiliary Division, and more British Army personnel into the country. Thus, the third phase of the war (roughly August 1920 \u2013 July 1921) involved the IRA taking on a greatly expanded British force, moving away from attacking well-defended barracks and instead using ambush tactics. To this end the IRA was re-organised into \"flying columns\"\u2014permanent guerrilla units, usually about 20 strong, although sometimes larger. In rural areas, the flying columns usually had bases in remote mountainous areas.\n\nThe most high-profile violence of the war took place in Dublin in November 1920 and is still known as Bloody Sunday. In the early hours of the morning, Collins' \"Squad\" killed fourteen British spies. In reprisal, that afternoon, British forces opened fire on a football crowd at Croke Park, killing 14 civilians. Towards the end of the day, two prominent Republicans and a friend of theirs were arrested and killed by Crown Forces.\n\nWhile most areas of the country saw some violence in 1919\u20131921, the brunt of the war was fought in Dublin and the southern province of Munster. In Munster, the IRA carried out a significant number of successful actions against British troops, for instance, the ambushing and killing of 16 of 18 Auxiliaries by Tom Barry's column at Kilmicheal in West Cork in November 1920, or Liam Lynch's men killing 13 British soldiers near Millstreet early in the next year. At the Crossbarry Ambush in March 1921, 100 or so of Barry's men fought a sizeable engagement with a British column of 1,200, escaping from the British encircling manoeuvre. In Dublin, the \"Squad\" and elements of the IRA Dublin Brigade were amalgamated into the \"Active Service Unit\", under Oscar Traynor, which tried to carry out at least three attacks on British troops a day. Usually, these consisted of shooting or grenade attacks on British patrols. Outside Dublin and Munster, there were only isolated areas of intense activity. For instance, the County Longford IRA under Se\u00e1n Mac Eoin carried out a number of well-planned ambushes and successfully defended the village of Ballinalee against Black and Tan reprisals in a three-hour gun battle. In County Mayo, large-scale guerrilla action did not break out until spring 1921, when two British forces were ambushed at Carrowkennedy and Tourmakeady. Elsewhere, fighting was more sporadic and less intense.\n\nIn Belfast, the war had a character all of its own. The city had a Protestant and unionist majority and IRA actions were responded to with reprisals against the Catholic population, including killings (such as the McMahon killings) and the burning of many homes \u2013 as on Belfast's Bloody Sunday. The IRA in Belfast and the North generally, although involved in protecting the Catholic community from loyalists and state forces, undertook a retaliatory arson campaign against factories and commercial premises. The violence in Belfast alone, which continued until October 1922 (long after the truce in the rest of the country), claimed the lives of between 400 and 500 people.\n\nIn April 1921, the IRA was again reorganised, in line with the D\u00e1il's endorsement of its actions, along the lines of a regular army. Divisions were created based on region, with commanders being given responsibility, in theory, for large geographical areas. In practice, this had little effect on the localised nature of the guerrilla warfare.\n\nIn May 1921, the IRA in Dublin attacked and burned the Custom House. The action was a serious setback as five members were killed and eighty captured.\n\nBy the end of the war in July 1921, the IRA was hard-pressed by the deployment of more British troops into the most active areas and a chronic shortage of arms and ammunition. It has been estimated that the IRA had only about 3,000 rifles (mostly captured from the British) during the war, with a larger number of shotguns and pistols. An ambitious plan to buy arms from Italy in 1921 collapsed when the money did not reach the arms dealers. Towards the end of the war, some Thompson submachine guns were imported from the United States; however 450 of these were intercepted by the American authorities and the remainder only reached Ireland shortly before the Truce.\n\nBy June 1921, Collins' assessment was that the IRA was within weeks, possibly even days, of collapse. It had few weapons or ammunition left. Moreover, almost 5,000 IRA men had been imprisoned or interned and over 500 killed. Collins and Mulcahy estimated that the number of effective guerrilla fighters was down to 2,000\u20133,000. However, in the summer of 1921, the war was abruptly ended.\n\nThe British recruited hundreds of World War I veterans into the RIC and sent them to Ireland. Because there was initially a shortage of RIC uniforms, the veterans at first wore a combination of dark green RIC uniforms and khaki British Army uniforms, which inspired the nickname \"Black and Tans\". The brutality of the Black and Tans is now well-known, although the greatest violence attributed to the Crown's forces was often that of the Auxiliary Division of the Constabulary. One of the strongest critics of the Black and Tans was King George V who in May 1921 told Lady Margery Greenwood that \"he hated the idea of the Black and Tans.\"\n\nThe IRA was also involved in the destruction of many stately homes in Munster. The Church of Ireland Gazette recorded numerous instances of Unionists and Loyalists being shot, burnt or forced from their homes during the early 1920s. In County Cork between 1920 and 1923 the IRA shot over 200 civilians of whom over 70 (or 36%) were Protestants: five times the percentage of Protestants in the civilian population. This was due to the historical inclination of Protestants towards loyalty to the United Kingdom. A convention of Irish Protestant Churches in Dublin in May 1922 signed a resolution placing \"on record\" that \"hostility to Protestants by reason of their religion has been almost, if not wholly, unknown in the twenty-six counties in which Protestants are in the minority.\"\n\nMany historic buildings in Ireland were destroyed during the war, most famously the Custom House in Dublin, which was disastrously attacked on de Valera's insistence, to the horror of the more militarily experienced Collins. As he feared, the destruction proved a pyrrhic victory for the Republic, with so many IRA men killed or captured that the IRA in Dublin suffered a severe blow.\n\nThis was also a period of social upheaval in Ireland, with frequent strikes as well as other manifestations of class conflict. In this regard, the IRA acted to a large degree as an agent of social control and stability, driven by the need to preserve cross-class unity in the national struggle, and on occasion being used to break strikes.\n\nAssessments of the effectiveness of the IRA's campaign vary. They were never in a position to engage in conventional warfare. The political, military and financial costs of remaining in Ireland were higher than the British government was prepared to pay and this in a sense forced them into negotiations with the Irish political leaders. According to historian Michael Hopkinson, the guerrilla warfare \"was often courageous and effective\". Historian David Fitzpatrick observes, \"The guerrilla fighters...were vastly outnumbered by the forces of the Crown... The success of the Irish Volunteers in surviving so long is therefore noteworthy.\"\n\nTruce and treaty\n\nDavid Lloyd George, the British Prime Minister, at the time, found himself under increasing pressure (both internationally and from within the British Isles) to try to salvage something from the situation. This was a complete reversal on his earlier position. He had consistently referred to the IRA as a \"murder gang\" up until then. An unexpected olive branch came from King George V, who, in a speech in Belfast called for reconciliation on all sides, changed the mood and enabled the British and Irish Republican governments to agree to a truce. The Truce was agreed on 11 July 1921. On 8 July, de Valera met General Nevil Macready, the British commander in chief in Ireland and agreed terms. The IRA was to retain its arms and the British Army was to remain in barracks for the duration of peace negotiations. Many IRA officers interpreted the truce only as a temporary break in fighting. They continued to recruit and train volunteers, with the result that the IRA had increased its number to over 72,000 men by early 1922.\n\nNegotiations on an Anglo-Irish Treaty took place in late 1921 in London. The Irish delegation was led by Arthur Griffith and Michael Collins.\n\nThe most contentious areas of the Treaty for the IRA were abolition of the Irish Republic declared in 1919, the status of the Irish Free State as a dominion in the British Commonwealth and the British retention of the so-called Treaty Ports on Ireland's south coast. These issues were the cause of a split in the IRA and ultimately, the Irish Civil War.\n\nUnder the Government of Ireland Act 1920, Ireland was partitioned, creating Northern Ireland and Southern Ireland. Under the terms of the Anglo-Irish agreement of 6 December 1921, which ended the war (1919\u201321), Northern Ireland was given the option of withdrawing from the new state, the Irish Free State, and remaining part of the United Kingdom. The Northern Ireland parliament chose to do that. An Irish Boundary Commission was then set up to review the border.\n\nIrish leaders expected that it would so reduce Northern Ireland's size, by transferring nationalist areas to the Irish Free State, as to make it economically unviable. Partition was not by itself the key breaking point between pro- and anti-Treaty campaigners; both sides expected the Boundary Commission to greatly reduce Northern Ireland. Moreover, Michael Collins was planning a clandestine guerrilla campaign against the Northern state using the IRA. In early 1922, he sent IRA units to the border areas and sent arms to northern units. It was only afterwards, when partition was confirmed, that a united Ireland became the preserve of anti-Treaty Republicans.\n\nIRA and the Anglo-Irish Treaty\n\nThe IRA leadership was deeply divided over the decision by the D\u00e1il to ratify the Treaty. Despite the fact that Michael Collins \u2013 the de facto leader of the IRA \u2013 had negotiated the Treaty, many IRA officers were against it. Of the General Headquarters (GHQ) staff, nine members were in favour of the Treaty while four opposed it. The majority of the IRA rank-and-file were against the Treaty; in January\u2013June 1922, their discontent developed into open defiance of the elected civilian Provisional government of Ireland.\n\nBoth sides agreed that the IRA's allegiance was to the (elected) D\u00e1il of the Irish Republic, but the anti-Treaty side argued that the decision of the D\u00e1il to accept the Treaty (and set aside the Irish Republic) meant that the IRA no longer owed that body its allegiance. They called for the IRA to withdraw from the authority of the D\u00e1il and to entrust the IRA Executive with control over the army. On 16 January, the first IRA division \u2013 the 2nd Southern Division led by Ernie O'Malley \u2013 repudiated the authority of the GHQ. A month later, on 18 February, Liam Forde, O/C of the IRA Mid-Limerick Brigade, issued a proclamation stating that: \"We no longer recognise the authority of the present head of the army, and renew our allegiance to the existing Irish Republic\". This was the first unit of the IRA to break with the pro-Treaty government.\n\nOn 22 March, Rory O'Connor held what was to become an infamous press conference and declared that the IRA would no longer obey the D\u00e1il as (he said) it had violated its Oath to uphold the Irish Republic. He went on to say that \"we repudiate the D\u00e1il ... We will set up an Executive which will issue orders to the IRA all over the country.\" In reply to the question on whether this meant they intended to create a military dictatorship, O'Connor said: \"You can take it that way if you like.\"\n\nOn 28 March, the (anti-Treaty) IRA Executive issued statement stating that Minister of Defence (Richard Mulcahy) and the Chief-of-Staff (Eoin O'Duffy) no longer exercised any control over the IRA. In addition, it ordered an end to the recruitment to the new military and police forces of the Provisional Government. Furthermore, it instructed all IRA units to reaffirm their allegiance to the Irish Republic on 2 April.\nThe stage was set for civil war over the Treaty.\n\nCivil War\n\nThe pro-treaty IRA soon became the nucleus of the new (regular) Irish National Army created by Collins and Richard Mulcahy. British pressure, and tensions between the pro- and anti-Treaty factions of the IRA, led to a bloody civil war, ending in the defeat of the anti-Treaty faction. On 24 May 1923, Frank Aiken, the (anti-treaty) IRA Chief-of-Staff, called a cease-fire. Many left political activity altogether, but a minority continued to insist that the new Irish Free State, created by the \"illegitimate\" Treaty, was an illegitimate state.  They asserted that their \"IRA Army Executive\" was the real government of a still-existing Irish Republic. The IRA of the Civil War and subsequent organisations that have used the name claim lineage from that group, which is covered in full at Irish Republican Army (1922\u20131969).\n\nFor information on later organisations using the name Irish Republican Army, see the table below. For a genealogy of organisations using the name IRA after 1922, see List of organisations known as the Irish Republican Army.\n\nSee also\nList of films featuring the Irish Republican Army\n\nReferences\n\nBibliography\n\nFurther reading\n\nExternal links\n\nBureau of Military History, 1913-1921 at militaryarchives.ie\nIrish Volunteers History, 1913-1922 at IVCO\n\n \nInstitutions of the Irish Republic (1919\u20131922)\nGuerrilla organizations\nIrish republican militant groups\nNational liberation armies\nAnti-imperialism in Europe \n\n **TEXT id-english-5859** \n The Continuity Irish Republican Army (Continuity IRA or CIRA), styling itself as the Irish Republican Army (), is an Irish republican paramilitary group that aims to bring about a united Ireland. It claims to be a direct continuation of the original Irish Republican Army and the national army of the Irish Republic that was proclaimed in 1916. It emerged from a split in the Provisional IRA in 1986 but did not become active until the Provisional IRA ceasefire of 1994. It is an illegal organisation in the Republic of Ireland and is designated a terrorist organisation in the United Kingdom, New Zealand and the United States. It has links with the political party Republican Sinn F\u00e9in (RSF).\n\nSince 1994, the CIRA has waged a campaign in Northern Ireland against the British Army and the Police Service of Northern Ireland (PSNI), formerly the Royal Ulster Constabulary. This is part of a wider campaign against the British security forces by dissident republican paramilitaries. It has targeted the security forces in gun attacks and bombings, as well as with grenades, mortars and rockets. The CIRA has also carried out bombings with the goal of causing economic harm and/or disruption, as well as many punishment attacks on alleged criminals.\n\nTo date, it has been responsible for the death of one PSNI officer. The CIRA is smaller and less active than the Real IRA, and there have been a number of splits within the organisation since the mid-2000s.\n\nOrigins\nThe Continuity IRA has its origins in a split in the Provisional IRA. In September 1986, the Provisional IRA held a General Army Convention (GAC), the organisation's supreme decision-making body. It was the first GAC in 16 years. The meeting, which like all such meetings was secret, was convened to discuss among other resolutions, the articles of the Provisional IRA constitution which dealt with abstentionism, specifically its opposition to the taking of seats in D\u00e1il \u00c9ireann (the parliament of the Republic of Ireland). The GAC passed motions (by the necessary two-thirds majority) allowing members of the Provisional IRA to discuss and debate the taking of parliamentary seats, and the removal of the ban on members of the organisation from supporting any successful republican candidate who took their seat in D\u00e1il \u00c9ireann.\n\nThe Provisional IRA convention delegates opposed to the change in the constitution claimed that the convention was gerrymandered \"by the creation of new IRA organisational structures for the convention, including the combinations of Sligo-Roscommon-Longford and Wicklow-Wexford-Waterford.\" The only IRA body that supported this viewpoint was the outgoing IRA Executive. Those members of the outgoing Executive who opposed the change comprised a quorum. They met, dismissed those in favour of the change, and set up a new Executive. They contacted Tom Maguire, who was a commander in the old IRA and had supported the Provisionals against the Official IRA (see Irish republican legitimatism), and asked him for support. Maguire had also been contacted by supporters of Gerry Adams, then president of Sinn F\u00e9in, and a supporter of the change in the Provisional IRA constitution.\n\nMaguire rejected Adams' supporters, supported the IRA Executive members opposed to the change, and named the new organisers the Continuity Army Council. In a 1986 statement, he rejected \"the legitimacy of an Army Council styling itself the Council of the Irish Republican Army which lends support to any person or organisation styling itself as Sinn F\u00e9in and prepared to enter the partition parliament of Leinster House.\" In 1987, Maguire described the \"Continuity Executive\" as the \"lawful Executive of the Irish Republican Army.\"\n\nCampaign\n\nInitially, the Continuity IRA did not reveal its existence, either in the form of press statements or paramilitary activity. Although the Garda S\u00edoch\u00e1na had suspicions that the organisation existed, they were unsure of its name, labelling it the \"Irish National Republican Army\". On 21 January 1994, on the 75th anniversary of the First D\u00e1il \u00c9ireann, Continuity IRA volunteers offered a \"final salute\" to Tom Maguire by firing over his grave, and a public statement and a photo were published in Saoirse Irish Freedom. In February 1994 it was reported that in previous months Garda\u00ed had found arms dumps along the Cooley Peninsula in County Louth that did not belong to the Provisional IRA, and forensics tests determined had been used for firing practice recently.\n\nIt was only after the Provisional IRA declared a ceasefire in 1994 that the Continuity IRA became active, announcing its intention to continue the campaign against British rule. The CIRA continues to oppose the Good Friday Agreement and, unlike the Provisional IRA (and the Real IRA in 1998), the CIRA has not announced a ceasefire or agreed to participate in weapons decommissioning\u2014nor is there any evidence that it will. In the 18th Independent Monitoring Commission's report, the RIRA, the CIRA and the Irish National Liberation Army (INLA) were deemed a potential future threat. The CIRA was labelled \"active, dangerous and committed and... capable of a greater level of violent and other crime\". Like the RIRA and RIRA splinter group \u00d3glaigh na h\u00c9ireann, it too sought funds for expansion. It is also known to have worked with the INLA.\n\nThe CIRA has been involved in a number of bombing and shooting incidents. Targets of the CIRA have included the British military, the Northern Ireland police (both the Royal Ulster Constabulary and its successor the Police Service of Northern Ireland). Since the Good Friday Agreement in 1998 the CIRA, along with other paramilitaries opposing the ceasefire, have been involved with a countless number of punishment shootings and beatings. By 2005 the CIRA was believed to be an established presence on the island of Great Britain with the capability of launching attacks. A bomb defused in Dublin in December 2005 was believed to have been the work of the CIRA. In February 2006, the Independent Monitoring Commission (IMC) blamed the CIRA for planting four bombs in Northern Ireland during the final quarter of 2005, as well as several hoax bomb warnings. The IMC also blamed the CIRA for the killings of two former CIRA members in Belfast, who had stolen CIRA weapons and established a rival organisation.\n\nThe CIRA continued to be active in both planning and undertaking attacks on the PSNI. The IMC said they tried to lure police into ambushes, while they have also taken to stoning and using petrol bombs. In addition, other assaults, robbery, tiger kidnapping, extortion, fuel laundering and smuggling were undertaken by the group. The CIRA also actively took part in recruiting and training members, including disgruntled former Provisional IRA members. As a result of this continued activity the IMC said the group remained \"a very serious threat\".\n\nOn 10 March 2009 the CIRA claimed responsibility for the fatal shooting of a PSNI officer in Craigavon, County Armagh\u2014the first police fatality in Northern Ireland since 1998. The officer was fatally shot by a sniper as he and a colleague investigated \"suspicious activity\" at a house nearby when a window was smashed by youths causing the occupant to phone the police. The PSNI officers responded to the emergency call, giving a CIRA sniper the chance to shoot and kill officer Stephen Carroll. Carroll was killed two days after the Real IRA's 2009 Massereene Barracks shooting at Massereene Barracks in Antrim. In a press interview with Republican Sinn F\u00e9in some days later, regarded by some to be the political wing of the Continuity IRA, Richard Walsh described the attacks as \"acts of war\".\n\nIn 2013, the Continuity IRA's 'South Down Brigade' threatened a Traveller family in Newry and published a statement in the local newspaper. There were negotiations with community representatives and the CIRA announced the threat was lifted. It was believed the threat was issued after a Traveller feud which resulted in a pipe bomb attack in Bessbrook, near Newry. The Continuity IRA is believed to be strongest in the County Fermanagh \u2013 North County Armagh area (Craigavon, Armagh and Lurgan). It is believed to be behind a number of attacks such as pipe bombings, rocket attacks, gun attacks, and the PSNI claimed it orchestrated riots a number of times to lure police officers into areas such as Kilwilkie in Lurgan and Drumbeg in Craigavon in order to attack them. It also claimed the group orchestrated a riot during a security alert in Lurgan. The alert turned out to be a hoax.\n\nOn Easter 2016, the Continuity IRA marched in paramilitary uniforms through North Lurgan, Co Armagh, without any hindrance from the PSNI who monitored the parade from a police helicopter.\n\nIn July and August 2019 the CIRA carried out attempted bomb attacks on the PSNI in Craigavon, County Armagh and Wattlebridge, County Fermanagh.\n\nOn 5 February 2020, a bomb planted by the CIRA was found by the PSNI in a lorry in Lurgan. The CIRA believed the lorry was going to be put on a North Channel ferry to Scotland in January 2020.\n\nClaim to legitimacy\n Similar to the claim put forward by the Provisional IRA after its split from the Official IRA in 1969, the Continuity IRA claims to be the legitimate continuation of the original Irish Republican Army or \u00d3glaigh na h\u00c9ireann. This argument is based on the view that the surviving anti-Treaty members of the Second D\u00e1il delegated their \"authority\" to the IRA Army Council in 1938. As further justification for this claim, Tom Maguire, one of those anti-Treaty members of the Second D\u00e1il, issued a statement in favour of the Continuity IRA, just as he had done in 1969 in favour of the Provisionals. J. Bowyer Bell, in his The Irish Troubles, describes Maguire's opinion in 1986: \"abstentionism was a basic tenet of republicanism, a moral issue of principle. Abstentionism gave the movement legitimacy, the right to wage war, to speak for a Republic all but established in the hearts of the people\". Maguire's stature was such that a delegation from Gerry Adams sought his support in 1986, but was rejected.&lt;ref&gt;Robert W. White, Ruair\u00ed \u00d3 Br\u00e1daigh, The Life and Politics of an Irish Revolutionary, 2006, p. 310.&lt;/ref&gt;\n\nRelationship to other organisations\nThese changes within the IRA were accompanied by changes on the political side and at the 1986 Sinn F\u00e9in Ard Fheis (party conference), which followed the IRA Convention, the party's policy of abstentionism, which forbade Sinn F\u00e9in elected representatives from taking seats in the Oireachtas, the parliament of the Republic, was dropped. On 2 November, the 628 delegates present cast their votes, the result being 429 to 161. The traditionalists, having lost at both conventions, walked out of the Mansion House, met that evening at the West County Hotel, and reformed as Republican Sinn F\u00e9in (RSF).\n\nAccording to a report in the Cork Examiner, the Continuity IRA's first chief of staff was D\u00e1ith\u00ed \u00d3 Conaill, who also served as the first chairman of RSF from 1986 to 1987. The Continuity IRA and RSF perceive themselves as forming a \"true\" Republican Movement.\n\nStructure and status\nThe leadership of the Continuity IRA is believed to be based in the provinces of Munster and Ulster. It was alleged that its chief of staff was a Limerick man and that a number of other key members were from that county, until their expulsion. D\u00e1ith\u00ed \u00d3 Conaill was the first chief of staff until 1991. In 2004 the United States (US) government believed the Continuity IRA consisted of fewer than fifty hardcore activists. In 2005, Irish Minister for Justice, Equality and Law Reform Michael McDowell told D\u00e1il \u00c9ireann that the organisation had a maximum of 150 members.\n\nThe CIRA is an illegal organisation under UK (section 11(1) of the Terrorism Act 2000) and ROI law due to the use of 'IRA' in the group's name, in a situation analogous to that of the Real Irish Republican Army (RIRA). Membership of the organisation is punishable by a sentence of up to ten years imprisonment under UK law. On 31 May 2001 Dermot Gannon became the first person to be convicted of membership of the CIRA solely on the word of a Garda S\u00edoch\u00e1na chief superintendent. On 13 July 2004, the US government designated the CIRA as a 'Foreign Terrorist Organization'. This made it illegal for Americans to provide material support to the CIRA, requires US financial institutions to block the group's assets and denies alleged CIRA members visas into the US.\n\nExternal aid and arsenal\nThe US government suspects the Continuity IRA of having received funds and arms from supporters in the United States. Security sources in Ireland have expressed the suspicion that, in co-operation with the RIRA, the Continuity IRA may have acquired arms and materiel from the Balkans. They also suspect that the Continuity IRA arsenal contains some weapons that were taken from Provisional IRA arms dumps, including a few dozen rifles, machine guns, and pistols; a small amount of the explosive Semtex; and a few dozen detonators.\n\nInternal tension and splits\nIn 2005, several members of the CIRA, who were serving prison sentences in Portlaoise Prison for paramilitary activity, left the organisation. Some transferred to the INLA landing of the prison, but the majority of those who left are now independent and on E4 landing. The remaining CIRA prisoners have moved to D Wing. Supporters of the Continuity IRA leadership claim that this resulted from an internal disagreement, which although brought to a conclusion, was followed by some people leaving the organisation anyway. Supporters of the disaffected members established the Concerned Group for Republican Prisoners. Most of those who had left went back to the CIRA, or dissociated themselves from the CGRP, which is now defunct.\n\nIn February 2006, the Independent Monitoring Commission claimed in a report on paramilitary activity that two groups, styling themselves as \"\u00d3glaigh na h\u00c9ireann\" and \"Saoirse na h\u00c9ireann\", had been formed after a split in the Continuity IRA either in early 2006 or late 2005. The \u00d3glaigh na h\u00c9ireann group was responsible for a number of pipe bomb attacks on the PSNI, bomb hoaxes, and robberies, the IMC also claimed the organisation was responsible for the killing of Andrew Burns on 12 February 2008 and was seeking to recruit former members of the RIRA. The Saoirse na h\u00c9ireann (SNH) group was composed of \"disaffected and largely young republicans\" and was responsible for a number of bomb hoaxes, two of which took place in September 2006. It was thought to have operated largely in republican areas of Belfast . The groups had apparently ceased operations by early 2009.\n\nIn 2007, the Continuity IRA was responsible for shooting dead two of its members who had left and attempted to create their own organisation. Upon leaving the CIRA, they had allegedly taken a number of guns with them. The Continuity IRA is believed by Garda\u00ed to have been involved in a number of gangland killings in Dublin and Limerick.\n\nIn July 2010, members of a \"militant Northern-based faction within the CIRA\" led by a well-known member from south Londonderry claimed to have overthrown the leadership of the organisation. They also claimed that an Army Convention representing \"95 per cent of volunteers\" had unanimously elected a new 12-member Army Executive, which in turn appointed a new seven-member Army Council. The moves came as a result of dissatisfication with the southern-based leadership and the apparent winding-down of military operations. A senior source from RSF said: \"We would see them [the purported new leadership] as just another splinter group that has broken away.\" This organisation is referred to as the Real CIRA.\n\nIn June 2011 CIRA member Liam Kenny was murdered, allegedly by drug dealers, at his home in Clondalkin, West Dublin. On 28 November 2011 an innocent man was mistakenly shot dead in retaliation for the murder of Liam Kenny. Limerick Real IRA volunteer Rose Lynch pleaded guilty to this murder at the Special Criminal Court and was sentenced to life imprisonment.\n\nIn July 2012 the CIRA announced it had a new leadership after expelling members who had been working against the organisation.\n\nIn April 2014 a former leading member of the Belfast Continuity IRA who had been expelled from the organisation, Tommy Crossan, was shot dead.\n\nIn popular culture\nThe CIRA are depicted in RT\u00c9's TV series crime drama Love/Hate''.\n\nNotes\n\nReferences\n\n \nIrish republican militant groups\nOrganised crime groups in Ireland\n1986 establishments in Ireland \n\n</pre> <p>As you can see in the above result, the closest match is the text itself that we used to search. The second closest match in an English text with a similar semantic meaning referring to IRA. This is what a multi-lingual embedding model can do.</p> <p>Find more examples on VectorDB-recipes repo</p>"},{"location":"notebooks/multi_lingual_example/#example-multi-lingual-semantic-search","title":"Example - Multi-lingual semantic search\u00b6","text":""},{"location":"notebooks/multi_lingual_example/#lancedb-embeddings-api-multi-lingual-semantic-search","title":"Lancedb Embeddings API: Multi-lingual semantic search\u00b6","text":"<p>In this example, we'll build a simple LanceDB table containing embeddings for different languages that can be used for universal semantic search.</p> <ul> <li>The Dataset used will be wikipedia dataset in English and French</li> <li>The Model used will be cohere's multi-lingual model</li> </ul> <p>In this example, we'll explore LanceDB's Embeddings API that allows you to create tables that automatically vectorize data once you define the config at the time of table creation. Let's dive right in!</p> <p>To learn more about LanceDB, visit our docs</p>"},{"location":"notebooks/multi_lingual_example/#create-datasets","title":"Create datasets\u00b6","text":"<p>For accessing the datasets, we'll use datasets library in streaming mode. We'll use english and french versions and embed them together. For semantic search the order should be irrelevant</p>"},{"location":"notebooks/multi_lingual_example/#lancedb-embeddings-api","title":"LanceDB Embeddings API\u00b6","text":"<p>Let's see how you can use the embeddings API to create an ingestion pipeline that automatically does all the vectorization for you both when ingesting new data or searching queries.</p>"},{"location":"notebooks/multi_lingual_example/#openai-api-example","title":"OpenAI API Example\u00b6","text":"<p>Let us take a look at openAI example first. LanceDB comes with OpenAI embedding function support.</p> <ul> <li>Create the instance of the available embedding function or create your own</li> <li>Create the scheme of the table, marking source end vector fields. Each embedding function can have multiple source and vector feilds</li> <li>Create a table with schema</li> </ul> <p>Doing this creates a table with where embedding function information is ingested as metadata so you can forget about all the modelling details and focus only ingesting and retrieving data.</p>"},{"location":"notebooks/multi_lingual_example/#cohere-embedding-table","title":"Cohere Embedding Table\u00b6","text":"<p>Now let's see another example using cohere embedding function which is also supported directly by LanceDB. We will follow the same steps.</p>"},{"location":"notebooks/multi_lingual_example/#ingest-data","title":"Ingest data\u00b6","text":"<p>Now, we have the table set up for ingesting the dataset.</p>"},{"location":"notebooks/multi_lingual_example/#searching-multi-lingual-embedding-space","title":"Searching multi-lingual embedding space\u00b6","text":"<p>Let us now search the table with a substring from a random batch in french</p>"},{"location":"notebooks/multi_modal_video_RAG/","title":"Multimodal RAG for video processing using LlamaIndex, OpenAI GPT4V","text":"In\u00a0[\u00a0]: Copied! <pre>%pip install llama-index-vector-stores-lancedb\n%pip install llama-index-multi-modal-llms-openai\n</pre> %pip install llama-index-vector-stores-lancedb %pip install llama-index-multi-modal-llms-openai In\u00a0[\u00a0]: Copied! <pre>%pip install llama-index-multi-modal-llms-openai\n%pip install llama-index-vector-stores-lancedb\n%pip install llama-index-embeddings-clip\n</pre> %pip install llama-index-multi-modal-llms-openai %pip install llama-index-vector-stores-lancedb %pip install llama-index-embeddings-clip In\u00a0[\u00a0]: Copied! <pre>%pip install llama_index ftfy regex tqdm\n%pip install -U openai-whisper\n%pip install git+https://github.com/openai/CLIP.git\n%pip install torch torchvision\n%pip install matplotlib scikit-image\n%pip install lancedb\n%pip install moviepy\n%pip install pytube\n%pip install pydub\n%pip install SpeechRecognition\n%pip install ffmpeg-python\n%pip install soundfile\n</pre> %pip install llama_index ftfy regex tqdm %pip install -U openai-whisper %pip install git+https://github.com/openai/CLIP.git %pip install torch torchvision %pip install matplotlib scikit-image %pip install lancedb %pip install moviepy %pip install pytube %pip install pydub %pip install SpeechRecognition %pip install ffmpeg-python %pip install soundfile In\u00a0[\u00a0]: Copied! <pre>from moviepy.editor import VideoFileClip\nfrom pathlib import Path\nimport speech_recognition as sr\nfrom pytube import YouTube\nfrom pprint import pprint\n</pre> from moviepy.editor import VideoFileClip from pathlib import Path import speech_recognition as sr from pytube import YouTube from pprint import pprint In\u00a0[\u00a0]: Copied! <pre>import os\n\nOPENAI_API_TOKEN = \"\"\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_TOKEN\n</pre> import os  OPENAI_API_TOKEN = \"\" os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_TOKEN In\u00a0[\u00a0]: Copied! <pre>video_url = \"https://www.youtube.com/watch?v=d_qvLDhkg00\"\noutput_video_path = \"./video_data/\"\noutput_folder = \"./mixed_data/\"\noutput_audio_path = \"./mixed_data/output_audio.wav\"\n\nfilepath = output_video_path + \"input_vid.mp4\"\nPath(output_folder).mkdir(parents=True, exist_ok=True)\n</pre> video_url = \"https://www.youtube.com/watch?v=d_qvLDhkg00\" output_video_path = \"./video_data/\" output_folder = \"./mixed_data/\" output_audio_path = \"./mixed_data/output_audio.wav\"  filepath = output_video_path + \"input_vid.mp4\" Path(output_folder).mkdir(parents=True, exist_ok=True) In\u00a0[\u00a0]: Copied! <pre>from PIL import Image\nimport matplotlib.pyplot as plt\nimport os\n\n\ndef plot_images(image_paths):\n    images_shown = 0\n    plt.figure(figsize=(16, 9))\n    for img_path in image_paths:\n        if os.path.isfile(img_path):\n            image = Image.open(img_path)\n\n            plt.subplot(2, 3, images_shown + 1)\n            plt.imshow(image)\n            plt.xticks([])\n            plt.yticks([])\n\n            images_shown += 1\n            if images_shown &gt;= 7:\n                break\n</pre> from PIL import Image import matplotlib.pyplot as plt import os   def plot_images(image_paths):     images_shown = 0     plt.figure(figsize=(16, 9))     for img_path in image_paths:         if os.path.isfile(img_path):             image = Image.open(img_path)              plt.subplot(2, 3, images_shown + 1)             plt.imshow(image)             plt.xticks([])             plt.yticks([])              images_shown += 1             if images_shown &gt;= 7:                 break In\u00a0[\u00a0]: Copied! <pre>def download_video(url, output_path):\n    \"\"\"\n    Download a video from a given url and save it to the output path.\n\n    Parameters:\n    url (str): The url of the video to download.\n    output_path (str): The path to save the video to.\n\n    Returns:\n    dict: A dictionary containing the metadata of the video.\n    \"\"\"\n    yt = YouTube(url)\n    metadata = {\"Author\": yt.author, \"Title\": yt.title, \"Views\": yt.views}\n    yt.streams.get_highest_resolution().download(\n        output_path=output_path, filename=\"input_vid.mp4\"\n    )\n    return metadata\n\n\ndef video_to_images(video_path, output_folder):\n    \"\"\"\n    Convert a video to a sequence of images and save them to the output folder.\n\n    Parameters:\n    video_path (str): The path to the video file.\n    output_folder (str): The path to the folder to save the images to.\n\n    \"\"\"\n    clip = VideoFileClip(video_path)\n    clip.write_images_sequence(\n        os.path.join(output_folder, \"frame%04d.png\"), fps=0.2\n    )\n\n\ndef video_to_audio(video_path, output_audio_path):\n    \"\"\"\n    Convert a video to audio and save it to the output path.\n\n    Parameters:\n    video_path (str): The path to the video file.\n    output_audio_path (str): The path to save the audio to.\n\n    \"\"\"\n    clip = VideoFileClip(video_path)\n    audio = clip.audio\n    audio.write_audiofile(output_audio_path)\n\n\ndef audio_to_text(audio_path):\n    \"\"\"\n    Convert audio to text using the SpeechRecognition library.\n\n    Parameters:\n    audio_path (str): The path to the audio file.\n\n    Returns:\n    test (str): The text recognized from the audio.\n\n    \"\"\"\n    recognizer = sr.Recognizer()\n    audio = sr.AudioFile(audio_path)\n\n    with audio as source:\n        # Record the audio data\n        audio_data = recognizer.record(source)\n\n        try:\n            # Recognize the speech\n            text = recognizer.recognize_whisper(audio_data)\n        except sr.UnknownValueError:\n            print(\"Speech recognition could not understand the audio.\")\n        except sr.RequestError as e:\n            print(f\"Could not request results from service; {e}\")\n\n    return text\n</pre> def download_video(url, output_path):     \"\"\"     Download a video from a given url and save it to the output path.      Parameters:     url (str): The url of the video to download.     output_path (str): The path to save the video to.      Returns:     dict: A dictionary containing the metadata of the video.     \"\"\"     yt = YouTube(url)     metadata = {\"Author\": yt.author, \"Title\": yt.title, \"Views\": yt.views}     yt.streams.get_highest_resolution().download(         output_path=output_path, filename=\"input_vid.mp4\"     )     return metadata   def video_to_images(video_path, output_folder):     \"\"\"     Convert a video to a sequence of images and save them to the output folder.      Parameters:     video_path (str): The path to the video file.     output_folder (str): The path to the folder to save the images to.      \"\"\"     clip = VideoFileClip(video_path)     clip.write_images_sequence(         os.path.join(output_folder, \"frame%04d.png\"), fps=0.2     )   def video_to_audio(video_path, output_audio_path):     \"\"\"     Convert a video to audio and save it to the output path.      Parameters:     video_path (str): The path to the video file.     output_audio_path (str): The path to save the audio to.      \"\"\"     clip = VideoFileClip(video_path)     audio = clip.audio     audio.write_audiofile(output_audio_path)   def audio_to_text(audio_path):     \"\"\"     Convert audio to text using the SpeechRecognition library.      Parameters:     audio_path (str): The path to the audio file.      Returns:     test (str): The text recognized from the audio.      \"\"\"     recognizer = sr.Recognizer()     audio = sr.AudioFile(audio_path)      with audio as source:         # Record the audio data         audio_data = recognizer.record(source)          try:             # Recognize the speech             text = recognizer.recognize_whisper(audio_data)         except sr.UnknownValueError:             print(\"Speech recognition could not understand the audio.\")         except sr.RequestError as e:             print(f\"Could not request results from service; {e}\")      return text In\u00a0[\u00a0]: Copied! <pre>try:\n    metadata_vid = download_video(video_url, output_video_path)\n    video_to_images(filepath, output_folder)\n    video_to_audio(filepath, output_audio_path)\n    text_data = audio_to_text(output_audio_path)\n\n    with open(output_folder + \"output_text.txt\", \"w\") as file:\n        file.write(text_data)\n    print(\"Text data saved to file\")\n    file.close()\n    os.remove(output_audio_path)\n    print(\"Audio file removed\")\n\nexcept Exception as e:\n    raise e\n</pre> try:     metadata_vid = download_video(video_url, output_video_path)     video_to_images(filepath, output_folder)     video_to_audio(filepath, output_audio_path)     text_data = audio_to_text(output_audio_path)      with open(output_folder + \"output_text.txt\", \"w\") as file:         file.write(text_data)     print(\"Text data saved to file\")     file.close()     os.remove(output_audio_path)     print(\"Audio file removed\")  except Exception as e:     raise e In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.indices import MultiModalVectorStoreIndex\nfrom llama_index.core import SimpleDirectoryReader, StorageContext\n\nfrom llama_index.core import SimpleDirectoryReader, StorageContext\nfrom llama_index.vector_stores.lancedb import LanceDBVectorStore\n\n\nfrom llama_index.core import SimpleDirectoryReader\n\ntext_store = LanceDBVectorStore(uri=\"lancedb\", table_name=\"text_collection\")\nimage_store = LanceDBVectorStore(uri=\"lancedb\", table_name=\"image_collection\")\nstorage_context = StorageContext.from_defaults(\n    vector_store=text_store, image_store=image_store\n)\n\n# Create the MultiModal index\ndocuments = SimpleDirectoryReader(output_folder).load_data()\n\nindex = MultiModalVectorStoreIndex.from_documents(\n    documents,\n    storage_context=storage_context,\n)\n</pre> from llama_index.core.indices import MultiModalVectorStoreIndex from llama_index.core import SimpleDirectoryReader, StorageContext  from llama_index.core import SimpleDirectoryReader, StorageContext from llama_index.vector_stores.lancedb import LanceDBVectorStore   from llama_index.core import SimpleDirectoryReader  text_store = LanceDBVectorStore(uri=\"lancedb\", table_name=\"text_collection\") image_store = LanceDBVectorStore(uri=\"lancedb\", table_name=\"image_collection\") storage_context = StorageContext.from_defaults(     vector_store=text_store, image_store=image_store )  # Create the MultiModal index documents = SimpleDirectoryReader(output_folder).load_data()  index = MultiModalVectorStoreIndex.from_documents(     documents,     storage_context=storage_context, ) In\u00a0[\u00a0]: Copied! <pre>retriever_engine = index.as_retriever(\n    similarity_top_k=5, image_similarity_top_k=5\n)\n</pre> retriever_engine = index.as_retriever(     similarity_top_k=5, image_similarity_top_k=5 ) In\u00a0[\u00a0]: Copied! <pre>import json\n\nmetadata_str = json.dumps(metadata_vid)\n\nqa_tmpl_str = (\n    \"Given the provided information, including relevant images and retrieved context from the video, \\\n accurately and precisely answer the query without any additional prior knowledge.\\n\"\n    \"Please ensure honesty and responsibility, refraining from any racist or sexist remarks.\\n\"\n    \"---------------------\\n\"\n    \"Context: {context_str}\\n\"\n    \"Metadata for video: {metadata_str} \\n\"\n    \"---------------------\\n\"\n    \"Query: {query_str}\\n\"\n    \"Answer: \"\n)\n</pre> import json  metadata_str = json.dumps(metadata_vid)  qa_tmpl_str = (     \"Given the provided information, including relevant images and retrieved context from the video, \\  accurately and precisely answer the query without any additional prior knowledge.\\n\"     \"Please ensure honesty and responsibility, refraining from any racist or sexist remarks.\\n\"     \"---------------------\\n\"     \"Context: {context_str}\\n\"     \"Metadata for video: {metadata_str} \\n\"     \"---------------------\\n\"     \"Query: {query_str}\\n\"     \"Answer: \" ) In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.response.notebook_utils import display_source_node\nfrom llama_index.core.schema import ImageNode\n\n\ndef retrieve(retriever_engine, query_str):\n    retrieval_results = retriever_engine.retrieve(query_str)\n\n    retrieved_image = []\n    retrieved_text = []\n    for res_node in retrieval_results:\n        if isinstance(res_node.node, ImageNode):\n            retrieved_image.append(res_node.node.metadata[\"file_path\"])\n        else:\n            display_source_node(res_node, source_length=200)\n            retrieved_text.append(res_node.text)\n\n    return retrieved_image, retrieved_text\n</pre> from llama_index.core.response.notebook_utils import display_source_node from llama_index.core.schema import ImageNode   def retrieve(retriever_engine, query_str):     retrieval_results = retriever_engine.retrieve(query_str)      retrieved_image = []     retrieved_text = []     for res_node in retrieval_results:         if isinstance(res_node.node, ImageNode):             retrieved_image.append(res_node.node.metadata[\"file_path\"])         else:             display_source_node(res_node, source_length=200)             retrieved_text.append(res_node.text)      return retrieved_image, retrieved_text In\u00a0[\u00a0]: Copied! <pre>query_str = \"Using examples from video, explain all things covered in the video regarding the gaussian function\"\n\nimg, txt = retrieve(retriever_engine=retriever_engine, query_str=query_str)\nimage_documents = SimpleDirectoryReader(\n    input_dir=output_folder, input_files=img\n).load_data()\ncontext_str = \"\".join(txt)\nplot_images(img)\n</pre> query_str = \"Using examples from video, explain all things covered in the video regarding the gaussian function\"  img, txt = retrieve(retriever_engine=retriever_engine, query_str=query_str) image_documents = SimpleDirectoryReader(     input_dir=output_folder, input_files=img ).load_data() context_str = \"\".join(txt) plot_images(img) <p>Node ID: bda08ef1-137c-4d69-9bcc-b7005a41a13cSimilarity: 0.7431071996688843Text: The basic function underlying a normal distribution, aka a Gaussian, is e to the negative x squared. But you might wonder why this function? Of all the expressions we could dream up that give you s...</p> <p>Node ID: 7d6d0f32-ce16-461b-be54-883241252e50Similarity: 0.7335695028305054Text: This step is actually pretty technical, it goes a little beyond what I want to talk about here. Often use these objects called moment generating functions, that gives you a very abstract argument t...</p> <p>Node ID: 519fb788-3927-4842-ad5c-88be61deaf65Similarity: 0.7069740295410156Text: The essence of what we want to compute is what the convolution between two copies of this function looks like. If you remember, in the last video, we had two different ways to visualize convolution...</p> <p>Node ID: f265c3fb-3c9f-4f36-aa2a-fb15efff9783Similarity: 0.706935465335846Text: This is the important point. All of the stuff that's involving s is now entirely separate from the integrated variable. This remaining integral is a little bit tricky. I did a whole video on it. It...</p> In\u00a0[\u00a0]: Copied! <pre>from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n\nopenai_mm_llm = OpenAIMultiModal(\n    model=\"gpt-4-vision-preview\", api_key=OPENAI_API_TOKEN, max_new_tokens=1500\n)\n\n\nresponse_1 = openai_mm_llm.complete(\n    prompt=qa_tmpl_str.format(\n        context_str=context_str, query_str=query_str, metadata_str=metadata_str\n    ),\n    image_documents=image_documents,\n)\n\npprint(response_1.text)\n</pre> from llama_index.multi_modal_llms.openai import OpenAIMultiModal  openai_mm_llm = OpenAIMultiModal(     model=\"gpt-4-vision-preview\", api_key=OPENAI_API_TOKEN, max_new_tokens=1500 )   response_1 = openai_mm_llm.complete(     prompt=qa_tmpl_str.format(         context_str=context_str, query_str=query_str, metadata_str=metadata_str     ),     image_documents=image_documents, )  pprint(response_1.text) <pre>('The video by 3Blue1Brown, titled \"A pretty reason why Gaussian + Gaussian = '\n 'Gaussian,\" covers several aspects of the Gaussian function, also known as '\n \"the normal distribution. Here's a summary of the key points discussed in the \"\n 'video:\\n'\n '\\n'\n '1. **Central Limit Theorem**: The video begins by discussing the central '\n 'limit theorem, which states that the sum of multiple copies of a random '\n 'variable tends to look like a normal distribution. As the number of '\n 'variables increases, the approximation to a normal distribution becomes '\n 'better.\\n'\n '\\n'\n '2. **Convolution of Random Variables**: The process of adding two random '\n 'variables is mathematically represented by a convolution of their respective '\n 'distributions. The video explains the concept of convolution and how it is '\n 'used to find the distribution of the sum of two random variables.\\n'\n '\\n'\n '3. **Gaussian Function**: The Gaussian function is more complex than just '\n '\\\\( e^{-x^2} \\\\). The full formula includes a scaling factor to ensure the '\n 'area under the curve is 1 (making it a valid probability distribution), a '\n 'standard deviation parameter \\\\( \\\\sigma \\\\) to describe the spread, and a '\n 'mean parameter \\\\( \\\\mu \\\\) to shift the center. However, the video focuses '\n 'on centered distributions with \\\\( \\\\mu = 0 \\\\).\\n'\n '\\n'\n '4. **Visualizing Convolution**: The video presents a visual method to '\n 'understand the convolution of two Gaussian functions using diagonal slices '\n 'on the xy-plane. This method involves looking at the probability density of '\n 'landing on a point (x, y) as \\\\( f(x) \\\\times g(y) \\\\), where f and g are '\n 'the two distributions being convolved.\\n'\n '\\n'\n '5. **Rotational Symmetry**: A key property of the Gaussian function is its '\n 'rotational symmetry, which is unique to bell curves. This symmetry is '\n 'exploited in the video to simplify the calculation of the convolution. By '\n 'rotating the graph 45 degrees, the computation becomes easier because the '\n 'integral only involves one variable.\\n'\n '\\n'\n '6. **Result of Convolution**: The video demonstrates that the convolution of '\n 'two Gaussian functions is another Gaussian function. This is a special '\n 'property because convolutions typically result in a different kind of '\n 'function. The standard deviation of the resulting Gaussian is \\\\( \\\\sqrt{2} '\n '\\\\times \\\\sigma \\\\) if the original Gaussians had the same standard '\n 'deviation.\\n'\n '\\n'\n '7. **Proof of Central Limit Theorem**: The video explains that the '\n 'convolution of two Gaussians being another Gaussian is a crucial step in '\n 'proving the central limit theorem. It shows that the Gaussian function is a '\n 'fixed point in the space of distributions, and since all distributions with '\n 'finite variance tend towards a single universal shape, that shape must be '\n 'the Gaussian.\\n'\n '\\n'\n '8. **Connection to Pi**: The video also touches on the connection between '\n 'the Gaussian function and the number Pi, which appears in the formula for '\n 'the normal distribution.\\n'\n '\\n'\n 'The video aims to provide an intuitive geometric argument for why the sum of '\n 'two normally distributed random variables is also normally distributed, and '\n 'how this relates to the central limit theorem and the special properties of '\n 'the Gaussian function.')\n</pre>"},{"location":"notebooks/multi_modal_video_RAG/#multimodal-rag-for-video-processing-using-llamaindex-openai-gpt4v","title":"Multimodal RAG for video processing using LlamaIndex, OpenAI GPT4V\u00b6","text":"<p>In this notebook, we showcase a Multimodal RAG architecture designed for video processing. We utilize OpenAI GPT4V MultiModal LLM class that employs CLIP to generate multimodal embeddings. Furthermore, we use LanceDBVectorStore for efficient vector storage.</p> <p>Steps:</p> <ol> <li><p>Download video from YouTube, process and store it.</p> </li> <li><p>Build Multi-Modal index and vector store for both texts and images.</p> </li> <li><p>Retrieve relevant images and context, use both to augment the prompt.</p> </li> <li><p>Using GPT4V for reasoning the correlations between the input query and augmented data and generating final response.</p> </li> </ol>"},{"location":"notebooks/multi_modal_video_RAG/#set-configuration-for-input-below","title":"Set configuration for input below\u00b6","text":""},{"location":"notebooks/multi_modal_video_RAG/#download-and-process-videos-into-appropriate-format-for-generatingstoring-embeddings","title":"Download and process videos into appropriate format for generating/storing embeddings\u00b6","text":""},{"location":"notebooks/multi_modal_video_RAG/#create-the-multi-modal-index","title":"Create the multi-modal index\u00b6","text":""},{"location":"notebooks/multi_modal_video_RAG/#use-index-as-retriever-to-fetch-top-k-5-in-this-example-results-from-the-multimodal-vector-index","title":"Use index as retriever to fetch top k (5 in this example) results from the multimodal vector index\u00b6","text":""},{"location":"notebooks/multi_modal_video_RAG/#set-the-rag-prompt-template","title":"Set the RAG  prompt template\u00b6","text":""},{"location":"notebooks/multi_modal_video_RAG/#retrieve-most-similar-textimage-embeddings-baseed-on-user-query-from-the-db","title":"Retrieve most similar text/image embeddings baseed on user query from the DB\u00b6","text":""},{"location":"notebooks/multi_modal_video_RAG/#add-query-now-fetch-relevant-details-including-images-and-augment-the-prompt-template","title":"Add query now, fetch relevant details including images and augment the prompt template\u00b6","text":""},{"location":"notebooks/multi_modal_video_RAG/#generate-final-response-using-gpt4v","title":"Generate final response using GPT4V\u00b6","text":""},{"location":"notebooks/reproducibility/","title":"Sync API","text":"<p>Let's first prepare the data. We will be using a CSV file with a bunch of quotes from Rick and Morty</p> In\u00a0[1]: Copied! <pre>!wget http://vectordb-recipes.s3.us-west-2.amazonaws.com/rick_and_morty_quotes.csv\n!head rick_and_morty_quotes.csv\n</pre> !wget http://vectordb-recipes.s3.us-west-2.amazonaws.com/rick_and_morty_quotes.csv !head rick_and_morty_quotes.csv <pre>--2024-12-17 11:54:43--  http://vectordb-recipes.s3.us-west-2.amazonaws.com/rick_and_morty_quotes.csv\nResolving vectordb-recipes.s3.us-west-2.amazonaws.com (vectordb-recipes.s3.us-west-2.amazonaws.com)... 52.92.138.34, 3.5.82.160, 52.218.236.161, ...\nConnecting to vectordb-recipes.s3.us-west-2.amazonaws.com (vectordb-recipes.s3.us-west-2.amazonaws.com)|52.92.138.34|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 8236 (8.0K) [text/csv]\nSaving to: \u2018rick_and_morty_quotes.csv.1\u2019\n\nrick_and_morty_quot 100%[===================&gt;]   8.04K  --.-KB/s    in 0s      \n\n2024-12-17 11:54:43 (77.8 MB/s) - \u2018rick_and_morty_quotes.csv.1\u2019 saved [8236/8236]\n\nid,author,quote\n1,Rick,\" Morty, you got to come on. You got to come with me.\"\n2,Morty,\" Rick, what\u2019s going on?\"\n3,Rick,\" I got a surprise for you, Morty.\"\n4,Morty,\" It\u2019s the middle of the night. What are you talking about?\"\n5,Rick,\" I got a surprise for you.\"\n6,Morty,\" Ow! Ow! You\u2019re tugging me too hard.\"\n7,Rick,\" I got a surprise for you, Morty.\"\n8,Rick,\" What do you think of this flying vehicle, Morty? I built it out of stuff I found in the garage.\"\n9,Morty,\" Yeah, Rick, it\u2019s great. Is this the surprise?\"\n</pre> <p>Let's load this into a pandas dataframe.</p> <p>It's got 3 columns, a quote id, the quote string, and the first name of the author of the quote:</p> In\u00a0[2]: Copied! <pre>import pandas as pd\ndf = pd.read_csv(\"rick_and_morty_quotes.csv\")\ndf.head()\n</pre> import pandas as pd df = pd.read_csv(\"rick_and_morty_quotes.csv\") df.head() Out[2]: id author quote 0 1 Rick Morty, you got to come on. You got to come wi... 1 2 Morty Rick, what\u2019s going on? 2 3 Rick I got a surprise for you, Morty. 3 4 Morty It\u2019s the middle of the night. What are you ta... 4 5 Rick I got a surprise for you. <p>We'll start with a local LanceDB connection</p> In\u00a0[3]: Copied! <pre>!pip install lancedb -q\n</pre> !pip install lancedb -q In\u00a0[\u00a0]: Copied! <pre>import lancedb\ndb = lancedb.connect(\"~/.lancedb\")\n</pre> import lancedb db = lancedb.connect(\"~/.lancedb\") <p>Creating a LanceDB table from a pandas dataframe is straightforward using <code>create_table</code>:</p> In\u00a0[5]: Copied! <pre>db.drop_table(\"rick_and_morty\", ignore_missing=True)\ntable = db.create_table(\"rick_and_morty\", df)\ntable.head().to_pandas()\n</pre> db.drop_table(\"rick_and_morty\", ignore_missing=True) table = db.create_table(\"rick_and_morty\", df) table.head().to_pandas() Out[5]: id author quote 0 1 Rick Morty, you got to come on. You got to come wi... 1 2 Morty Rick, what\u2019s going on? 2 3 Rick I got a surprise for you, Morty. 3 4 Morty It\u2019s the middle of the night. What are you ta... 4 5 Rick I got a surprise for you. <p>Now, since Rick is the smartest man in the multiverse, he deserves to have his quotes attributed to his full name: Richard Daniel Sanchez.</p> <p>This can be done via <code>LanceTable.update</code>. It needs two arguments:</p> <ol> <li>A <code>where</code> string filter (sql syntax) to determine the rows to update</li> <li>A dict of <code>values</code> where the keys are the column names to update and the values are the new values</li> </ol> In\u00a0[6]: Copied! <pre>table.update(where=\"author='Rick'\", values={\"author\": \"Richard Daniel Sanchez\"})\ntable.to_pandas()\n</pre> table.update(where=\"author='Rick'\", values={\"author\": \"Richard Daniel Sanchez\"}) table.to_pandas() Out[6]: id author quote 0 2 Morty Rick, what\u2019s going on? 1 4 Morty It\u2019s the middle of the night. What are you ta... 2 6 Morty Ow! Ow! You\u2019re tugging me too hard. 3 9 Morty Yeah, Rick, it\u2019s great. Is this the surprise? 4 11 Morty What?! A bomb?! ... ... ... ... 94 80 Richard Daniel Sanchez There you are, Morty. Listen to me. I got an ... 95 82 Richard Daniel Sanchez It\u2019s pretty obvious, Morty. I froze him. Now ... 96 84 Richard Daniel Sanchez Do you have any concept of how much higher th... 97 86 Richard Daniel Sanchez I\u2019ll do it later, Morty. He\u2019ll be fine. Let\u2019s... 98 97 Richard Daniel Sanchez There she is. All right. Come on, Morty. Let\u2019... <p>99 rows \u00d7 3 columns</p> <p>Ok so this is a vector database, so we need actual vectors. We'll use sentence transformers here to avoid having to deal with API keys.</p> <p>Let's create a basic model using the \"all-MiniLM-L6-v2\" model and embed the quotes:</p> In\u00a0[7]: Copied! <pre>from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\")\nvectors = model.encode(df.quote.values.tolist(),\n                       convert_to_numpy=True,\n                       normalize_embeddings=True).tolist()\n</pre> from sentence_transformers import SentenceTransformer model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\") vectors = model.encode(df.quote.values.tolist(),                        convert_to_numpy=True,                        normalize_embeddings=True).tolist() <p>We can then convert the vectors into a pyarrow Table and merge it to the LanceDB Table.</p> <p>For the merge to work successfully, we need to have an overlapping column. Here the natural choice is to use the id column:</p> In\u00a0[8]: Copied! <pre>from lance.vector import vec_to_table\nimport numpy as np\nimport pyarrow as pa\n</pre> from lance.vector import vec_to_table import numpy as np import pyarrow as pa In\u00a0[9]: Copied! <pre>embeddings = vec_to_table(vectors)\nembeddings = embeddings.append_column(\"id\", pa.array(np.arange(len(table))+1))\nembeddings.to_pandas().head()\n</pre> embeddings = vec_to_table(vectors) embeddings = embeddings.append_column(\"id\", pa.array(np.arange(len(table))+1)) embeddings.to_pandas().head() Out[9]: vector id 0 [-0.10369808, -0.038807657, -0.07471153, -0.05... 1 1 [-0.11813704, -0.0533092, 0.025554786, -0.0242... 2 2 [-0.09807682, -0.035231438, -0.04206024, -0.06... 3 3 [0.032292824, 0.038136397, 0.013615396, 0.0335... 4 4 [-0.050369408, -0.0043397923, 0.013419108, -0.... 5 <p>And now we'll use the <code>LanceTable.merge</code> function to add the vector column into the LanceTable:</p> In\u00a0[10]: Copied! <pre>table.merge(embeddings, left_on=\"id\")\ntable.head().to_pandas()\n</pre> table.merge(embeddings, left_on=\"id\") table.head().to_pandas() Out[10]: id author quote vector 0 2 Morty Rick, what\u2019s going on? [-0.11813704, -0.0533092, 0.025554786, -0.0242... 1 4 Morty It\u2019s the middle of the night. What are you ta... [0.032292824, 0.038136397, 0.013615396, 0.0335... 2 6 Morty Ow! Ow! You\u2019re tugging me too hard. [-0.035019904, -0.070963725, 0.003859435, -0.0... 3 9 Morty Yeah, Rick, it\u2019s great. Is this the surprise? [-0.12578955, -0.019364933, 0.01606114, -0.082... 4 11 Morty What?! A bomb?! [0.0018287548, 0.07033146, -0.023754105, 0.047... <p>If we look at the schema, we see that <code>all-MiniLM-L6-v2</code> produces 384-dimensional vectors:</p> In\u00a0[11]: Copied! <pre>table.schema\n</pre> table.schema Out[11]: <pre>id: int64\nauthor: string\nquote: string\nvector: fixed_size_list&lt;item: float&gt;[384]\n  child 0, item: float</pre> <p>Suppose we used the table and found that the <code>all-MiniLM-L6-v2</code> model doesn't produce ideal results. Instead we want to try a larger model. How do we use the new embeddings without losing the change history?</p> <p>First, major operations are automatically versioned in LanceDB. Version 1 is the table creation, with the initial insertion of data. Versions 2 and 3 represents the update (deletion + append) Version 4 is adding the new column.</p> In\u00a0[12]: Copied! <pre>table.list_versions()\n</pre> table.list_versions() Out[12]: <pre>[{'version': 1,\n  'timestamp': datetime.datetime(2024, 12, 17, 11, 57, 21, 613932),\n  'metadata': {}},\n {'version': 2,\n  'timestamp': datetime.datetime(2024, 12, 17, 11, 57, 21, 626525),\n  'metadata': {}},\n {'version': 3,\n  'timestamp': datetime.datetime(2024, 12, 17, 11, 57, 27, 91378),\n  'metadata': {}},\n {'version': 4,\n  'timestamp': datetime.datetime(2024, 12, 17, 11, 58, 4, 513085),\n  'metadata': {}}]</pre> <p>We can restore version 3, before we added the old vector column</p> In\u00a0[13]: Copied! <pre>table.restore(3)\ntable.head().to_pandas()\n</pre> table.restore(3) table.head().to_pandas() Out[13]: id author quote 0 2 Morty Rick, what\u2019s going on? 1 4 Morty It\u2019s the middle of the night. What are you ta... 2 6 Morty Ow! Ow! You\u2019re tugging me too hard. 3 9 Morty Yeah, Rick, it\u2019s great. Is this the surprise? 4 11 Morty What?! A bomb?! <p>Notice that we now have one more, not less versions. When we restore an old version, we're not deleting the version history, we're just creating a new version where the schema and data is equivalent to the restored old version. In this way, we can keep track of all of the changes and always rollback to a previous state.</p> In\u00a0[14]: Copied! <pre>table.list_versions()\n</pre> table.list_versions() Out[14]: <pre>[{'version': 1,\n  'timestamp': datetime.datetime(2024, 12, 17, 11, 57, 21, 613932),\n  'metadata': {}},\n {'version': 2,\n  'timestamp': datetime.datetime(2024, 12, 17, 11, 57, 21, 626525),\n  'metadata': {}},\n {'version': 3,\n  'timestamp': datetime.datetime(2024, 12, 17, 11, 57, 27, 91378),\n  'metadata': {}},\n {'version': 4,\n  'timestamp': datetime.datetime(2024, 12, 17, 11, 58, 4, 513085),\n  'metadata': {}},\n {'version': 5,\n  'timestamp': datetime.datetime(2024, 12, 17, 11, 58, 27, 153807),\n  'metadata': {}}]</pre> In\u00a0[\u00a0]: Copied! <pre>model = SentenceTransformer(\"all-mpnet-base-v2\", device=\"cpu\")\nvectors = model.encode(df.quote.values.tolist(),\n                       convert_to_numpy=True,\n                       normalize_embeddings=True).tolist()\nembeddings = vec_to_table(vectors)\nembeddings = embeddings.append_column(\"id\", pa.array(np.arange(len(table))+1))\ntable.merge(embeddings, left_on=\"id\")\n</pre> model = SentenceTransformer(\"all-mpnet-base-v2\", device=\"cpu\") vectors = model.encode(df.quote.values.tolist(),                        convert_to_numpy=True,                        normalize_embeddings=True).tolist() embeddings = vec_to_table(vectors) embeddings = embeddings.append_column(\"id\", pa.array(np.arange(len(table))+1)) table.merge(embeddings, left_on=\"id\") In\u00a0[16]: Copied! <pre>table.schema\n</pre> table.schema Out[16]: <pre>id: int64\nauthor: string\nquote: string\nvector: fixed_size_list&lt;item: float&gt;[768]\n  child 0, item: float</pre> In\u00a0[17]: Copied! <pre>table.delete(\"author != 'Richard Daniel Sanchez'\")\n</pre> table.delete(\"author != 'Richard Daniel Sanchez'\") <p>We can see that the number of rows has been reduced to 30</p> In\u00a0[18]: Copied! <pre>len(table)\n</pre> len(table) Out[18]: <pre>28</pre> <p>Ok we had our fun, let's get back to the full quote set</p> In\u00a0[20]: Copied! <pre>table.restore(6)\n</pre> table.restore(6) In\u00a0[21]: Copied! <pre>len(table)\n</pre> len(table) Out[21]: <pre>99</pre> In\u00a0[22]: Copied! <pre>table.version\n</pre> table.version Out[22]: <pre>8</pre> <p>Versions:</p> <ul> <li>1 - Create and append</li> <li>2 - Update (deletion)</li> <li>3 - Update (append)</li> <li>4 - Merge (vector column)</li> <li>5 - Restore (4)</li> <li>6 - Merge (new vector column)</li> <li>7 - Deletion</li> <li>8 - Restore</li> </ul> <p>We never had to explicitly manage the versioning. And we never had to create expensive and slow snapshots. LanceDB automatically tracks the full history of operations and supports fast rollbacks. In production this is critical for debugging issues and minimizing downtime by rolling back to a previously successful state in seconds.</p>"},{"location":"notebooks/reproducibility/#sync-api","title":"Sync API\u00b6","text":"<p>Reproducibility is critical for AI. For code, it's easy to keep track of changes using Github or Gitlab. For data, it's not as easy. Most of the time, we're manually writing complicated data tracking code, wrestling with an external tool, and dealing with expensive duplicate snapshot copies with low granularity.</p> <p>While working with most other vector databases, if we loaded in the wrong data (or any other such mistakes), we have to blow away the index, correct the mistake, and then completely rebuild it. It's really difficult to rollback to an earlier state, and any such corrective action destroys historical data and evidence, which may be useful down the line to debug and diagnose issues.</p> <p>To our knowledge, LanceDB is the first and only vector database that supports full reproducibility and rollbacks natively. Taking advantage of the Lance columnar data format, LanceDB supports:</p> <ul> <li>Automatic versioning</li> <li>Instant rollback</li> <li>Appends, updates, deletions</li> <li>Schema evolution</li> </ul> <p>This makes auditing, tracking, and reproducibility a breeze!</p> <p>Let's see how this all works.</p>"},{"location":"notebooks/reproducibility/#pickle-rick","title":"Pickle Rick!\u00b6","text":""},{"location":"notebooks/reproducibility/#updates","title":"Updates\u00b6","text":""},{"location":"notebooks/reproducibility/#schema-evolution","title":"Schema evolution\u00b6","text":""},{"location":"notebooks/reproducibility/#rollback","title":"Rollback\u00b6","text":""},{"location":"notebooks/reproducibility/#switching-models","title":"Switching Models\u00b6","text":"<p>Now we'll switch to the <code>all-mpnet-base-v2</code> model and add the vectors to the restored dataset again. Note that this step can take a couple of minutes.</p>"},{"location":"notebooks/reproducibility/#deletion","title":"Deletion\u00b6","text":"<p>What if the whole show was just Rick-isms? Let's delete any quote not said by Rick:</p>"},{"location":"notebooks/reproducibility/#history","title":"History\u00b6","text":"<p>We now have 9 versions in the data. We can review the operations that corresponds to each version below:</p>"},{"location":"notebooks/reproducibility/#summary","title":"Summary\u00b6","text":""},{"location":"notebooks/reproducibility_async/","title":"Async API","text":"<p>Let's first prepare the data. We will be using a CSV file with a bunch of quotes from Rick and Morty</p> In\u00a0[50]: Copied! <pre>!wget http://vectordb-recipes.s3.us-west-2.amazonaws.com/rick_and_morty_quotes.csv\n!head rick_and_morty_quotes.csv\n</pre> !wget http://vectordb-recipes.s3.us-west-2.amazonaws.com/rick_and_morty_quotes.csv !head rick_and_morty_quotes.csv <pre>--2024-12-17 15:58:31--  http://vectordb-recipes.s3.us-west-2.amazonaws.com/rick_and_morty_quotes.csv\nResolving vectordb-recipes.s3.us-west-2.amazonaws.com (vectordb-recipes.s3.us-west-2.amazonaws.com)... 3.5.84.162, 3.5.76.76, 52.92.228.138, ...\nConnecting to vectordb-recipes.s3.us-west-2.amazonaws.com (vectordb-recipes.s3.us-west-2.amazonaws.com)|3.5.84.162|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 8236 (8.0K) [text/csv]\nSaving to: \u2018rick_and_morty_quotes.csv.3\u2019\n\nrick_and_morty_quot 100%[===================&gt;]   8.04K  --.-KB/s    in 0s      \n\n2024-12-17 15:58:31 (160 MB/s) - \u2018rick_and_morty_quotes.csv.3\u2019 saved [8236/8236]\n\nid,author,quote\n1,Rick,\" Morty, you got to come on. You got to come with me.\"\n2,Morty,\" Rick, what\u2019s going on?\"\n3,Rick,\" I got a surprise for you, Morty.\"\n4,Morty,\" It\u2019s the middle of the night. What are you talking about?\"\n5,Rick,\" I got a surprise for you.\"\n6,Morty,\" Ow! Ow! You\u2019re tugging me too hard.\"\n7,Rick,\" I got a surprise for you, Morty.\"\n8,Rick,\" What do you think of this flying vehicle, Morty? I built it out of stuff I found in the garage.\"\n9,Morty,\" Yeah, Rick, it\u2019s great. Is this the surprise?\"\n</pre> <p>Let's load this into a pandas dataframe.</p> <p>It's got 3 columns, a quote id, the quote string, and the first name of the author of the quote:</p> In\u00a0[51]: Copied! <pre>import pandas as pd\ndf = pd.read_csv(\"rick_and_morty_quotes.csv\")\ndf.head()\n</pre> import pandas as pd df = pd.read_csv(\"rick_and_morty_quotes.csv\") df.head() Out[51]: id author quote 0 1 Rick Morty, you got to come on. You got to come wi... 1 2 Morty Rick, what\u2019s going on? 2 3 Rick I got a surprise for you, Morty. 3 4 Morty It\u2019s the middle of the night. What are you ta... 4 5 Rick I got a surprise for you. <p>Creating a LanceDB table from a pandas dataframe is straightforward using <code>create_table</code></p> <p>We'll start with a local LanceDB connection</p> In\u00a0[35]: Copied! <pre>!pip install lancedb -q\n</pre> !pip install lancedb -q In\u00a0[52]: Copied! <pre>import lancedb\nasync_db = await lancedb.connect_async(\"~/.lancedb\")\n</pre> import lancedb async_db = await lancedb.connect_async(\"~/.lancedb\") In\u00a0[53]: Copied! <pre>await async_db.drop_table(\"rick_and_morty\")\nasync_table = await async_db.create_table(\"rick_and_morty\", df, mode=\"overwrite\")\nawait async_table.to_pandas()\n</pre> await async_db.drop_table(\"rick_and_morty\") async_table = await async_db.create_table(\"rick_and_morty\", df, mode=\"overwrite\") await async_table.to_pandas() <pre>[2024-12-17T23:58:46Z WARN  lance::dataset::write::insert] No existing dataset at ~/.lancedb/rick_and_morty.lance, it will be created\n</pre> Out[53]: id author quote 0 1 Rick Morty, you got to come on. You got to come wi... 1 2 Morty Rick, what\u2019s going on? 2 3 Rick I got a surprise for you, Morty. 3 4 Morty It\u2019s the middle of the night. What are you ta... 4 5 Rick I got a surprise for you. 5 6 Morty Ow! Ow! You\u2019re tugging me too hard. 6 7 Rick I got a surprise for you, Morty. 7 8 Rick What do you think of this flying vehicle, Mor... 8 9 Morty Yeah, Rick, it\u2019s great. Is this the surprise? 9 10 Rick Morty, I had to I had to I had to I had to ma... <p>Now, since Rick is the smartest man in the multiverse, he deserves to have his quotes attributed to his full name: Richard Daniel Sanchez.</p> <p>This can be done via <code>LanceTable.update</code>. It needs two arguments:</p> <ol> <li>A <code>where</code> string filter (sql syntax) to determine the rows to update</li> <li>A dict of <code>updates</code> where the keys are the column names to update and the values are the new values</li> </ol> In\u00a0[54]: Copied! <pre>await async_table.update(where=\"author='Morty'\", updates={\"author\": \"Richard Daniel Sanchez\"})\nawait async_table.to_pandas()\n</pre> await async_table.update(where=\"author='Morty'\", updates={\"author\": \"Richard Daniel Sanchez\"}) await async_table.to_pandas() Out[54]: id author quote 0 1 Rick Morty, you got to come on. You got to come wi... 1 3 Rick I got a surprise for you, Morty. 2 5 Rick I got a surprise for you. 3 7 Rick I got a surprise for you, Morty. 4 8 Rick What do you think of this flying vehicle, Mor... 5 10 Rick Morty, I had to I had to I had to I had to ma... 6 12 Rick We\u2019re gonna drop it down there just get a who... 7 14 Rick Come on, Morty. Just take it easy, Morty. It\u2019... 8 16 Rick When I drop the bomb you know, I want you to ... 9 18 Rick And Jessica\u2019s gonna be Eve,\u2026 <p>Let's add a <code>new_id</code> column to the table, where each value is the original <code>id</code> plus 1.</p> In\u00a0[55]: Copied! <pre>await async_table.add_columns({\"new_id\": \"id + 1\"})\nawait async_table.to_pandas()\n</pre> await async_table.add_columns({\"new_id\": \"id + 1\"}) await async_table.to_pandas() Out[55]: id author quote new_id 0 1 Rick Morty, you got to come on. You got to come wi... 2 1 3 Rick I got a surprise for you, Morty. 4 2 5 Rick I got a surprise for you. 6 3 7 Rick I got a surprise for you, Morty. 8 4 8 Rick What do you think of this flying vehicle, Mor... 9 5 10 Rick Morty, I had to I had to I had to I had to ma... 11 6 12 Rick We\u2019re gonna drop it down there just get a who... 13 7 14 Rick Come on, Morty. Just take it easy, Morty. It\u2019... 15 8 16 Rick When I drop the bomb you know, I want you to ... 17 9 18 Rick And Jessica\u2019s gonna be Eve,\u2026 19 <p>If we look at the schema, we see that a new int64 column was added</p> In\u00a0[56]: Copied! <pre>await async_table.schema()\n</pre> await async_table.schema() Out[56]: <pre>id: int64\nauthor: string\nquote: string\nnew_id: int64</pre> <p>Suppose we used the table and found that the new column should be a different value. How do we use another new column without losing the change history?</p> <p>First, major operations are automatically versioned in LanceDB. Version 1 is the table creation, with the initial insertion of data. Versions 2 and 3 represents the update (deletion + append) Version 4 is adding the new column.</p> In\u00a0[57]: Copied! <pre>await async_table.checkout_latest()\nawait async_table.list_versions()\n</pre> await async_table.checkout_latest() await async_table.list_versions() Out[57]: <pre>[{'version': 1,\n  'timestamp': datetime.datetime(2024, 12, 17, 15, 58, 46, 983259),\n  'metadata': {}},\n {'version': 2,\n  'timestamp': datetime.datetime(2024, 12, 17, 15, 59, 0, 291948),\n  'metadata': {}},\n {'version': 3,\n  'timestamp': datetime.datetime(2024, 12, 17, 15, 59, 8, 381165),\n  'metadata': {}}]</pre> <p>We can restore version 3, before we added the <code>new_id</code> vector column</p> In\u00a0[58]: Copied! <pre>await async_table.checkout(2)\nawait async_table.restore()\nawait async_table.to_pandas()\n</pre> await async_table.checkout(2) await async_table.restore() await async_table.to_pandas() Out[58]: id author quote 0 1 Rick Morty, you got to come on. You got to come wi... 1 3 Rick I got a surprise for you, Morty. 2 5 Rick I got a surprise for you. 3 7 Rick I got a surprise for you, Morty. 4 8 Rick What do you think of this flying vehicle, Mor... 5 10 Rick Morty, I had to I had to I had to I had to ma... 6 12 Rick We\u2019re gonna drop it down there just get a who... 7 14 Rick Come on, Morty. Just take it easy, Morty. It\u2019... 8 16 Rick When I drop the bomb you know, I want you to ... 9 18 Rick And Jessica\u2019s gonna be Eve,\u2026 <p>Notice that we now have one more, not less versions. When we restore an old version, we're not deleting the version history, we're just creating a new version where the schema and data is equivalent to the restored old version. In this way, we can keep track of all of the changes and always rollback to a previous state.</p> In\u00a0[59]: Copied! <pre>await async_table.list_versions()\n</pre> await async_table.list_versions() Out[59]: <pre>[{'version': 1,\n  'timestamp': datetime.datetime(2024, 12, 17, 15, 58, 46, 983259),\n  'metadata': {}},\n {'version': 2,\n  'timestamp': datetime.datetime(2024, 12, 17, 15, 59, 0, 291948),\n  'metadata': {}},\n {'version': 3,\n  'timestamp': datetime.datetime(2024, 12, 17, 15, 59, 8, 381165),\n  'metadata': {}},\n {'version': 4,\n  'timestamp': datetime.datetime(2024, 12, 17, 15, 59, 22, 800694),\n  'metadata': {}}]</pre> In\u00a0[60]: Copied! <pre>await async_table.add_columns({\"new_id\": \"id + 10\"})\n</pre> await async_table.add_columns({\"new_id\": \"id + 10\"}) In\u00a0[61]: Copied! <pre>await async_table.schema()\n</pre> await async_table.schema() Out[61]: <pre>id: int64\nauthor: string\nquote: string\nnew_id: int64</pre> In\u00a0[62]: Copied! <pre>await async_table.delete(\"author != 'Richard Daniel Sanchez'\")\n</pre> await async_table.delete(\"author != 'Richard Daniel Sanchez'\") <p>We can see that the number of rows has been reduced to 30</p> In\u00a0[63]: Copied! <pre>await async_table.count_rows()\n</pre> await async_table.count_rows() Out[63]: <pre>34</pre> <p>Ok we had our fun, let's get back to the full quote set</p> In\u00a0[67]: Copied! <pre>await async_table.checkout(5)\nawait async_table.restore()\n</pre> await async_table.checkout(5) await async_table.restore() In\u00a0[68]: Copied! <pre>await async_table.count_rows()\n</pre> await async_table.count_rows() Out[68]: <pre>99</pre> In\u00a0[32]: Copied! <pre>await async_table.version()\n</pre> await async_table.version() Out[32]: <pre>6</pre> <p>Versions:</p> <ul> <li>1 - Create</li> <li>2 - Update</li> <li>3 - Add a new column</li> <li>4 - Restore (2)</li> <li>5 - Add a new column</li> <li>6 - Delete</li> <li>7 - Restore</li> </ul> <p>We never had to explicitly manage the versioning. And we never had to create expensive and slow snapshots. LanceDB automatically tracks the full history of operations I created and supports fast rollbacks. In production this is critical for debugging issues and minimizing downtime by rolling back to a previously successful state in seconds.</p>"},{"location":"notebooks/reproducibility_async/#async-api","title":"Async API\u00b6","text":"<p>We demonstrate the following functionalities suppored by LanceDB using our asynchonous APIs:</p> <ul> <li>Automatic versioning</li> <li>Instant rollback</li> <li>Appends, updates, deletions</li> <li>Schema evolution</li> </ul>"},{"location":"notebooks/reproducibility_async/#updates","title":"Updates\u00b6","text":""},{"location":"notebooks/reproducibility_async/#schema-evolution","title":"Schema evolution\u00b6","text":""},{"location":"notebooks/reproducibility_async/#rollback","title":"Rollback\u00b6","text":""},{"location":"notebooks/reproducibility_async/#add-another-new-column","title":"Add another new column\u00b6","text":"<p>Now we'll change the value of the <code>new_id</code> column and add it to the restored dataset again</p>"},{"location":"notebooks/reproducibility_async/#deletion","title":"Deletion\u00b6","text":"<p>What if the whole show was just Rick-isms? Let's delete any quote not said by Rick</p>"},{"location":"notebooks/reproducibility_async/#history","title":"History\u00b6","text":"<p>We now have 9 versions in the data. We can review the operations that corresponds to each version below:</p>"},{"location":"notebooks/reproducibility_async/#summary","title":"Summary\u00b6","text":""},{"location":"notebooks/tables_guide/","title":"Tables guide","text":"In\u00a0[2]: Copied! <pre>!pip install lancedb -qq\n</pre> !pip install lancedb -qq In\u00a0[3]: Copied! <pre>import lancedb\ndb = lancedb.connect(\"./.lancedb\")\n</pre> import lancedb db = lancedb.connect(\"./.lancedb\") <p>LanceDB allows ingesting data from various sources - <code>dict</code>, <code>list[dict]</code>, <code>pd.DataFrame</code>, <code>pa.Table</code> or a <code>Iterator[pa.RecordBatch]</code>. Let's take a look at some of the these.</p> In\u00a0[4]: Copied! <pre>import lancedb\n\ndb = lancedb.connect(\"./.lancedb\")\n\ndata = [{\"vector\": [1.1, 1.2], \"lat\": 45.5, \"long\": -122.7},\n        {\"vector\": [0.2, 1.8], \"lat\": 40.1, \"long\": -74.1}]\n\ndb.create_table(\"my_table\", data)\n\ndb[\"my_table\"].head()\n</pre> import lancedb  db = lancedb.connect(\"./.lancedb\")  data = [{\"vector\": [1.1, 1.2], \"lat\": 45.5, \"long\": -122.7},         {\"vector\": [0.2, 1.8], \"lat\": 40.1, \"long\": -74.1}]  db.create_table(\"my_table\", data)  db[\"my_table\"].head() Out[4]: <pre>pyarrow.Table\nvector: fixed_size_list&lt;item: float&gt;[2]\n  child 0, item: float\nlat: double\nlong: double\n----\nvector: [[[1.1,1.2],[0.2,1.8]]]\nlat: [[45.5,40.1]]\nlong: [[-122.7,-74.1]]</pre> In\u00a0[5]: Copied! <pre>import pandas as pd\n\ndata = pd.DataFrame(\n    {\n        \"vector\": [[1.1, 1.2, 1.3, 1.4], [0.2, 1.8, 0.4, 3.6]],\n        \"lat\": [45.5, 40.1],\n        \"long\": [-122.7, -74.1],\n    }\n)\ndb.create_table(\"my_table_pandas\", data)\ndb[\"my_table_pandas\"].head()\n</pre> import pandas as pd  data = pd.DataFrame(     {         \"vector\": [[1.1, 1.2, 1.3, 1.4], [0.2, 1.8, 0.4, 3.6]],         \"lat\": [45.5, 40.1],         \"long\": [-122.7, -74.1],     } ) db.create_table(\"my_table_pandas\", data) db[\"my_table_pandas\"].head() Out[5]: <pre>pyarrow.Table\nvector: fixed_size_list&lt;item: float&gt;[2]\n  child 0, item: float\nlat: double\nlong: double\n----\nvector: [[[1.1,1.2],[0.2,1.8]]]\nlat: [[45.5,40.1]]\nlong: [[-122.7,-74.1]]</pre> <p>Data is converted to Arrow before being written to disk. For maximum control over how data is saved, either provide the PyArrow schema to convert to or else provide a PyArrow Table directly.</p> In\u00a0[6]: Copied! <pre>import pyarrow as pa\n\ncustom_schema = pa.schema([\npa.field(\"vector\", pa.list_(pa.float32(), 4)),\npa.field(\"lat\", pa.float32()),\npa.field(\"long\", pa.float32())\n])\n\ntable = db.create_table(\"table3\", data, schema=custom_schema, mode=\"overwrite\")\ntable.schema\n</pre> import pyarrow as pa  custom_schema = pa.schema([ pa.field(\"vector\", pa.list_(pa.float32(), 4)), pa.field(\"lat\", pa.float32()), pa.field(\"long\", pa.float32()) ])  table = db.create_table(\"table3\", data, schema=custom_schema, mode=\"overwrite\") table.schema <pre>[2024-01-31T18:59:33Z WARN  lance::dataset] No existing dataset at /Users/qian/Work/LanceDB/lancedb/docs/src/notebooks/.lancedb/table3.lance, it will be created\n</pre> Out[6]: <pre>vector: fixed_size_list&lt;item: float&gt;[2]\n  child 0, item: float\nlat: float\nlong: float</pre> In\u00a0[7]: Copied! <pre>import numpy as np\n\ndim = 16\ntotal = 2\nschema = pa.schema(\n    [\n        pa.field(\"vector\", pa.list_(pa.float16(), dim)),\n        pa.field(\"text\", pa.string())\n    ]\n)\ndata = pa.Table.from_arrays(\n    [\n        pa.array([np.random.randn(dim).astype(np.float16) for _ in range(total)],\n                pa.list_(pa.float16(), dim)),\n        pa.array([\"foo\", \"bar\"])\n    ],\n    [\"vector\", \"text\"],\n)\n\ntbl = db.create_table(\"f16_tbl\", data, schema=schema)\ntbl.schema\n</pre> import numpy as np  dim = 16 total = 2 schema = pa.schema(     [         pa.field(\"vector\", pa.list_(pa.float16(), dim)),         pa.field(\"text\", pa.string())     ] ) data = pa.Table.from_arrays(     [         pa.array([np.random.randn(dim).astype(np.float16) for _ in range(total)],                 pa.list_(pa.float16(), dim)),         pa.array([\"foo\", \"bar\"])     ],     [\"vector\", \"text\"], )  tbl = db.create_table(\"f16_tbl\", data, schema=schema) tbl.schema Out[7]: <pre>vector: fixed_size_list&lt;item: halffloat&gt;[16]\n  child 0, item: halffloat\ntext: string</pre> In\u00a0[8]: Copied! <pre>from lancedb.pydantic import Vector, LanceModel\n\nclass Content(LanceModel):\n    movie_id: int\n    vector: Vector(128)\n    genres: str\n    title: str\n    imdb_id: int\n        \n    @property\n    def imdb_url(self) -&gt; str:\n        return f\"https://www.imdb.com/title/tt{self.imdb_id}\"\n\nimport pyarrow as pa\ndb = lancedb.connect(\"~/.lancedb\")\ntable_name = \"movielens_small\"\ntable = db.create_table(table_name, schema=Content)\ntable.schema\n</pre> from lancedb.pydantic import Vector, LanceModel  class Content(LanceModel):     movie_id: int     vector: Vector(128)     genres: str     title: str     imdb_id: int              @property     def imdb_url(self) -&gt; str:         return f\"https://www.imdb.com/title/tt{self.imdb_id}\"  import pyarrow as pa db = lancedb.connect(\"~/.lancedb\") table_name = \"movielens_small\" table = db.create_table(table_name, schema=Content) table.schema Out[8]: <pre>movie_id: int64 not null\nvector: fixed_size_list&lt;item: float&gt;[128] not null\n  child 0, item: float\ngenres: string not null\ntitle: string not null\nimdb_id: int64 not null</pre> In\u00a0[9]: Copied! <pre>import pyarrow as pa\n\ndef make_batches():\n    for i in range(5):\n        yield pa.RecordBatch.from_arrays(\n            [\n                pa.array([[3.1, 4.1], [5.9, 26.5]],\n                        pa.list_(pa.float32(), 2)),\n                pa.array([\"foo\", \"bar\"]),\n                pa.array([10.0, 20.0]),\n            ],\n            [\"vector\", \"item\", \"price\"],\n        )\n\nschema = pa.schema([\n    pa.field(\"vector\", pa.list_(pa.float32(), 2)),\n    pa.field(\"item\", pa.utf8()),\n    pa.field(\"price\", pa.float32()),\n])\n\ndb.create_table(\"table4\", make_batches(), schema=schema)\n</pre> import pyarrow as pa  def make_batches():     for i in range(5):         yield pa.RecordBatch.from_arrays(             [                 pa.array([[3.1, 4.1], [5.9, 26.5]],                         pa.list_(pa.float32(), 2)),                 pa.array([\"foo\", \"bar\"]),                 pa.array([10.0, 20.0]),             ],             [\"vector\", \"item\", \"price\"],         )  schema = pa.schema([     pa.field(\"vector\", pa.list_(pa.float32(), 2)),     pa.field(\"item\", pa.utf8()),     pa.field(\"price\", pa.float32()), ])  db.create_table(\"table4\", make_batches(), schema=schema) Out[9]: <pre>LanceTable(table4)</pre> In\u00a0[10]: Copied! <pre>import pyarrow as pa\nimport pandas as pd\n\nclass PydanticSchema(LanceModel):\n    vector: Vector(2)\n    item: str\n    price: float\n\ndef make_batches():\n    for i in range(5):\n        yield pd.DataFrame(\n                {\n                    \"vector\": [[3.1, 4.1], [1, 1]],\n                    \"item\": [\"foo\", \"bar\"],\n                    \"price\": [10.0, 20.0],\n                })\n\ntbl = db.create_table(\"table5\", make_batches(), schema=PydanticSchema)\ntbl.schema\n</pre> import pyarrow as pa import pandas as pd  class PydanticSchema(LanceModel):     vector: Vector(2)     item: str     price: float  def make_batches():     for i in range(5):         yield pd.DataFrame(                 {                     \"vector\": [[3.1, 4.1], [1, 1]],                     \"item\": [\"foo\", \"bar\"],                     \"price\": [10.0, 20.0],                 })  tbl = db.create_table(\"table5\", make_batches(), schema=PydanticSchema) tbl.schema Out[10]: <pre>vector: fixed_size_list&lt;item: float&gt;[2] not null\n  child 0, item: float\nitem: string not null\nprice: double not null</pre> In\u00a0[11]: Copied! <pre>import lancedb\nfrom lancedb.pydantic import LanceModel, Vector\n\nclass Model(LanceModel):\n      vector: Vector(2)\n\ntbl = db.create_table(\"table6\", schema=Model.to_arrow_schema())\n</pre> import lancedb from lancedb.pydantic import LanceModel, Vector  class Model(LanceModel):       vector: Vector(2)  tbl = db.create_table(\"table6\", schema=Model.to_arrow_schema()) In\u00a0[12]: Copied! <pre>db.table_names()\n</pre> db.table_names() Out[12]: <pre>['table6', 'table4', 'table5', 'movielens_small']</pre> In\u00a0[13]: Copied! <pre>tbl = db.open_table(\"table4\")\ntbl.to_pandas()\n</pre> tbl = db.open_table(\"table4\") tbl.to_pandas() Out[13]: vector item price 0 [3.1, 4.1] foo 10.0 1 [5.9, 26.5] bar 20.0 2 [3.1, 4.1] foo 10.0 3 [5.9, 26.5] bar 20.0 4 [3.1, 4.1] foo 10.0 5 [5.9, 26.5] bar 20.0 6 [3.1, 4.1] foo 10.0 7 [5.9, 26.5] bar 20.0 8 [3.1, 4.1] foo 10.0 9 [5.9, 26.5] bar 20.0 In\u00a0[14]: Copied! <pre>data = [\n        {\"vector\": [1.3, 1.4], \"item\": \"fizz\", \"price\": 100.0},\n        {\"vector\": [9.5, 56.2], \"item\": \"buzz\", \"price\": 200.0}\n]\ntbl.add(data)\n</pre> data = [         {\"vector\": [1.3, 1.4], \"item\": \"fizz\", \"price\": 100.0},         {\"vector\": [9.5, 56.2], \"item\": \"buzz\", \"price\": 200.0} ] tbl.add(data) <p>You can also add a large dataset batch in one go using Iterator of supported data types</p> In\u00a0[15]: Copied! <pre>def make_batches():\n    for i in range(5):\n        yield [\n                  {\"vector\": [3.1, 4.1], \"item\": \"foo\", \"price\": 10.0},\n                  {\"vector\": [1, 1], \"item\": \"bar\", \"price\": 20.0},\n              ]\ntbl.add(make_batches())\n</pre> def make_batches():     for i in range(5):         yield [                   {\"vector\": [3.1, 4.1], \"item\": \"foo\", \"price\": 10.0},                   {\"vector\": [1, 1], \"item\": \"bar\", \"price\": 20.0},               ] tbl.add(make_batches()) In\u00a0[16]: Copied! <pre>print(len(tbl))\n      \ntbl.delete(\"price = 20.0\")\n      \nlen(tbl)\n</pre> print(len(tbl))        tbl.delete(\"price = 20.0\")        len(tbl) <pre>22\n</pre> Out[16]: <pre>12</pre> In\u00a0[17]: Copied! <pre>to_remove = [\"foo\", \"buzz\"]\nto_remove = \", \".join(str(v) for v in to_remove)\nprint(tbl.to_pandas())\ntbl.delete(f\"item IN ({to_remove})\")\n</pre> to_remove = [\"foo\", \"buzz\"] to_remove = \", \".join(str(v) for v in to_remove) print(tbl.to_pandas()) tbl.delete(f\"item IN ({to_remove})\")  <pre>         vector  item  price\n0    [3.1, 4.1]   foo   10.0\n1    [3.1, 4.1]   foo   10.0\n2    [3.1, 4.1]   foo   10.0\n3    [3.1, 4.1]   foo   10.0\n4    [3.1, 4.1]   foo   10.0\n5    [1.3, 1.4]  fizz  100.0\n6   [9.5, 56.2]  buzz  200.0\n7    [3.1, 4.1]   foo   10.0\n8    [3.1, 4.1]   foo   10.0\n9    [3.1, 4.1]   foo   10.0\n10   [3.1, 4.1]   foo   10.0\n11   [3.1, 4.1]   foo   10.0\n</pre> <pre>\n---------------------------------------------------------------------------\nOSError                                   Traceback (most recent call last)\nCell In[17], line 4\n      2 to_remove = \", \".join(str(v) for v in to_remove)\n      3 print(tbl.to_pandas())\n----&gt; 4 tbl.delete(f\"item IN ({to_remove})\")\n\nFile ~/Work/LanceDB/lancedb/docs/doc-venv/lib/python3.11/site-packages/lancedb/table.py:872, in LanceTable.delete(self, where)\n    871 def delete(self, where: str):\n--&gt; 872     self._dataset.delete(where)\n\nFile ~/Work/LanceDB/lancedb/docs/doc-venv/lib/python3.11/site-packages/lance/dataset.py:596, in LanceDataset.delete(self, predicate)\n    594 if isinstance(predicate, pa.compute.Expression):\n    595     predicate = str(predicate)\n--&gt; 596 self._ds.delete(predicate)\n\nOSError: LanceError(IO): Error during planning: column foo does not exist, /Users/runner/work/lance/lance/rust/lance-core/src/error.rs:212:23</pre> In\u00a0[\u00a0]: Copied! <pre>df = pd.DataFrame(\n                    {\n                        \"vector\": [[3.1, 4.1], [1, 1]],\n                        \"item\": [\"foo\", \"bar\"],\n                        \"price\": [10.0, 20.0],\n                    })\n\ntbl = db.create_table(\"table7\", data=df, mode=\"overwrite\")\n</pre> df = pd.DataFrame(                     {                         \"vector\": [[3.1, 4.1], [1, 1]],                         \"item\": [\"foo\", \"bar\"],                         \"price\": [10.0, 20.0],                     })  tbl = db.create_table(\"table7\", data=df, mode=\"overwrite\") In\u00a0[\u00a0]: Copied! <pre>to_remove = [10.0, 20.0]\nto_remove = \", \".join(str(v) for v in to_remove)\n\ntbl.delete(f\"price IN ({to_remove})\")\n</pre> to_remove = [10.0, 20.0] to_remove = \", \".join(str(v) for v in to_remove)  tbl.delete(f\"price IN ({to_remove})\") In\u00a0[\u00a0]: Copied! <pre>tbl.to_pandas()\n</pre> tbl.to_pandas() Out[\u00a0]: vector item price In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/tables_guide/#lancedb-tables","title":"LanceDB Tables\u00b6","text":"<p>A Table is a collection of Records in a LanceDB Database.</p> <p></p>"},{"location":"notebooks/tables_guide/#from-list-of-tuples-or-dictionaries","title":"From list of tuples or dictionaries\u00b6","text":""},{"location":"notebooks/tables_guide/#from-pandas-dataframe","title":"From pandas DataFrame\u00b6","text":""},{"location":"notebooks/tables_guide/#from-an-arrow-table","title":"From an Arrow Table\u00b6","text":"<p>You can also create LanceDB tables directly from pyarrow tables. LanceDB supports float16 type.</p>"},{"location":"notebooks/tables_guide/#from-pydantic-models","title":"From Pydantic Models\u00b6","text":"<p>LanceDB supports to create Apache Arrow Schema from a Pydantic BaseModel.</p>"},{"location":"notebooks/tables_guide/#using-iterators-writing-large-datasets","title":"Using Iterators / Writing Large Datasets\u00b6","text":"<p>It is recommended to use itertators to add large datasets in batches when creating your table in one go. This does not create multiple versions of your dataset unlike manually adding batches using <code>table.add()</code></p> <p>LanceDB additionally supports pyarrow's <code>RecordBatch</code> Iterators or other generators producing supported data types.</p>"},{"location":"notebooks/tables_guide/#heres-an-example-using-using-recordbatch-iterator-for-creating-tables","title":"Here's an example using using <code>RecordBatch</code> iterator for creating tables.\u00b6","text":""},{"location":"notebooks/tables_guide/#using-pandas-dataframe-iterator-and-pydantic-schema","title":"Using pandas <code>DataFrame</code> Iterator and Pydantic Schema\u00b6","text":"<p>You can set the schema via pyarrow schema object or using Pydantic object</p>"},{"location":"notebooks/tables_guide/#creating-empty-table","title":"Creating Empty Table\u00b6","text":"<p>You can create an empty table by just passing the schema and later add to it using <code>table.add()</code></p>"},{"location":"notebooks/tables_guide/#open-existing-tables","title":"Open Existing Tables\u00b6","text":"<p>If you forget the name of your table, you can always get a listing of all table names:</p>"},{"location":"notebooks/tables_guide/#adding-to-table","title":"Adding to table\u00b6","text":"<p>After a table has been created, you can always add more data to it using</p> <p>You can add any of the valid data structures accepted by LanceDB table, i.e, <code>dict</code>, <code>list[dict]</code>, <code>pd.DataFrame</code>, or a <code>Iterator[pa.RecordBatch]</code>. Here are some examples.</p>"},{"location":"notebooks/tables_guide/#adding-via-iterator","title":"Adding via Iterator\u00b6","text":"<p>here, we'll use pandas DataFrame Iterator</p>"},{"location":"notebooks/tables_guide/#deleting-from-a-table","title":"Deleting from a Table\u00b6","text":"<p>Use the <code>delete()</code> method on tables to delete rows from a table. To choose which rows to delete, provide a filter that matches on the metadata columns. This can delete any number of rows that match the filter, like:</p> <pre>tbl.delete('item = \"fizz\"')\n</pre>"},{"location":"notebooks/tables_guide/#delete-from-a-list-of-values","title":"Delete from a list of values\u00b6","text":""},{"location":"notebooks/diffusiondb/datagen/","title":"Datagen","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"Dataset hf://poloclub/diffusiondb\n\"\"\"\n</pre> \"\"\"Dataset hf://poloclub/diffusiondb \"\"\" In\u00a0[\u00a0]: Copied! <pre>import io\nfrom argparse import ArgumentParser\nfrom multiprocessing import Pool\n</pre> import io from argparse import ArgumentParser from multiprocessing import Pool In\u00a0[\u00a0]: Copied! <pre>import lance\nimport pyarrow as pa\nfrom datasets import load_dataset\nfrom transformers import CLIPModel, CLIPProcessor, CLIPTokenizerFast\n</pre> import lance import pyarrow as pa from datasets import load_dataset from transformers import CLIPModel, CLIPProcessor, CLIPTokenizerFast In\u00a0[\u00a0]: Copied! <pre>MODEL_ID = \"openai/clip-vit-base-patch32\"\n</pre> MODEL_ID = \"openai/clip-vit-base-patch32\" In\u00a0[\u00a0]: Copied! <pre>device = \"cuda\"\n</pre> device = \"cuda\" In\u00a0[\u00a0]: Copied! <pre>tokenizer = CLIPTokenizerFast.from_pretrained(MODEL_ID)\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n</pre> tokenizer = CLIPTokenizerFast.from_pretrained(MODEL_ID) model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device) processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\") In\u00a0[\u00a0]: Copied! <pre>schema = pa.schema(\n    [\n        pa.field(\"prompt\", pa.string()),\n        pa.field(\"seed\", pa.uint32()),\n        pa.field(\"step\", pa.uint16()),\n        pa.field(\"cfg\", pa.float32()),\n        pa.field(\"sampler\", pa.string()),\n        pa.field(\"width\", pa.uint16()),\n        pa.field(\"height\", pa.uint16()),\n        pa.field(\"timestamp\", pa.timestamp(\"s\")),\n        pa.field(\"image_nsfw\", pa.float32()),\n        pa.field(\"prompt_nsfw\", pa.float32()),\n        pa.field(\"vector\", pa.list_(pa.float32(), 512)),\n        pa.field(\"image\", pa.binary()),\n    ]\n)\n</pre> schema = pa.schema(     [         pa.field(\"prompt\", pa.string()),         pa.field(\"seed\", pa.uint32()),         pa.field(\"step\", pa.uint16()),         pa.field(\"cfg\", pa.float32()),         pa.field(\"sampler\", pa.string()),         pa.field(\"width\", pa.uint16()),         pa.field(\"height\", pa.uint16()),         pa.field(\"timestamp\", pa.timestamp(\"s\")),         pa.field(\"image_nsfw\", pa.float32()),         pa.field(\"prompt_nsfw\", pa.float32()),         pa.field(\"vector\", pa.list_(pa.float32(), 512)),         pa.field(\"image\", pa.binary()),     ] ) In\u00a0[\u00a0]: Copied! <pre>def pil_to_bytes(img) -&gt; list[bytes]:\n    buf = io.BytesIO()\n    img.save(buf, format=\"PNG\")\n    return buf.getvalue()\n</pre> def pil_to_bytes(img) -&gt; list[bytes]:     buf = io.BytesIO()     img.save(buf, format=\"PNG\")     return buf.getvalue() In\u00a0[\u00a0]: Copied! <pre>def generate_clip_embeddings(batch) -&gt; pa.RecordBatch:\n    image = processor(text=None, images=batch[\"image\"], return_tensors=\"pt\")[\n        \"pixel_values\"\n    ].to(device)\n    img_emb = model.get_image_features(image)\n    batch[\"vector\"] = img_emb.cpu().tolist()\n\n    with Pool() as p:\n        batch[\"image_bytes\"] = p.map(pil_to_bytes, batch[\"image\"])\n    return batch\n</pre> def generate_clip_embeddings(batch) -&gt; pa.RecordBatch:     image = processor(text=None, images=batch[\"image\"], return_tensors=\"pt\")[         \"pixel_values\"     ].to(device)     img_emb = model.get_image_features(image)     batch[\"vector\"] = img_emb.cpu().tolist()      with Pool() as p:         batch[\"image_bytes\"] = p.map(pil_to_bytes, batch[\"image\"])     return batch In\u00a0[\u00a0]: Copied! <pre>def datagen(args):\n    \"\"\"Generate DiffusionDB dataset, and use CLIP model to generate image embeddings.\"\"\"\n    dataset = load_dataset(\"poloclub/diffusiondb\", args.subset)\n    data = []\n    for b in dataset.map(\n        generate_clip_embeddings, batched=True, batch_size=256, remove_columns=[\"image\"]\n    )[\"train\"]:\n        b[\"image\"] = b[\"image_bytes\"]\n        del b[\"image_bytes\"]\n        data.append(b)\n    tbl = pa.Table.from_pylist(data, schema=schema)\n    return tbl\n</pre> def datagen(args):     \"\"\"Generate DiffusionDB dataset, and use CLIP model to generate image embeddings.\"\"\"     dataset = load_dataset(\"poloclub/diffusiondb\", args.subset)     data = []     for b in dataset.map(         generate_clip_embeddings, batched=True, batch_size=256, remove_columns=[\"image\"]     )[\"train\"]:         b[\"image\"] = b[\"image_bytes\"]         del b[\"image_bytes\"]         data.append(b)     tbl = pa.Table.from_pylist(data, schema=schema)     return tbl In\u00a0[\u00a0]: Copied! <pre>def main():\n    parser = ArgumentParser()\n    parser.add_argument(\n        \"-o\", \"--output\", metavar=\"DIR\", help=\"Output lance directory\", required=True\n    )\n    parser.add_argument(\n        \"-s\",\n        \"--subset\",\n        choices=[\"2m_all\", \"2m_first_10k\", \"2m_first_100k\"],\n        default=\"2m_first_10k\",\n        help=\"subset of the hg dataset\",\n    )\n\n    args = parser.parse_args()\n\n    batches = datagen(args)\n    lance.write_dataset(batches, args.output)\n</pre> def main():     parser = ArgumentParser()     parser.add_argument(         \"-o\", \"--output\", metavar=\"DIR\", help=\"Output lance directory\", required=True     )     parser.add_argument(         \"-s\",         \"--subset\",         choices=[\"2m_all\", \"2m_first_10k\", \"2m_first_100k\"],         default=\"2m_first_10k\",         help=\"subset of the hg dataset\",     )      args = parser.parse_args()      batches = datagen(args)     lance.write_dataset(batches, args.output) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    main()\n</pre> if __name__ == \"__main__\":     main()"},{"location":"overview/","title":"LanceDB Documentation","text":"<p>LanceDB is an open-source vector database for AI that's designed to store, manage, query and retrieve embeddings on large-scale multi-modal data. The core of LanceDB is written in Rust \ud83e\udd80 and is built on top of Lance, an open-source columnar data format designed for performant ML workloads and fast random access.</p> <p>Both the database and the underlying data format are designed from the ground up to be easy-to-use, scalable and cost-effective.</p> <p>Hosted LanceDB</p> <p>If you want S3 cost-efficiency and local performance via a simple serverless API, checkout LanceDB Cloud. For private deployments, high performance at extreme scale, or if you have strict security requirements, talk to us about LanceDB Enterprise. Learn more</p> <p></p>"},{"location":"overview/#truly-multi-modal","title":"Truly multi-modal","text":"<p>Most existing vector databases that store and query just the embeddings and their metadata. The actual data is stored elsewhere, requiring you to manage their storage and versioning separately.</p> <p>LanceDB supports storage of the actual data itself, alongside the embeddings and metadata. You can persist your images, videos, text documents, audio files and more in the Lance format, which provides automatic data versioning and blazing fast retrievals and filtering via LanceDB.</p>"},{"location":"overview/#open-source-and-cloud-solutions","title":"Open-source and cloud solutions","text":"<p>LanceDB is available in two flavors: OSS and Cloud.</p> <p>LanceDB OSS is an open-source, batteries-included embedded vector database that you can run on your own infrastructure. \"Embedded\" means that it runs in-process, making it incredibly simple to self-host your own AI retrieval workflows for RAG and more. No servers, no hassle.</p> <p>LanceDB Cloud is a SaaS (software-as-a-service) solution that runs serverless in the cloud, making the storage clearly separated from compute. It's designed to be cost-effective and highly scalable without breaking the bank. LanceDB Cloud is currently in private beta with general availability coming soon, but you can apply for early access with the private beta release by signing up below.</p> <p>Try out LanceDB Cloud (Public Beta) Now</p>"},{"location":"overview/#why-use-lancedb","title":"Why use LanceDB?","text":"<ul> <li> <p>Embedded (OSS) and serverless (Cloud) - no need to manage servers</p> </li> <li> <p>Fast production-scale vector similarity, full-text &amp; hybrid search and a SQL query interface (via DataFusion)</p> </li> <li> <p>Python, Javascript/Typescript, and Rust support</p> </li> <li> <p>Store, query &amp; manage multi-modal data (text, images, videos, point clouds, etc.), not just the embeddings and metadata</p> </li> <li> <p>Tight integration with the Arrow ecosystem, allowing true zero-copy access in shared memory with SIMD and GPU acceleration</p> </li> <li> <p>Automatic data versioning to manage versions of your data without needing extra infrastructure</p> </li> <li> <p>Disk-based index &amp; storage, allowing for massive scalability without breaking the bank</p> </li> <li> <p>Ingest your favorite data formats directly, like pandas DataFrames, Pydantic objects, Polars (coming soon), and more</p> </li> </ul>"},{"location":"overview/#documentation-guide","title":"Documentation guide","text":"<p>The following pages go deeper into the internal of LanceDB and how to use it.</p> <ul> <li>Quick start: Get started with LanceDB and vector DB concepts</li> <li>Vector search concepts: Understand the basics of vector search</li> <li>Working with tables: Learn how to work with tables and their associated functions</li> <li>Indexing: Understand how to create indexes</li> <li>Vector search: Learn how to perform vector similarity search</li> <li>Full-text search (native): Learn how to perform full-text search</li> <li>Full-text search (tantivy-based): Learn how to perform full-text search using Tantivy</li> <li>Managing embeddings: Managing embeddings and the embedding functions API in LanceDB</li> <li>Ecosystem Integrations: Integrate LanceDB with other tools in the data ecosystem</li> <li>Python API Reference: Python OSS and Cloud API references</li> <li>JavaScript API Reference: JavaScript OSS and Cloud API references</li> <li>Rust API Reference: Rust API reference</li> </ul>"},{"location":"overview/faq/","title":"LanceDB FAQ | Frequently Asked Questions","text":"<p>This section covers some common questions and issues that you may encounter when using LanceDB.</p>"},{"location":"overview/faq/#is-lancedb-open-source","title":"Is LanceDB open source?","text":"<p>Yes, LanceDB is an open source vector database available under an Apache 2.0 license. We also have a serverless SaaS solution, LanceDB Cloud, available under a commercial license.</p>"},{"location":"overview/faq/#what-is-the-difference-between-lance-and-lancedb","title":"What is the difference between Lance and LanceDB?","text":"<p>Lance is a modern columnar data format for AI, written in Rust \ud83e\udd80. It's perfect for building search engines, feature stores and being the foundation of large-scale ML training jobs requiring high performance IO and shuffles. It also has native support for storing, querying, and inspecting deeply nested data for robotics or large blobs like images, point clouds, and more.</p> <p>LanceDB is the vector database that's built on top of Lance, and utilizes the underlying optimized storage format to build efficient disk-based indexes that power semantic search &amp; retrieval applications, from RAGs to QA Bots to recommender systems.</p>"},{"location":"overview/faq/#why-invent-another-data-format-instead-of-using-parquet","title":"Why invent another data format instead of using Parquet?","text":"<p>As we mention in our talk titled \"Lance, a modern columnar data format\", Parquet and other tabular formats that derive from it are rather dated (Parquet is over 10 years old), especially when it comes to random access on vectors. We needed a format that's able to handle the complex trade-offs involved in shuffling, scanning, OLAP and filtering large datasets involving vectors, and our extensive experiments with Parquet didn't yield sufficient levels of performance for modern ML. Our benchmarks show that Lance is up to 1000x faster than Parquet for random access, which we believe justifies our decision to create a new data format for AI.</p>"},{"location":"overview/faq/#why-build-in-rust","title":"Why build in Rust? \ud83e\udd80","text":"<p>We believe that the Rust ecosystem has attained mainstream maturity and that Rust will form the underpinnings of large parts of the data and ML landscape in a few years. Performance, latency and reliability are paramount to a vector DB, and building in Rust allows us to iterate and release updates more rapidly due to Rust's safety guarantees. Both Lance (the data format) and LanceDB (the database) are written entirely in Rust. We also provide Python, JavaScript, and Rust client libraries to interact with the database.</p>"},{"location":"overview/faq/#what-is-the-difference-between-lancedb-oss-and-lancedb-cloud","title":"What is the difference between LanceDB OSS and LanceDB Cloud?","text":"<p>LanceDB OSS is an embedded (in-process) solution that can be used as the vector store of choice for your LLM and RAG applications. It can be embedded inside an existing application backend, or used in-process alongside existing ML and data engineering pipelines.</p> <p>LanceDB Cloud is a serverless solution \u2014 the database and data sit on the cloud and we manage the scalability of the application side via a remote client, without the need to manage any infrastructure.</p> <p>Both flavors of LanceDB benefit from the blazing fast Lance data format and are built on the same open source foundations.</p>"},{"location":"overview/faq/#what-makes-lancedb-different","title":"What makes LanceDB different?","text":"<p>LanceDB is among the few embedded vector DBs out there that we believe can unlock a whole new class of LLM-powered applications in the browser or via edge functions. Lance's multi-modal nature allows you to store the raw data, metadata and the embeddings all at once, unlike other solutions that typically store just the embeddings and metadata.</p> <p>The Lance data format that powers our storage system also provides true zero-copy access and seamless interoperability with numerous other data formats (like Pandas, Polars, Pydantic) via Apache Arrow, as well as automatic data versioning and data management without needing extra infrastructure.</p>"},{"location":"overview/faq/#how-large-of-a-dataset-can-lancedb-handle","title":"How large of a dataset can LanceDB handle?","text":"<p>LanceDB and its underlying data format, Lance, are built to scale to really large amounts of data (hundreds of terabytes). We are currently working with customers who regularly perform operations on 200M+ vectors, and we're fast approaching billion scale and beyond, which are well-handled by our disk-based indexes, without you having to break the bank.</p>"},{"location":"overview/faq/#do-i-need-to-build-an-ann-index-to-run-vector-search","title":"Do I need to build an ANN index to run vector search?","text":"<p>No. LanceDB is blazing fast (due to its disk-based index) for even brute force kNN search, within reason. In our benchmarks, computing 100K pairs of 1000-dimension vectors takes less than 20ms. For small datasets of ~100K records or applications that can accept ~100ms latency, an ANN index is usually not necessary.</p> <p>For large-scale (&gt;1M) or higher dimension vectors, it is beneficial to create an ANN index. See the ANN indexes section for more details.</p>"},{"location":"overview/faq/#does-lancedb-support-full-text-search","title":"Does LanceDB support full-text search?","text":"<p>Yes, LanceDB supports full-text search (FTS) via Tantivy. Our current FTS integration is Python-only, and our goal is to push it down to the Rust level in future versions to enable much more powerful search capabilities available to our Python, JavaScript and Rust clients. Follow along in the Github issue</p>"},{"location":"overview/faq/#how-can-i-speed-up-data-inserts","title":"How can I speed up data inserts?","text":"<p>It's highly recommend to perform bulk inserts via batches (for e.g., Pandas DataFrames or lists of dicts in Python) to speed up inserts for large datasets. Inserting records one at a time is slow and can result in suboptimal performance because each insert creates a new data fragment on disk. Batching inserts allows LanceDB to create larger fragments (and their associated manifests), which are more efficient to read and write.</p>"},{"location":"overview/faq/#do-i-need-to-set-a-refine-factor-when-using-an-index","title":"Do I need to set a refine factor when using an index?","text":"<p>Yes. LanceDB uses PQ, or Product Quantization, to compress vectors and speed up search when using an ANN index. However, because PQ is a lossy compression algorithm, it tends to reduce recall while also reducing the index size. To address this trade-off, we introduce a process called refinement. The normal process computes distances by operating on the compressed PQ vectors. The refinement factor (rf) is a multiplier that takes the top-k similar PQ vectors to a given query, fetches <code>rf * k</code> full vectors and computes the raw vector distances between them and the query vector, reordering the top-k results based on these scores instead.</p> <p>For example, if you're retrieving the top 10 results and set <code>refine_factor</code> to 25, LanceDB will fetch the 250 most similar vectors (according to PQ), compute the distances again based on the full vectors for those 250 and then re-rank based on their scores. This can significantly improve recall, with a small added latency cost (typically a few milliseconds), so it's recommended you set a <code>refine_factor</code> of anywhere between 5-50 and measure its impact on latency prior to deploying your solution.</p>"},{"location":"overview/faq/#how-can-i-improve-ivf-pq-recall-while-keeping-latency-low","title":"How can I improve IVF-PQ recall while keeping latency low?","text":"<p>When using an IVF-PQ index, there's a trade-off between recall and latency at query time. You can improve recall by increasing the number of probes and the <code>refine_factor</code>. In our benchmark on the GIST-1M dataset, we show that it's possible to achieve &gt;0.95 recall with a latency of under 10 ms on most systems, using ~50 probes and a <code>refine_factor</code> of 50. This is, of course, subject to the dataset at hand and a quick sensitivity study can be performed on your own data. You can find more details on the benchmark in our blog post.</p> <p></p>"},{"location":"overview/faq/#how-do-i-connect-to-minio","title":"How do I connect to MinIO?","text":"<p>MinIO supports an S3 compatible API. In order to connect to a MinIO instance, you need to:</p> <ul> <li>Set the envvar <code>AWS_ENDPOINT</code> to the URL of your MinIO API</li> <li>Set the envvars <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> with your MinIO credential</li> <li>Call <code>lancedb.connect(\"s3://minio_bucket_name\")</code></li> </ul>"},{"location":"overview/faq/#where-can-i-find-benchmarks-for-lancedb","title":"Where can I find benchmarks for LanceDB?","text":"<p>Refer to this post for recent benchmarks.</p>"},{"location":"overview/faq/#how-much-data-can-lancedb-practically-manage-without-effecting-performance","title":"How much data can LanceDB practically manage without effecting performance?","text":"<p>We target good performance on ~10-50 billion rows and ~10-30 TB of data.</p>"},{"location":"overview/faq/#does-lancedb-support-concurrent-operations","title":"Does LanceDB support concurrent operations?","text":"<p>LanceDB can handle concurrent reads very well, and can scale horizontally. The main constraint is how well the storage layer you've chosen scales. For writes, we support concurrent writing, though too many concurrent writers can lead to failing writes as there is a limited number of times a writer retries a commit</p> <p>Multiprocessing with LanceDB</p> <p>For multiprocessing you should probably not use <code>fork</code> as lance is multi-threaded internally and <code>fork</code> and multi-thread do not work well.Refer to this discussion</p>"},{"location":"overview/quickstart/","title":"Getting Started with LanceDB Locally","text":"<p>LanceDB can be run in a number of ways:</p> <ul> <li>Embedded within an existing backend (like your Django, Flask, Node.js or FastAPI application)</li> <li>Directly from a client application like a Jupyter notebook for analytical workloads</li> <li>Deployed as a remote serverless database</li> </ul> <p></p>"},{"location":"overview/quickstart/#installation","title":"Installation","text":"PythonTypescript<sup>1</sup>Rust <pre><code>pip install lancedb\n</code></pre> @lancedb/lancedbvectordb (deprecated) <pre><code>npm install @lancedb/lancedb\n</code></pre> <p>Bundling <code>@lancedb/lancedb</code> apps with Webpack</p> <p>Since LanceDB contains a prebuilt Node binary, you must configure <code>next.config.js</code> to exclude it from webpack. This is required for both using Next.js and deploying a LanceDB app on Vercel.</p> <pre><code>/** @type {import('next').NextConfig} */\nmodule.exports = ({\nwebpack(config) {\n    config.externals.push({ '@lancedb/lancedb': '@lancedb/lancedb' })\n    return config;\n}\n})\n</code></pre> <p>Yarn users</p> <p>Unlike other package managers, Yarn does not automatically resolve peer dependencies. If you are using Yarn, you will need to manually install 'apache-arrow':</p> <pre><code>yarn add apache-arrow\n</code></pre> <pre><code>npm install vectordb\n</code></pre> <p>Bundling <code>vectordb</code> apps with Webpack</p> <p>Since LanceDB contains a prebuilt Node binary, you must configure <code>next.config.js</code> to exclude it from webpack. This is required for both using Next.js and deploying a LanceDB app on Vercel.</p> <pre><code>/** @type {import('next').NextConfig} */\nmodule.exports = ({\nwebpack(config) {\n    config.externals.push({ vectordb: 'vectordb' })\n    return config;\n}\n})\n</code></pre> <p>Yarn users</p> <p>Unlike other package managers, Yarn does not automatically resolve peer dependencies. If you are using Yarn, you will need to manually install 'apache-arrow':</p> <pre><code>yarn add apache-arrow\n</code></pre> <pre><code>cargo add lancedb\n</code></pre> <p>To use the lancedb create, you first need to install protobuf.</p> macOSUbuntu/Debian <pre><code>brew install protobuf\n</code></pre> <pre><code>sudo apt install -y protobuf-compiler libssl-dev\n</code></pre> <p>Please also make sure you're using the same version of Arrow as in the lancedb crate</p>"},{"location":"overview/quickstart/#preview-releases","title":"Preview releases","text":"<p>Stable releases are created about every 2 weeks. For the latest features and bug fixes, you can install the preview release. These releases receive the same level of testing as stable releases, but are not guaranteed to be available for more than 6 months after they are released. Once your application is stable, we recommend switching to stable releases.</p> PythonTypescript<sup>1</sup>Rust <pre><code>pip install --pre --extra-index-url https://pypi.fury.io/lancedb/ lancedb\n</code></pre> @lancedb/lancedbvectordb (deprecated) <pre><code>npm install @lancedb/lancedb@preview\n</code></pre> <pre><code>npm install vectordb@preview\n</code></pre> <p>We don't push preview releases to crates.io, but you can referent the tag in GitHub within your Cargo dependencies:</p> <pre><code>[dependencies]\nlancedb = { git = \"https://github.com/lancedb/lancedb.git\", tag = \"vX.Y.Z-beta.N\" }\n</code></pre>"},{"location":"overview/quickstart/#connect-to-a-database","title":"Connect to a database","text":"PythonTypescript<sup>1</sup>Rust Sync APIAsync API <pre><code>import lancedb\nimport pandas as pd\nimport pyarrow as pa\n\nuri = \"data/sample-lancedb\"\ndb = lancedb.connect(uri)\n</code></pre> <pre><code>import lancedb\nimport pandas as pd\nimport pyarrow as pa\n\nuri = \"data/sample-lancedb\"\ndb = await lancedb.connect_async(uri)\n</code></pre> @lancedb/lancedbvectordb (deprecated) <pre><code>import * as lancedb from \"@lancedb/lancedb\";\nimport * as arrow from \"apache-arrow\";\n\nconst db = await lancedb.connect(databaseDir);\n</code></pre> <pre><code>const lancedb = require(\"vectordb\");\nconst uri = \"data/sample-lancedb\";\nconst db = await lancedb.connect(uri);\n</code></pre> <pre><code>#[tokio::main]\nasync fn main() -&gt; Result&lt;()&gt; {\n    let uri = \"data/sample-lancedb\";\n    let db = connect(uri).execute().await?;\n}\n</code></pre> <p>See examples/simple.rs for a full working example.</p> <p>LanceDB will create the directory if it doesn't exist (including parent directories).</p> <p>If you need a reminder of the uri, you can call <code>db.uri()</code>.</p>"},{"location":"overview/quickstart/#create-a-table","title":"Create a table","text":""},{"location":"overview/quickstart/#create-a-table-from-initial-data","title":"Create a table from initial data","text":"<p>If you have data to insert into the table at creation time, you can simultaneously create a table and insert the data into it. The schema of the data will be used as the schema of the table.</p> PythonTypescript<sup>1</sup>Rust <p>If the table already exists, LanceDB will raise an error by default. If you want to overwrite the table, you can pass in <code>mode=\"overwrite\"</code> to the <code>create_table</code> method.</p> Sync APIAsync API <pre><code>data = [\n    {\"vector\": [3.1, 4.1], \"item\": \"foo\", \"price\": 10.0},\n    {\"vector\": [5.9, 26.5], \"item\": \"bar\", \"price\": 20.0},\n]\n\ntbl = db.create_table(\"my_table\", data=data)\n</code></pre> <p>You can also pass in a pandas DataFrame directly:</p> <pre><code>df = pd.DataFrame(\n    [\n        {\"vector\": [3.1, 4.1], \"item\": \"foo\", \"price\": 10.0},\n        {\"vector\": [5.9, 26.5], \"item\": \"bar\", \"price\": 20.0},\n    ]\n)\ntbl = db.create_table(\"table_from_df\", data=df)\n</code></pre> <pre><code>data = [\n    {\"vector\": [3.1, 4.1], \"item\": \"foo\", \"price\": 10.0},\n    {\"vector\": [5.9, 26.5], \"item\": \"bar\", \"price\": 20.0},\n]\n\ntbl = await db.create_table(\"my_table_async\", data=data)\n</code></pre> <p>You can also pass in a pandas DataFrame directly:</p> <pre><code>df = pd.DataFrame(\n    [\n        {\"vector\": [3.1, 4.1], \"item\": \"foo\", \"price\": 10.0},\n        {\"vector\": [5.9, 26.5], \"item\": \"bar\", \"price\": 20.0},\n    ]\n)\n\ntbl = await db.create_table(\"table_from_df_async\", df)\n</code></pre> @lancedb/lancedbvectordb (deprecated) <pre><code>const _tbl = await db.createTable(\n  \"myTable\",\n  [\n    { vector: [3.1, 4.1], item: \"foo\", price: 10.0 },\n    { vector: [5.9, 26.5], item: \"bar\", price: 20.0 },\n  ],\n  { mode: \"overwrite\" },\n);\n</code></pre> <pre><code>const tbl = await db.createTable(\n  \"myTable\",\n  [\n    { vector: [3.1, 4.1], item: \"foo\", price: 10.0 },\n    { vector: [5.9, 26.5], item: \"bar\", price: 20.0 },\n  ],\n  { writeMode: lancedb.WriteMode.Overwrite },\n);\n</code></pre> <p>If the table already exists, LanceDB will raise an error by default. If you want to overwrite the table, you can pass in <code>mode:\"overwrite\"</code> to the <code>createTable</code> function.</p> <pre><code>let initial_data = create_some_records()?;\nlet tbl = db\n    .create_table(\"my_table\", initial_data)\n    .execute()\n    .await\n    .unwrap();\n</code></pre> <p>If the table already exists, LanceDB will raise an error by default.  See the mode option for details on how to overwrite (or open) existing tables instead.</p> <p>Providing</p> <p>The Rust SDK currently expects data to be provided as an Arrow RecordBatchReader Support for additional formats (such as serde or polars) is on the roadmap.</p> <p>Under the hood, LanceDB reads in the Apache Arrow data and persists it to disk using the Lance format.</p> <p>Automatic embedding generation with Embedding API</p> <p>When working with embedding models, it is recommended to use the LanceDB embedding API to automatically create vector representation of the data and queries in the background. See the quickstart example or the embedding API guide</p>"},{"location":"overview/quickstart/#create-an-empty-table","title":"Create an empty table","text":"<p>Sometimes you may not have the data to insert into the table at creation time. In this case, you can create an empty table and specify the schema, so that you can add data to the table at a later time (as long as it conforms to the schema). This is similar to a <code>CREATE TABLE</code> statement in SQL.</p> PythonTypescript<sup>1</sup>Rust Sync APIAsync API <pre><code>schema = pa.schema([pa.field(\"vector\", pa.list_(pa.float32(), list_size=2))])\ntbl = db.create_table(\"empty_table\", schema=schema)\n</code></pre> <pre><code>schema = pa.schema([pa.field(\"vector\", pa.list_(pa.float32(), list_size=2))])\ntbl = await db.create_table(\"empty_table_async\", schema=schema)\n</code></pre> <p>You can define schema in Pydantic</p> <p>LanceDB comes with Pydantic support, which allows you to define the schema of your data using Pydantic models. This makes it easy to work with LanceDB tables and data. Learn more about all supported types in tables guide.</p> @lancedb/lancedbvectordb (deprecated) <pre><code>const schema = new arrow.Schema([\n  new arrow.Field(\"id\", new arrow.Int32()),\n  new arrow.Field(\"name\", new arrow.Utf8()),\n]);\n\nconst emptyTbl = await db.createEmptyTable(\"empty_table\", schema);\n</code></pre> <pre><code>const schema = new arrow.Schema([\n  new arrow.Field(\"id\", new arrow.Int32()),\n  new arrow.Field(\"name\", new arrow.Utf8()),\n]);\n\nconst empty_tbl = await db.createTable({ name: \"empty_table\", schema });\n</code></pre> <pre><code>let schema = Arc::new(Schema::new(vec![\n    Field::new(\"id\", DataType::Int32, false),\n    Field::new(\"item\", DataType::Utf8, true),\n]));\ndb.create_empty_table(\"empty_table\", schema).execute().await\n</code></pre>"},{"location":"overview/quickstart/#open-an-existing-table","title":"Open an existing table","text":"<p>Once created, you can open a table as follows:</p> PythonTypescript<sup>1</sup>Rust Sync APIAsync API <pre><code>tbl = db.open_table(\"my_table\")\n</code></pre> <pre><code>tbl = await db.open_table(\"my_table_async\")\n</code></pre> @lancedb/lancedbvectordb (deprecated) <pre><code>const _tbl = await db.openTable(\"myTable\");\n</code></pre> <pre><code>const tbl = await db.openTable(\"myTable\");\n</code></pre> <pre><code>let table = db.open_table(\"my_table\").execute().await.unwrap();\n</code></pre> <p>If you forget the name of your table, you can always get a listing of all table names:</p> PythonTypescript<sup>1</sup>Rust Sync APIAsync API <pre><code>print(db.table_names())\n</code></pre> <pre><code>print(await db.table_names())\n</code></pre> @lancedb/lancedbvectordb (deprecated) <pre><code>const tableNames = await db.tableNames();\n</code></pre> <pre><code>console.log(await db.tableNames());\n</code></pre> <pre><code>println!(\"{:?}\", db.table_names().execute().await?);\n</code></pre>"},{"location":"overview/quickstart/#add-data-to-a-table","title":"Add data to a table","text":"<p>After a table has been created, you can always add more data to it as follows:</p> PythonTypescript<sup>1</sup>Rust Sync APIAsync API <pre><code># Option 1: Add a list of dicts to a table\ndata = [\n    {\"vector\": [1.3, 1.4], \"item\": \"fizz\", \"price\": 100.0},\n    {\"vector\": [9.5, 56.2], \"item\": \"buzz\", \"price\": 200.0},\n]\ntbl.add(data)\n\n# Option 2: Add a pandas DataFrame to a table\ndf = pd.DataFrame(data)\ntbl.add(data)\n</code></pre> <pre><code># Option 1: Add a list of dicts to a table\ndata = [\n    {\"vector\": [1.3, 1.4], \"item\": \"fizz\", \"price\": 100.0},\n    {\"vector\": [9.5, 56.2], \"item\": \"buzz\", \"price\": 200.0},\n]\nawait tbl.add(data)\n\n# Option 2: Add a pandas DataFrame to a table\ndf = pd.DataFrame(data)\nawait tbl.add(data)\n</code></pre> @lancedb/lancedbvectordb (deprecated) <pre><code>const data = [\n  { vector: [1.3, 1.4], item: \"fizz\", price: 100.0 },\n  { vector: [9.5, 56.2], item: \"buzz\", price: 200.0 },\n];\nawait tbl.add(data);\n</code></pre> <pre><code>const newData = Array.from({ length: 500 }, (_, i) =&gt; ({\n  vector: [i, i + 1],\n  item: \"fizz\",\n  price: i * 0.1,\n}));\nawait tbl.add(newData);\n</code></pre> <pre><code>let new_data = create_some_records()?;\ntbl.add(new_data).execute().await.unwrap();\n</code></pre>"},{"location":"overview/quickstart/#search-for-nearest-neighbors","title":"Search for nearest neighbors","text":"<p>Once you've embedded the query, you can find its nearest neighbors as follows:</p> PythonTypescript<sup>1</sup>Rust Sync APIAsync API <pre><code>tbl.search([100, 100]).limit(2).to_pandas()\n</code></pre> <pre><code>await tbl.vector_search([100, 100]).limit(2).to_pandas()\n</code></pre> <p>This returns a pandas DataFrame with the results.</p> @lancedb/lancedbvectordb (deprecated) <pre><code>const res = await tbl.search([100, 100]).limit(2).toArray();\n</code></pre> <pre><code>const query = await tbl.search([100, 100]).limit(2).execute();\n</code></pre> <pre><code>use futures::TryStreamExt;\n\ntable\n    .query()\n    .limit(2)\n    .nearest_to(&amp;[1.0; 128])?\n    .execute()\n    .await?\n    .try_collect::&lt;Vec&lt;_&gt;&gt;()\n    .await\n</code></pre> <p>Query</p> <p>Rust does not yet support automatic execution of embedding functions.  You will need to calculate embeddings yourself.  Support for this is on the roadmap and can be tracked at https://github.com/lancedb/lancedb/issues/994</p> <p>Query vectors can be provided as Arrow arrays or a Vec/slice of Rust floats. Support for additional formats (e.g. <code>polars::series::Series</code>) is on the roadmap.</p> <p>By default, LanceDB runs a brute-force scan over dataset to find the K nearest neighbours (KNN). For tables with more than 50K vectors, creating an ANN index is recommended to speed up search performance. LanceDB allows you to create an ANN index on a table as follows:</p> PythonTypescript<sup>1</sup>Rust Sync APIAsync API <pre><code>tbl.create_index(num_sub_vectors=1)\n</code></pre> <pre><code>await tbl.create_index(\"vector\")\n</code></pre> @lancedb/lancedbvectordb (deprecated) <pre><code>await tbl.createIndex(\"vector\");\n</code></pre> <pre><code>await tbl.createIndex({\n  type: \"ivf_pq\",\n  num_partitions: 2,\n  num_sub_vectors: 2,\n});\n</code></pre> <pre><code>table.create_index(&amp;[\"vector\"], Index::Auto).execute().await\n</code></pre> <p>Why do I need to create an index manually?</p> <p>LanceDB does not automatically create the ANN index for two reasons. The first is that it's optimized for really fast retrievals via a disk-based index, and the second is that data and query workloads can be very diverse, so there's no one-size-fits-all index configuration. LanceDB provides many parameters to fine-tune index size, query latency and accuracy. See the section on ANN indexes for more details.</p>"},{"location":"overview/quickstart/#delete-rows-from-a-table","title":"Delete rows from a table","text":"<p>Use the <code>delete()</code> method on tables to delete rows from a table. To choose which rows to delete, provide a filter that matches on the metadata columns. This can delete any number of rows that match the filter.</p> PythonTypescript<sup>1</sup>Rust Sync APIAsync API <pre><code>tbl.delete('item = \"fizz\"')\n</code></pre> <pre><code>await tbl.delete('item = \"fizz\"')\n</code></pre> @lancedb/lancedbvectordb (deprecated) <pre><code>await tbl.delete('item = \"fizz\"');\n</code></pre> <pre><code>await tbl.delete('item = \"fizz\"');\n</code></pre> <pre><code>tbl.delete(\"id &gt; 24\").await.unwrap();\n</code></pre> <p>The deletion predicate is a SQL expression that supports the same expressions as the <code>where()</code> clause (<code>only_if()</code> in Rust) on a search. They can be as simple or complex as needed. To see what expressions are supported, see the SQL filters section.</p> PythonTypescript<sup>1</sup>Rust Sync APIAsync API <p>Read more: lancedb.table.Table.delete</p> <p>Read more: lancedb.table.AsyncTable.delete</p> @lancedb/lancedbvectordb (deprecated) <p>Read more: lancedb.Table.delete</p> <p>Read more: vectordb.Table.delete</p> <p>Read more: lancedb::Table::delete</p>"},{"location":"overview/quickstart/#drop-a-table","title":"Drop a table","text":"<p>Use the <code>drop_table()</code> method on the database to remove a table.</p> PythonTypescript<sup>1</sup>Rust Sync APIAsync API <pre><code>db.drop_table(\"my_table\")\n</code></pre> <pre><code>await db.drop_table(\"my_table_async\")\n</code></pre> <p>This permanently removes the table and is not recoverable, unlike deleting rows. By default, if the table does not exist an exception is raised. To suppress this, you can pass in <code>ignore_missing=True</code>.</p> @lancedb/lancedbvectordb (deprecated) <pre><code>await db.dropTable(\"myTable\");\n</code></pre> <pre><code>await db.dropTable(\"myTable\");\n</code></pre> <p>This permanently removes the table and is not recoverable, unlike deleting rows. If the table does not exist an exception is raised.</p> <pre><code>db.drop_table(\"my_table\").await.unwrap();\n</code></pre>"},{"location":"overview/quickstart/#using-the-embedding-api","title":"Using the Embedding API","text":"<p>You can use the embedding API when working with embedding models. It automatically vectorizes the data at ingestion and query time and comes with built-in integrations with popular embedding models like Openai, Hugging Face, Sentence Transformers, CLIP and more.</p> PythonTypescript<sup>1</sup>Rust Sync APIAsync API <pre><code>from lancedb.pydantic import LanceModel, Vector\nfrom lancedb.embeddings import get_registry\n\n\ndb = lancedb.connect(\"/tmp/db\")\nfunc = get_registry().get(\"openai\").create(name=\"text-embedding-ada-002\")\n\nclass Words(LanceModel):\n    text: str = func.SourceField()\n    vector: Vector(func.ndims()) = func.VectorField()\n\ntable = db.create_table(\"words\", schema=Words, mode=\"overwrite\")\ntable.add([{\"text\": \"hello world\"}, {\"text\": \"goodbye world\"}])\n\nquery = \"greetings\"\nactual = table.search(query).limit(1).to_pydantic(Words)[0]\nprint(actual.text)\n</code></pre> <p>Coming soon to the async API. https://github.com/lancedb/lancedb/issues/1938</p> @lancedb/lancedb <pre><code>import * as lancedb from \"@lancedb/lancedb\";\nimport \"@lancedb/lancedb/embedding/openai\";\nimport { LanceSchema, getRegistry, register } from \"@lancedb/lancedb/embedding\";\nimport { EmbeddingFunction } from \"@lancedb/lancedb/embedding\";\nimport { type Float, Float32, Utf8 } from \"apache-arrow\";\nconst db = await lancedb.connect(databaseDir);\nconst func = getRegistry()\n  .get(\"openai\")\n  ?.create({ model: \"text-embedding-ada-002\" }) as EmbeddingFunction;\n\nconst wordsSchema = LanceSchema({\n  text: func.sourceField(new Utf8()),\n  vector: func.vectorField(),\n});\nconst tbl = await db.createEmptyTable(\"words\", wordsSchema, {\n  mode: \"overwrite\",\n});\nawait tbl.add([{ text: \"hello world\" }, { text: \"goodbye world\" }]);\n\nconst query = \"greetings\";\nconst actual = (await tbl.search(query).limit(1).toArray())[0];\n</code></pre> <pre><code>use std::{iter::once, sync::Arc};\n\nuse arrow_array::{Float64Array, Int32Array, RecordBatch, RecordBatchIterator, StringArray};\nuse arrow_schema::{DataType, Field, Schema};\nuse futures::StreamExt;\nuse lancedb::{\n    arrow::IntoArrow,\n    connect,\n    embeddings::{openai::OpenAIEmbeddingFunction, EmbeddingDefinition, EmbeddingFunction},\n    query::{ExecutableQuery, QueryBase},\n    Result,\n};\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;()&gt; {\n    let tempdir = tempfile::tempdir().unwrap();\n    let tempdir = tempdir.path().to_str().unwrap();\n    let api_key = std::env::var(\"OPENAI_API_KEY\").expect(\"OPENAI_API_KEY is not set\");\n    let embedding = Arc::new(OpenAIEmbeddingFunction::new_with_model(\n        api_key,\n        \"text-embedding-3-large\",\n    )?);\n\n    let db = connect(tempdir).execute().await?;\n    db.embedding_registry()\n        .register(\"openai\", embedding.clone())?;\n\n    let table = db\n        .create_table(\"vectors\", make_data())\n        .add_embedding(EmbeddingDefinition::new(\n            \"text\",\n            \"openai\",\n            Some(\"embeddings\"),\n        ))?\n        .execute()\n        .await?;\n\n    let query = Arc::new(StringArray::from_iter_values(once(\"something warm\")));\n    let query_vector = embedding.compute_query_embeddings(query)?;\n    let mut results = table\n        .vector_search(query_vector)?\n        .limit(1)\n        .execute()\n        .await?;\n\n    let rb = results.next().await.unwrap()?;\n    let out = rb\n        .column_by_name(\"text\")\n        .unwrap()\n        .as_any()\n        .downcast_ref::&lt;StringArray&gt;()\n        .unwrap();\n    let text = out.iter().next().unwrap().unwrap();\n    println!(\"Closest match: {}\", text);\n    Ok(())\n}\n</code></pre> <p>Learn about using the existing integrations and creating custom embedding functions in the embedding API guide.</p>"},{"location":"overview/quickstart/#whats-next","title":"What's next","text":"<p>This section covered the very basics of using LanceDB. If you're learning about vector databases for the first time, you may want to read the page on indexing to get familiar with the concepts.</p> <p>If you've already worked with other vector databases, you may want to read the guides to learn how to work with LanceDB in more detail.</p> <ol> <li> <p>The <code>vectordb</code> package is a legacy package that is  deprecated in favor of <code>@lancedb/lancedb</code>.  The <code>vectordb</code> package will continue to receive bug fixes and security updates until September 2024.  We recommend all new projects use <code>@lancedb/lancedb</code>.  See the migration guide for more information.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"python/duckdb/","title":"DuckDB","text":"<p>In Python, LanceDB tables can also be queried with DuckDB, an in-process SQL OLAP database. This means you can write complex SQL queries to analyze your data in LanceDB.</p> <p>This integration is done via Apache Arrow, which provides zero-copy data sharing between LanceDB and DuckDB. DuckDB is capable of passing down column selections and basic filters to LanceDB, reducing the amount of data that needs to be scanned to perform your query. Finally, the integration allows streaming data from LanceDB tables, allowing you to aggregate tables that won't fit into memory. All of this uses the same mechanism described in DuckDB's blog post DuckDB quacks Arrow.</p> <p>We can demonstrate this by first installing <code>duckdb</code> and <code>lancedb</code>.</p> <pre><code>pip install duckdb lancedb\n</code></pre> <p>We will re-use the dataset created previously:</p> <pre><code>import lancedb\n\ndb = lancedb.connect(\"data/sample-lancedb\")\ndata = [\n    {\"vector\": [3.1, 4.1], \"item\": \"foo\", \"price\": 10.0},\n    {\"vector\": [5.9, 26.5], \"item\": \"bar\", \"price\": 20.0}\n]\ntable = db.create_table(\"pd_table\", data=data)\n</code></pre> <p>The <code>to_lance</code> method converts the LanceDB table to a <code>LanceDataset</code>, which is accessible to DuckDB through the Arrow compatibility layer. To query the resulting Lance dataset in DuckDB, all you need to do is reference the dataset by the same name in your SQL query.</p> <pre><code>import duckdb\n\narrow_table = table.to_lance()\n\nduckdb.query(\"SELECT * FROM arrow_table\")\n</code></pre> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   vector    \u2502  item   \u2502 price  \u2502\n\u2502   float[]   \u2502 varchar \u2502 double \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 [3.1, 4.1]  \u2502 foo     \u2502   10.0 \u2502\n\u2502 [5.9, 26.5] \u2502 bar     \u2502   20.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>You can very easily run any other DuckDB SQL queries on your data.</p> <pre><code>duckdb.query(\"SELECT mean(price) FROM arrow_table\")\n</code></pre> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 mean(price) \u2502\n\u2502   double    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502        15.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"python/pandas_and_pyarrow/","title":"Pandas and PyArrow","text":"<p>Because Lance is built on top of Apache Arrow, LanceDB is tightly integrated with the Python data ecosystem, including Pandas and PyArrow. The sequence of steps in a typical workflow is shown below.</p>"},{"location":"python/pandas_and_pyarrow/#create-dataset","title":"Create dataset","text":"<p>First, we need to connect to a LanceDB database.</p> Sync APIAsync API <pre><code>import lancedb\n\nuri = \"data/sample-lancedb\"\ndb = lancedb.connect(uri)\n</code></pre> <pre><code>import lancedb\n\nuri = \"data/sample-lancedb\"\nasync_db = await lancedb.connect_async(uri)\n</code></pre> <p>We can load a Pandas <code>DataFrame</code> to LanceDB directly.</p> Sync APIAsync API <pre><code>import pandas as pd\n\ndata = pd.DataFrame(\n    {\n        \"vector\": [[3.1, 4.1], [5.9, 26.5]],\n        \"item\": [\"foo\", \"bar\"],\n        \"price\": [10.0, 20.0],\n    }\n)\ntable = db.create_table(\"pd_table\", data=data)\n</code></pre> <pre><code>import pandas as pd\n\ndata = pd.DataFrame(\n    {\n        \"vector\": [[3.1, 4.1], [5.9, 26.5]],\n        \"item\": [\"foo\", \"bar\"],\n        \"price\": [10.0, 20.0],\n    }\n)\nawait async_db.create_table(\"pd_table_async\", data=data)\n</code></pre> <p>Similar to the <code>pyarrow.write_dataset()</code> method, LanceDB's <code>db.create_table()</code> accepts data in a variety of forms.</p> <p>If you have a dataset that is larger than memory, you can create a table with <code>Iterator[pyarrow.RecordBatch]</code> to lazily load the data:</p> Sync APIAsync API <pre><code>from typing import Iterable\n\nimport pyarrow as pa\n\ndef make_batches() -&gt; Iterable[pa.RecordBatch]:\n    for i in range(5):\n        yield pa.RecordBatch.from_arrays(\n            [\n                pa.array([[3.1, 4.1], [5.9, 26.5]]),\n                pa.array([\"foo\", \"bar\"]),\n                pa.array([10.0, 20.0]),\n            ],\n            [\"vector\", \"item\", \"price\"],\n        )\n\n\nschema = pa.schema(\n    [\n        pa.field(\"vector\", pa.list_(pa.float32())),\n        pa.field(\"item\", pa.utf8()),\n        pa.field(\"price\", pa.float32()),\n    ]\n)\ntable = db.create_table(\"iterable_table\", data=make_batches(), schema=schema)\n</code></pre> <pre><code>from typing import Iterable\n\nimport pyarrow as pa\n\ndef make_batches() -&gt; Iterable[pa.RecordBatch]:\n    for i in range(5):\n        yield pa.RecordBatch.from_arrays(\n            [\n                pa.array([[3.1, 4.1], [5.9, 26.5]]),\n                pa.array([\"foo\", \"bar\"]),\n                pa.array([10.0, 20.0]),\n            ],\n            [\"vector\", \"item\", \"price\"],\n        )\n\n\nschema = pa.schema(\n    [\n        pa.field(\"vector\", pa.list_(pa.float32())),\n        pa.field(\"item\", pa.utf8()),\n        pa.field(\"price\", pa.float32()),\n    ]\n)\nawait async_db.create_table(\n    \"iterable_table_async\", data=make_batches(), schema=schema\n)\n</code></pre> <p>You will find detailed instructions of creating a LanceDB dataset in Getting Started and API sections.</p>"},{"location":"python/pandas_and_pyarrow/#vector-search","title":"Vector search","text":"<p>We can now perform similarity search via the LanceDB Python API.</p> Sync APIAsync API <pre><code># Open the table previously created.\ntable = db.open_table(\"pd_table\")\n\nquery_vector = [100, 100]\n# Pandas DataFrame\ndf = table.search(query_vector).limit(1).to_pandas()\nprint(df)\n</code></pre> <pre><code># Open the table previously created.\nasync_tbl = await async_db.open_table(\"pd_table_async\")\n\nquery_vector = [100, 100]\n# Pandas DataFrame\ndf = await (await async_tbl.search(query_vector)).limit(1).to_pandas()\nprint(df)\n</code></pre> <pre><code>    vector     item  price    _distance\n0  [5.9, 26.5]  bar   20.0  14257.05957\n</code></pre> <p>If you have a simple filter, it's faster to provide a <code>where</code> clause to LanceDB's <code>search</code> method. For more complex filters or aggregations, you can always resort to using the underlying <code>DataFrame</code> methods after performing a search.</p> Sync APIAsync API <pre><code># Apply the filter via LanceDB\nresults = table.search([100, 100]).where(\"price &lt; 15\").to_pandas()\nassert len(results) == 1\nassert results[\"item\"].iloc[0] == \"foo\"\n\n# Apply the filter via Pandas\ndf = results = table.search([100, 100]).to_pandas()\nresults = df[df.price &lt; 15]\nassert len(results) == 1\nassert results[\"item\"].iloc[0] == \"foo\"\n</code></pre> <pre><code># Apply the filter via LanceDB\nresults = await (await async_tbl.search([100, 100])).where(\"price &lt; 15\").to_pandas()\nassert len(results) == 1\nassert results[\"item\"].iloc[0] == \"foo\"\n\n# Apply the filter via Pandas\ndf = results = await (await async_tbl.search([100, 100])).to_pandas()\nresults = df[df.price &lt; 15]\nassert len(results) == 1\nassert results[\"item\"].iloc[0] == \"foo\"\n</code></pre>"},{"location":"python/polars_arrow/","title":"Polars","text":"<p>LanceDB supports Polars, a blazingly fast DataFrame library for Python written in Rust. Just like in Pandas, the Polars integration is enabled by PyArrow under the hood. A deeper integration between Lance Tables and Polars DataFrames is in progress, but at the moment, you can read a Polars DataFrame into LanceDB and output the search results from a query to a Polars DataFrame.</p>"},{"location":"python/polars_arrow/#create-query-lancedb-table","title":"Create &amp; Query LanceDB Table","text":""},{"location":"python/polars_arrow/#from-polars-dataframe","title":"From Polars DataFrame","text":"<p>First, we connect to a LanceDB database.</p> Sync APIAsync API <pre><code>import lancedb\n\nuri = \"data/sample-lancedb\"\ndb = lancedb.connect(uri)\n</code></pre> <pre><code>import lancedb\n\nuri = \"data/sample-lancedb\"\nasync_db = await lancedb.connect_async(uri)\n</code></pre> <p>We can load a Polars <code>DataFrame</code> to LanceDB directly.</p> Sync APIAsync API <pre><code>import polars as pl\n\ndata = pl.DataFrame(\n    {\n        \"vector\": [[3.1, 4.1], [5.9, 26.5]],\n        \"item\": [\"foo\", \"bar\"],\n        \"price\": [10.0, 20.0],\n    }\n)\ntable = db.create_table(\"pl_table\", data=data)\n</code></pre> <pre><code>import polars as pl\n\ndata = pl.DataFrame(\n    {\n        \"vector\": [[3.1, 4.1], [5.9, 26.5]],\n        \"item\": [\"foo\", \"bar\"],\n        \"price\": [10.0, 20.0],\n    }\n)\ntable = await db.create_table(\"pl_table_async\", data=data)\n</code></pre> <p>We can now perform similarity search via the LanceDB Python API.</p> Sync APIAsync API <pre><code>query = [3.0, 4.0]\nresult = table.search(query).limit(1).to_polars()\nprint(result)\nprint(type(result))\n</code></pre> <pre><code>query = [3.0, 4.0]\nresult = await (await table.search(query)).limit(1).to_polars()\nprint(result)\nprint(type(result))\n</code></pre> <p>In addition to the selected columns, LanceDB also returns a vector and also the <code>_distance</code> column which is the distance between the query vector and the returned vector.</p> <pre><code>shape: (1, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 vector        \u2506 item \u2506 price \u2506 _distance \u2502\n\u2502 ---           \u2506 ---  \u2506 ---   \u2506 ---       \u2502\n\u2502 array[f32, 2] \u2506 str  \u2506 f64   \u2506 f32       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 [3.1, 4.1]    \u2506 foo  \u2506 10.0  \u2506 0.0       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&lt;class 'polars.dataframe.frame.DataFrame'&gt;\n</code></pre> <p>Note that the type of the result from a table search is a Polars DataFrame.</p>"},{"location":"python/polars_arrow/#from-pydantic-models","title":"From Pydantic Models","text":"<p>Alternately, we can create an empty LanceDB Table using a Pydantic schema and populate it with a Polars DataFrame.</p> <pre><code>import polars as pl\n\nfrom lancedb.pydantic import Vector, LanceModel\n\nclass Item(LanceModel):\n    vector: Vector(2)\n    item: str\n    price: float\n\n\ntable = db.create_table(\"pydantic_table\", schema=Item)\ndf = pl.DataFrame(data)\n# Add Polars DataFrame to table\ntable.add(df)\n</code></pre> <p>The table can now be queried as usual.</p> <pre><code>query = [3.0, 4.0]\nresult = table.search(query).limit(1).to_polars()\nprint(result)\nprint(type(result))\n</code></pre> <pre><code>shape: (1, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 vector        \u2506 item \u2506 price \u2506 _distance \u2502\n\u2502 ---           \u2506 ---  \u2506 ---   \u2506 ---       \u2502\n\u2502 array[f32, 2] \u2506 str  \u2506 f64   \u2506 f32       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 [3.1, 4.1]    \u2506 foo  \u2506 10.0  \u2506 0.02      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&lt;class 'polars.dataframe.frame.DataFrame'&gt;\n</code></pre> <p>This result is the same as the previous one, with a DataFrame returned.</p>"},{"location":"python/polars_arrow/#dump-table-to-lazyframe","title":"Dump Table to LazyFrame","text":"<p>As you iterate on your application, you'll likely need to work with the whole table's data pretty frequently. LanceDB tables can also be converted directly into a polars LazyFrame for further processing.</p> <pre><code>ldf = table.to_polars()\nprint(type(ldf))\n</code></pre> <p>Unlike the search result from a query, we can see that the type of the result is a LazyFrame.</p> <pre><code>&lt;class 'polars.lazyframe.frame.LazyFrame'&gt;\n</code></pre> <p>We can now work with the LazyFrame as we would in Polars, and collect the first result.</p> <pre><code>print(ldf.first().collect())\n</code></pre> <pre><code>shape: (1, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 vector        \u2506 item \u2506 price \u2502\n\u2502 ---           \u2506 ---  \u2506 ---   \u2502\n\u2502 array[f32, 2] \u2506 str  \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 [3.1, 4.1]    \u2506 foo  \u2506 10.0  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The reason it's beneficial to not convert the LanceDB Table to a DataFrame is because the table can potentially be way larger than memory, and Polars LazyFrames allow us to work with such larger-than-memory datasets by not loading it into memory all at once.</p>"},{"location":"python/pydantic/","title":"Pydantic","text":"<p>Pydantic is a data validation library in Python. LanceDB integrates with Pydantic for schema inference, data ingestion, and query result casting. Using LanceModel, users can seamlessly integrate Pydantic with the rest of the LanceDB APIs.</p> <pre><code>import lancedb\nfrom lancedb.pydantic import Vector, LanceModel\n\nclass PersonModel(LanceModel):\n    name: str\n    age: int\n    vector: Vector(2)\n\n\nurl = \"./example\"\ndb = lancedb.connect(url)\ntable = db.create_table(\"person\", schema=PersonModel)\ntable.add(\n    [\n        PersonModel(name=\"bob\", age=1, vector=[1.0, 2.0]),\n        PersonModel(name=\"alice\", age=2, vector=[3.0, 4.0]),\n    ]\n)\nassert table.count_rows() == 2\nperson = table.search([0.0, 0.0]).limit(1).to_pydantic(PersonModel)\nassert person[0].name == \"bob\"\n</code></pre>"},{"location":"python/pydantic/#vector-field","title":"Vector Field","text":"<p>LanceDB provides a <code>Vector(dim)</code> method to define a vector Field in a Pydantic Model.</p>"},{"location":"python/pydantic/#lancedb.pydantic.Vector","title":"lancedb.pydantic.Vector","text":"<pre><code>Vector(dim: int, value_type: DataType = pa.float32(), nullable: bool = True) -&gt; Type[FixedSizeListMixin]\n</code></pre> <p>Pydantic Vector Type.</p> <p>Warning</p> <p>Experimental feature.</p> <p>Parameters:</p> <ul> <li> <code>dim</code>               (<code>int</code>)           \u2013            <p>The dimension of the vector.</p> </li> <li> <code>value_type</code>               (<code>DataType</code>, default:                   <code>float32()</code> )           \u2013            <p>The value type of the vector, by default pa.float32()</p> </li> <li> <code>nullable</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether the vector is nullable, by default it is True.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pydantic\n&gt;&gt;&gt; from lancedb.pydantic import Vector\n...\n&gt;&gt;&gt; class MyModel(pydantic.BaseModel):\n...     id: int\n...     url: str\n...     embeddings: Vector(768)\n&gt;&gt;&gt; schema = pydantic_to_schema(MyModel)\n&gt;&gt;&gt; assert schema == pa.schema([\n...     pa.field(\"id\", pa.int64(), False),\n...     pa.field(\"url\", pa.utf8(), False),\n...     pa.field(\"embeddings\", pa.list_(pa.float32(), 768))\n... ])\n</code></pre> Source code in <code>lancedb/pydantic.py</code> <pre><code>def Vector(\n    dim: int, value_type: pa.DataType = pa.float32(), nullable: bool = True\n) -&gt; Type[FixedSizeListMixin]:\n    \"\"\"Pydantic Vector Type.\n\n    !!! warning\n        Experimental feature.\n\n    Parameters\n    ----------\n    dim : int\n        The dimension of the vector.\n    value_type : pyarrow.DataType, optional\n        The value type of the vector, by default pa.float32()\n    nullable : bool, optional\n        Whether the vector is nullable, by default it is True.\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; import pydantic\n    &gt;&gt;&gt; from lancedb.pydantic import Vector\n    ...\n    &gt;&gt;&gt; class MyModel(pydantic.BaseModel):\n    ...     id: int\n    ...     url: str\n    ...     embeddings: Vector(768)\n    &gt;&gt;&gt; schema = pydantic_to_schema(MyModel)\n    &gt;&gt;&gt; assert schema == pa.schema([\n    ...     pa.field(\"id\", pa.int64(), False),\n    ...     pa.field(\"url\", pa.utf8(), False),\n    ...     pa.field(\"embeddings\", pa.list_(pa.float32(), 768))\n    ... ])\n    \"\"\"\n\n    # TODO: make a public parameterized type.\n    class FixedSizeList(list, FixedSizeListMixin):\n        def __repr__(self):\n            return f\"FixedSizeList(dim={dim})\"\n\n        @staticmethod\n        def nullable() -&gt; bool:\n            return nullable\n\n        @staticmethod\n        def dim() -&gt; int:\n            return dim\n\n        @staticmethod\n        def value_arrow_type() -&gt; pa.DataType:\n            return value_type\n\n        @classmethod\n        def __get_pydantic_core_schema__(\n            cls, _source_type: Any, _handler: pydantic.GetCoreSchemaHandler\n        ) -&gt; CoreSchema:\n            return core_schema.no_info_after_validator_function(\n                cls,\n                core_schema.list_schema(\n                    min_length=dim,\n                    max_length=dim,\n                    items_schema=core_schema.float_schema(),\n                ),\n            )\n\n        @classmethod\n        def __get_validators__(cls) -&gt; Generator[Callable, None, None]:\n            yield cls.validate\n\n        # For pydantic v1\n        @classmethod\n        def validate(cls, v):\n            if not isinstance(v, (list, range, np.ndarray)) or len(v) != dim:\n                raise TypeError(\"A list of numbers or numpy.ndarray is needed\")\n            return cls(v)\n\n        if PYDANTIC_VERSION.major &lt; 2:\n\n            @classmethod\n            def __modify_schema__(cls, field_schema: Dict[str, Any]):\n                field_schema[\"items\"] = {\"type\": \"number\"}\n                field_schema[\"maxItems\"] = dim\n                field_schema[\"minItems\"] = dim\n\n    return FixedSizeList\n</code></pre>"},{"location":"python/pydantic/#type-conversion","title":"Type Conversion","text":"<p>LanceDB automatically convert Pydantic fields to Apache Arrow DataType.</p> <p>Current supported type conversions:</p> Pydantic Field Type PyArrow Data Type <code>int</code> <code>pyarrow.int64</code> <code>float</code> <code>pyarrow.float64</code> <code>bool</code> <code>pyarrow.bool</code> <code>str</code> <code>pyarrow.utf8()</code> <code>list</code> <code>pyarrow.List</code> <code>BaseModel</code> <code>pyarrow.Struct</code> <code>Vector(n)</code> <code>pyarrow.FixedSizeList(float32, n)</code> <p>LanceDB supports to create Apache Arrow Schema from a Pydantic BaseModel via pydantic_to_schema() method.</p>"},{"location":"python/pydantic/#lancedb.pydantic.pydantic_to_schema","title":"lancedb.pydantic.pydantic_to_schema","text":"<pre><code>pydantic_to_schema(model: Type[BaseModel]) -&gt; Schema\n</code></pre> <p>Convert a Pydantic Model to a    PyArrow Schema.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Type[BaseModel]</code>)           \u2013            <p>The Pydantic BaseModel to convert to Arrow Schema.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Schema</code>           \u2013            <p>The Arrow Schema</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from typing import List, Optional\n&gt;&gt;&gt; import pydantic\n&gt;&gt;&gt; from lancedb.pydantic import pydantic_to_schema, Vector\n&gt;&gt;&gt; class FooModel(pydantic.BaseModel):\n...     id: int\n...     s: str\n...     vec: Vector(1536)  # fixed_size_list&lt;item: float32&gt;[1536]\n...     li: List[int]\n...\n&gt;&gt;&gt; schema = pydantic_to_schema(FooModel)\n&gt;&gt;&gt; assert schema == pa.schema([\n...     pa.field(\"id\", pa.int64(), False),\n...     pa.field(\"s\", pa.utf8(), False),\n...     pa.field(\"vec\", pa.list_(pa.float32(), 1536)),\n...     pa.field(\"li\", pa.list_(pa.int64()), False),\n... ])\n</code></pre> Source code in <code>lancedb/pydantic.py</code> <pre><code>def pydantic_to_schema(model: Type[pydantic.BaseModel]) -&gt; pa.Schema:\n    \"\"\"Convert a [Pydantic Model][pydantic.BaseModel] to a\n       [PyArrow Schema][pyarrow.Schema].\n\n    Parameters\n    ----------\n    model : Type[pydantic.BaseModel]\n        The Pydantic BaseModel to convert to Arrow Schema.\n\n    Returns\n    -------\n    pyarrow.Schema\n        The Arrow Schema\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; from typing import List, Optional\n    &gt;&gt;&gt; import pydantic\n    &gt;&gt;&gt; from lancedb.pydantic import pydantic_to_schema, Vector\n    &gt;&gt;&gt; class FooModel(pydantic.BaseModel):\n    ...     id: int\n    ...     s: str\n    ...     vec: Vector(1536)  # fixed_size_list&lt;item: float32&gt;[1536]\n    ...     li: List[int]\n    ...\n    &gt;&gt;&gt; schema = pydantic_to_schema(FooModel)\n    &gt;&gt;&gt; assert schema == pa.schema([\n    ...     pa.field(\"id\", pa.int64(), False),\n    ...     pa.field(\"s\", pa.utf8(), False),\n    ...     pa.field(\"vec\", pa.list_(pa.float32(), 1536)),\n    ...     pa.field(\"li\", pa.list_(pa.int64()), False),\n    ... ])\n    \"\"\"\n    fields = _pydantic_model_to_fields(model)\n    return pa.schema(fields)\n</code></pre>"},{"location":"python/python/","title":"Python API Reference","text":"<p>This section contains the API reference for the Python API. There is a synchronous and an asynchronous API client.</p> <p>The general flow of using the API is:</p> <ol> <li>Use lancedb.connect or lancedb.connect_async to connect to a database.</li> <li>Use the returned lancedb.DBConnection or lancedb.AsyncConnection to    create or open tables.</li> <li>Use the returned lancedb.table.Table or lancedb.AsyncTable to query    or modify tables.</li> </ol>"},{"location":"python/python/#installation","title":"Installation","text":"<pre><code>pip install lancedb\n</code></pre> <p>The following methods describe the synchronous API client. There is also an asynchronous API client.</p>"},{"location":"python/python/#connections-synchronous","title":"Connections (Synchronous)","text":""},{"location":"python/python/#lancedb.connect","title":"lancedb.connect","text":"<pre><code>connect(uri: URI, *, api_key: Optional[str] = None, region: str = 'us-east-1', host_override: Optional[str] = None, read_consistency_interval: Optional[timedelta] = None, request_thread_pool: Optional[Union[int, ThreadPoolExecutor]] = None, client_config: Union[ClientConfig, Dict[str, Any], None] = None, storage_options: Optional[Dict[str, str]] = None, **kwargs: Any) -&gt; DBConnection\n</code></pre> <p>Connect to a LanceDB database.</p> <p>Parameters:</p> <ul> <li> <code>uri</code>               (<code>URI</code>)           \u2013            <p>The uri of the database.</p> </li> <li> <code>api_key</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>If presented, connect to LanceDB cloud. Otherwise, connect to a database on file system or cloud storage. Can be set via environment variable <code>LANCEDB_API_KEY</code>.</p> </li> <li> <code>region</code>               (<code>str</code>, default:                   <code>'us-east-1'</code> )           \u2013            <p>The region to use for LanceDB Cloud.</p> </li> <li> <code>host_override</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The override url for LanceDB Cloud.</p> </li> <li> <code>read_consistency_interval</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>(For LanceDB OSS only) The interval at which to check for updates to the table from other processes. If None, then consistency is not checked. For performance reasons, this is the default. For strong consistency, set this to zero seconds. Then every read will check for updates from other processes. As a compromise, you can set this to a non-zero timedelta for eventual consistency. If more than that interval has passed since the last check, then the table will be checked for updates. Note: this consistency only applies to read operations. Write operations are always consistent.</p> </li> <li> <code>client_config</code>               (<code>Union[ClientConfig, Dict[str, Any], None]</code>, default:                   <code>None</code> )           \u2013            <p>Configuration options for the LanceDB Cloud HTTP client. If a dict, then the keys are the attributes of the ClientConfig class. If None, then the default configuration is used.</p> </li> <li> <code>storage_options</code>               (<code>Optional[Dict[str, str]]</code>, default:                   <code>None</code> )           \u2013            <p>Additional options for the storage backend. See available options at https://lancedb.github.io/lancedb/guides/storage/</p> </li> </ul> <p>Examples:</p> <p>For a local directory, provide a path for the database:</p> <pre><code>&gt;&gt;&gt; import lancedb\n&gt;&gt;&gt; db = lancedb.connect(\"~/.lancedb\")\n</code></pre> <p>For object storage, use a URI prefix:</p> <pre><code>&gt;&gt;&gt; db = lancedb.connect(\"s3://my-bucket/lancedb\",\n...                      storage_options={\"aws_access_key_id\": \"***\"})\n</code></pre> <p>Connect to LanceDB cloud:</p> <pre><code>&gt;&gt;&gt; db = lancedb.connect(\"db://my_database\", api_key=\"ldb_...\",\n...                      client_config={\"retry_config\": {\"retries\": 5}})\n</code></pre> <p>Returns:</p> <ul> <li> <code>conn</code> (              <code>DBConnection</code> )          \u2013            <p>A connection to a LanceDB database.</p> </li> </ul> Source code in <code>lancedb/__init__.py</code> <pre><code>def connect(\n    uri: URI,\n    *,\n    api_key: Optional[str] = None,\n    region: str = \"us-east-1\",\n    host_override: Optional[str] = None,\n    read_consistency_interval: Optional[timedelta] = None,\n    request_thread_pool: Optional[Union[int, ThreadPoolExecutor]] = None,\n    client_config: Union[ClientConfig, Dict[str, Any], None] = None,\n    storage_options: Optional[Dict[str, str]] = None,\n    **kwargs: Any,\n) -&gt; DBConnection:\n    \"\"\"Connect to a LanceDB database.\n\n    Parameters\n    ----------\n    uri: str or Path\n        The uri of the database.\n    api_key: str, optional\n        If presented, connect to LanceDB cloud.\n        Otherwise, connect to a database on file system or cloud storage.\n        Can be set via environment variable `LANCEDB_API_KEY`.\n    region: str, default \"us-east-1\"\n        The region to use for LanceDB Cloud.\n    host_override: str, optional\n        The override url for LanceDB Cloud.\n    read_consistency_interval: timedelta, default None\n        (For LanceDB OSS only)\n        The interval at which to check for updates to the table from other\n        processes. If None, then consistency is not checked. For performance\n        reasons, this is the default. For strong consistency, set this to\n        zero seconds. Then every read will check for updates from other\n        processes. As a compromise, you can set this to a non-zero timedelta\n        for eventual consistency. If more than that interval has passed since\n        the last check, then the table will be checked for updates. Note: this\n        consistency only applies to read operations. Write operations are\n        always consistent.\n    client_config: ClientConfig or dict, optional\n        Configuration options for the LanceDB Cloud HTTP client. If a dict, then\n        the keys are the attributes of the ClientConfig class. If None, then the\n        default configuration is used.\n    storage_options: dict, optional\n        Additional options for the storage backend. See available options at\n        &lt;https://lancedb.github.io/lancedb/guides/storage/&gt;\n\n    Examples\n    --------\n\n    For a local directory, provide a path for the database:\n\n    &gt;&gt;&gt; import lancedb\n    &gt;&gt;&gt; db = lancedb.connect(\"~/.lancedb\")\n\n    For object storage, use a URI prefix:\n\n    &gt;&gt;&gt; db = lancedb.connect(\"s3://my-bucket/lancedb\",\n    ...                      storage_options={\"aws_access_key_id\": \"***\"})\n\n    Connect to LanceDB cloud:\n\n    &gt;&gt;&gt; db = lancedb.connect(\"db://my_database\", api_key=\"ldb_...\",\n    ...                      client_config={\"retry_config\": {\"retries\": 5}})\n\n    Returns\n    -------\n    conn : DBConnection\n        A connection to a LanceDB database.\n    \"\"\"\n    if isinstance(uri, str) and uri.startswith(\"db://\"):\n        if api_key is None:\n            api_key = os.environ.get(\"LANCEDB_API_KEY\")\n        if api_key is None:\n            raise ValueError(f\"api_key is required to connected LanceDB cloud: {uri}\")\n        if isinstance(request_thread_pool, int):\n            request_thread_pool = ThreadPoolExecutor(request_thread_pool)\n        return RemoteDBConnection(\n            uri,\n            api_key,\n            region,\n            host_override,\n            # TODO: remove this (deprecation warning downstream)\n            request_thread_pool=request_thread_pool,\n            client_config=client_config,\n            storage_options=storage_options,\n            **kwargs,\n        )\n\n    if kwargs:\n        raise ValueError(f\"Unknown keyword arguments: {kwargs}\")\n    return LanceDBConnection(\n        uri,\n        read_consistency_interval=read_consistency_interval,\n        storage_options=storage_options,\n    )\n</code></pre>"},{"location":"python/python/#lancedb.db.DBConnection","title":"lancedb.db.DBConnection","text":"<p>               Bases: <code>EnforceOverrides</code></p> <p>An active LanceDB connection interface.</p> Source code in <code>lancedb/db.py</code> <pre><code>class DBConnection(EnforceOverrides):\n    \"\"\"An active LanceDB connection interface.\"\"\"\n\n    @abstractmethod\n    def table_names(\n        self, page_token: Optional[str] = None, limit: int = 10\n    ) -&gt; Iterable[str]:\n        \"\"\"List all tables in this database, in sorted order\n\n        Parameters\n        ----------\n        page_token: str, optional\n            The token to use for pagination. If not present, start from the beginning.\n            Typically, this token is last table name from the previous page.\n            Only supported by LanceDb Cloud.\n        limit: int, default 10\n            The size of the page to return.\n            Only supported by LanceDb Cloud.\n\n        Returns\n        -------\n        Iterable of str\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def create_table(\n        self,\n        name: str,\n        data: Optional[DATA] = None,\n        schema: Optional[Union[pa.Schema, LanceModel]] = None,\n        mode: str = \"create\",\n        exist_ok: bool = False,\n        on_bad_vectors: str = \"error\",\n        fill_value: float = 0.0,\n        embedding_functions: Optional[List[EmbeddingFunctionConfig]] = None,\n        *,\n        storage_options: Optional[Dict[str, str]] = None,\n        data_storage_version: Optional[str] = None,\n        enable_v2_manifest_paths: Optional[bool] = None,\n    ) -&gt; Table:\n        \"\"\"Create a [Table][lancedb.table.Table] in the database.\n\n        Parameters\n        ----------\n        name: str\n            The name of the table.\n        data: The data to initialize the table, *optional*\n            User must provide at least one of `data` or `schema`.\n            Acceptable types are:\n\n            - list-of-dict\n\n            - pandas.DataFrame\n\n            - pyarrow.Table or pyarrow.RecordBatch\n        schema: The schema of the table, *optional*\n            Acceptable types are:\n\n            - pyarrow.Schema\n\n            - [LanceModel][lancedb.pydantic.LanceModel]\n        mode: str; default \"create\"\n            The mode to use when creating the table.\n            Can be either \"create\" or \"overwrite\".\n            By default, if the table already exists, an exception is raised.\n            If you want to overwrite the table, use mode=\"overwrite\".\n        exist_ok: bool, default False\n            If a table by the same name already exists, then raise an exception\n            if exist_ok=False. If exist_ok=True, then open the existing table;\n            it will not add the provided data but will validate against any\n            schema that's specified.\n        on_bad_vectors: str, default \"error\"\n            What to do if any of the vectors are not the same size or contains NaNs.\n            One of \"error\", \"drop\", \"fill\".\n        fill_value: float\n            The value to use when filling vectors. Only used if on_bad_vectors=\"fill\".\n        storage_options: dict, optional\n            Additional options for the storage backend. Options already set on the\n            connection will be inherited by the table, but can be overridden here.\n            See available options at\n            &lt;https://lancedb.github.io/lancedb/guides/storage/&gt;\n        data_storage_version: optional, str, default \"stable\"\n            Deprecated.  Set `storage_options` when connecting to the database and set\n            `new_table_data_storage_version` in the options.\n        enable_v2_manifest_paths: optional, bool, default False\n            Deprecated.  Set `storage_options` when connecting to the database and set\n            `new_table_enable_v2_manifest_paths` in the options.\n        Returns\n        -------\n        LanceTable\n            A reference to the newly created table.\n\n        !!! note\n\n            The vector index won't be created by default.\n            To create the index, call the `create_index` method on the table.\n\n        Examples\n        --------\n\n        Can create with list of tuples or dictionaries:\n\n        &gt;&gt;&gt; import lancedb\n        &gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n        &gt;&gt;&gt; data = [{\"vector\": [1.1, 1.2], \"lat\": 45.5, \"long\": -122.7},\n        ...         {\"vector\": [0.2, 1.8], \"lat\": 40.1, \"long\":  -74.1}]\n        &gt;&gt;&gt; db.create_table(\"my_table\", data)\n        LanceTable(name='my_table', version=1, ...)\n        &gt;&gt;&gt; db[\"my_table\"].head()\n        pyarrow.Table\n        vector: fixed_size_list&lt;item: float&gt;[2]\n          child 0, item: float\n        lat: double\n        long: double\n        ----\n        vector: [[[1.1,1.2],[0.2,1.8]]]\n        lat: [[45.5,40.1]]\n        long: [[-122.7,-74.1]]\n\n        You can also pass a pandas DataFrame:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; data = pd.DataFrame({\n        ...    \"vector\": [[1.1, 1.2], [0.2, 1.8]],\n        ...    \"lat\": [45.5, 40.1],\n        ...    \"long\": [-122.7, -74.1]\n        ... })\n        &gt;&gt;&gt; db.create_table(\"table2\", data)\n        LanceTable(name='table2', version=1, ...)\n        &gt;&gt;&gt; db[\"table2\"].head()\n        pyarrow.Table\n        vector: fixed_size_list&lt;item: float&gt;[2]\n          child 0, item: float\n        lat: double\n        long: double\n        ----\n        vector: [[[1.1,1.2],[0.2,1.8]]]\n        lat: [[45.5,40.1]]\n        long: [[-122.7,-74.1]]\n\n        Data is converted to Arrow before being written to disk. For maximum\n        control over how data is saved, either provide the PyArrow schema to\n        convert to or else provide a [PyArrow Table](pyarrow.Table) directly.\n\n        &gt;&gt;&gt; import pyarrow as pa\n        &gt;&gt;&gt; custom_schema = pa.schema([\n        ...   pa.field(\"vector\", pa.list_(pa.float32(), 2)),\n        ...   pa.field(\"lat\", pa.float32()),\n        ...   pa.field(\"long\", pa.float32())\n        ... ])\n        &gt;&gt;&gt; db.create_table(\"table3\", data, schema = custom_schema)\n        LanceTable(name='table3', version=1, ...)\n        &gt;&gt;&gt; db[\"table3\"].head()\n        pyarrow.Table\n        vector: fixed_size_list&lt;item: float&gt;[2]\n          child 0, item: float\n        lat: float\n        long: float\n        ----\n        vector: [[[1.1,1.2],[0.2,1.8]]]\n        lat: [[45.5,40.1]]\n        long: [[-122.7,-74.1]]\n\n\n        It is also possible to create an table from `[Iterable[pa.RecordBatch]]`:\n\n\n        &gt;&gt;&gt; import pyarrow as pa\n        &gt;&gt;&gt; def make_batches():\n        ...     for i in range(5):\n        ...         yield pa.RecordBatch.from_arrays(\n        ...             [\n        ...                 pa.array([[3.1, 4.1], [5.9, 26.5]],\n        ...                     pa.list_(pa.float32(), 2)),\n        ...                 pa.array([\"foo\", \"bar\"]),\n        ...                 pa.array([10.0, 20.0]),\n        ...             ],\n        ...             [\"vector\", \"item\", \"price\"],\n        ...         )\n        &gt;&gt;&gt; schema=pa.schema([\n        ...     pa.field(\"vector\", pa.list_(pa.float32(), 2)),\n        ...     pa.field(\"item\", pa.utf8()),\n        ...     pa.field(\"price\", pa.float32()),\n        ... ])\n        &gt;&gt;&gt; db.create_table(\"table4\", make_batches(), schema=schema)\n        LanceTable(name='table4', version=1, ...)\n\n        \"\"\"\n        raise NotImplementedError\n\n    def __getitem__(self, name: str) -&gt; LanceTable:\n        return self.open_table(name)\n\n    def open_table(\n        self,\n        name: str,\n        *,\n        storage_options: Optional[Dict[str, str]] = None,\n        index_cache_size: Optional[int] = None,\n    ) -&gt; Table:\n        \"\"\"Open a Lance Table in the database.\n\n        Parameters\n        ----------\n        name: str\n            The name of the table.\n        index_cache_size: int, default 256\n            Set the size of the index cache, specified as a number of entries\n\n            The exact meaning of an \"entry\" will depend on the type of index:\n            * IVF - there is one entry for each IVF partition\n            * BTREE - there is one entry for the entire index\n\n            This cache applies to the entire opened table, across all indices.\n            Setting this value higher will increase performance on larger datasets\n            at the expense of more RAM\n        storage_options: dict, optional\n            Additional options for the storage backend. Options already set on the\n            connection will be inherited by the table, but can be overridden here.\n            See available options at\n            &lt;https://lancedb.github.io/lancedb/guides/storage/&gt;\n\n        Returns\n        -------\n        A LanceTable object representing the table.\n        \"\"\"\n        raise NotImplementedError\n\n    def drop_table(self, name: str):\n        \"\"\"Drop a table from the database.\n\n        Parameters\n        ----------\n        name: str\n            The name of the table.\n        \"\"\"\n        raise NotImplementedError\n\n    def rename_table(self, cur_name: str, new_name: str):\n        \"\"\"Rename a table in the database.\n\n        Parameters\n        ----------\n        cur_name: str\n            The current name of the table.\n        new_name: str\n            The new name of the table.\n        \"\"\"\n        raise NotImplementedError\n\n    def drop_database(self):\n        \"\"\"\n        Drop database\n        This is the same thing as dropping all the tables\n        \"\"\"\n        raise NotImplementedError\n\n    def drop_all_tables(self):\n        \"\"\"\n        Drop all tables from the database\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def uri(self) -&gt; str:\n        return self._uri\n</code></pre>"},{"location":"python/python/#lancedb.db.DBConnection.table_names","title":"table_names  <code>abstractmethod</code>","text":"<pre><code>table_names(page_token: Optional[str] = None, limit: int = 10) -&gt; Iterable[str]\n</code></pre> <p>List all tables in this database, in sorted order</p> <p>Parameters:</p> <ul> <li> <code>page_token</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The token to use for pagination. If not present, start from the beginning. Typically, this token is last table name from the previous page. Only supported by LanceDb Cloud.</p> </li> <li> <code>limit</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the page to return. Only supported by LanceDb Cloud.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Iterable of str</code>           \u2013            </li> </ul> Source code in <code>lancedb/db.py</code> <pre><code>@abstractmethod\ndef table_names(\n    self, page_token: Optional[str] = None, limit: int = 10\n) -&gt; Iterable[str]:\n    \"\"\"List all tables in this database, in sorted order\n\n    Parameters\n    ----------\n    page_token: str, optional\n        The token to use for pagination. If not present, start from the beginning.\n        Typically, this token is last table name from the previous page.\n        Only supported by LanceDb Cloud.\n    limit: int, default 10\n        The size of the page to return.\n        Only supported by LanceDb Cloud.\n\n    Returns\n    -------\n    Iterable of str\n    \"\"\"\n    pass\n</code></pre>"},{"location":"python/python/#lancedb.db.DBConnection.create_table","title":"create_table  <code>abstractmethod</code>","text":"<pre><code>create_table(name: str, data: Optional[DATA] = None, schema: Optional[Union[Schema, LanceModel]] = None, mode: str = 'create', exist_ok: bool = False, on_bad_vectors: str = 'error', fill_value: float = 0.0, embedding_functions: Optional[List[EmbeddingFunctionConfig]] = None, *, storage_options: Optional[Dict[str, str]] = None, data_storage_version: Optional[str] = None, enable_v2_manifest_paths: Optional[bool] = None) -&gt; Table\n</code></pre> <p>Create a Table in the database.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the table.</p> </li> <li> <code>data</code>               (<code>Optional[DATA]</code>, default:                   <code>None</code> )           \u2013            <p>User must provide at least one of <code>data</code> or <code>schema</code>. Acceptable types are:</p> <ul> <li> <p>list-of-dict</p> </li> <li> <p>pandas.DataFrame</p> </li> <li> <p>pyarrow.Table or pyarrow.RecordBatch</p> </li> </ul> </li> <li> <code>schema</code>               (<code>Optional[Union[Schema, LanceModel]]</code>, default:                   <code>None</code> )           \u2013            <p>Acceptable types are:</p> <ul> <li> <p>pyarrow.Schema</p> </li> <li> <p>LanceModel</p> </li> </ul> </li> <li> <code>mode</code>               (<code>str</code>, default:                   <code>'create'</code> )           \u2013            <p>The mode to use when creating the table. Can be either \"create\" or \"overwrite\". By default, if the table already exists, an exception is raised. If you want to overwrite the table, use mode=\"overwrite\".</p> </li> <li> <code>exist_ok</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If a table by the same name already exists, then raise an exception if exist_ok=False. If exist_ok=True, then open the existing table; it will not add the provided data but will validate against any schema that's specified.</p> </li> <li> <code>on_bad_vectors</code>               (<code>str</code>, default:                   <code>'error'</code> )           \u2013            <p>What to do if any of the vectors are not the same size or contains NaNs. One of \"error\", \"drop\", \"fill\".</p> </li> <li> <code>fill_value</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>The value to use when filling vectors. Only used if on_bad_vectors=\"fill\".</p> </li> <li> <code>storage_options</code>               (<code>Optional[Dict[str, str]]</code>, default:                   <code>None</code> )           \u2013            <p>Additional options for the storage backend. Options already set on the connection will be inherited by the table, but can be overridden here. See available options at https://lancedb.github.io/lancedb/guides/storage/</p> </li> <li> <code>data_storage_version</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Deprecated.  Set <code>storage_options</code> when connecting to the database and set <code>new_table_data_storage_version</code> in the options.</p> </li> <li> <code>enable_v2_manifest_paths</code>               (<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>Deprecated.  Set <code>storage_options</code> when connecting to the database and set <code>new_table_enable_v2_manifest_paths</code> in the options.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LanceTable</code>           \u2013            <p>A reference to the newly created table.</p> </li> <li> <code>!!! note</code>           \u2013            <p>The vector index won't be created by default. To create the index, call the <code>create_index</code> method on the table.</p> </li> </ul> <p>Examples:</p> <p>Can create with list of tuples or dictionaries:</p> <pre><code>&gt;&gt;&gt; import lancedb\n&gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n&gt;&gt;&gt; data = [{\"vector\": [1.1, 1.2], \"lat\": 45.5, \"long\": -122.7},\n...         {\"vector\": [0.2, 1.8], \"lat\": 40.1, \"long\":  -74.1}]\n&gt;&gt;&gt; db.create_table(\"my_table\", data)\nLanceTable(name='my_table', version=1, ...)\n&gt;&gt;&gt; db[\"my_table\"].head()\npyarrow.Table\nvector: fixed_size_list&lt;item: float&gt;[2]\n  child 0, item: float\nlat: double\nlong: double\n----\nvector: [[[1.1,1.2],[0.2,1.8]]]\nlat: [[45.5,40.1]]\nlong: [[-122.7,-74.1]]\n</code></pre> <p>You can also pass a pandas DataFrame:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = pd.DataFrame({\n...    \"vector\": [[1.1, 1.2], [0.2, 1.8]],\n...    \"lat\": [45.5, 40.1],\n...    \"long\": [-122.7, -74.1]\n... })\n&gt;&gt;&gt; db.create_table(\"table2\", data)\nLanceTable(name='table2', version=1, ...)\n&gt;&gt;&gt; db[\"table2\"].head()\npyarrow.Table\nvector: fixed_size_list&lt;item: float&gt;[2]\n  child 0, item: float\nlat: double\nlong: double\n----\nvector: [[[1.1,1.2],[0.2,1.8]]]\nlat: [[45.5,40.1]]\nlong: [[-122.7,-74.1]]\n</code></pre> <p>Data is converted to Arrow before being written to disk. For maximum control over how data is saved, either provide the PyArrow schema to convert to or else provide a PyArrow Table directly.</p> <pre><code>&gt;&gt;&gt; import pyarrow as pa\n&gt;&gt;&gt; custom_schema = pa.schema([\n...   pa.field(\"vector\", pa.list_(pa.float32(), 2)),\n...   pa.field(\"lat\", pa.float32()),\n...   pa.field(\"long\", pa.float32())\n... ])\n&gt;&gt;&gt; db.create_table(\"table3\", data, schema = custom_schema)\nLanceTable(name='table3', version=1, ...)\n&gt;&gt;&gt; db[\"table3\"].head()\npyarrow.Table\nvector: fixed_size_list&lt;item: float&gt;[2]\n  child 0, item: float\nlat: float\nlong: float\n----\nvector: [[[1.1,1.2],[0.2,1.8]]]\nlat: [[45.5,40.1]]\nlong: [[-122.7,-74.1]]\n</code></pre> <p>It is also possible to create an table from <code>[Iterable[pa.RecordBatch]]</code>:</p> <pre><code>&gt;&gt;&gt; import pyarrow as pa\n&gt;&gt;&gt; def make_batches():\n...     for i in range(5):\n...         yield pa.RecordBatch.from_arrays(\n...             [\n...                 pa.array([[3.1, 4.1], [5.9, 26.5]],\n...                     pa.list_(pa.float32(), 2)),\n...                 pa.array([\"foo\", \"bar\"]),\n...                 pa.array([10.0, 20.0]),\n...             ],\n...             [\"vector\", \"item\", \"price\"],\n...         )\n&gt;&gt;&gt; schema=pa.schema([\n...     pa.field(\"vector\", pa.list_(pa.float32(), 2)),\n...     pa.field(\"item\", pa.utf8()),\n...     pa.field(\"price\", pa.float32()),\n... ])\n&gt;&gt;&gt; db.create_table(\"table4\", make_batches(), schema=schema)\nLanceTable(name='table4', version=1, ...)\n</code></pre> Source code in <code>lancedb/db.py</code> <pre><code>@abstractmethod\ndef create_table(\n    self,\n    name: str,\n    data: Optional[DATA] = None,\n    schema: Optional[Union[pa.Schema, LanceModel]] = None,\n    mode: str = \"create\",\n    exist_ok: bool = False,\n    on_bad_vectors: str = \"error\",\n    fill_value: float = 0.0,\n    embedding_functions: Optional[List[EmbeddingFunctionConfig]] = None,\n    *,\n    storage_options: Optional[Dict[str, str]] = None,\n    data_storage_version: Optional[str] = None,\n    enable_v2_manifest_paths: Optional[bool] = None,\n) -&gt; Table:\n    \"\"\"Create a [Table][lancedb.table.Table] in the database.\n\n    Parameters\n    ----------\n    name: str\n        The name of the table.\n    data: The data to initialize the table, *optional*\n        User must provide at least one of `data` or `schema`.\n        Acceptable types are:\n\n        - list-of-dict\n\n        - pandas.DataFrame\n\n        - pyarrow.Table or pyarrow.RecordBatch\n    schema: The schema of the table, *optional*\n        Acceptable types are:\n\n        - pyarrow.Schema\n\n        - [LanceModel][lancedb.pydantic.LanceModel]\n    mode: str; default \"create\"\n        The mode to use when creating the table.\n        Can be either \"create\" or \"overwrite\".\n        By default, if the table already exists, an exception is raised.\n        If you want to overwrite the table, use mode=\"overwrite\".\n    exist_ok: bool, default False\n        If a table by the same name already exists, then raise an exception\n        if exist_ok=False. If exist_ok=True, then open the existing table;\n        it will not add the provided data but will validate against any\n        schema that's specified.\n    on_bad_vectors: str, default \"error\"\n        What to do if any of the vectors are not the same size or contains NaNs.\n        One of \"error\", \"drop\", \"fill\".\n    fill_value: float\n        The value to use when filling vectors. Only used if on_bad_vectors=\"fill\".\n    storage_options: dict, optional\n        Additional options for the storage backend. Options already set on the\n        connection will be inherited by the table, but can be overridden here.\n        See available options at\n        &lt;https://lancedb.github.io/lancedb/guides/storage/&gt;\n    data_storage_version: optional, str, default \"stable\"\n        Deprecated.  Set `storage_options` when connecting to the database and set\n        `new_table_data_storage_version` in the options.\n    enable_v2_manifest_paths: optional, bool, default False\n        Deprecated.  Set `storage_options` when connecting to the database and set\n        `new_table_enable_v2_manifest_paths` in the options.\n    Returns\n    -------\n    LanceTable\n        A reference to the newly created table.\n\n    !!! note\n\n        The vector index won't be created by default.\n        To create the index, call the `create_index` method on the table.\n\n    Examples\n    --------\n\n    Can create with list of tuples or dictionaries:\n\n    &gt;&gt;&gt; import lancedb\n    &gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n    &gt;&gt;&gt; data = [{\"vector\": [1.1, 1.2], \"lat\": 45.5, \"long\": -122.7},\n    ...         {\"vector\": [0.2, 1.8], \"lat\": 40.1, \"long\":  -74.1}]\n    &gt;&gt;&gt; db.create_table(\"my_table\", data)\n    LanceTable(name='my_table', version=1, ...)\n    &gt;&gt;&gt; db[\"my_table\"].head()\n    pyarrow.Table\n    vector: fixed_size_list&lt;item: float&gt;[2]\n      child 0, item: float\n    lat: double\n    long: double\n    ----\n    vector: [[[1.1,1.2],[0.2,1.8]]]\n    lat: [[45.5,40.1]]\n    long: [[-122.7,-74.1]]\n\n    You can also pass a pandas DataFrame:\n\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; data = pd.DataFrame({\n    ...    \"vector\": [[1.1, 1.2], [0.2, 1.8]],\n    ...    \"lat\": [45.5, 40.1],\n    ...    \"long\": [-122.7, -74.1]\n    ... })\n    &gt;&gt;&gt; db.create_table(\"table2\", data)\n    LanceTable(name='table2', version=1, ...)\n    &gt;&gt;&gt; db[\"table2\"].head()\n    pyarrow.Table\n    vector: fixed_size_list&lt;item: float&gt;[2]\n      child 0, item: float\n    lat: double\n    long: double\n    ----\n    vector: [[[1.1,1.2],[0.2,1.8]]]\n    lat: [[45.5,40.1]]\n    long: [[-122.7,-74.1]]\n\n    Data is converted to Arrow before being written to disk. For maximum\n    control over how data is saved, either provide the PyArrow schema to\n    convert to or else provide a [PyArrow Table](pyarrow.Table) directly.\n\n    &gt;&gt;&gt; import pyarrow as pa\n    &gt;&gt;&gt; custom_schema = pa.schema([\n    ...   pa.field(\"vector\", pa.list_(pa.float32(), 2)),\n    ...   pa.field(\"lat\", pa.float32()),\n    ...   pa.field(\"long\", pa.float32())\n    ... ])\n    &gt;&gt;&gt; db.create_table(\"table3\", data, schema = custom_schema)\n    LanceTable(name='table3', version=1, ...)\n    &gt;&gt;&gt; db[\"table3\"].head()\n    pyarrow.Table\n    vector: fixed_size_list&lt;item: float&gt;[2]\n      child 0, item: float\n    lat: float\n    long: float\n    ----\n    vector: [[[1.1,1.2],[0.2,1.8]]]\n    lat: [[45.5,40.1]]\n    long: [[-122.7,-74.1]]\n\n\n    It is also possible to create an table from `[Iterable[pa.RecordBatch]]`:\n\n\n    &gt;&gt;&gt; import pyarrow as pa\n    &gt;&gt;&gt; def make_batches():\n    ...     for i in range(5):\n    ...         yield pa.RecordBatch.from_arrays(\n    ...             [\n    ...                 pa.array([[3.1, 4.1], [5.9, 26.5]],\n    ...                     pa.list_(pa.float32(), 2)),\n    ...                 pa.array([\"foo\", \"bar\"]),\n    ...                 pa.array([10.0, 20.0]),\n    ...             ],\n    ...             [\"vector\", \"item\", \"price\"],\n    ...         )\n    &gt;&gt;&gt; schema=pa.schema([\n    ...     pa.field(\"vector\", pa.list_(pa.float32(), 2)),\n    ...     pa.field(\"item\", pa.utf8()),\n    ...     pa.field(\"price\", pa.float32()),\n    ... ])\n    &gt;&gt;&gt; db.create_table(\"table4\", make_batches(), schema=schema)\n    LanceTable(name='table4', version=1, ...)\n\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python/python/#lancedb.db.DBConnection.open_table","title":"open_table","text":"<pre><code>open_table(name: str, *, storage_options: Optional[Dict[str, str]] = None, index_cache_size: Optional[int] = None) -&gt; Table\n</code></pre> <p>Open a Lance Table in the database.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the table.</p> </li> <li> <code>index_cache_size</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Set the size of the index cache, specified as a number of entries</p> <p>The exact meaning of an \"entry\" will depend on the type of index: * IVF - there is one entry for each IVF partition * BTREE - there is one entry for the entire index</p> <p>This cache applies to the entire opened table, across all indices. Setting this value higher will increase performance on larger datasets at the expense of more RAM</p> </li> <li> <code>storage_options</code>               (<code>Optional[Dict[str, str]]</code>, default:                   <code>None</code> )           \u2013            <p>Additional options for the storage backend. Options already set on the connection will be inherited by the table, but can be overridden here. See available options at https://lancedb.github.io/lancedb/guides/storage/</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>A LanceTable object representing the table.</code>           \u2013            </li> </ul> Source code in <code>lancedb/db.py</code> <pre><code>def open_table(\n    self,\n    name: str,\n    *,\n    storage_options: Optional[Dict[str, str]] = None,\n    index_cache_size: Optional[int] = None,\n) -&gt; Table:\n    \"\"\"Open a Lance Table in the database.\n\n    Parameters\n    ----------\n    name: str\n        The name of the table.\n    index_cache_size: int, default 256\n        Set the size of the index cache, specified as a number of entries\n\n        The exact meaning of an \"entry\" will depend on the type of index:\n        * IVF - there is one entry for each IVF partition\n        * BTREE - there is one entry for the entire index\n\n        This cache applies to the entire opened table, across all indices.\n        Setting this value higher will increase performance on larger datasets\n        at the expense of more RAM\n    storage_options: dict, optional\n        Additional options for the storage backend. Options already set on the\n        connection will be inherited by the table, but can be overridden here.\n        See available options at\n        &lt;https://lancedb.github.io/lancedb/guides/storage/&gt;\n\n    Returns\n    -------\n    A LanceTable object representing the table.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python/python/#lancedb.db.DBConnection.drop_table","title":"drop_table","text":"<pre><code>drop_table(name: str)\n</code></pre> <p>Drop a table from the database.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the table.</p> </li> </ul> Source code in <code>lancedb/db.py</code> <pre><code>def drop_table(self, name: str):\n    \"\"\"Drop a table from the database.\n\n    Parameters\n    ----------\n    name: str\n        The name of the table.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python/python/#lancedb.db.DBConnection.rename_table","title":"rename_table","text":"<pre><code>rename_table(cur_name: str, new_name: str)\n</code></pre> <p>Rename a table in the database.</p> <p>Parameters:</p> <ul> <li> <code>cur_name</code>               (<code>str</code>)           \u2013            <p>The current name of the table.</p> </li> <li> <code>new_name</code>               (<code>str</code>)           \u2013            <p>The new name of the table.</p> </li> </ul> Source code in <code>lancedb/db.py</code> <pre><code>def rename_table(self, cur_name: str, new_name: str):\n    \"\"\"Rename a table in the database.\n\n    Parameters\n    ----------\n    cur_name: str\n        The current name of the table.\n    new_name: str\n        The new name of the table.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python/python/#lancedb.db.DBConnection.drop_database","title":"drop_database","text":"<pre><code>drop_database()\n</code></pre> <p>Drop database This is the same thing as dropping all the tables</p> Source code in <code>lancedb/db.py</code> <pre><code>def drop_database(self):\n    \"\"\"\n    Drop database\n    This is the same thing as dropping all the tables\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python/python/#lancedb.db.DBConnection.drop_all_tables","title":"drop_all_tables","text":"<pre><code>drop_all_tables()\n</code></pre> <p>Drop all tables from the database</p> Source code in <code>lancedb/db.py</code> <pre><code>def drop_all_tables(self):\n    \"\"\"\n    Drop all tables from the database\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python/python/#tables-synchronous","title":"Tables (Synchronous)","text":""},{"location":"python/python/#lancedb.table.Table","title":"lancedb.table.Table","text":"<p>               Bases: <code>ABC</code></p> <p>A Table is a collection of Records in a LanceDB Database.</p> <p>Examples:</p> <p>Create using DBConnection.create_table (more examples in that method's documentation).</p> <pre><code>&gt;&gt;&gt; import lancedb\n&gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n&gt;&gt;&gt; table = db.create_table(\"my_table\", data=[{\"vector\": [1.1, 1.2], \"b\": 2}])\n&gt;&gt;&gt; table.head()\npyarrow.Table\nvector: fixed_size_list&lt;item: float&gt;[2]\n  child 0, item: float\nb: int64\n----\nvector: [[[1.1,1.2]]]\nb: [[2]]\n</code></pre> <p>Can append new data with Table.add().</p> <pre><code>&gt;&gt;&gt; table.add([{\"vector\": [0.5, 1.3], \"b\": 4}])\n</code></pre> <p>Can query the table with Table.search.</p> <pre><code>&gt;&gt;&gt; table.search([0.4, 0.4]).select([\"b\", \"vector\"]).to_pandas()\n   b      vector  _distance\n0  4  [0.5, 1.3]       0.82\n1  2  [1.1, 1.2]       1.13\n</code></pre> <p>Search queries are much faster when an index is created. See Table.create_index.</p> Source code in <code>lancedb/table.py</code> <pre><code>class Table(ABC):\n    \"\"\"\n    A Table is a collection of Records in a LanceDB Database.\n\n    Examples\n    --------\n\n    Create using [DBConnection.create_table][lancedb.DBConnection.create_table]\n    (more examples in that method's documentation).\n\n    &gt;&gt;&gt; import lancedb\n    &gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n    &gt;&gt;&gt; table = db.create_table(\"my_table\", data=[{\"vector\": [1.1, 1.2], \"b\": 2}])\n    &gt;&gt;&gt; table.head()\n    pyarrow.Table\n    vector: fixed_size_list&lt;item: float&gt;[2]\n      child 0, item: float\n    b: int64\n    ----\n    vector: [[[1.1,1.2]]]\n    b: [[2]]\n\n    Can append new data with [Table.add()][lancedb.table.Table.add].\n\n    &gt;&gt;&gt; table.add([{\"vector\": [0.5, 1.3], \"b\": 4}])\n\n    Can query the table with [Table.search][lancedb.table.Table.search].\n\n    &gt;&gt;&gt; table.search([0.4, 0.4]).select([\"b\", \"vector\"]).to_pandas()\n       b      vector  _distance\n    0  4  [0.5, 1.3]       0.82\n    1  2  [1.1, 1.2]       1.13\n\n    Search queries are much faster when an index is created. See\n    [Table.create_index][lancedb.table.Table.create_index].\n    \"\"\"\n\n    @property\n    @abstractmethod\n    def name(self) -&gt; str:\n        \"\"\"The name of this Table\"\"\"\n        raise NotImplementedError\n\n    @property\n    @abstractmethod\n    def version(self) -&gt; int:\n        \"\"\"The version of this Table\"\"\"\n        raise NotImplementedError\n\n    @property\n    @abstractmethod\n    def schema(self) -&gt; pa.Schema:\n        \"\"\"The [Arrow Schema](https://arrow.apache.org/docs/python/api/datatypes.html#)\n        of this Table\n\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    @abstractmethod\n    def embedding_functions(self) -&gt; Dict[str, EmbeddingFunctionConfig]:\n        \"\"\"\n        Get a mapping from vector column name to it's configured embedding function.\n        \"\"\"\n\n    @abstractmethod\n    def count_rows(self, filter: Optional[str] = None) -&gt; int:\n        \"\"\"\n        Count the number of rows in the table.\n\n        Parameters\n        ----------\n        filter: str, optional\n            A SQL where clause to filter the rows to count.\n        \"\"\"\n        raise NotImplementedError\n\n    def to_pandas(self) -&gt; \"pandas.DataFrame\":\n        \"\"\"Return the table as a pandas DataFrame.\n\n        Returns\n        -------\n        pd.DataFrame\n        \"\"\"\n        return self.to_arrow().to_pandas()\n\n    @abstractmethod\n    def to_arrow(self) -&gt; pa.Table:\n        \"\"\"Return the table as a pyarrow Table.\n\n        Returns\n        -------\n        pa.Table\n        \"\"\"\n        raise NotImplementedError\n\n    def create_index(\n        self,\n        metric=\"l2\",\n        num_partitions=256,\n        num_sub_vectors=96,\n        vector_column_name: str = VECTOR_COLUMN_NAME,\n        replace: bool = True,\n        accelerator: Optional[str] = None,\n        index_cache_size: Optional[int] = None,\n        *,\n        index_type: VectorIndexType = \"IVF_PQ\",\n        wait_timeout: Optional[timedelta] = None,\n        num_bits: int = 8,\n        max_iterations: int = 50,\n        sample_rate: int = 256,\n        m: int = 20,\n        ef_construction: int = 300,\n    ):\n        \"\"\"Create an index on the table.\n\n        Parameters\n        ----------\n        metric: str, default \"l2\"\n            The distance metric to use when creating the index.\n            Valid values are \"l2\", \"cosine\", \"dot\", or \"hamming\".\n            l2 is euclidean distance.\n            Hamming is available only for binary vectors.\n        num_partitions: int, default 256\n            The number of IVF partitions to use when creating the index.\n            Default is 256.\n        num_sub_vectors: int, default 96\n            The number of PQ sub-vectors to use when creating the index.\n            Default is 96.\n        vector_column_name: str, default \"vector\"\n            The vector column name to create the index.\n        replace: bool, default True\n            - If True, replace the existing index if it exists.\n\n            - If False, raise an error if duplicate index exists.\n        accelerator: str, default None\n            If set, use the given accelerator to create the index.\n            Only support \"cuda\" for now.\n        index_cache_size : int, optional\n            The size of the index cache in number of entries. Default value is 256.\n        num_bits: int\n            The number of bits to encode sub-vectors. Only used with the IVF_PQ index.\n            Only 4 and 8 are supported.\n        wait_timeout: timedelta, optional\n            The timeout to wait if indexing is asynchronous.\n        \"\"\"\n        raise NotImplementedError\n\n    def drop_index(self, name: str) -&gt; None:\n        \"\"\"\n        Drop an index from the table.\n\n        Parameters\n        ----------\n        name: str\n            The name of the index to drop.\n\n        Notes\n        -----\n        This does not delete the index from disk, it just removes it from the table.\n        To delete the index, run [optimize][lancedb.table.Table.optimize]\n        after dropping the index.\n\n        Use [list_indices][lancedb.table.Table.list_indices] to find the names of\n        the indices.\n        \"\"\"\n        raise NotImplementedError\n\n    def wait_for_index(\n        self, index_names: Iterable[str], timeout: timedelta = timedelta(seconds=300)\n    ) -&gt; None:\n        \"\"\"\n        Wait for indexing to complete for the given index names.\n        This will poll the table until all the indices are fully indexed,\n        or raise a timeout exception if the timeout is reached.\n\n        Parameters\n        ----------\n        index_names: str\n            The name of the indices to poll\n        timeout: timedelta\n            Timeout to wait for asynchronous indexing. The default is 5 minutes.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def create_scalar_index(\n        self,\n        column: str,\n        *,\n        replace: bool = True,\n        index_type: ScalarIndexType = \"BTREE\",\n        wait_timeout: Optional[timedelta] = None,\n    ):\n        \"\"\"Create a scalar index on a column.\n\n        Parameters\n        ----------\n        column : str\n            The column to be indexed.  Must be a boolean, integer, float,\n            or string column.\n        replace : bool, default True\n            Replace the existing index if it exists.\n        index_type: Literal[\"BTREE\", \"BITMAP\", \"LABEL_LIST\"], default \"BTREE\"\n            The type of index to create.\n        wait_timeout: timedelta, optional\n            The timeout to wait if indexing is asynchronous.\n        Examples\n        --------\n\n        Scalar indices, like vector indices, can be used to speed up scans.  A scalar\n        index can speed up scans that contain filter expressions on the indexed column.\n        For example, the following scan will be faster if the column ``my_col`` has\n        a scalar index:\n\n        &gt;&gt;&gt; import lancedb # doctest: +SKIP\n        &gt;&gt;&gt; db = lancedb.connect(\"/data/lance\") # doctest: +SKIP\n        &gt;&gt;&gt; img_table = db.open_table(\"images\") # doctest: +SKIP\n        &gt;&gt;&gt; my_df = img_table.search().where(\"my_col = 7\", # doctest: +SKIP\n        ...                                  prefilter=True).to_pandas()\n\n        Scalar indices can also speed up scans containing a vector search and a\n        prefilter:\n\n        &gt;&gt;&gt; import lancedb # doctest: +SKIP\n        &gt;&gt;&gt; db = lancedb.connect(\"/data/lance\") # doctest: +SKIP\n        &gt;&gt;&gt; img_table = db.open_table(\"images\") # doctest: +SKIP\n        &gt;&gt;&gt; img_table.search([1, 2, 3, 4], vector_column_name=\"vector\") # doctest: +SKIP\n        ...     .where(\"my_col != 7\", prefilter=True)\n        ...     .to_pandas()\n\n        Scalar indices can only speed up scans for basic filters using\n        equality, comparison, range (e.g. ``my_col BETWEEN 0 AND 100``), and set\n        membership (e.g. `my_col IN (0, 1, 2)`)\n\n        Scalar indices can be used if the filter contains multiple indexed columns and\n        the filter criteria are AND'd or OR'd together\n        (e.g. ``my_col &lt; 0 AND other_col&gt; 100``)\n\n        Scalar indices may be used if the filter contains non-indexed columns but,\n        depending on the structure of the filter, they may not be usable.  For example,\n        if the column ``not_indexed`` does not have a scalar index then the filter\n        ``my_col = 0 OR not_indexed = 1`` will not be able to use any scalar index on\n        ``my_col``.\n        \"\"\"\n        raise NotImplementedError\n\n    def create_fts_index(\n        self,\n        field_names: Union[str, List[str]],\n        *,\n        ordering_field_names: Optional[Union[str, List[str]]] = None,\n        replace: bool = False,\n        writer_heap_size: Optional[int] = 1024 * 1024 * 1024,\n        use_tantivy: bool = True,\n        tokenizer_name: Optional[str] = None,\n        with_position: bool = True,\n        # tokenizer configs:\n        base_tokenizer: BaseTokenizerType = \"simple\",\n        language: str = \"English\",\n        max_token_length: Optional[int] = 40,\n        lower_case: bool = True,\n        stem: bool = False,\n        remove_stop_words: bool = False,\n        ascii_folding: bool = False,\n        wait_timeout: Optional[timedelta] = None,\n    ):\n        \"\"\"Create a full-text search index on the table.\n\n        Warning - this API is highly experimental and is highly likely to change\n        in the future.\n\n        Parameters\n        ----------\n        field_names: str or list of str\n            The name(s) of the field to index.\n            can be only str if use_tantivy=True for now.\n        replace: bool, default False\n            If True, replace the existing index if it exists. Note that this is\n            not yet an atomic operation; the index will be temporarily\n            unavailable while the new index is being created.\n        writer_heap_size: int, default 1GB\n            Only available with use_tantivy=True\n        ordering_field_names:\n            A list of unsigned type fields to index to optionally order\n            results on at search time.\n            only available with use_tantivy=True\n        tokenizer_name: str, default \"default\"\n            The tokenizer to use for the index. Can be \"raw\", \"default\" or the 2 letter\n            language code followed by \"_stem\". So for english it would be \"en_stem\".\n            For available languages see: https://docs.rs/tantivy/latest/tantivy/tokenizer/enum.Language.html\n        use_tantivy: bool, default True\n            If True, use the legacy full-text search implementation based on tantivy.\n            If False, use the new full-text search implementation based on lance-index.\n        with_position: bool, default True\n            Only available with use_tantivy=False\n            If False, do not store the positions of the terms in the text.\n            This can reduce the size of the index and improve indexing speed.\n            But it will raise an exception for phrase queries.\n        base_tokenizer : str, default \"simple\"\n            The base tokenizer to use for tokenization. Options are:\n            - \"simple\": Splits text by whitespace and punctuation.\n            - \"whitespace\": Split text by whitespace, but not punctuation.\n            - \"raw\": No tokenization. The entire text is treated as a single token.\n        language : str, default \"English\"\n            The language to use for tokenization.\n        max_token_length : int, default 40\n            The maximum token length to index. Tokens longer than this length will be\n            ignored.\n        lower_case : bool, default True\n            Whether to convert the token to lower case. This makes queries\n            case-insensitive.\n        stem : bool, default False\n            Whether to stem the token. Stemming reduces words to their root form.\n            For example, in English \"running\" and \"runs\" would both be reduced to \"run\".\n        remove_stop_words : bool, default False\n            Whether to remove stop words. Stop words are common words that are often\n            removed from text before indexing. For example, in English \"the\" and \"and\".\n        ascii_folding : bool, default False\n            Whether to fold ASCII characters. This converts accented characters to\n            their ASCII equivalent. For example, \"caf\u00e9\" would be converted to \"cafe\".\n        wait_timeout: timedelta, optional\n            The timeout to wait if indexing is asynchronous.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def add(\n        self,\n        data: DATA,\n        mode: AddMode = \"append\",\n        on_bad_vectors: OnBadVectorsType = \"error\",\n        fill_value: float = 0.0,\n    ):\n        \"\"\"Add more data to the [Table](Table).\n\n        Parameters\n        ----------\n        data: DATA\n            The data to insert into the table. Acceptable types are:\n\n            - list-of-dict\n\n            - pandas.DataFrame\n\n            - pyarrow.Table or pyarrow.RecordBatch\n        mode: str\n            The mode to use when writing the data. Valid values are\n            \"append\" and \"overwrite\".\n        on_bad_vectors: str, default \"error\"\n            What to do if any of the vectors are not the same size or contains NaNs.\n            One of \"error\", \"drop\", \"fill\".\n        fill_value: float, default 0.\n            The value to use when filling vectors. Only used if on_bad_vectors=\"fill\".\n\n        \"\"\"\n        raise NotImplementedError\n\n    def merge_insert(self, on: Union[str, Iterable[str]]) -&gt; LanceMergeInsertBuilder:\n        \"\"\"\n        Returns a [`LanceMergeInsertBuilder`][lancedb.merge.LanceMergeInsertBuilder]\n        that can be used to create a \"merge insert\" operation\n\n        This operation can add rows, update rows, and remove rows all in a single\n        transaction. It is a very generic tool that can be used to create\n        behaviors like \"insert if not exists\", \"update or insert (i.e. upsert)\",\n        or even replace a portion of existing data with new data (e.g. replace\n        all data where month=\"january\")\n\n        The merge insert operation works by combining new data from a\n        **source table** with existing data in a **target table** by using a\n        join.  There are three categories of records.\n\n        \"Matched\" records are records that exist in both the source table and\n        the target table. \"Not matched\" records exist only in the source table\n        (e.g. these are new data) \"Not matched by source\" records exist only\n        in the target table (this is old data)\n\n        The builder returned by this method can be used to customize what\n        should happen for each category of data.\n\n        Please note that the data may appear to be reordered as part of this\n        operation.  This is because updated rows will be deleted from the\n        dataset and then reinserted at the end with the new values.\n\n        Parameters\n        ----------\n\n        on: Union[str, Iterable[str]]\n            A column (or columns) to join on.  This is how records from the\n            source table and target table are matched.  Typically this is some\n            kind of key or id column.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import lancedb\n        &gt;&gt;&gt; data = pa.table({\"a\": [2, 1, 3], \"b\": [\"a\", \"b\", \"c\"]})\n        &gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n        &gt;&gt;&gt; table = db.create_table(\"my_table\", data)\n        &gt;&gt;&gt; new_data = pa.table({\"a\": [2, 3, 4], \"b\": [\"x\", \"y\", \"z\"]})\n        &gt;&gt;&gt; # Perform a \"upsert\" operation\n        &gt;&gt;&gt; table.merge_insert(\"a\")             \\\\\n        ...      .when_matched_update_all()     \\\\\n        ...      .when_not_matched_insert_all() \\\\\n        ...      .execute(new_data)\n        &gt;&gt;&gt; # The order of new rows is non-deterministic since we use\n        &gt;&gt;&gt; # a hash-join as part of this operation and so we sort here\n        &gt;&gt;&gt; table.to_arrow().sort_by(\"a\").to_pandas()\n           a  b\n        0  1  b\n        1  2  x\n        2  3  y\n        3  4  z\n        \"\"\"\n        on = [on] if isinstance(on, str) else list(iter(on))\n\n        return LanceMergeInsertBuilder(self, on)\n\n    @abstractmethod\n    def search(\n        self,\n        query: Optional[\n            Union[VEC, str, \"PIL.Image.Image\", Tuple, FullTextQuery]\n        ] = None,\n        vector_column_name: Optional[str] = None,\n        query_type: QueryType = \"auto\",\n        ordering_field_name: Optional[str] = None,\n        fts_columns: Optional[Union[str, List[str]]] = None,\n    ) -&gt; LanceQueryBuilder:\n        \"\"\"Create a search query to find the nearest neighbors\n        of the given query vector. We currently support [vector search][search]\n        and [full-text search][experimental-full-text-search].\n\n        All query options are defined in\n        [LanceQueryBuilder][lancedb.query.LanceQueryBuilder].\n\n        Examples\n        --------\n        &gt;&gt;&gt; import lancedb\n        &gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n        &gt;&gt;&gt; data = [\n        ...    {\"original_width\": 100, \"caption\": \"bar\", \"vector\": [0.1, 2.3, 4.5]},\n        ...    {\"original_width\": 2000, \"caption\": \"foo\",  \"vector\": [0.5, 3.4, 1.3]},\n        ...    {\"original_width\": 3000, \"caption\": \"test\", \"vector\": [0.3, 6.2, 2.6]}\n        ... ]\n        &gt;&gt;&gt; table = db.create_table(\"my_table\", data)\n        &gt;&gt;&gt; query = [0.4, 1.4, 2.4]\n        &gt;&gt;&gt; (table.search(query)\n        ...     .where(\"original_width &gt; 1000\", prefilter=True)\n        ...     .select([\"caption\", \"original_width\", \"vector\"])\n        ...     .limit(2)\n        ...     .to_pandas())\n          caption  original_width           vector  _distance\n        0     foo            2000  [0.5, 3.4, 1.3]   5.220000\n        1    test            3000  [0.3, 6.2, 2.6]  23.089996\n\n        Parameters\n        ----------\n        query: list/np.ndarray/str/PIL.Image.Image, default None\n            The targetted vector to search for.\n\n            - *default None*.\n            Acceptable types are: list, np.ndarray, PIL.Image.Image\n\n            - If None then the select/where/limit clauses are applied to filter\n            the table\n        vector_column_name: str, optional\n            The name of the vector column to search.\n\n            The vector column needs to be a pyarrow fixed size list type\n\n            - If not specified then the vector column is inferred from\n            the table schema\n\n            - If the table has multiple vector columns then the *vector_column_name*\n            needs to be specified. Otherwise, an error is raised.\n        query_type: str\n            *default \"auto\"*.\n            Acceptable types are: \"vector\", \"fts\", \"hybrid\", or \"auto\"\n\n            - If \"auto\" then the query type is inferred from the query;\n\n                - If `query` is a list/np.ndarray then the query type is\n                \"vector\";\n\n                - If `query` is a PIL.Image.Image then either do vector search,\n                or raise an error if no corresponding embedding function is found.\n\n            - If `query` is a string, then the query type is \"vector\" if the\n            table has embedding functions else the query type is \"fts\"\n\n        Returns\n        -------\n        LanceQueryBuilder\n            A query builder object representing the query.\n            Once executed, the query returns\n\n            - selected columns\n\n            - the vector\n\n            - and also the \"_distance\" column which is the distance between the query\n            vector and the returned vector.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def _execute_query(\n        self,\n        query: Query,\n        *,\n        batch_size: Optional[int] = None,\n        timeout: Optional[timedelta] = None,\n    ) -&gt; pa.RecordBatchReader: ...\n\n    @abstractmethod\n    def _explain_plan(self, query: Query, verbose: Optional[bool] = False) -&gt; str: ...\n\n    @abstractmethod\n    def _analyze_plan(self, query: Query) -&gt; str: ...\n\n    @abstractmethod\n    def _do_merge(\n        self,\n        merge: LanceMergeInsertBuilder,\n        new_data: DATA,\n        on_bad_vectors: OnBadVectorsType,\n        fill_value: float,\n    ): ...\n\n    @abstractmethod\n    def delete(self, where: str):\n        \"\"\"Delete rows from the table.\n\n        This can be used to delete a single row, many rows, all rows, or\n        sometimes no rows (if your predicate matches nothing).\n\n        Parameters\n        ----------\n        where: str\n            The SQL where clause to use when deleting rows.\n\n            - For example, 'x = 2' or 'x IN (1, 2, 3)'.\n\n            The filter must not be empty, or it will error.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import lancedb\n        &gt;&gt;&gt; data = [\n        ...    {\"x\": 1, \"vector\": [1.0, 2]},\n        ...    {\"x\": 2, \"vector\": [3.0, 4]},\n        ...    {\"x\": 3, \"vector\": [5.0, 6]}\n        ... ]\n        &gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n        &gt;&gt;&gt; table = db.create_table(\"my_table\", data)\n        &gt;&gt;&gt; table.to_pandas()\n           x      vector\n        0  1  [1.0, 2.0]\n        1  2  [3.0, 4.0]\n        2  3  [5.0, 6.0]\n        &gt;&gt;&gt; table.delete(\"x = 2\")\n        &gt;&gt;&gt; table.to_pandas()\n           x      vector\n        0  1  [1.0, 2.0]\n        1  3  [5.0, 6.0]\n\n        If you have a list of values to delete, you can combine them into a\n        stringified list and use the `IN` operator:\n\n        &gt;&gt;&gt; to_remove = [1, 5]\n        &gt;&gt;&gt; to_remove = \", \".join([str(v) for v in to_remove])\n        &gt;&gt;&gt; to_remove\n        '1, 5'\n        &gt;&gt;&gt; table.delete(f\"x IN ({to_remove})\")\n        &gt;&gt;&gt; table.to_pandas()\n           x      vector\n        0  3  [5.0, 6.0]\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def update(\n        self,\n        where: Optional[str] = None,\n        values: Optional[dict] = None,\n        *,\n        values_sql: Optional[Dict[str, str]] = None,\n    ):\n        \"\"\"\n        This can be used to update zero to all rows depending on how many\n        rows match the where clause. If no where clause is provided, then\n        all rows will be updated.\n\n        Either `values` or `values_sql` must be provided. You cannot provide\n        both.\n\n        Parameters\n        ----------\n        where: str, optional\n            The SQL where clause to use when updating rows. For example, 'x = 2'\n            or 'x IN (1, 2, 3)'. The filter must not be empty, or it will error.\n        values: dict, optional\n            The values to update. The keys are the column names and the values\n            are the values to set.\n        values_sql: dict, optional\n            The values to update, expressed as SQL expression strings. These can\n            reference existing columns. For example, {\"x\": \"x + 1\"} will increment\n            the x column by 1.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import lancedb\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; data = pd.DataFrame({\"x\": [1, 2, 3], \"vector\": [[1.0, 2], [3, 4], [5, 6]]})\n        &gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n        &gt;&gt;&gt; table = db.create_table(\"my_table\", data)\n        &gt;&gt;&gt; table.to_pandas()\n           x      vector\n        0  1  [1.0, 2.0]\n        1  2  [3.0, 4.0]\n        2  3  [5.0, 6.0]\n        &gt;&gt;&gt; table.update(where=\"x = 2\", values={\"vector\": [10.0, 10]})\n        &gt;&gt;&gt; table.to_pandas()\n           x        vector\n        0  1    [1.0, 2.0]\n        1  3    [5.0, 6.0]\n        2  2  [10.0, 10.0]\n        &gt;&gt;&gt; table.update(values_sql={\"x\": \"x + 1\"})\n        &gt;&gt;&gt; table.to_pandas()\n           x        vector\n        0  2    [1.0, 2.0]\n        1  4    [5.0, 6.0]\n        2  3  [10.0, 10.0]\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def cleanup_old_versions(\n        self,\n        older_than: Optional[timedelta] = None,\n        *,\n        delete_unverified: bool = False,\n    ) -&gt; \"CleanupStats\":\n        \"\"\"\n        Clean up old versions of the table, freeing disk space.\n\n        Parameters\n        ----------\n        older_than: timedelta, default None\n            The minimum age of the version to delete. If None, then this defaults\n            to two weeks.\n        delete_unverified: bool, default False\n            Because they may be part of an in-progress transaction, files newer\n            than 7 days old are not deleted by default. If you are sure that\n            there are no in-progress transactions, then you can set this to True\n            to delete all files older than `older_than`.\n\n        Returns\n        -------\n        CleanupStats\n            The stats of the cleanup operation, including how many bytes were\n            freed.\n\n        See Also\n        --------\n        [Table.optimize][lancedb.table.Table.optimize]: A more comprehensive\n            optimization operation that includes cleanup as well as other operations.\n\n        Notes\n        -----\n        This function is not available in LanceDb Cloud (since LanceDB\n        Cloud manages cleanup for you automatically)\n        \"\"\"\n\n    @abstractmethod\n    def compact_files(self, *args, **kwargs):\n        \"\"\"\n        Run the compaction process on the table.\n        This can be run after making several small appends to optimize the table\n        for faster reads.\n\n        Arguments are passed onto Lance's\n        [compact_files][lance.dataset.DatasetOptimizer.compact_files].\n        For most cases, the default should be fine.\n\n        See Also\n        --------\n        [Table.optimize][lancedb.table.Table.optimize]: A more comprehensive\n            optimization operation that includes cleanup as well as other operations.\n\n        Notes\n        -----\n        This function is not available in LanceDB Cloud (since LanceDB\n        Cloud manages compaction for you automatically)\n        \"\"\"\n\n    @abstractmethod\n    def optimize(\n        self,\n        *,\n        cleanup_older_than: Optional[timedelta] = None,\n        delete_unverified: bool = False,\n        retrain: bool = False,\n    ):\n        \"\"\"\n        Optimize the on-disk data and indices for better performance.\n\n        Modeled after ``VACUUM`` in PostgreSQL.\n\n        Optimization covers three operations:\n\n         * Compaction: Merges small files into larger ones\n         * Prune: Removes old versions of the dataset\n         * Index: Optimizes the indices, adding new data to existing indices\n\n        Parameters\n        ----------\n        cleanup_older_than: timedelta, optional default 7 days\n            All files belonging to versions older than this will be removed.  Set\n            to 0 days to remove all versions except the latest.  The latest version\n            is never removed.\n        delete_unverified: bool, default False\n            Files leftover from a failed transaction may appear to be part of an\n            in-progress operation (e.g. appending new data) and these files will not\n            be deleted unless they are at least 7 days old. If delete_unverified is True\n            then these files will be deleted regardless of their age.\n        retrain: bool, default False\n            If True, retrain the vector indices, this would refine the IVF clustering\n            and quantization, which may improve the search accuracy. It's faster than\n            re-creating the index from scratch, so it's recommended to try this first,\n            when the data distribution has changed significantly.\n\n        Experimental API\n        ----------------\n\n        The optimization process is undergoing active development and may change.\n        Our goal with these changes is to improve the performance of optimization and\n        reduce the complexity.\n\n        That being said, it is essential today to run optimize if you want the best\n        performance.  It should be stable and safe to use in production, but it our\n        hope that the API may be simplified (or not even need to be called) in the\n        future.\n\n        The frequency an application shoudl call optimize is based on the frequency of\n        data modifications.  If data is frequently added, deleted, or updated then\n        optimize should be run frequently.  A good rule of thumb is to run optimize if\n        you have added or modified 100,000 or more records or run more than 20 data\n        modification operations.\n        \"\"\"\n\n    @abstractmethod\n    def list_indices(self) -&gt; Iterable[IndexConfig]:\n        \"\"\"\n        List all indices that have been created with\n        [Table.create_index][lancedb.table.Table.create_index]\n        \"\"\"\n\n    @abstractmethod\n    def index_stats(self, index_name: str) -&gt; Optional[IndexStatistics]:\n        \"\"\"\n        Retrieve statistics about an index\n\n        Parameters\n        ----------\n        index_name: str\n            The name of the index to retrieve statistics for\n\n        Returns\n        -------\n        IndexStatistics or None\n            The statistics about the index. Returns None if the index does not exist.\n        \"\"\"\n\n    @abstractmethod\n    def add_columns(\n        self, transforms: Dict[str, str] | pa.Field | List[pa.Field] | pa.Schema\n    ):\n        \"\"\"\n        Add new columns with defined values.\n\n        Parameters\n        ----------\n        transforms: Dict[str, str], pa.Field, List[pa.Field], pa.Schema\n            A map of column name to a SQL expression to use to calculate the\n            value of the new column. These expressions will be evaluated for\n            each row in the table, and can reference existing columns.\n            Alternatively, a pyarrow Field or Schema can be provided to add\n            new columns with the specified data types. The new columns will\n            be initialized with null values.\n        \"\"\"\n\n    @abstractmethod\n    def alter_columns(self, *alterations: Iterable[Dict[str, str]]):\n        \"\"\"\n        Alter column names and nullability.\n\n        Parameters\n        ----------\n        alterations : Iterable[Dict[str, Any]]\n            A sequence of dictionaries, each with the following keys:\n            - \"path\": str\n                The column path to alter. For a top-level column, this is the name.\n                For a nested column, this is the dot-separated path, e.g. \"a.b.c\".\n            - \"rename\": str, optional\n                The new name of the column. If not specified, the column name is\n                not changed.\n            - \"data_type\": pyarrow.DataType, optional\n               The new data type of the column. Existing values will be casted\n               to this type. If not specified, the column data type is not changed.\n            - \"nullable\": bool, optional\n                Whether the column should be nullable. If not specified, the column\n                nullability is not changed. Only non-nullable columns can be changed\n                to nullable. Currently, you cannot change a nullable column to\n                non-nullable.\n        \"\"\"\n\n    @abstractmethod\n    def drop_columns(self, columns: Iterable[str]):\n        \"\"\"\n        Drop columns from the table.\n\n        Parameters\n        ----------\n        columns : Iterable[str]\n            The names of the columns to drop.\n        \"\"\"\n\n    @abstractmethod\n    def checkout(self, version: int):\n        \"\"\"\n        Checks out a specific version of the Table\n\n        Any read operation on the table will now access the data at the checked out\n        version. As a consequence, calling this method will disable any read consistency\n        interval that was previously set.\n\n        This is a read-only operation that turns the table into a sort of \"view\"\n        or \"detached head\".  Other table instances will not be affected.  To make the\n        change permanent you can use the `[Self::restore]` method.\n\n        Any operation that modifies the table will fail while the table is in a checked\n        out state.\n\n        To return the table to a normal state use `[Self::checkout_latest]`\n        \"\"\"\n\n    @abstractmethod\n    def checkout_latest(self):\n        \"\"\"\n        Ensures the table is pointing at the latest version\n\n        This can be used to manually update a table when the read_consistency_interval\n        is None\n        It can also be used to undo a `[Self::checkout]` operation\n        \"\"\"\n\n    @abstractmethod\n    def restore(self, version: Optional[int] = None):\n        \"\"\"Restore a version of the table. This is an in-place operation.\n\n        This creates a new version where the data is equivalent to the\n        specified previous version. Data is not copied (as of python-v0.2.1).\n\n        Parameters\n        ----------\n        version : int, default None\n            The version to restore. If unspecified then restores the currently\n            checked out version. If the currently checked out version is the\n            latest version then this is a no-op.\n        \"\"\"\n\n    @abstractmethod\n    def list_versions(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"List all versions of the table\"\"\"\n\n    @cached_property\n    def _dataset_uri(self) -&gt; str:\n        return _table_uri(self._conn.uri, self.name)\n\n    def _get_fts_index_path(self) -&gt; Tuple[str, pa_fs.FileSystem, bool]:\n        from .remote.table import RemoteTable\n\n        if isinstance(self, RemoteTable) or get_uri_scheme(self._dataset_uri) != \"file\":\n            return (\"\", None, False)\n        path = join_uri(self._dataset_uri, \"_indices\", \"fts\")\n        fs, path = fs_from_uri(path)\n        index_exists = fs.get_file_info(path).type != pa_fs.FileType.NotFound\n        return (path, fs, index_exists)\n\n    @abstractmethod\n    def uses_v2_manifest_paths(self) -&gt; bool:\n        \"\"\"\n        Check if the table is using the new v2 manifest paths.\n\n        Returns\n        -------\n        bool\n            True if the table is using the new v2 manifest paths, False otherwise.\n        \"\"\"\n\n    @abstractmethod\n    def migrate_v2_manifest_paths(self):\n        \"\"\"\n        Migrate the manifest paths to the new format.\n\n        This will update the manifest to use the new v2 format for paths.\n\n        This function is idempotent, and can be run multiple times without\n        changing the state of the object store.\n\n        !!! danger\n\n            This should not be run while other concurrent operations are happening.\n            And it should also run until completion before resuming other operations.\n\n        You can use\n        [Table.uses_v2_manifest_paths][lancedb.table.Table.uses_v2_manifest_paths]\n        to check if the table is already using the new path style.\n        \"\"\"\n</code></pre>"},{"location":"python/python/#lancedb.table.Table.name","title":"name  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of this Table</p>"},{"location":"python/python/#lancedb.table.Table.version","title":"version  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>version: int\n</code></pre> <p>The version of this Table</p>"},{"location":"python/python/#lancedb.table.Table.schema","title":"schema  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>schema: Schema\n</code></pre> <p>The Arrow Schema of this Table</p>"},{"location":"python/python/#lancedb.table.Table.embedding_functions","title":"embedding_functions  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>embedding_functions: Dict[str, EmbeddingFunctionConfig]\n</code></pre> <p>Get a mapping from vector column name to it's configured embedding function.</p>"},{"location":"python/python/#lancedb.table.Table.count_rows","title":"count_rows  <code>abstractmethod</code>","text":"<pre><code>count_rows(filter: Optional[str] = None) -&gt; int\n</code></pre> <p>Count the number of rows in the table.</p> <p>Parameters:</p> <ul> <li> <code>filter</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>A SQL where clause to filter the rows to count.</p> </li> </ul> Source code in <code>lancedb/table.py</code> <pre><code>@abstractmethod\ndef count_rows(self, filter: Optional[str] = None) -&gt; int:\n    \"\"\"\n    Count the number of rows in the table.\n\n    Parameters\n    ----------\n    filter: str, optional\n        A SQL where clause to filter the rows to count.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python/python/#lancedb.table.Table.to_pandas","title":"to_pandas","text":"<pre><code>to_pandas() -&gt; 'pandas.DataFrame'\n</code></pre> <p>Return the table as a pandas DataFrame.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            </li> </ul> Source code in <code>lancedb/table.py</code> <pre><code>def to_pandas(self) -&gt; \"pandas.DataFrame\":\n    \"\"\"Return the table as a pandas DataFrame.\n\n    Returns\n    -------\n    pd.DataFrame\n    \"\"\"\n    return self.to_arrow().to_pandas()\n</code></pre>"},{"location":"python/python/#lancedb.table.Table.to_arrow","title":"to_arrow  <code>abstractmethod</code>","text":"<pre><code>to_arrow() -&gt; Table\n</code></pre> <p>Return the table as a pyarrow Table.</p> <p>Returns:</p> <ul> <li> <code>Table</code>           \u2013            </li> </ul> Source code in <code>lancedb/table.py</code> <pre><code>@abstractmethod\ndef to_arrow(self) -&gt; pa.Table:\n    \"\"\"Return the table as a pyarrow Table.\n\n    Returns\n    -------\n    pa.Table\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python/python/#lancedb.table.Table.create_index","title":"create_index","text":"<pre><code>create_index(metric='l2', num_partitions=256, num_sub_vectors=96, vector_column_name: str = VECTOR_COLUMN_NAME, replace: bool = True, accelerator: Optional[str] = None, index_cache_size: Optional[int] = None, *, index_type: VectorIndexType = 'IVF_PQ', wait_timeout: Optional[timedelta] = None, num_bits: int = 8, max_iterations: int = 50, sample_rate: int = 256, m: int = 20, ef_construction: int = 300)\n</code></pre> <p>Create an index on the table.</p> <p>Parameters:</p> <ul> <li> <code>metric</code>           \u2013            <p>The distance metric to use when creating the index. Valid values are \"l2\", \"cosine\", \"dot\", or \"hamming\". l2 is euclidean distance. Hamming is available only for binary vectors.</p> </li> <li> <code>num_partitions</code>           \u2013            <p>The number of IVF partitions to use when creating the index. Default is 256.</p> </li> <li> <code>num_sub_vectors</code>           \u2013            <p>The number of PQ sub-vectors to use when creating the index. Default is 96.</p> </li> <li> <code>vector_column_name</code>               (<code>str</code>, default:                   <code>VECTOR_COLUMN_NAME</code> )           \u2013            <p>The vector column name to create the index.</p> </li> <li> <code>replace</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <ul> <li> <p>If True, replace the existing index if it exists.</p> </li> <li> <p>If False, raise an error if duplicate index exists.</p> </li> </ul> </li> <li> <code>accelerator</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>If set, use the given accelerator to create the index. Only support \"cuda\" for now.</p> </li> <li> <code>index_cache_size</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>The size of the index cache in number of entries. Default value is 256.</p> </li> <li> <code>num_bits</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The number of bits to encode sub-vectors. Only used with the IVF_PQ index. Only 4 and 8 are supported.</p> </li> <li> <code>wait_timeout</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>The timeout to wait if indexing is asynchronous.</p> </li> </ul> Source code in <code>lancedb/table.py</code> <pre><code>def create_index(\n    self,\n    metric=\"l2\",\n    num_partitions=256,\n    num_sub_vectors=96,\n    vector_column_name: str = VECTOR_COLUMN_NAME,\n    replace: bool = True,\n    accelerator: Optional[str] = None,\n    index_cache_size: Optional[int] = None,\n    *,\n    index_type: VectorIndexType = \"IVF_PQ\",\n    wait_timeout: Optional[timedelta] = None,\n    num_bits: int = 8,\n    max_iterations: int = 50,\n    sample_rate: int = 256,\n    m: int = 20,\n    ef_construction: int = 300,\n):\n    \"\"\"Create an index on the table.\n\n    Parameters\n    ----------\n    metric: str, default \"l2\"\n        The distance metric to use when creating the index.\n        Valid values are \"l2\", \"cosine\", \"dot\", or \"hamming\".\n        l2 is euclidean distance.\n        Hamming is available only for binary vectors.\n    num_partitions: int, default 256\n        The number of IVF partitions to use when creating the index.\n        Default is 256.\n    num_sub_vectors: int, default 96\n        The number of PQ sub-vectors to use when creating the index.\n        Default is 96.\n    vector_column_name: str, default \"vector\"\n        The vector column name to create the index.\n    replace: bool, default True\n        - If True, replace the existing index if it exists.\n\n        - If False, raise an error if duplicate index exists.\n    accelerator: str, default None\n        If set, use the given accelerator to create the index.\n        Only support \"cuda\" for now.\n    index_cache_size : int, optional\n        The size of the index cache in number of entries. Default value is 256.\n    num_bits: int\n        The number of bits to encode sub-vectors. Only used with the IVF_PQ index.\n        Only 4 and 8 are supported.\n    wait_timeout: timedelta, optional\n        The timeout to wait if indexing is asynchronous.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python/python/#lancedb.table.Table.drop_index","title":"drop_index","text":"<pre><code>drop_index(name: str) -&gt; None\n</code></pre> <p>Drop an index from the table.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the index to drop.</p> </li> </ul> Notes <p>This does not delete the index from disk, it just removes it from the table. To delete the index, run optimize after dropping the index.</p> <p>Use list_indices to find the names of the indices.</p> Source code in <code>lancedb/table.py</code> <pre><code>def drop_index(self, name: str) -&gt; None:\n    \"\"\"\n    Drop an index from the table.\n\n    Parameters\n    ----------\n    name: str\n        The name of the index to drop.\n\n    Notes\n    -----\n    This does not delete the index from disk, it just removes it from the table.\n    To delete the index, run [optimize][lancedb.table.Table.optimize]\n    after dropping the index.\n\n    Use [list_indices][lancedb.table.Table.list_indices] to find the names of\n    the indices.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python/python/#lancedb.table.Table.wait_for_index","title":"wait_for_index","text":"<pre><code>wait_for_index(index_names: Iterable[str], timeout: timedelta = timedelta(seconds=300)) -&gt; None\n</code></pre> <p>Wait for indexing to complete for the given index names. This will poll the table until all the indices are fully indexed, or raise a timeout exception if the timeout is reached.</p> <p>Parameters:</p> <ul> <li> <code>index_names</code>               (<code>Iterable[str]</code>)           \u2013            <p>The name of the indices to poll</p> </li> <li> <code>timeout</code>               (<code>timedelta</code>, default:                   <code>timedelta(seconds=300)</code> )           \u2013            <p>Timeout to wait for asynchronous indexing. The default is 5 minutes.</p> </li> </ul> Source code in <code>lancedb/table.py</code> <pre><code>def wait_for_index(\n    self, index_names: Iterable[str], timeout: timedelta = timedelta(seconds=300)\n) -&gt; None:\n    \"\"\"\n    Wait for indexing to complete for the given index names.\n    This will poll the table until all the indices are fully indexed,\n    or raise a timeout exception if the timeout is reached.\n\n    Parameters\n    ----------\n    index_names: str\n        The name of the indices to poll\n    timeout: timedelta\n        Timeout to wait for asynchronous indexing. The default is 5 minutes.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python/python/#lancedb.table.Table.create_scalar_index","title":"create_scalar_index  <code>abstractmethod</code>","text":"<pre><code>create_scalar_index(column: str, *, replace: bool = True, index_type: ScalarIndexType = 'BTREE', wait_timeout: Optional[timedelta] = None)\n</code></pre> <p>Create a scalar index on a column.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>str</code>)           \u2013            <p>The column to be indexed.  Must be a boolean, integer, float, or string column.</p> </li> <li> <code>replace</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Replace the existing index if it exists.</p> </li> <li> <code>index_type</code>               (<code>ScalarIndexType</code>, default:                   <code>'BTREE'</code> )           \u2013            <p>The type of index to create.</p> </li> <li> <code>wait_timeout</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>The timeout to wait if indexing is asynchronous.</p> </li> </ul> <p>Examples:</p> <p>Scalar indices, like vector indices, can be used to speed up scans.  A scalar index can speed up scans that contain filter expressions on the indexed column. For example, the following scan will be faster if the column <code>my_col</code> has a scalar index:</p> <pre><code>&gt;&gt;&gt; import lancedb\n&gt;&gt;&gt; db = lancedb.connect(\"/data/lance\")\n&gt;&gt;&gt; img_table = db.open_table(\"images\")\n&gt;&gt;&gt; my_df = img_table.search().where(\"my_col = 7\",\n...                                  prefilter=True).to_pandas()\n</code></pre> <p>Scalar indices can also speed up scans containing a vector search and a prefilter:</p> <pre><code>&gt;&gt;&gt; import lancedb\n&gt;&gt;&gt; db = lancedb.connect(\"/data/lance\")\n&gt;&gt;&gt; img_table = db.open_table(\"images\")\n&gt;&gt;&gt; img_table.search([1, 2, 3, 4], vector_column_name=\"vector\")\n...     .where(\"my_col != 7\", prefilter=True)\n...     .to_pandas()\n</code></pre> <p>Scalar indices can only speed up scans for basic filters using equality, comparison, range (e.g. <code>my_col BETWEEN 0 AND 100</code>), and set membership (e.g. <code>my_col IN (0, 1, 2)</code>)</p> <p>Scalar indices can be used if the filter contains multiple indexed columns and the filter criteria are AND'd or OR'd together (e.g. <code>my_col &lt; 0 AND other_col&gt; 100</code>)</p> <p>Scalar indices may be used if the filter contains non-indexed columns but, depending on the structure of the filter, they may not be usable.  For example, if the column <code>not_indexed</code> does not have a scalar index then the filter <code>my_col = 0 OR not_indexed = 1</code> will not be able to use any scalar index on <code>my_col</code>.</p> Source code in <code>lancedb/table.py</code> <pre><code>@abstractmethod\ndef create_scalar_index(\n    self,\n    column: str,\n    *,\n    replace: bool = True,\n    index_type: ScalarIndexType = \"BTREE\",\n    wait_timeout: Optional[timedelta] = None,\n):\n    \"\"\"Create a scalar index on a column.\n\n    Parameters\n    ----------\n    column : str\n        The column to be indexed.  Must be a boolean, integer, float,\n        or string column.\n    replace : bool, default True\n        Replace the existing index if it exists.\n    index_type: Literal[\"BTREE\", \"BITMAP\", \"LABEL_LIST\"], default \"BTREE\"\n        The type of index to create.\n    wait_timeout: timedelta, optional\n        The timeout to wait if indexing is asynchronous.\n    Examples\n    --------\n\n    Scalar indices, like vector indices, can be used to speed up scans.  A scalar\n    index can speed up scans that contain filter expressions on the indexed column.\n    For example, the following scan will be faster if the column ``my_col`` has\n    a scalar index:\n\n    &gt;&gt;&gt; import lancedb # doctest: +SKIP\n    &gt;&gt;&gt; db = lancedb.connect(\"/data/lance\") # doctest: +SKIP\n    &gt;&gt;&gt; img_table = db.open_table(\"images\") # doctest: +SKIP\n    &gt;&gt;&gt; my_df = img_table.search().where(\"my_col = 7\", # doctest: +SKIP\n    ...                                  prefilter=True).to_pandas()\n\n    Scalar indices can also speed up scans containing a vector search and a\n    prefilter:\n\n    &gt;&gt;&gt; import lancedb # doctest: +SKIP\n    &gt;&gt;&gt; db = lancedb.connect(\"/data/lance\") # doctest: +SKIP\n    &gt;&gt;&gt; img_table = db.open_table(\"images\") # doctest: +SKIP\n    &gt;&gt;&gt; img_table.search([1, 2, 3, 4], vector_column_name=\"vector\") # doctest: +SKIP\n    ...     .where(\"my_col != 7\", prefilter=True)\n    ...     .to_pandas()\n\n    Scalar indices can only speed up scans for basic filters using\n    equality, comparison, range (e.g. ``my_col BETWEEN 0 AND 100``), and set\n    membership (e.g. `my_col IN (0, 1, 2)`)\n\n    Scalar indices can be used if the filter contains multiple indexed columns and\n    the filter criteria are AND'd or OR'd together\n    (e.g. ``my_col &lt; 0 AND other_col&gt; 100``)\n\n    Scalar indices may be used if the filter contains non-indexed columns but,\n    depending on the structure of the filter, they may not be usable.  For example,\n    if the column ``not_indexed`` does not have a scalar index then the filter\n    ``my_col = 0 OR not_indexed = 1`` will not be able to use any scalar index on\n    ``my_col``.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python/python/#lancedb.table.Table.create_fts_index","title":"create_fts_index","text":"<pre><code>create_fts_index(field_names: Union[str, List[str]], *, ordering_field_names: Optional[Union[str, List[str]]] = None, replace: bool = False, writer_heap_size: Optional[int] = 1024 * 1024 * 1024, use_tantivy: bool = True, tokenizer_name: Optional[str] = None, with_position: bool = True, base_tokenizer: BaseTokenizerType = 'simple', language: str = 'English', max_token_length: Optional[int] = 40, lower_case: bool = True, stem: bool = False, remove_stop_words: bool = False, ascii_folding: bool = False, wait_timeout: Optional[timedelta] = None)\n</code></pre> <p>Create a full-text search index on the table.</p> <p>Warning - this API is highly experimental and is highly likely to change in the future.</p> <p>Parameters:</p> <ul> <li> <code>field_names</code>               (<code>Union[str, List[str]]</code>)           \u2013            <p>The name(s) of the field to index. can be only str if use_tantivy=True for now.</p> </li> <li> <code>replace</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, replace the existing index if it exists. Note that this is not yet an atomic operation; the index will be temporarily unavailable while the new index is being created.</p> </li> <li> <code>writer_heap_size</code>               (<code>Optional[int]</code>, default:                   <code>1024 * 1024 * 1024</code> )           \u2013            <p>Only available with use_tantivy=True</p> </li> <li> <code>ordering_field_names</code>               (<code>Optional[Union[str, List[str]]]</code>, default:                   <code>None</code> )           \u2013            <p>A list of unsigned type fields to index to optionally order results on at search time. only available with use_tantivy=True</p> </li> <li> <code>tokenizer_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The tokenizer to use for the index. Can be \"raw\", \"default\" or the 2 letter language code followed by \"_stem\". So for english it would be \"en_stem\". For available languages see: https://docs.rs/tantivy/latest/tantivy/tokenizer/enum.Language.html</p> </li> <li> <code>use_tantivy</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, use the legacy full-text search implementation based on tantivy. If False, use the new full-text search implementation based on lance-index.</p> </li> <li> <code>with_position</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Only available with use_tantivy=False If False, do not store the positions of the terms in the text. This can reduce the size of the index and improve indexing speed. But it will raise an exception for phrase queries.</p> </li> <li> <code>base_tokenizer</code>               (<code>str</code>, default:                   <code>\"simple\"</code> )           \u2013            <p>The base tokenizer to use for tokenization. Options are: - \"simple\": Splits text by whitespace and punctuation. - \"whitespace\": Split text by whitespace, but not punctuation. - \"raw\": No tokenization. The entire text is treated as a single token.</p> </li> <li> <code>language</code>               (<code>str</code>, default:                   <code>\"English\"</code> )           \u2013            <p>The language to use for tokenization.</p> </li> <li> <code>max_token_length</code>               (<code>int</code>, default:                   <code>40</code> )           \u2013            <p>The maximum token length to index. Tokens longer than this length will be ignored.</p> </li> <li> <code>lower_case</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to convert the token to lower case. This makes queries case-insensitive.</p> </li> <li> <code>stem</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to stem the token. Stemming reduces words to their root form. For example, in English \"running\" and \"runs\" would both be reduced to \"run\".</p> </li> <li> <code>remove_stop_words</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to remove stop words. Stop words are common words that are often removed from text before indexing. For example, in English \"the\" and \"and\".</p> </li> <li> <code>ascii_folding</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to fold ASCII characters. This converts accented characters to their ASCII equivalent. For example, \"caf\u00e9\" would be converted to \"cafe\".</p> </li> <li> <code>wait_timeout</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>The timeout to wait if indexing is asynchronous.</p> </li> </ul> Source code in <code>lancedb/table.py</code> <pre><code>def create_fts_index(\n    self,\n    field_names: Union[str, List[str]],\n    *,\n    ordering_field_names: Optional[Union[str, List[str]]] = None,\n    replace: bool = False,\n    writer_heap_size: Optional[int] = 1024 * 1024 * 1024,\n    use_tantivy: bool = True,\n    tokenizer_name: Optional[str] = None,\n    with_position: bool = True,\n    # tokenizer configs:\n    base_tokenizer: BaseTokenizerType = \"simple\",\n    language: str = \"English\",\n    max_token_length: Optional[int] = 40,\n    lower_case: bool = True,\n    stem: bool = False,\n    remove_stop_words: bool = False,\n    ascii_folding: bool = False,\n    wait_timeout: Optional[timedelta] = None,\n):\n    \"\"\"Create a full-text search index on the table.\n\n    Warning - this API is highly experimental and is highly likely to change\n    in the future.\n\n    Parameters\n    ----------\n    field_names: str or list of str\n        The name(s) of the field to index.\n        can be only str if use_tantivy=True for now.\n    replace: bool, default False\n        If True, replace the existing index if it exists. Note that this is\n        not yet an atomic operation; the index will be temporarily\n        unavailable while the new index is being created.\n    writer_heap_size: int, default 1GB\n        Only available with use_tantivy=True\n    ordering_field_names:\n        A list of unsigned type fields to index to optionally order\n        results on at search time.\n        only available with use_tantivy=True\n    tokenizer_name: str, default \"default\"\n        The tokenizer to use for the index. Can be \"raw\", \"default\" or the 2 letter\n        language code followed by \"_stem\". So for english it would be \"en_stem\".\n        For available languages see: https://docs.rs/tantivy/latest/tantivy/tokenizer/enum.Language.html\n    use_tantivy: bool, default True\n        If True, use the legacy full-text search implementation based on tantivy.\n        If False, use the new full-text search implementation based on lance-index.\n    with_position: bool, default True\n        Only available with use_tantivy=False\n        If False, do not store the positions of the terms in the text.\n        This can reduce the size of the index and improve indexing speed.\n        But it will raise an exception for phrase queries.\n    base_tokenizer : str, default \"simple\"\n        The base tokenizer to use for tokenization. Options are:\n        - \"simple\": Splits text by whitespace and punctuation.\n        - \"whitespace\": Split text by whitespace, but not punctuation.\n        - \"raw\": No tokenization. The entire text is treated as a single token.\n    language : str, default \"English\"\n        The language to use for tokenization.\n    max_token_length : int, default 40\n        The maximum token length to index. Tokens longer than this length will be\n        ignored.\n    lower_case : bool, default True\n        Whether to convert the token to lower case. This makes queries\n        case-insensitive.\n    stem : bool, default False\n        Whether to stem the token. Stemming reduces words to their root form.\n        For example, in English \"running\" and \"runs\" would both be reduced to \"run\".\n    remove_stop_words : bool, default False\n        Whether to remove stop words. Stop words are common words that are often\n        removed from text before indexing. For example, in English \"the\" and \"and\".\n    ascii_folding : bool, default False\n        Whether to fold ASCII characters. This converts accented characters to\n        their ASCII equivalent. For example, \"caf\u00e9\" would be converted to \"cafe\".\n    wait_timeout: timedelta, optional\n        The timeout to wait if indexing is asynchronous.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python/python/#lancedb.table.Table.add","title":"add  <code>abstractmethod</code>","text":"<pre><code>add(data: DATA, mode: AddMode = 'append', on_bad_vectors: OnBadVectorsType = 'error', fill_value: float = 0.0)\n</code></pre> <p>Add more data to the Table.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>DATA</code>)           \u2013            <p>The data to insert into the table. Acceptable types are:</p> <ul> <li> <p>list-of-dict</p> </li> <li> <p>pandas.DataFrame</p> </li> <li> <p>pyarrow.Table or pyarrow.RecordBatch</p> </li> </ul> </li> <li> <code>mode</code>               (<code>AddMode</code>, default:                   <code>'append'</code> )           \u2013            <p>The mode to use when writing the data. Valid values are \"append\" and \"overwrite\".</p> </li> <li> <code>on_bad_vectors</code>               (<code>OnBadVectorsType</code>, default:                   <code>'error'</code> )           \u2013            <p>What to do if any of the vectors are not the same size or contains NaNs. One of \"error\", \"drop\", \"fill\".</p> </li> <li> <code>fill_value</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>The value to use when filling vectors. Only used if on_bad_vectors=\"fill\".</p> </li> </ul> Source code in <code>lancedb/table.py</code> <pre><code>@abstractmethod\ndef add(\n    self,\n    data: DATA,\n    mode: AddMode = \"append\",\n    on_bad_vectors: OnBadVectorsType = \"error\",\n    fill_value: float = 0.0,\n):\n    \"\"\"Add more data to the [Table](Table).\n\n    Parameters\n    ----------\n    data: DATA\n        The data to insert into the table. Acceptable types are:\n\n        - list-of-dict\n\n        - pandas.DataFrame\n\n        - pyarrow.Table or pyarrow.RecordBatch\n    mode: str\n        The mode to use when writing the data. Valid values are\n        \"append\" and \"overwrite\".\n    on_bad_vectors: str, default \"error\"\n        What to do if any of the vectors are not the same size or contains NaNs.\n        One of \"error\", \"drop\", \"fill\".\n    fill_value: float, default 0.\n        The value to use when filling vectors. Only used if on_bad_vectors=\"fill\".\n\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python/python/#lancedb.table.Table.merge_insert","title":"merge_insert","text":"<pre><code>merge_insert(on: Union[str, Iterable[str]]) -&gt; LanceMergeInsertBuilder\n</code></pre> <p>Returns a <code>LanceMergeInsertBuilder</code> that can be used to create a \"merge insert\" operation</p> <p>This operation can add rows, update rows, and remove rows all in a single transaction. It is a very generic tool that can be used to create behaviors like \"insert if not exists\", \"update or insert (i.e. upsert)\", or even replace a portion of existing data with new data (e.g. replace all data where month=\"january\")</p> <p>The merge insert operation works by combining new data from a source table with existing data in a target table by using a join.  There are three categories of records.</p> <p>\"Matched\" records are records that exist in both the source table and the target table. \"Not matched\" records exist only in the source table (e.g. these are new data) \"Not matched by source\" records exist only in the target table (this is old data)</p> <p>The builder returned by this method can be used to customize what should happen for each category of data.</p> <p>Please note that the data may appear to be reordered as part of this operation.  This is because updated rows will be deleted from the dataset and then reinserted at the end with the new values.</p> <p>Parameters:</p> <ul> <li> <code>on</code>               (<code>Union[str, Iterable[str]]</code>)           \u2013            <p>A column (or columns) to join on.  This is how records from the source table and target table are matched.  Typically this is some kind of key or id column.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import lancedb\n&gt;&gt;&gt; data = pa.table({\"a\": [2, 1, 3], \"b\": [\"a\", \"b\", \"c\"]})\n&gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n&gt;&gt;&gt; table = db.create_table(\"my_table\", data)\n&gt;&gt;&gt; new_data = pa.table({\"a\": [2, 3, 4], \"b\": [\"x\", \"y\", \"z\"]})\n&gt;&gt;&gt; # Perform a \"upsert\" operation\n&gt;&gt;&gt; table.merge_insert(\"a\")             \\\n...      .when_matched_update_all()     \\\n...      .when_not_matched_insert_all() \\\n...      .execute(new_data)\n&gt;&gt;&gt; # The order of new rows is non-deterministic since we use\n&gt;&gt;&gt; # a hash-join as part of this operation and so we sort here\n&gt;&gt;&gt; table.to_arrow().sort_by(\"a\").to_pandas()\n   a  b\n0  1  b\n1  2  x\n2  3  y\n3  4  z\n</code></pre> Source code in <code>lancedb/table.py</code> <pre><code>def merge_insert(self, on: Union[str, Iterable[str]]) -&gt; LanceMergeInsertBuilder:\n    \"\"\"\n    Returns a [`LanceMergeInsertBuilder`][lancedb.merge.LanceMergeInsertBuilder]\n    that can be used to create a \"merge insert\" operation\n\n    This operation can add rows, update rows, and remove rows all in a single\n    transaction. It is a very generic tool that can be used to create\n    behaviors like \"insert if not exists\", \"update or insert (i.e. upsert)\",\n    or even replace a portion of existing data with new data (e.g. replace\n    all data where month=\"january\")\n\n    The merge insert operation works by combining new data from a\n    **source table** with existing data in a **target table** by using a\n    join.  There are three categories of records.\n\n    \"Matched\" records are records that exist in both the source table and\n    the target table. \"Not matched\" records exist only in the source table\n    (e.g. these are new data) \"Not matched by source\" records exist only\n    in the target table (this is old data)\n\n    The builder returned by this method can be used to customize what\n    should happen for each category of data.\n\n    Please note that the data may appear to be reordered as part of this\n    operation.  This is because updated rows will be deleted from the\n    dataset and then reinserted at the end with the new values.\n\n    Parameters\n    ----------\n\n    on: Union[str, Iterable[str]]\n        A column (or columns) to join on.  This is how records from the\n        source table and target table are matched.  Typically this is some\n        kind of key or id column.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import lancedb\n    &gt;&gt;&gt; data = pa.table({\"a\": [2, 1, 3], \"b\": [\"a\", \"b\", \"c\"]})\n    &gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n    &gt;&gt;&gt; table = db.create_table(\"my_table\", data)\n    &gt;&gt;&gt; new_data = pa.table({\"a\": [2, 3, 4], \"b\": [\"x\", \"y\", \"z\"]})\n    &gt;&gt;&gt; # Perform a \"upsert\" operation\n    &gt;&gt;&gt; table.merge_insert(\"a\")             \\\\\n    ...      .when_matched_update_all()     \\\\\n    ...      .when_not_matched_insert_all() \\\\\n    ...      .execute(new_data)\n    &gt;&gt;&gt; # The order of new rows is non-deterministic since we use\n    &gt;&gt;&gt; # a hash-join as part of this operation and so we sort here\n    &gt;&gt;&gt; table.to_arrow().sort_by(\"a\").to_pandas()\n       a  b\n    0  1  b\n    1  2  x\n    2  3  y\n    3  4  z\n    \"\"\"\n    on = [on] if isinstance(on, str) else list(iter(on))\n\n    return LanceMergeInsertBuilder(self, on)\n</code></pre>"},{"location":"python/python/#lancedb.table.Table.search","title":"search  <code>abstractmethod</code>","text":"<pre><code>search(query: Optional[Union[VEC, str, 'PIL.Image.Image', Tuple, FullTextQuery]] = None, vector_column_name: Optional[str] = None, query_type: QueryType = 'auto', ordering_field_name: Optional[str] = None, fts_columns: Optional[Union[str, List[str]]] = None) -&gt; LanceQueryBuilder\n</code></pre> <p>Create a search query to find the nearest neighbors of the given query vector. We currently support vector search and [full-text search][experimental-full-text-search].</p> <p>All query options are defined in LanceQueryBuilder.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import lancedb\n&gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n&gt;&gt;&gt; data = [\n...    {\"original_width\": 100, \"caption\": \"bar\", \"vector\": [0.1, 2.3, 4.5]},\n...    {\"original_width\": 2000, \"caption\": \"foo\",  \"vector\": [0.5, 3.4, 1.3]},\n...    {\"original_width\": 3000, \"caption\": \"test\", \"vector\": [0.3, 6.2, 2.6]}\n... ]\n&gt;&gt;&gt; table = db.create_table(\"my_table\", data)\n&gt;&gt;&gt; query = [0.4, 1.4, 2.4]\n&gt;&gt;&gt; (table.search(query)\n...     .where(\"original_width &gt; 1000\", prefilter=True)\n...     .select([\"caption\", \"original_width\", \"vector\"])\n...     .limit(2)\n...     .to_pandas())\n  caption  original_width           vector  _distance\n0     foo            2000  [0.5, 3.4, 1.3]   5.220000\n1    test            3000  [0.3, 6.2, 2.6]  23.089996\n</code></pre> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>Optional[Union[VEC, str, 'PIL.Image.Image', Tuple, FullTextQuery]]</code>, default:                   <code>None</code> )           \u2013            <p>The targetted vector to search for.</p> <ul> <li> <p>default None. Acceptable types are: list, np.ndarray, PIL.Image.Image</p> </li> <li> <p>If None then the select/where/limit clauses are applied to filter the table</p> </li> </ul> </li> <li> <code>vector_column_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The name of the vector column to search.</p> <p>The vector column needs to be a pyarrow fixed size list type</p> <ul> <li> <p>If not specified then the vector column is inferred from the table schema</p> </li> <li> <p>If the table has multiple vector columns then the vector_column_name needs to be specified. Otherwise, an error is raised.</p> </li> </ul> </li> <li> <code>query_type</code>               (<code>QueryType</code>, default:                   <code>'auto'</code> )           \u2013            <p>default \"auto\". Acceptable types are: \"vector\", \"fts\", \"hybrid\", or \"auto\"</p> <ul> <li> <p>If \"auto\" then the query type is inferred from the query;</p> <ul> <li> <p>If <code>query</code> is a list/np.ndarray then the query type is \"vector\";</p> </li> <li> <p>If <code>query</code> is a PIL.Image.Image then either do vector search, or raise an error if no corresponding embedding function is found.</p> </li> </ul> </li> <li> <p>If <code>query</code> is a string, then the query type is \"vector\" if the table has embedding functions else the query type is \"fts\"</p> </li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> <code>LanceQueryBuilder</code>           \u2013            <p>A query builder object representing the query. Once executed, the query returns</p> <ul> <li> <p>selected columns</p> </li> <li> <p>the vector</p> </li> <li> <p>and also the \"_distance\" column which is the distance between the query vector and the returned vector.</p> </li> </ul> </li> </ul> Source code in <code>lancedb/table.py</code> <pre><code>@abstractmethod\ndef search(\n    self,\n    query: Optional[\n        Union[VEC, str, \"PIL.Image.Image\", Tuple, FullTextQuery]\n    ] = None,\n    vector_column_name: Optional[str] = None,\n    query_type: QueryType = \"auto\",\n    ordering_field_name: Optional[str] = None,\n    fts_columns: Optional[Union[str, List[str]]] = None,\n) -&gt; LanceQueryBuilder:\n    \"\"\"Create a search query to find the nearest neighbors\n    of the given query vector. We currently support [vector search][search]\n    and [full-text search][experimental-full-text-search].\n\n    All query options are defined in\n    [LanceQueryBuilder][lancedb.query.LanceQueryBuilder].\n\n    Examples\n    --------\n    &gt;&gt;&gt; import lancedb\n    &gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n    &gt;&gt;&gt; data = [\n    ...    {\"original_width\": 100, \"caption\": \"bar\", \"vector\": [0.1, 2.3, 4.5]},\n    ...    {\"original_width\": 2000, \"caption\": \"foo\",  \"vector\": [0.5, 3.4, 1.3]},\n    ...    {\"original_width\": 3000, \"caption\": \"test\", \"vector\": [0.3, 6.2, 2.6]}\n    ... ]\n    &gt;&gt;&gt; table = db.create_table(\"my_table\", data)\n    &gt;&gt;&gt; query = [0.4, 1.4, 2.4]\n    &gt;&gt;&gt; (table.search(query)\n    ...     .where(\"original_width &gt; 1000\", prefilter=True)\n    ...     .select([\"caption\", \"original_width\", \"vector\"])\n    ...     .limit(2)\n    ...     .to_pandas())\n      caption  original_width           vector  _distance\n    0     foo            2000  [0.5, 3.4, 1.3]   5.220000\n    1    test            3000  [0.3, 6.2, 2.6]  23.089996\n\n    Parameters\n    ----------\n    query: list/np.ndarray/str/PIL.Image.Image, default None\n        The targetted vector to search for.\n\n        - *default None*.\n        Acceptable types are: list, np.ndarray, PIL.Image.Image\n\n        - If None then the select/where/limit clauses are applied to filter\n        the table\n    vector_column_name: str, optional\n        The name of the vector column to search.\n\n        The vector column needs to be a pyarrow fixed size list type\n\n        - If not specified then the vector column is inferred from\n        the table schema\n\n        - If the table has multiple vector columns then the *vector_column_name*\n        needs to be specified. Otherwise, an error is raised.\n    query_type: str\n        *default \"auto\"*.\n        Acceptable types are: \"vector\", \"fts\", \"hybrid\", or \"auto\"\n\n        - If \"auto\" then the query type is inferred from the query;\n\n            - If `query` is a list/np.ndarray then the query type is\n            \"vector\";\n\n            - If `query` is a PIL.Image.Image then either do vector search,\n            or raise an error if no corresponding embedding function is found.\n\n        - If `query` is a string, then the query type is \"vector\" if the\n        table has embedding functions else the query type is \"fts\"\n\n    Returns\n    -------\n    LanceQueryBuilder\n        A query builder object representing the query.\n        Once executed, the query returns\n\n        - selected columns\n\n        - the vector\n\n        - and also the \"_distance\" column which is the distance between the query\n        vector and the returned vector.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python/python/#lancedb.table.Table.delete","title":"delete  <code>abstractmethod</code>","text":"<pre><code>delete(where: str)\n</code></pre> <p>Delete rows from the table.</p> <p>This can be used to delete a single row, many rows, all rows, or sometimes no rows (if your predicate matches nothing).</p> <p>Parameters:</p> <ul> <li> <code>where</code>               (<code>str</code>)           \u2013            <p>The SQL where clause to use when deleting rows.</p> <ul> <li>For example, 'x = 2' or 'x IN (1, 2, 3)'.</li> </ul> <p>The filter must not be empty, or it will error.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import lancedb\n&gt;&gt;&gt; data = [\n...    {\"x\": 1, \"vector\": [1.0, 2]},\n...    {\"x\": 2, \"vector\": [3.0, 4]},\n...    {\"x\": 3, \"vector\": [5.0, 6]}\n... ]\n&gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n&gt;&gt;&gt; table = db.create_table(\"my_table\", data)\n&gt;&gt;&gt; table.to_pandas()\n   x      vector\n0  1  [1.0, 2.0]\n1  2  [3.0, 4.0]\n2  3  [5.0, 6.0]\n&gt;&gt;&gt; table.delete(\"x = 2\")\n&gt;&gt;&gt; table.to_pandas()\n   x      vector\n0  1  [1.0, 2.0]\n1  3  [5.0, 6.0]\n</code></pre> <p>If you have a list of values to delete, you can combine them into a stringified list and use the <code>IN</code> operator:</p> <pre><code>&gt;&gt;&gt; to_remove = [1, 5]\n&gt;&gt;&gt; to_remove = \", \".join([str(v) for v in to_remove])\n&gt;&gt;&gt; to_remove\n'1, 5'\n&gt;&gt;&gt; table.delete(f\"x IN ({to_remove})\")\n&gt;&gt;&gt; table.to_pandas()\n   x      vector\n0  3  [5.0, 6.0]\n</code></pre> Source code in <code>lancedb/table.py</code> <pre><code>@abstractmethod\ndef delete(self, where: str):\n    \"\"\"Delete rows from the table.\n\n    This can be used to delete a single row, many rows, all rows, or\n    sometimes no rows (if your predicate matches nothing).\n\n    Parameters\n    ----------\n    where: str\n        The SQL where clause to use when deleting rows.\n\n        - For example, 'x = 2' or 'x IN (1, 2, 3)'.\n\n        The filter must not be empty, or it will error.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import lancedb\n    &gt;&gt;&gt; data = [\n    ...    {\"x\": 1, \"vector\": [1.0, 2]},\n    ...    {\"x\": 2, \"vector\": [3.0, 4]},\n    ...    {\"x\": 3, \"vector\": [5.0, 6]}\n    ... ]\n    &gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n    &gt;&gt;&gt; table = db.create_table(\"my_table\", data)\n    &gt;&gt;&gt; table.to_pandas()\n       x      vector\n    0  1  [1.0, 2.0]\n    1  2  [3.0, 4.0]\n    2  3  [5.0, 6.0]\n    &gt;&gt;&gt; table.delete(\"x = 2\")\n    &gt;&gt;&gt; table.to_pandas()\n       x      vector\n    0  1  [1.0, 2.0]\n    1  3  [5.0, 6.0]\n\n    If you have a list of values to delete, you can combine them into a\n    stringified list and use the `IN` operator:\n\n    &gt;&gt;&gt; to_remove = [1, 5]\n    &gt;&gt;&gt; to_remove = \", \".join([str(v) for v in to_remove])\n    &gt;&gt;&gt; to_remove\n    '1, 5'\n    &gt;&gt;&gt; table.delete(f\"x IN ({to_remove})\")\n    &gt;&gt;&gt; table.to_pandas()\n       x      vector\n    0  3  [5.0, 6.0]\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python/python/#lancedb.table.Table.update","title":"update  <code>abstractmethod</code>","text":"<pre><code>update(where: Optional[str] = None, values: Optional[dict] = None, *, values_sql: Optional[Dict[str, str]] = None)\n</code></pre> <p>This can be used to update zero to all rows depending on how many rows match the where clause. If no where clause is provided, then all rows will be updated.</p> <p>Either <code>values</code> or <code>values_sql</code> must be provided. You cannot provide both.</p> <p>Parameters:</p> <ul> <li> <code>where</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The SQL where clause to use when updating rows. For example, 'x = 2' or 'x IN (1, 2, 3)'. The filter must not be empty, or it will error.</p> </li> <li> <code>values</code>               (<code>Optional[dict]</code>, default:                   <code>None</code> )           \u2013            <p>The values to update. The keys are the column names and the values are the values to set.</p> </li> <li> <code>values_sql</code>               (<code>Optional[Dict[str, str]]</code>, default:                   <code>None</code> )           \u2013            <p>The values to update, expressed as SQL expression strings. These can reference existing columns. For example, {\"x\": \"x + 1\"} will increment the x column by 1.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import lancedb\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = pd.DataFrame({\"x\": [1, 2, 3], \"vector\": [[1.0, 2], [3, 4], [5, 6]]})\n&gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n&gt;&gt;&gt; table = db.create_table(\"my_table\", data)\n&gt;&gt;&gt; table.to_pandas()\n   x      vector\n0  1  [1.0, 2.0]\n1  2  [3.0, 4.0]\n2  3  [5.0, 6.0]\n&gt;&gt;&gt; table.update(where=\"x = 2\", values={\"vector\": [10.0, 10]})\n&gt;&gt;&gt; table.to_pandas()\n   x        vector\n0  1    [1.0, 2.0]\n1  3    [5.0, 6.0]\n2  2  [10.0, 10.0]\n&gt;&gt;&gt; table.update(values_sql={\"x\": \"x + 1\"})\n&gt;&gt;&gt; table.to_pandas()\n   x        vector\n0  2    [1.0, 2.0]\n1  4    [5.0, 6.0]\n2  3  [10.0, 10.0]\n</code></pre> Source code in <code>lancedb/table.py</code> <pre><code>@abstractmethod\ndef update(\n    self,\n    where: Optional[str] = None,\n    values: Optional[dict] = None,\n    *,\n    values_sql: Optional[Dict[str, str]] = None,\n):\n    \"\"\"\n    This can be used to update zero to all rows depending on how many\n    rows match the where clause. If no where clause is provided, then\n    all rows will be updated.\n\n    Either `values` or `values_sql` must be provided. You cannot provide\n    both.\n\n    Parameters\n    ----------\n    where: str, optional\n        The SQL where clause to use when updating rows. For example, 'x = 2'\n        or 'x IN (1, 2, 3)'. The filter must not be empty, or it will error.\n    values: dict, optional\n        The values to update. The keys are the column names and the values\n        are the values to set.\n    values_sql: dict, optional\n        The values to update, expressed as SQL expression strings. These can\n        reference existing columns. For example, {\"x\": \"x + 1\"} will increment\n        the x column by 1.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import lancedb\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; data = pd.DataFrame({\"x\": [1, 2, 3], \"vector\": [[1.0, 2], [3, 4], [5, 6]]})\n    &gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n    &gt;&gt;&gt; table = db.create_table(\"my_table\", data)\n    &gt;&gt;&gt; table.to_pandas()\n       x      vector\n    0  1  [1.0, 2.0]\n    1  2  [3.0, 4.0]\n    2  3  [5.0, 6.0]\n    &gt;&gt;&gt; table.update(where=\"x = 2\", values={\"vector\": [10.0, 10]})\n    &gt;&gt;&gt; table.to_pandas()\n       x        vector\n    0  1    [1.0, 2.0]\n    1  3    [5.0, 6.0]\n    2  2  [10.0, 10.0]\n    &gt;&gt;&gt; table.update(values_sql={\"x\": \"x + 1\"})\n    &gt;&gt;&gt; table.to_pandas()\n       x        vector\n    0  2    [1.0, 2.0]\n    1  4    [5.0, 6.0]\n    2  3  [10.0, 10.0]\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python/python/#lancedb.table.Table.cleanup_old_versions","title":"cleanup_old_versions  <code>abstractmethod</code>","text":"<pre><code>cleanup_old_versions(older_than: Optional[timedelta] = None, *, delete_unverified: bool = False) -&gt; 'CleanupStats'\n</code></pre> <p>Clean up old versions of the table, freeing disk space.</p> <p>Parameters:</p> <ul> <li> <code>older_than</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>The minimum age of the version to delete. If None, then this defaults to two weeks.</p> </li> <li> <code>delete_unverified</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Because they may be part of an in-progress transaction, files newer than 7 days old are not deleted by default. If you are sure that there are no in-progress transactions, then you can set this to True to delete all files older than <code>older_than</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>CleanupStats</code>           \u2013            <p>The stats of the cleanup operation, including how many bytes were freed.</p> </li> </ul> See Also <p>Table.optimize: A more comprehensive     optimization operation that includes cleanup as well as other operations.</p> Notes <p>This function is not available in LanceDb Cloud (since LanceDB Cloud manages cleanup for you automatically)</p> Source code in <code>lancedb/table.py</code> <pre><code>@abstractmethod\ndef cleanup_old_versions(\n    self,\n    older_than: Optional[timedelta] = None,\n    *,\n    delete_unverified: bool = False,\n) -&gt; \"CleanupStats\":\n    \"\"\"\n    Clean up old versions of the table, freeing disk space.\n\n    Parameters\n    ----------\n    older_than: timedelta, default None\n        The minimum age of the version to delete. If None, then this defaults\n        to two weeks.\n    delete_unverified: bool, default False\n        Because they may be part of an in-progress transaction, files newer\n        than 7 days old are not deleted by default. If you are sure that\n        there are no in-progress transactions, then you can set this to True\n        to delete all files older than `older_than`.\n\n    Returns\n    -------\n    CleanupStats\n        The stats of the cleanup operation, including how many bytes were\n        freed.\n\n    See Also\n    --------\n    [Table.optimize][lancedb.table.Table.optimize]: A more comprehensive\n        optimization operation that includes cleanup as well as other operations.\n\n    Notes\n    -----\n    This function is not available in LanceDb Cloud (since LanceDB\n    Cloud manages cleanup for you automatically)\n    \"\"\"\n</code></pre>"},{"location":"python/python/#lancedb.table.Table.compact_files","title":"compact_files  <code>abstractmethod</code>","text":"<pre><code>compact_files(*args, **kwargs)\n</code></pre> <p>Run the compaction process on the table. This can be run after making several small appends to optimize the table for faster reads.</p> <p>Arguments are passed onto Lance's compact_files. For most cases, the default should be fine.</p> See Also <p>Table.optimize: A more comprehensive     optimization operation that includes cleanup as well as other operations.</p> Notes <p>This function is not available in LanceDB Cloud (since LanceDB Cloud manages compaction for you automatically)</p> Source code in <code>lancedb/table.py</code> <pre><code>@abstractmethod\ndef compact_files(self, *args, **kwargs):\n    \"\"\"\n    Run the compaction process on the table.\n    This can be run after making several small appends to optimize the table\n    for faster reads.\n\n    Arguments are passed onto Lance's\n    [compact_files][lance.dataset.DatasetOptimizer.compact_files].\n    For most cases, the default should be fine.\n\n    See Also\n    --------\n    [Table.optimize][lancedb.table.Table.optimize]: A more comprehensive\n        optimization operation that includes cleanup as well as other operations.\n\n    Notes\n    -----\n    This function is not available in LanceDB Cloud (since LanceDB\n    Cloud manages compaction for you automatically)\n    \"\"\"\n</code></pre>"},{"location":"python/python/#lancedb.table.Table.optimize","title":"optimize  <code>abstractmethod</code>","text":"<pre><code>optimize(*, cleanup_older_than: Optional[timedelta] = None, delete_unverified: bool = False, retrain: bool = False)\n</code></pre> <p>Optimize the on-disk data and indices for better performance.</p> <p>Modeled after <code>VACUUM</code> in PostgreSQL.</p> <p>Optimization covers three operations:</p> <ul> <li>Compaction: Merges small files into larger ones</li> <li>Prune: Removes old versions of the dataset</li> <li>Index: Optimizes the indices, adding new data to existing indices</li> </ul> <p>Parameters:</p> <ul> <li> <code>cleanup_older_than</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>All files belonging to versions older than this will be removed.  Set to 0 days to remove all versions except the latest.  The latest version is never removed.</p> </li> <li> <code>delete_unverified</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Files leftover from a failed transaction may appear to be part of an in-progress operation (e.g. appending new data) and these files will not be deleted unless they are at least 7 days old. If delete_unverified is True then these files will be deleted regardless of their age.</p> </li> <li> <code>retrain</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, retrain the vector indices, this would refine the IVF clustering and quantization, which may improve the search accuracy. It's faster than re-creating the index from scratch, so it's recommended to try this first, when the data distribution has changed significantly.</p> </li> </ul> Experimental API <p>The optimization process is undergoing active development and may change. Our goal with these changes is to improve the performance of optimization and reduce the complexity.</p> <p>That being said, it is essential today to run optimize if you want the best performance.  It should be stable and safe to use in production, but it our hope that the API may be simplified (or not even need to be called) in the future.</p> <p>The frequency an application shoudl call optimize is based on the frequency of data modifications.  If data is frequently added, deleted, or updated then optimize should be run frequently.  A good rule of thumb is to run optimize if you have added or modified 100,000 or more records or run more than 20 data modification operations.</p> Source code in <code>lancedb/table.py</code> <pre><code>@abstractmethod\ndef optimize(\n    self,\n    *,\n    cleanup_older_than: Optional[timedelta] = None,\n    delete_unverified: bool = False,\n    retrain: bool = False,\n):\n    \"\"\"\n    Optimize the on-disk data and indices for better performance.\n\n    Modeled after ``VACUUM`` in PostgreSQL.\n\n    Optimization covers three operations:\n\n     * Compaction: Merges small files into larger ones\n     * Prune: Removes old versions of the dataset\n     * Index: Optimizes the indices, adding new data to existing indices\n\n    Parameters\n    ----------\n    cleanup_older_than: timedelta, optional default 7 days\n        All files belonging to versions older than this will be removed.  Set\n        to 0 days to remove all versions except the latest.  The latest version\n        is never removed.\n    delete_unverified: bool, default False\n        Files leftover from a failed transaction may appear to be part of an\n        in-progress operation (e.g. appending new data) and these files will not\n        be deleted unless they are at least 7 days old. If delete_unverified is True\n        then these files will be deleted regardless of their age.\n    retrain: bool, default False\n        If True, retrain the vector indices, this would refine the IVF clustering\n        and quantization, which may improve the search accuracy. It's faster than\n        re-creating the index from scratch, so it's recommended to try this first,\n        when the data distribution has changed significantly.\n\n    Experimental API\n    ----------------\n\n    The optimization process is undergoing active development and may change.\n    Our goal with these changes is to improve the performance of optimization and\n    reduce the complexity.\n\n    That being said, it is essential today to run optimize if you want the best\n    performance.  It should be stable and safe to use in production, but it our\n    hope that the API may be simplified (or not even need to be called) in the\n    future.\n\n    The frequency an application shoudl call optimize is based on the frequency of\n    data modifications.  If data is frequently added, deleted, or updated then\n    optimize should be run frequently.  A good rule of thumb is to run optimize if\n    you have added or modified 100,000 or more records or run more than 20 data\n    modification operations.\n    \"\"\"\n</code></pre>"},{"location":"python/python/#lancedb.table.Table.list_indices","title":"list_indices  <code>abstractmethod</code>","text":"<pre><code>list_indices() -&gt; Iterable[IndexConfig]\n</code></pre> <p>List all indices that have been created with Table.create_index</p> Source code in <code>lancedb/table.py</code> <pre><code>@abstractmethod\ndef list_indices(self) -&gt; Iterable[IndexConfig]:\n    \"\"\"\n    List all indices that have been created with\n    [Table.create_index][lancedb.table.Table.create_index]\n    \"\"\"\n</code></pre>"},{"location":"python/python/#lancedb.table.Table.index_stats","title":"index_stats  <code>abstractmethod</code>","text":"<pre><code>index_stats(index_name: str) -&gt; Optional[IndexStatistics]\n</code></pre> <p>Retrieve statistics about an index</p> <p>Parameters:</p> <ul> <li> <code>index_name</code>               (<code>str</code>)           \u2013            <p>The name of the index to retrieve statistics for</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>IndexStatistics or None</code>           \u2013            <p>The statistics about the index. Returns None if the index does not exist.</p> </li> </ul> Source code in <code>lancedb/table.py</code> <pre><code>@abstractmethod\ndef index_stats(self, index_name: str) -&gt; Optional[IndexStatistics]:\n    \"\"\"\n    Retrieve statistics about an index\n\n    Parameters\n    ----------\n    index_name: str\n        The name of the index to retrieve statistics for\n\n    Returns\n    -------\n    IndexStatistics or None\n        The statistics about the index. Returns None if the index does not exist.\n    \"\"\"\n</code></pre>"},{"location":"python/python/#lancedb.table.Table.add_columns","title":"add_columns  <code>abstractmethod</code>","text":"<pre><code>add_columns(transforms: Dict[str, str] | Field | List[Field] | Schema)\n</code></pre> <p>Add new columns with defined values.</p> <p>Parameters:</p> <ul> <li> <code>transforms</code>               (<code>Dict[str, str] | Field | List[Field] | Schema</code>)           \u2013            <p>A map of column name to a SQL expression to use to calculate the value of the new column. These expressions will be evaluated for each row in the table, and can reference existing columns. Alternatively, a pyarrow Field or Schema can be provided to add new columns with the specified data types. The new columns will be initialized with null values.</p> </li> </ul> Source code in <code>lancedb/table.py</code> <pre><code>@abstractmethod\ndef add_columns(\n    self, transforms: Dict[str, str] | pa.Field | List[pa.Field] | pa.Schema\n):\n    \"\"\"\n    Add new columns with defined values.\n\n    Parameters\n    ----------\n    transforms: Dict[str, str], pa.Field, List[pa.Field], pa.Schema\n        A map of column name to a SQL expression to use to calculate the\n        value of the new column. These expressions will be evaluated for\n        each row in the table, and can reference existing columns.\n        Alternatively, a pyarrow Field or Schema can be provided to add\n        new columns with the specified data types. The new columns will\n        be initialized with null values.\n    \"\"\"\n</code></pre>"},{"location":"python/python/#lancedb.table.Table.alter_columns","title":"alter_columns  <code>abstractmethod</code>","text":"<pre><code>alter_columns(*alterations: Iterable[Dict[str, str]])\n</code></pre> <p>Alter column names and nullability.</p> <p>Parameters:</p> <ul> <li> <code>alterations</code>               (<code>Iterable[Dict[str, Any]]</code>, default:                   <code>()</code> )           \u2013            <p>A sequence of dictionaries, each with the following keys: - \"path\": str     The column path to alter. For a top-level column, this is the name.     For a nested column, this is the dot-separated path, e.g. \"a.b.c\". - \"rename\": str, optional     The new name of the column. If not specified, the column name is     not changed. - \"data_type\": pyarrow.DataType, optional    The new data type of the column. Existing values will be casted    to this type. If not specified, the column data type is not changed. - \"nullable\": bool, optional     Whether the column should be nullable. If not specified, the column     nullability is not changed. Only non-nullable columns can be changed     to nullable. Currently, you cannot change a nullable column to     non-nullable.</p> </li> </ul> Source code in <code>lancedb/table.py</code> <pre><code>@abstractmethod\ndef alter_columns(self, *alterations: Iterable[Dict[str, str]]):\n    \"\"\"\n    Alter column names and nullability.\n\n    Parameters\n    ----------\n    alterations : Iterable[Dict[str, Any]]\n        A sequence of dictionaries, each with the following keys:\n        - \"path\": str\n            The column path to alter. For a top-level column, this is the name.\n            For a nested column, this is the dot-separated path, e.g. \"a.b.c\".\n        - \"rename\": str, optional\n            The new name of the column. If not specified, the column name is\n            not changed.\n        - \"data_type\": pyarrow.DataType, optional\n           The new data type of the column. Existing values will be casted\n           to this type. If not specified, the column data type is not changed.\n        - \"nullable\": bool, optional\n            Whether the column should be nullable. If not specified, the column\n            nullability is not changed. Only non-nullable columns can be changed\n            to nullable. Currently, you cannot change a nullable column to\n            non-nullable.\n    \"\"\"\n</code></pre>"},{"location":"python/python/#lancedb.table.Table.drop_columns","title":"drop_columns  <code>abstractmethod</code>","text":"<pre><code>drop_columns(columns: Iterable[str])\n</code></pre> <p>Drop columns from the table.</p> <p>Parameters:</p> <ul> <li> <code>columns</code>               (<code>Iterable[str]</code>)           \u2013            <p>The names of the columns to drop.</p> </li> </ul> Source code in <code>lancedb/table.py</code> <pre><code>@abstractmethod\ndef drop_columns(self, columns: Iterable[str]):\n    \"\"\"\n    Drop columns from the table.\n\n    Parameters\n    ----------\n    columns : Iterable[str]\n        The names of the columns to drop.\n    \"\"\"\n</code></pre>"},{"location":"python/python/#lancedb.table.Table.checkout","title":"checkout  <code>abstractmethod</code>","text":"<pre><code>checkout(version: int)\n</code></pre> <p>Checks out a specific version of the Table</p> <p>Any read operation on the table will now access the data at the checked out version. As a consequence, calling this method will disable any read consistency interval that was previously set.</p> <p>This is a read-only operation that turns the table into a sort of \"view\" or \"detached head\".  Other table instances will not be affected.  To make the change permanent you can use the <code>[Self::restore]</code> method.</p> <p>Any operation that modifies the table will fail while the table is in a checked out state.</p> <p>To return the table to a normal state use <code>[Self::checkout_latest]</code></p> Source code in <code>lancedb/table.py</code> <pre><code>@abstractmethod\ndef checkout(self, version: int):\n    \"\"\"\n    Checks out a specific version of the Table\n\n    Any read operation on the table will now access the data at the checked out\n    version. As a consequence, calling this method will disable any read consistency\n    interval that was previously set.\n\n    This is a read-only operation that turns the table into a sort of \"view\"\n    or \"detached head\".  Other table instances will not be affected.  To make the\n    change permanent you can use the `[Self::restore]` method.\n\n    Any operation that modifies the table will fail while the table is in a checked\n    out state.\n\n    To return the table to a normal state use `[Self::checkout_latest]`\n    \"\"\"\n</code></pre>"},{"location":"python/python/#lancedb.table.Table.checkout_latest","title":"checkout_latest  <code>abstractmethod</code>","text":"<pre><code>checkout_latest()\n</code></pre> <p>Ensures the table is pointing at the latest version</p> <p>This can be used to manually update a table when the read_consistency_interval is None It can also be used to undo a <code>[Self::checkout]</code> operation</p> Source code in <code>lancedb/table.py</code> <pre><code>@abstractmethod\ndef checkout_latest(self):\n    \"\"\"\n    Ensures the table is pointing at the latest version\n\n    This can be used to manually update a table when the read_consistency_interval\n    is None\n    It can also be used to undo a `[Self::checkout]` operation\n    \"\"\"\n</code></pre>"},{"location":"python/python/#lancedb.table.Table.restore","title":"restore  <code>abstractmethod</code>","text":"<pre><code>restore(version: Optional[int] = None)\n</code></pre> <p>Restore a version of the table. This is an in-place operation.</p> <p>This creates a new version where the data is equivalent to the specified previous version. Data is not copied (as of python-v0.2.1).</p> <p>Parameters:</p> <ul> <li> <code>version</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>The version to restore. If unspecified then restores the currently checked out version. If the currently checked out version is the latest version then this is a no-op.</p> </li> </ul> Source code in <code>lancedb/table.py</code> <pre><code>@abstractmethod\ndef restore(self, version: Optional[int] = None):\n    \"\"\"Restore a version of the table. This is an in-place operation.\n\n    This creates a new version where the data is equivalent to the\n    specified previous version. Data is not copied (as of python-v0.2.1).\n\n    Parameters\n    ----------\n    version : int, default None\n        The version to restore. If unspecified then restores the currently\n        checked out version. If the currently checked out version is the\n        latest version then this is a no-op.\n    \"\"\"\n</code></pre>"},{"location":"python/python/#lancedb.table.Table.list_versions","title":"list_versions  <code>abstractmethod</code>","text":"<pre><code>list_versions() -&gt; List[Dict[str, Any]]\n</code></pre> <p>List all versions of the table</p> Source code in <code>lancedb/table.py</code> <pre><code>@abstractmethod\ndef list_versions(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"List all versions of the table\"\"\"\n</code></pre>"},{"location":"python/python/#lancedb.table.Table.uses_v2_manifest_paths","title":"uses_v2_manifest_paths  <code>abstractmethod</code>","text":"<pre><code>uses_v2_manifest_paths() -&gt; bool\n</code></pre> <p>Check if the table is using the new v2 manifest paths.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if the table is using the new v2 manifest paths, False otherwise.</p> </li> </ul> Source code in <code>lancedb/table.py</code> <pre><code>@abstractmethod\ndef uses_v2_manifest_paths(self) -&gt; bool:\n    \"\"\"\n    Check if the table is using the new v2 manifest paths.\n\n    Returns\n    -------\n    bool\n        True if the table is using the new v2 manifest paths, False otherwise.\n    \"\"\"\n</code></pre>"},{"location":"python/python/#lancedb.table.Table.migrate_v2_manifest_paths","title":"migrate_v2_manifest_paths  <code>abstractmethod</code>","text":"<pre><code>migrate_v2_manifest_paths()\n</code></pre> <p>Migrate the manifest paths to the new format.</p> <p>This will update the manifest to use the new v2 format for paths.</p> <p>This function is idempotent, and can be run multiple times without changing the state of the object store.</p> <p>Danger</p> <p>This should not be run while other concurrent operations are happening. And it should also run until completion before resuming other operations.</p> <p>You can use Table.uses_v2_manifest_paths to check if the table is already using the new path style.</p> Source code in <code>lancedb/table.py</code> <pre><code>@abstractmethod\ndef migrate_v2_manifest_paths(self):\n    \"\"\"\n    Migrate the manifest paths to the new format.\n\n    This will update the manifest to use the new v2 format for paths.\n\n    This function is idempotent, and can be run multiple times without\n    changing the state of the object store.\n\n    !!! danger\n\n        This should not be run while other concurrent operations are happening.\n        And it should also run until completion before resuming other operations.\n\n    You can use\n    [Table.uses_v2_manifest_paths][lancedb.table.Table.uses_v2_manifest_paths]\n    to check if the table is already using the new path style.\n    \"\"\"\n</code></pre>"},{"location":"python/python/#querying-synchronous","title":"Querying (Synchronous)","text":""},{"location":"python/python/#lancedb.query.Query","title":"lancedb.query.Query","text":"<p>               Bases: <code>BaseModel</code></p> <p>A LanceDB Query</p> <p>Queries are constructed by the <code>Table.search</code> method.  This class is a python representation of the query.  Normally you will not need to interact with this class directly.  You can build up a query and execute it using collection methods such as <code>to_batches()</code>, <code>to_arrow()</code>, <code>to_pandas()</code>, etc.</p> <p>However, you can use the <code>to_query()</code> method to get the underlying query object. This can be useful for serializing a query or using it in a different context.</p> <p>Attributes:</p> <ul> <li> <code>filter</code>               (<code>Optional[str]</code>)           \u2013            <p>sql filter to refine the query with</p> </li> <li> <code>limit</code>               (<code>Optional[int]</code>)           \u2013            <p>The limit on the number of results to return.  If this is a vector or FTS query, then this is required.  If this is a plain SQL query, then this is optional.</p> </li> <li> <code>offset</code>               (<code>Optional[int]</code>)           \u2013            <p>The offset to start fetching results from</p> <p>This is ignored for vector / FTS search (will be None).</p> </li> <li> <code>columns</code>               (<code>Optional[Union[List[str], Dict[str, str]]]</code>)           \u2013            <p>which columns to return in the results</p> <p>This can be a list of column names or a dictionary.  If it is a dictionary, then the keys are the column names and the values are sql expressions to use to calculate the result.</p> <p>If this is None then all columns are returned.  This can be expensive.</p> </li> <li> <code>with_row_id</code>               (<code>Optional[bool]</code>)           \u2013            <p>if True then include the row id in the results</p> </li> <li> <code>vector</code>               (<code>Optional[Union[List[float], List[List[float]], Array, List[Array]]]</code>)           \u2013            <p>the vector to search for, if this a vector search or hybrid search.  It will be None for full text search and plain SQL filtering.</p> </li> <li> <code>vector_column</code>               (<code>Optional[str]</code>)           \u2013            <p>the name of the vector column to use for vector search</p> <p>If this is None then a default vector column will be used.</p> </li> <li> <code>distance_type</code>               (<code>Optional[str]</code>)           \u2013            <p>the distance type to use for vector search</p> <p>This can be l2 (default), cosine and dot.  See metric definitions for more details.</p> <p>If this is not a vector search this will be None.</p> </li> <li> <code>postfilter</code>               (<code>bool</code>)           \u2013            <p>if True then apply the filter after vector / FTS search.  This is ignored for plain SQL filtering.</p> </li> <li> <code>nprobes</code>               (<code>Optional[int]</code>)           \u2013            <p>The number of IVF partitions to search.  If this is None then a default number of partitions will be used.</p> <ul> <li> <p>A higher number makes search more accurate but also slower.</p> </li> <li> <p>See discussion in [Querying an ANN Index][querying-an-ann-index] for   tuning advice.</p> </li> </ul> <p>Will be None if this is not a vector search.</p> </li> <li> <code>refine_factor</code>               (<code>Optional[int]</code>)           \u2013            <p>Refine the results by reading extra elements and re-ranking them in memory.</p> <ul> <li> <p>A higher number makes search more accurate but also slower.</p> </li> <li> <p>See discussion in [Querying an ANN Index][querying-an-ann-index] for   tuning advice.</p> </li> </ul> <p>Will be None if this is not a vector search.</p> </li> <li> <code>lower_bound</code>               (<code>Optional[float]</code>)           \u2013            <p>The lower bound for distance search</p> <p>Only results with a distance greater than or equal to this value will be returned.</p> <p>This will only be set on vector search.</p> </li> <li> <code>upper_bound</code>               (<code>Optional[float]</code>)           \u2013            <p>The upper bound for distance search</p> <p>Only results with a distance less than or equal to this value will be returned.</p> <p>This will only be set on vector search.</p> </li> <li> <code>ef</code>               (<code>Optional[int]</code>)           \u2013            <p>The size of the nearest neighbor list maintained during HNSW search</p> <p>This will only be set on vector search.</p> </li> <li> <code>full_text_query</code>               (<code>Optional[Union[str, dict]]</code>)           \u2013            <p>The full text search query</p> <p>This can be a string or a dictionary.  A dictionary will be used to search multiple columns.  The keys are the column names and the values are the search queries.</p> <p>This will only be set on FTS or hybrid queries.</p> </li> <li> <code>fast_search</code>               (<code>Optional[bool]</code>)           \u2013            <p>Skip a flat search of unindexed data. This will improve search performance but search results will not include unindexed data.</p> <p>The default is False</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>class Query(pydantic.BaseModel):\n    \"\"\"A LanceDB Query\n\n    Queries are constructed by the `Table.search` method.  This class is a\n    python representation of the query.  Normally you will not need to interact\n    with this class directly.  You can build up a query and execute it using\n    collection methods such as `to_batches()`, `to_arrow()`, `to_pandas()`,\n    etc.\n\n    However, you can use the `to_query()` method to get the underlying query object.\n    This can be useful for serializing a query or using it in a different context.\n\n    Attributes\n    ----------\n    filter : Optional[str]\n        sql filter to refine the query with\n    limit : Optional[int]\n        The limit on the number of results to return.  If this is a vector or FTS query,\n        then this is required.  If this is a plain SQL query, then this is optional.\n    offset: Optional[int]\n        The offset to start fetching results from\n\n        This is ignored for vector / FTS search (will be None).\n    columns : Optional[Union[List[str], Dict[str, str]]]\n        which columns to return in the results\n\n        This can be a list of column names or a dictionary.  If it is a dictionary,\n        then the keys are the column names and the values are sql expressions to\n        use to calculate the result.\n\n        If this is None then all columns are returned.  This can be expensive.\n    with_row_id : Optional[bool]\n        if True then include the row id in the results\n    vector : Optional[Union[List[float], List[List[float]], pa.Array, List[pa.Array]]]\n        the vector to search for, if this a vector search or hybrid search.  It will\n        be None for full text search and plain SQL filtering.\n    vector_column : Optional[str]\n        the name of the vector column to use for vector search\n\n        If this is None then a default vector column will be used.\n    distance_type : Optional[str]\n        the distance type to use for vector search\n\n        This can be l2 (default), cosine and dot.  See [metric definitions][search] for\n        more details.\n\n        If this is not a vector search this will be None.\n    postfilter : bool\n        if True then apply the filter after vector / FTS search.  This is ignored for\n        plain SQL filtering.\n    nprobes : Optional[int]\n        The number of IVF partitions to search.  If this is None then a default\n        number of partitions will be used.\n\n        - A higher number makes search more accurate but also slower.\n\n        - See discussion in [Querying an ANN Index][querying-an-ann-index] for\n          tuning advice.\n\n        Will be None if this is not a vector search.\n    refine_factor : Optional[int]\n        Refine the results by reading extra elements and re-ranking them in memory.\n\n        - A higher number makes search more accurate but also slower.\n\n        - See discussion in [Querying an ANN Index][querying-an-ann-index] for\n          tuning advice.\n\n        Will be None if this is not a vector search.\n    lower_bound : Optional[float]\n        The lower bound for distance search\n\n        Only results with a distance greater than or equal to this value\n        will be returned.\n\n        This will only be set on vector search.\n    upper_bound : Optional[float]\n        The upper bound for distance search\n\n        Only results with a distance less than or equal to this value\n        will be returned.\n\n        This will only be set on vector search.\n    ef : Optional[int]\n        The size of the nearest neighbor list maintained during HNSW search\n\n        This will only be set on vector search.\n    full_text_query : Optional[Union[str, dict]]\n        The full text search query\n\n        This can be a string or a dictionary.  A dictionary will be used to search\n        multiple columns.  The keys are the column names and the values are the\n        search queries.\n\n        This will only be set on FTS or hybrid queries.\n    fast_search: Optional[bool]\n        Skip a flat search of unindexed data. This will improve\n        search performance but search results will not include unindexed data.\n\n        The default is False\n    \"\"\"\n\n    # The name of the vector column to use for vector search.\n    vector_column: Optional[str] = None\n\n    # vector to search for\n    #\n    # Note: today this will be floats on the sync path and pa.Array on the async\n    # path though in the future we should unify this to pa.Array everywhere\n    vector: Annotated[\n        Optional[Union[List[float], List[List[float]], pa.Array, List[pa.Array]]],\n        ensure_vector_query,\n    ] = None\n\n    # sql filter to refine the query with\n    filter: Optional[str] = None\n\n    # if True then apply the filter after vector search\n    postfilter: Optional[bool] = None\n\n    # full text search query\n    full_text_query: Optional[FullTextSearchQuery] = None\n\n    # top k results to return\n    limit: Optional[int] = None\n\n    # distance type to use for vector search\n    distance_type: Optional[str] = None\n\n    # which columns to return in the results\n    columns: Optional[Union[List[str], Dict[str, str]]] = None\n\n    # number of IVF partitions to search\n    nprobes: Optional[int] = None\n\n    # lower bound for distance search\n    lower_bound: Optional[float] = None\n\n    # upper bound for distance search\n    upper_bound: Optional[float] = None\n\n    # multiplier for the number of results to inspect for reranking\n    refine_factor: Optional[int] = None\n\n    # if true, include the row id in the results\n    with_row_id: Optional[bool] = None\n\n    # offset to start fetching results from\n    offset: Optional[int] = None\n\n    # if true, will only search the indexed data\n    fast_search: Optional[bool] = None\n\n    # size of the nearest neighbor list maintained during HNSW search\n    ef: Optional[int] = None\n\n    # Bypass the vector index and use a brute force search\n    bypass_vector_index: Optional[bool] = None\n\n    @classmethod\n    def from_inner(cls, req: PyQueryRequest) -&gt; Self:\n        query = cls()\n        query.limit = req.limit\n        query.offset = req.offset\n        query.filter = req.filter\n        query.full_text_query = req.full_text_search\n        query.columns = req.select\n        query.with_row_id = req.with_row_id\n        query.vector_column = req.column\n        query.vector = req.query_vector\n        query.distance_type = req.distance_type\n        query.nprobes = req.nprobes\n        query.lower_bound = req.lower_bound\n        query.upper_bound = req.upper_bound\n        query.ef = req.ef\n        query.refine_factor = req.refine_factor\n        query.bypass_vector_index = req.bypass_vector_index\n        query.postfilter = req.postfilter\n        if req.full_text_search is not None:\n            query.full_text_query = FullTextSearchQuery(\n                columns=req.full_text_search.columns,\n                query=req.full_text_search.query,\n                limit=req.full_text_search.limit,\n                wand_factor=req.full_text_search.wand_factor,\n            )\n        return query\n\n    # This tells pydantic to allow custom types (needed for the `vector` query since\n    # pa.Array wouln't be allowed otherwise)\n    if PYDANTIC_VERSION.major &lt; 2:  # Pydantic 1.x compat\n\n        class Config:\n            arbitrary_types_allowed = True\n    else:\n        model_config = {\"arbitrary_types_allowed\": True}\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceQueryBuilder","title":"lancedb.query.LanceQueryBuilder","text":"<p>               Bases: <code>ABC</code></p> <p>An abstract query builder. Subclasses are defined for vector search, full text search, hybrid, and plain SQL filtering.</p> Source code in <code>lancedb/query.py</code> <pre><code>class LanceQueryBuilder(ABC):\n    \"\"\"An abstract query builder. Subclasses are defined for vector search,\n    full text search, hybrid, and plain SQL filtering.\n    \"\"\"\n\n    @classmethod\n    def create(\n        cls,\n        table: \"Table\",\n        query: Optional[Union[np.ndarray, str, \"PIL.Image.Image\", Tuple]],\n        query_type: str,\n        vector_column_name: str,\n        ordering_field_name: Optional[str] = None,\n        fts_columns: Optional[Union[str, List[str]]] = None,\n        fast_search: bool = None,\n    ) -&gt; Self:\n        \"\"\"\n        Create a query builder based on the given query and query type.\n\n        Parameters\n        ----------\n        table: Table\n            The table to query.\n        query: Optional[Union[np.ndarray, str, \"PIL.Image.Image\", Tuple]]\n            The query to use. If None, an empty query builder is returned\n            which performs simple SQL filtering.\n        query_type: str\n            The type of query to perform. One of \"vector\", \"fts\", \"hybrid\", or \"auto\".\n            If \"auto\", the query type is inferred based on the query.\n        vector_column_name: str\n            The name of the vector column to use for vector search.\n        fast_search: bool\n            Skip flat search of unindexed data.\n        \"\"\"\n        # Check hybrid search first as it supports empty query pattern\n        if query_type == \"hybrid\":\n            # hybrid fts and vector query\n            return LanceHybridQueryBuilder(\n                table, query, vector_column_name, fts_columns=fts_columns\n            )\n\n        if query is None:\n            return LanceEmptyQueryBuilder(table)\n\n        # remember the string query for reranking purpose\n        str_query = query if isinstance(query, str) else None\n\n        # convert \"auto\" query_type to \"vector\", \"fts\"\n        # or \"hybrid\" and convert the query to vector if needed\n        query, query_type = cls._resolve_query(\n            table, query, query_type, vector_column_name\n        )\n\n        if query_type == \"hybrid\":\n            return LanceHybridQueryBuilder(\n                table, query, vector_column_name, fts_columns=fts_columns\n            )\n\n        if isinstance(query, (str, FullTextQuery)):\n            # fts\n            return LanceFtsQueryBuilder(\n                table,\n                query,\n                ordering_field_name=ordering_field_name,\n                fts_columns=fts_columns,\n            )\n\n        if isinstance(query, list):\n            query = np.array(query, dtype=np.float32)\n        elif isinstance(query, np.ndarray):\n            query = query.astype(np.float32)\n        else:\n            raise TypeError(f\"Unsupported query type: {type(query)}\")\n\n        return LanceVectorQueryBuilder(\n            table, query, vector_column_name, str_query, fast_search\n        )\n\n    @classmethod\n    def _resolve_query(cls, table, query, query_type, vector_column_name):\n        # If query_type is fts, then query must be a string.\n        # otherwise raise TypeError\n        if query_type == \"fts\":\n            if not isinstance(query, (str, FullTextQuery)):\n                raise TypeError(\n                    f\"'fts' query must be a string or FullTextQuery: {type(query)}\"\n                )\n            return query, query_type\n        elif query_type == \"vector\":\n            query = cls._query_to_vector(table, query, vector_column_name)\n            return query, query_type\n        elif query_type == \"auto\":\n            if isinstance(query, (list, np.ndarray)):\n                return query, \"vector\"\n            else:\n                conf = table.embedding_functions.get(vector_column_name)\n                if conf is not None:\n                    query = conf.function.compute_query_embeddings_with_retry(query)[0]\n                    return query, \"vector\"\n                else:\n                    return query, \"fts\"\n        else:\n            raise ValueError(\n                f\"Invalid query_type, must be 'vector', 'fts', or 'auto': {query_type}\"\n            )\n\n    @classmethod\n    def _query_to_vector(cls, table, query, vector_column_name):\n        if isinstance(query, (list, np.ndarray)):\n            return query\n        conf = table.embedding_functions.get(vector_column_name)\n        if conf is not None:\n            return conf.function.compute_query_embeddings_with_retry(query)[0]\n        else:\n            msg = f\"No embedding function for {vector_column_name}\"\n            raise ValueError(msg)\n\n    def __init__(self, table: \"Table\"):\n        self._table = table\n        self._limit = None\n        self._offset = None\n        self._columns = None\n        self._where = None\n        self._postfilter = None\n        self._with_row_id = None\n        self._vector = None\n        self._text = None\n        self._ef = None\n        self._bypass_vector_index = None\n\n    @deprecation.deprecated(\n        deprecated_in=\"0.3.1\",\n        removed_in=\"0.4.0\",\n        current_version=__version__,\n        details=\"Use to_pandas() instead\",\n    )\n    def to_df(self) -&gt; \"pd.DataFrame\":\n        \"\"\"\n        *Deprecated alias for `to_pandas()`. Please use `to_pandas()` instead.*\n\n        Execute the query and return the results as a pandas DataFrame.\n        In addition to the selected columns, LanceDB also returns a vector\n        and also the \"_distance\" column which is the distance between the query\n        vector and the returned vector.\n        \"\"\"\n        return self.to_pandas()\n\n    def to_pandas(\n        self,\n        flatten: Optional[Union[int, bool]] = None,\n        *,\n        timeout: Optional[timedelta] = None,\n    ) -&gt; \"pd.DataFrame\":\n        \"\"\"\n        Execute the query and return the results as a pandas DataFrame.\n        In addition to the selected columns, LanceDB also returns a vector\n        and also the \"_distance\" column which is the distance between the query\n        vector and the returned vector.\n\n        Parameters\n        ----------\n        flatten: Optional[Union[int, bool]]\n            If flatten is True, flatten all nested columns.\n            If flatten is an integer, flatten the nested columns up to the\n            specified depth.\n            If unspecified, do not flatten the nested columns.\n        timeout: Optional[timedelta]\n            The maximum time to wait for the query to complete.\n            If None, wait indefinitely.\n        \"\"\"\n        tbl = flatten_columns(self.to_arrow(timeout=timeout), flatten)\n        return tbl.to_pandas()\n\n    @abstractmethod\n    def to_arrow(self, *, timeout: Optional[timedelta] = None) -&gt; pa.Table:\n        \"\"\"\n        Execute the query and return the results as an\n        [Apache Arrow Table](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html#pyarrow.Table).\n\n        In addition to the selected columns, LanceDB also returns a vector\n        and also the \"_distance\" column which is the distance between the query\n        vector and the returned vectors.\n\n        Parameters\n        ----------\n        timeout: Optional[timedelta]\n            The maximum time to wait for the query to complete.\n            If None, wait indefinitely.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def to_batches(\n        self,\n        /,\n        batch_size: Optional[int] = None,\n        *,\n        timeout: Optional[timedelta] = None,\n    ) -&gt; pa.RecordBatchReader:\n        \"\"\"\n        Execute the query and return the results as a pyarrow\n        [RecordBatchReader](https://arrow.apache.org/docs/python/generated/pyarrow.RecordBatchReader.html)\n\n        Parameters\n        ----------\n        batch_size: int\n            The maximum number of selected records in a RecordBatch object.\n        timeout: Optional[timedelta]\n            The maximum time to wait for the query to complete.\n            If None, wait indefinitely.\n        \"\"\"\n        raise NotImplementedError\n\n    def to_list(self, *, timeout: Optional[timedelta] = None) -&gt; List[dict]:\n        \"\"\"\n        Execute the query and return the results as a list of dictionaries.\n\n        Each list entry is a dictionary with the selected column names as keys,\n        or all table columns if `select` is not called. The vector and the \"_distance\"\n        fields are returned whether or not they're explicitly selected.\n\n        Parameters\n        ----------\n        timeout: Optional[timedelta]\n            The maximum time to wait for the query to complete.\n            If None, wait indefinitely.\n        \"\"\"\n        return self.to_arrow(timeout=timeout).to_pylist()\n\n    def to_pydantic(\n        self, model: Type[LanceModel], *, timeout: Optional[timedelta] = None\n    ) -&gt; List[LanceModel]:\n        \"\"\"Return the table as a list of pydantic models.\n\n        Parameters\n        ----------\n        model: Type[LanceModel]\n            The pydantic model to use.\n        timeout: Optional[timedelta]\n            The maximum time to wait for the query to complete.\n            If None, wait indefinitely.\n\n        Returns\n        -------\n        List[LanceModel]\n        \"\"\"\n        return [\n            model(**{k: v for k, v in row.items() if k in model.field_names()})\n            for row in self.to_arrow(timeout=timeout).to_pylist()\n        ]\n\n    def to_polars(self, *, timeout: Optional[timedelta] = None) -&gt; \"pl.DataFrame\":\n        \"\"\"\n        Execute the query and return the results as a Polars DataFrame.\n        In addition to the selected columns, LanceDB also returns a vector\n        and also the \"_distance\" column which is the distance between the query\n        vector and the returned vector.\n\n        Parameters\n        ----------\n        timeout: Optional[timedelta]\n            The maximum time to wait for the query to complete.\n            If None, wait indefinitely.\n        \"\"\"\n        import polars as pl\n\n        return pl.from_arrow(self.to_arrow(timeout=timeout))\n\n    def limit(self, limit: Union[int, None]) -&gt; Self:\n        \"\"\"Set the maximum number of results to return.\n\n        Parameters\n        ----------\n        limit: int\n            The maximum number of results to return.\n            The default query limit is 10 results.\n            For ANN/KNN queries, you must specify a limit.\n            For plain searches, all records are returned if limit not set.\n            *WARNING* if you have a large dataset, setting\n            the limit to a large number, e.g. the table size,\n            can potentially result in reading a\n            large amount of data into memory and cause\n            out of memory issues.\n\n        Returns\n        -------\n        LanceQueryBuilder\n            The LanceQueryBuilder object.\n        \"\"\"\n        if limit is None or limit &lt;= 0:\n            if isinstance(self, LanceVectorQueryBuilder):\n                raise ValueError(\"Limit is required for ANN/KNN queries\")\n            else:\n                self._limit = None\n        else:\n            self._limit = limit\n        return self\n\n    def offset(self, offset: int) -&gt; Self:\n        \"\"\"Set the offset for the results.\n\n        Parameters\n        ----------\n        offset: int\n            The offset to start fetching results from.\n\n        Returns\n        -------\n        LanceQueryBuilder\n            The LanceQueryBuilder object.\n        \"\"\"\n        if offset is None or offset &lt;= 0:\n            self._offset = 0\n        else:\n            self._offset = offset\n        return self\n\n    def select(self, columns: Union[list[str], dict[str, str]]) -&gt; Self:\n        \"\"\"Set the columns to return.\n\n        Parameters\n        ----------\n        columns: list of str, or dict of str to str default None\n            List of column names to be fetched.\n            Or a dictionary of column names to SQL expressions.\n            All columns are fetched if None or unspecified.\n\n        Returns\n        -------\n        LanceQueryBuilder\n            The LanceQueryBuilder object.\n        \"\"\"\n        if isinstance(columns, list) or isinstance(columns, dict):\n            self._columns = columns\n        else:\n            raise ValueError(\"columns must be a list or a dictionary\")\n        return self\n\n    def where(self, where: str, prefilter: bool = True) -&gt; Self:\n        \"\"\"Set the where clause.\n\n        Parameters\n        ----------\n        where: str\n            The where clause which is a valid SQL where clause. See\n            `Lance filter pushdown &lt;https://lancedb.github.io/lance/read_and_write.html#filter-push-down&gt;`_\n            for valid SQL expressions.\n        prefilter: bool, default True\n            If True, apply the filter before vector search, otherwise the\n            filter is applied on the result of vector search.\n            This feature is **EXPERIMENTAL** and may be removed and modified\n            without warning in the future.\n\n        Returns\n        -------\n        LanceQueryBuilder\n            The LanceQueryBuilder object.\n        \"\"\"\n        self._where = where\n        self._postfilter = not prefilter\n        return self\n\n    def with_row_id(self, with_row_id: bool) -&gt; Self:\n        \"\"\"Set whether to return row ids.\n\n        Parameters\n        ----------\n        with_row_id: bool\n            If True, return _rowid column in the results.\n\n        Returns\n        -------\n        LanceQueryBuilder\n            The LanceQueryBuilder object.\n        \"\"\"\n        self._with_row_id = with_row_id\n        return self\n\n    def explain_plan(self, verbose: Optional[bool] = False) -&gt; str:\n        \"\"\"Return the execution plan for this query.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import lancedb\n        &gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n        &gt;&gt;&gt; table = db.create_table(\"my_table\", [{\"vector\": [99.0, 99]}])\n        &gt;&gt;&gt; query = [100, 100]\n        &gt;&gt;&gt; plan = table.search(query).explain_plan(True)\n        &gt;&gt;&gt; print(plan) # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n        ProjectionExec: expr=[vector@0 as vector, _distance@2 as _distance]\n        GlobalLimitExec: skip=0, fetch=10\n          FilterExec: _distance@2 IS NOT NULL\n            SortExec: TopK(fetch=10), expr=[_distance@2 ASC NULLS LAST], preserve_partitioning=[false]\n              KNNVectorDistance: metric=l2\n                LanceScan: uri=..., projection=[vector], row_id=true, row_addr=false, ordered=false\n\n        Parameters\n        ----------\n        verbose : bool, default False\n            Use a verbose output format.\n\n        Returns\n        -------\n        plan : str\n        \"\"\"  # noqa: E501\n        return self._table._explain_plan(self.to_query_object(), verbose=verbose)\n\n    def analyze_plan(self) -&gt; str:\n        \"\"\"\n        Run the query and return its execution plan with runtime metrics.\n\n        This returns detailed metrics for each step, such as elapsed time,\n        rows processed, bytes read, and I/O stats. It is useful for debugging\n        and performance tuning.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import lancedb\n        &gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n        &gt;&gt;&gt; table = db.create_table(\"my_table\", [{\"vector\": [99.0, 99]}])\n        &gt;&gt;&gt; query = [100, 100]\n        &gt;&gt;&gt; plan = table.search(query).analyze_plan()\n        &gt;&gt;&gt; print(plan)  # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n        AnalyzeExec verbose=true, metrics=[]\n          ProjectionExec: expr=[...], metrics=[...]\n            GlobalLimitExec: skip=0, fetch=10, metrics=[...]\n              FilterExec: _distance@2 IS NOT NULL,\n              metrics=[output_rows=..., elapsed_compute=...]\n                SortExec: TopK(fetch=10), expr=[...],\n                preserve_partitioning=[...],\n                metrics=[output_rows=..., elapsed_compute=..., row_replacements=...]\n                  KNNVectorDistance: metric=l2,\n                  metrics=[output_rows=..., elapsed_compute=..., output_batches=...]\n                    LanceScan: uri=..., projection=[vector], row_id=true,\n                    row_addr=false, ordered=false,\n                    metrics=[output_rows=..., elapsed_compute=...,\n                    bytes_read=..., iops=..., requests=...]\n\n        Returns\n        -------\n        plan : str\n            The physical query execution plan with runtime metrics.\n        \"\"\"\n        return self._table._analyze_plan(self.to_query_object())\n\n    def vector(self, vector: Union[np.ndarray, list]) -&gt; Self:\n        \"\"\"Set the vector to search for.\n\n        Parameters\n        ----------\n        vector: np.ndarray or list\n            The vector to search for.\n\n        Returns\n        -------\n        LanceQueryBuilder\n            The LanceQueryBuilder object.\n        \"\"\"\n        raise NotImplementedError\n\n    def text(self, text: str | FullTextQuery) -&gt; Self:\n        \"\"\"Set the text to search for.\n\n        Parameters\n        ----------\n        text: str | FullTextQuery\n            If a string, it is treated as a MatchQuery.\n            If a FullTextQuery object, it is used directly.\n\n        Returns\n        -------\n        LanceQueryBuilder\n            The LanceQueryBuilder object.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def rerank(self, reranker: Reranker) -&gt; Self:\n        \"\"\"Rerank the results using the specified reranker.\n\n        Parameters\n        ----------\n        reranker: Reranker\n            The reranker to use.\n\n        Returns\n        -------\n\n        The LanceQueryBuilder object.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def to_query_object(self) -&gt; Query:\n        \"\"\"Return a serializable representation of the query\n\n        Returns\n        -------\n        Query\n            The serializable representation of the query\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceQueryBuilder.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(table: 'Table', query: Optional[Union[ndarray, str, 'PIL.Image.Image', Tuple]], query_type: str, vector_column_name: str, ordering_field_name: Optional[str] = None, fts_columns: Optional[Union[str, List[str]]] = None, fast_search: bool = None) -&gt; Self\n</code></pre> <p>Create a query builder based on the given query and query type.</p> <p>Parameters:</p> <ul> <li> <code>table</code>               (<code>'Table'</code>)           \u2013            <p>The table to query.</p> </li> <li> <code>query</code>               (<code>Optional[Union[ndarray, str, 'PIL.Image.Image', Tuple]]</code>)           \u2013            <p>The query to use. If None, an empty query builder is returned which performs simple SQL filtering.</p> </li> <li> <code>query_type</code>               (<code>str</code>)           \u2013            <p>The type of query to perform. One of \"vector\", \"fts\", \"hybrid\", or \"auto\". If \"auto\", the query type is inferred based on the query.</p> </li> <li> <code>vector_column_name</code>               (<code>str</code>)           \u2013            <p>The name of the vector column to use for vector search.</p> </li> <li> <code>fast_search</code>               (<code>bool</code>, default:                   <code>None</code> )           \u2013            <p>Skip flat search of unindexed data.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    table: \"Table\",\n    query: Optional[Union[np.ndarray, str, \"PIL.Image.Image\", Tuple]],\n    query_type: str,\n    vector_column_name: str,\n    ordering_field_name: Optional[str] = None,\n    fts_columns: Optional[Union[str, List[str]]] = None,\n    fast_search: bool = None,\n) -&gt; Self:\n    \"\"\"\n    Create a query builder based on the given query and query type.\n\n    Parameters\n    ----------\n    table: Table\n        The table to query.\n    query: Optional[Union[np.ndarray, str, \"PIL.Image.Image\", Tuple]]\n        The query to use. If None, an empty query builder is returned\n        which performs simple SQL filtering.\n    query_type: str\n        The type of query to perform. One of \"vector\", \"fts\", \"hybrid\", or \"auto\".\n        If \"auto\", the query type is inferred based on the query.\n    vector_column_name: str\n        The name of the vector column to use for vector search.\n    fast_search: bool\n        Skip flat search of unindexed data.\n    \"\"\"\n    # Check hybrid search first as it supports empty query pattern\n    if query_type == \"hybrid\":\n        # hybrid fts and vector query\n        return LanceHybridQueryBuilder(\n            table, query, vector_column_name, fts_columns=fts_columns\n        )\n\n    if query is None:\n        return LanceEmptyQueryBuilder(table)\n\n    # remember the string query for reranking purpose\n    str_query = query if isinstance(query, str) else None\n\n    # convert \"auto\" query_type to \"vector\", \"fts\"\n    # or \"hybrid\" and convert the query to vector if needed\n    query, query_type = cls._resolve_query(\n        table, query, query_type, vector_column_name\n    )\n\n    if query_type == \"hybrid\":\n        return LanceHybridQueryBuilder(\n            table, query, vector_column_name, fts_columns=fts_columns\n        )\n\n    if isinstance(query, (str, FullTextQuery)):\n        # fts\n        return LanceFtsQueryBuilder(\n            table,\n            query,\n            ordering_field_name=ordering_field_name,\n            fts_columns=fts_columns,\n        )\n\n    if isinstance(query, list):\n        query = np.array(query, dtype=np.float32)\n    elif isinstance(query, np.ndarray):\n        query = query.astype(np.float32)\n    else:\n        raise TypeError(f\"Unsupported query type: {type(query)}\")\n\n    return LanceVectorQueryBuilder(\n        table, query, vector_column_name, str_query, fast_search\n    )\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceQueryBuilder.to_df","title":"to_df","text":"<pre><code>to_df() -&gt; 'pd.DataFrame'\n</code></pre> <p>Deprecated alias for <code>to_pandas()</code>. Please use <code>to_pandas()</code> instead.</p> <p>Execute the query and return the results as a pandas DataFrame. In addition to the selected columns, LanceDB also returns a vector and also the \"_distance\" column which is the distance between the query vector and the returned vector.</p> Source code in <code>lancedb/query.py</code> <pre><code>@deprecation.deprecated(\n    deprecated_in=\"0.3.1\",\n    removed_in=\"0.4.0\",\n    current_version=__version__,\n    details=\"Use to_pandas() instead\",\n)\ndef to_df(self) -&gt; \"pd.DataFrame\":\n    \"\"\"\n    *Deprecated alias for `to_pandas()`. Please use `to_pandas()` instead.*\n\n    Execute the query and return the results as a pandas DataFrame.\n    In addition to the selected columns, LanceDB also returns a vector\n    and also the \"_distance\" column which is the distance between the query\n    vector and the returned vector.\n    \"\"\"\n    return self.to_pandas()\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceQueryBuilder.to_pandas","title":"to_pandas","text":"<pre><code>to_pandas(flatten: Optional[Union[int, bool]] = None, *, timeout: Optional[timedelta] = None) -&gt; 'pd.DataFrame'\n</code></pre> <p>Execute the query and return the results as a pandas DataFrame. In addition to the selected columns, LanceDB also returns a vector and also the \"_distance\" column which is the distance between the query vector and the returned vector.</p> <p>Parameters:</p> <ul> <li> <code>flatten</code>               (<code>Optional[Union[int, bool]]</code>, default:                   <code>None</code> )           \u2013            <p>If flatten is True, flatten all nested columns. If flatten is an integer, flatten the nested columns up to the specified depth. If unspecified, do not flatten the nested columns.</p> </li> <li> <code>timeout</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum time to wait for the query to complete. If None, wait indefinitely.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def to_pandas(\n    self,\n    flatten: Optional[Union[int, bool]] = None,\n    *,\n    timeout: Optional[timedelta] = None,\n) -&gt; \"pd.DataFrame\":\n    \"\"\"\n    Execute the query and return the results as a pandas DataFrame.\n    In addition to the selected columns, LanceDB also returns a vector\n    and also the \"_distance\" column which is the distance between the query\n    vector and the returned vector.\n\n    Parameters\n    ----------\n    flatten: Optional[Union[int, bool]]\n        If flatten is True, flatten all nested columns.\n        If flatten is an integer, flatten the nested columns up to the\n        specified depth.\n        If unspecified, do not flatten the nested columns.\n    timeout: Optional[timedelta]\n        The maximum time to wait for the query to complete.\n        If None, wait indefinitely.\n    \"\"\"\n    tbl = flatten_columns(self.to_arrow(timeout=timeout), flatten)\n    return tbl.to_pandas()\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceQueryBuilder.to_arrow","title":"to_arrow  <code>abstractmethod</code>","text":"<pre><code>to_arrow(*, timeout: Optional[timedelta] = None) -&gt; Table\n</code></pre> <p>Execute the query and return the results as an Apache Arrow Table.</p> <p>In addition to the selected columns, LanceDB also returns a vector and also the \"_distance\" column which is the distance between the query vector and the returned vectors.</p> <p>Parameters:</p> <ul> <li> <code>timeout</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum time to wait for the query to complete. If None, wait indefinitely.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>@abstractmethod\ndef to_arrow(self, *, timeout: Optional[timedelta] = None) -&gt; pa.Table:\n    \"\"\"\n    Execute the query and return the results as an\n    [Apache Arrow Table](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html#pyarrow.Table).\n\n    In addition to the selected columns, LanceDB also returns a vector\n    and also the \"_distance\" column which is the distance between the query\n    vector and the returned vectors.\n\n    Parameters\n    ----------\n    timeout: Optional[timedelta]\n        The maximum time to wait for the query to complete.\n        If None, wait indefinitely.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceQueryBuilder.to_batches","title":"to_batches  <code>abstractmethod</code>","text":"<pre><code>to_batches(batch_size: Optional[int] = None, *, timeout: Optional[timedelta] = None) -&gt; RecordBatchReader\n</code></pre> <p>Execute the query and return the results as a pyarrow RecordBatchReader</p> <p>Parameters:</p> <ul> <li> <code>batch_size</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum number of selected records in a RecordBatch object.</p> </li> <li> <code>timeout</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum time to wait for the query to complete. If None, wait indefinitely.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>@abstractmethod\ndef to_batches(\n    self,\n    /,\n    batch_size: Optional[int] = None,\n    *,\n    timeout: Optional[timedelta] = None,\n) -&gt; pa.RecordBatchReader:\n    \"\"\"\n    Execute the query and return the results as a pyarrow\n    [RecordBatchReader](https://arrow.apache.org/docs/python/generated/pyarrow.RecordBatchReader.html)\n\n    Parameters\n    ----------\n    batch_size: int\n        The maximum number of selected records in a RecordBatch object.\n    timeout: Optional[timedelta]\n        The maximum time to wait for the query to complete.\n        If None, wait indefinitely.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceQueryBuilder.to_list","title":"to_list","text":"<pre><code>to_list(*, timeout: Optional[timedelta] = None) -&gt; List[dict]\n</code></pre> <p>Execute the query and return the results as a list of dictionaries.</p> <p>Each list entry is a dictionary with the selected column names as keys, or all table columns if <code>select</code> is not called. The vector and the \"_distance\" fields are returned whether or not they're explicitly selected.</p> <p>Parameters:</p> <ul> <li> <code>timeout</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum time to wait for the query to complete. If None, wait indefinitely.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def to_list(self, *, timeout: Optional[timedelta] = None) -&gt; List[dict]:\n    \"\"\"\n    Execute the query and return the results as a list of dictionaries.\n\n    Each list entry is a dictionary with the selected column names as keys,\n    or all table columns if `select` is not called. The vector and the \"_distance\"\n    fields are returned whether or not they're explicitly selected.\n\n    Parameters\n    ----------\n    timeout: Optional[timedelta]\n        The maximum time to wait for the query to complete.\n        If None, wait indefinitely.\n    \"\"\"\n    return self.to_arrow(timeout=timeout).to_pylist()\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceQueryBuilder.to_pydantic","title":"to_pydantic","text":"<pre><code>to_pydantic(model: Type[LanceModel], *, timeout: Optional[timedelta] = None) -&gt; List[LanceModel]\n</code></pre> <p>Return the table as a list of pydantic models.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Type[LanceModel]</code>)           \u2013            <p>The pydantic model to use.</p> </li> <li> <code>timeout</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum time to wait for the query to complete. If None, wait indefinitely.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[LanceModel]</code>           \u2013            </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def to_pydantic(\n    self, model: Type[LanceModel], *, timeout: Optional[timedelta] = None\n) -&gt; List[LanceModel]:\n    \"\"\"Return the table as a list of pydantic models.\n\n    Parameters\n    ----------\n    model: Type[LanceModel]\n        The pydantic model to use.\n    timeout: Optional[timedelta]\n        The maximum time to wait for the query to complete.\n        If None, wait indefinitely.\n\n    Returns\n    -------\n    List[LanceModel]\n    \"\"\"\n    return [\n        model(**{k: v for k, v in row.items() if k in model.field_names()})\n        for row in self.to_arrow(timeout=timeout).to_pylist()\n    ]\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceQueryBuilder.to_polars","title":"to_polars","text":"<pre><code>to_polars(*, timeout: Optional[timedelta] = None) -&gt; 'pl.DataFrame'\n</code></pre> <p>Execute the query and return the results as a Polars DataFrame. In addition to the selected columns, LanceDB also returns a vector and also the \"_distance\" column which is the distance between the query vector and the returned vector.</p> <p>Parameters:</p> <ul> <li> <code>timeout</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum time to wait for the query to complete. If None, wait indefinitely.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def to_polars(self, *, timeout: Optional[timedelta] = None) -&gt; \"pl.DataFrame\":\n    \"\"\"\n    Execute the query and return the results as a Polars DataFrame.\n    In addition to the selected columns, LanceDB also returns a vector\n    and also the \"_distance\" column which is the distance between the query\n    vector and the returned vector.\n\n    Parameters\n    ----------\n    timeout: Optional[timedelta]\n        The maximum time to wait for the query to complete.\n        If None, wait indefinitely.\n    \"\"\"\n    import polars as pl\n\n    return pl.from_arrow(self.to_arrow(timeout=timeout))\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceQueryBuilder.limit","title":"limit","text":"<pre><code>limit(limit: Union[int, None]) -&gt; Self\n</code></pre> <p>Set the maximum number of results to return.</p> <p>Parameters:</p> <ul> <li> <code>limit</code>               (<code>Union[int, None]</code>)           \u2013            <p>The maximum number of results to return. The default query limit is 10 results. For ANN/KNN queries, you must specify a limit. For plain searches, all records are returned if limit not set. WARNING if you have a large dataset, setting the limit to a large number, e.g. the table size, can potentially result in reading a large amount of data into memory and cause out of memory issues.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LanceQueryBuilder</code>           \u2013            <p>The LanceQueryBuilder object.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def limit(self, limit: Union[int, None]) -&gt; Self:\n    \"\"\"Set the maximum number of results to return.\n\n    Parameters\n    ----------\n    limit: int\n        The maximum number of results to return.\n        The default query limit is 10 results.\n        For ANN/KNN queries, you must specify a limit.\n        For plain searches, all records are returned if limit not set.\n        *WARNING* if you have a large dataset, setting\n        the limit to a large number, e.g. the table size,\n        can potentially result in reading a\n        large amount of data into memory and cause\n        out of memory issues.\n\n    Returns\n    -------\n    LanceQueryBuilder\n        The LanceQueryBuilder object.\n    \"\"\"\n    if limit is None or limit &lt;= 0:\n        if isinstance(self, LanceVectorQueryBuilder):\n            raise ValueError(\"Limit is required for ANN/KNN queries\")\n        else:\n            self._limit = None\n    else:\n        self._limit = limit\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceQueryBuilder.offset","title":"offset","text":"<pre><code>offset(offset: int) -&gt; Self\n</code></pre> <p>Set the offset for the results.</p> <p>Parameters:</p> <ul> <li> <code>offset</code>               (<code>int</code>)           \u2013            <p>The offset to start fetching results from.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LanceQueryBuilder</code>           \u2013            <p>The LanceQueryBuilder object.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def offset(self, offset: int) -&gt; Self:\n    \"\"\"Set the offset for the results.\n\n    Parameters\n    ----------\n    offset: int\n        The offset to start fetching results from.\n\n    Returns\n    -------\n    LanceQueryBuilder\n        The LanceQueryBuilder object.\n    \"\"\"\n    if offset is None or offset &lt;= 0:\n        self._offset = 0\n    else:\n        self._offset = offset\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceQueryBuilder.select","title":"select","text":"<pre><code>select(columns: Union[list[str], dict[str, str]]) -&gt; Self\n</code></pre> <p>Set the columns to return.</p> <p>Parameters:</p> <ul> <li> <code>columns</code>               (<code>Union[list[str], dict[str, str]]</code>)           \u2013            <p>List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LanceQueryBuilder</code>           \u2013            <p>The LanceQueryBuilder object.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def select(self, columns: Union[list[str], dict[str, str]]) -&gt; Self:\n    \"\"\"Set the columns to return.\n\n    Parameters\n    ----------\n    columns: list of str, or dict of str to str default None\n        List of column names to be fetched.\n        Or a dictionary of column names to SQL expressions.\n        All columns are fetched if None or unspecified.\n\n    Returns\n    -------\n    LanceQueryBuilder\n        The LanceQueryBuilder object.\n    \"\"\"\n    if isinstance(columns, list) or isinstance(columns, dict):\n        self._columns = columns\n    else:\n        raise ValueError(\"columns must be a list or a dictionary\")\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceQueryBuilder.where","title":"where","text":"<pre><code>where(where: str, prefilter: bool = True) -&gt; Self\n</code></pre> <p>Set the where clause.</p> <p>Parameters:</p> <ul> <li> <code>where</code>               (<code>str</code>)           \u2013            <p>The where clause which is a valid SQL where clause. See <code>Lance filter pushdown &lt;https://lancedb.github.io/lance/read_and_write.html#filter-push-down&gt;</code>_ for valid SQL expressions.</p> </li> <li> <code>prefilter</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, apply the filter before vector search, otherwise the filter is applied on the result of vector search. This feature is EXPERIMENTAL and may be removed and modified without warning in the future.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LanceQueryBuilder</code>           \u2013            <p>The LanceQueryBuilder object.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def where(self, where: str, prefilter: bool = True) -&gt; Self:\n    \"\"\"Set the where clause.\n\n    Parameters\n    ----------\n    where: str\n        The where clause which is a valid SQL where clause. See\n        `Lance filter pushdown &lt;https://lancedb.github.io/lance/read_and_write.html#filter-push-down&gt;`_\n        for valid SQL expressions.\n    prefilter: bool, default True\n        If True, apply the filter before vector search, otherwise the\n        filter is applied on the result of vector search.\n        This feature is **EXPERIMENTAL** and may be removed and modified\n        without warning in the future.\n\n    Returns\n    -------\n    LanceQueryBuilder\n        The LanceQueryBuilder object.\n    \"\"\"\n    self._where = where\n    self._postfilter = not prefilter\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceQueryBuilder.with_row_id","title":"with_row_id","text":"<pre><code>with_row_id(with_row_id: bool) -&gt; Self\n</code></pre> <p>Set whether to return row ids.</p> <p>Parameters:</p> <ul> <li> <code>with_row_id</code>               (<code>bool</code>)           \u2013            <p>If True, return _rowid column in the results.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LanceQueryBuilder</code>           \u2013            <p>The LanceQueryBuilder object.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def with_row_id(self, with_row_id: bool) -&gt; Self:\n    \"\"\"Set whether to return row ids.\n\n    Parameters\n    ----------\n    with_row_id: bool\n        If True, return _rowid column in the results.\n\n    Returns\n    -------\n    LanceQueryBuilder\n        The LanceQueryBuilder object.\n    \"\"\"\n    self._with_row_id = with_row_id\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceQueryBuilder.explain_plan","title":"explain_plan","text":"<pre><code>explain_plan(verbose: Optional[bool] = False) -&gt; str\n</code></pre> <p>Return the execution plan for this query.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import lancedb\n&gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n&gt;&gt;&gt; table = db.create_table(\"my_table\", [{\"vector\": [99.0, 99]}])\n&gt;&gt;&gt; query = [100, 100]\n&gt;&gt;&gt; plan = table.search(query).explain_plan(True)\n&gt;&gt;&gt; print(plan)\nProjectionExec: expr=[vector@0 as vector, _distance@2 as _distance]\nGlobalLimitExec: skip=0, fetch=10\n  FilterExec: _distance@2 IS NOT NULL\n    SortExec: TopK(fetch=10), expr=[_distance@2 ASC NULLS LAST], preserve_partitioning=[false]\n      KNNVectorDistance: metric=l2\n        LanceScan: uri=..., projection=[vector], row_id=true, row_addr=false, ordered=false\n</code></pre> <p>Parameters:</p> <ul> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Use a verbose output format.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>plan</code> (              <code>str</code> )          \u2013            </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def explain_plan(self, verbose: Optional[bool] = False) -&gt; str:\n    \"\"\"Return the execution plan for this query.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import lancedb\n    &gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n    &gt;&gt;&gt; table = db.create_table(\"my_table\", [{\"vector\": [99.0, 99]}])\n    &gt;&gt;&gt; query = [100, 100]\n    &gt;&gt;&gt; plan = table.search(query).explain_plan(True)\n    &gt;&gt;&gt; print(plan) # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n    ProjectionExec: expr=[vector@0 as vector, _distance@2 as _distance]\n    GlobalLimitExec: skip=0, fetch=10\n      FilterExec: _distance@2 IS NOT NULL\n        SortExec: TopK(fetch=10), expr=[_distance@2 ASC NULLS LAST], preserve_partitioning=[false]\n          KNNVectorDistance: metric=l2\n            LanceScan: uri=..., projection=[vector], row_id=true, row_addr=false, ordered=false\n\n    Parameters\n    ----------\n    verbose : bool, default False\n        Use a verbose output format.\n\n    Returns\n    -------\n    plan : str\n    \"\"\"  # noqa: E501\n    return self._table._explain_plan(self.to_query_object(), verbose=verbose)\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceQueryBuilder.analyze_plan","title":"analyze_plan","text":"<pre><code>analyze_plan() -&gt; str\n</code></pre> <p>Run the query and return its execution plan with runtime metrics.</p> <p>This returns detailed metrics for each step, such as elapsed time, rows processed, bytes read, and I/O stats. It is useful for debugging and performance tuning.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import lancedb\n&gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n&gt;&gt;&gt; table = db.create_table(\"my_table\", [{\"vector\": [99.0, 99]}])\n&gt;&gt;&gt; query = [100, 100]\n&gt;&gt;&gt; plan = table.search(query).analyze_plan()\n&gt;&gt;&gt; print(plan)\nAnalyzeExec verbose=true, metrics=[]\n  ProjectionExec: expr=[...], metrics=[...]\n    GlobalLimitExec: skip=0, fetch=10, metrics=[...]\n      FilterExec: _distance@2 IS NOT NULL,\n      metrics=[output_rows=..., elapsed_compute=...]\n        SortExec: TopK(fetch=10), expr=[...],\n        preserve_partitioning=[...],\n        metrics=[output_rows=..., elapsed_compute=..., row_replacements=...]\n          KNNVectorDistance: metric=l2,\n          metrics=[output_rows=..., elapsed_compute=..., output_batches=...]\n            LanceScan: uri=..., projection=[vector], row_id=true,\n            row_addr=false, ordered=false,\n            metrics=[output_rows=..., elapsed_compute=...,\n            bytes_read=..., iops=..., requests=...]\n</code></pre> <p>Returns:</p> <ul> <li> <code>plan</code> (              <code>str</code> )          \u2013            <p>The physical query execution plan with runtime metrics.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def analyze_plan(self) -&gt; str:\n    \"\"\"\n    Run the query and return its execution plan with runtime metrics.\n\n    This returns detailed metrics for each step, such as elapsed time,\n    rows processed, bytes read, and I/O stats. It is useful for debugging\n    and performance tuning.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import lancedb\n    &gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n    &gt;&gt;&gt; table = db.create_table(\"my_table\", [{\"vector\": [99.0, 99]}])\n    &gt;&gt;&gt; query = [100, 100]\n    &gt;&gt;&gt; plan = table.search(query).analyze_plan()\n    &gt;&gt;&gt; print(plan)  # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n    AnalyzeExec verbose=true, metrics=[]\n      ProjectionExec: expr=[...], metrics=[...]\n        GlobalLimitExec: skip=0, fetch=10, metrics=[...]\n          FilterExec: _distance@2 IS NOT NULL,\n          metrics=[output_rows=..., elapsed_compute=...]\n            SortExec: TopK(fetch=10), expr=[...],\n            preserve_partitioning=[...],\n            metrics=[output_rows=..., elapsed_compute=..., row_replacements=...]\n              KNNVectorDistance: metric=l2,\n              metrics=[output_rows=..., elapsed_compute=..., output_batches=...]\n                LanceScan: uri=..., projection=[vector], row_id=true,\n                row_addr=false, ordered=false,\n                metrics=[output_rows=..., elapsed_compute=...,\n                bytes_read=..., iops=..., requests=...]\n\n    Returns\n    -------\n    plan : str\n        The physical query execution plan with runtime metrics.\n    \"\"\"\n    return self._table._analyze_plan(self.to_query_object())\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceQueryBuilder.vector","title":"vector","text":"<pre><code>vector(vector: Union[ndarray, list]) -&gt; Self\n</code></pre> <p>Set the vector to search for.</p> <p>Parameters:</p> <ul> <li> <code>vector</code>               (<code>Union[ndarray, list]</code>)           \u2013            <p>The vector to search for.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LanceQueryBuilder</code>           \u2013            <p>The LanceQueryBuilder object.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def vector(self, vector: Union[np.ndarray, list]) -&gt; Self:\n    \"\"\"Set the vector to search for.\n\n    Parameters\n    ----------\n    vector: np.ndarray or list\n        The vector to search for.\n\n    Returns\n    -------\n    LanceQueryBuilder\n        The LanceQueryBuilder object.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceQueryBuilder.text","title":"text","text":"<pre><code>text(text: str | FullTextQuery) -&gt; Self\n</code></pre> <p>Set the text to search for.</p> <p>Parameters:</p> <ul> <li> <code>text</code>               (<code>str | FullTextQuery</code>)           \u2013            <p>If a string, it is treated as a MatchQuery. If a FullTextQuery object, it is used directly.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LanceQueryBuilder</code>           \u2013            <p>The LanceQueryBuilder object.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def text(self, text: str | FullTextQuery) -&gt; Self:\n    \"\"\"Set the text to search for.\n\n    Parameters\n    ----------\n    text: str | FullTextQuery\n        If a string, it is treated as a MatchQuery.\n        If a FullTextQuery object, it is used directly.\n\n    Returns\n    -------\n    LanceQueryBuilder\n        The LanceQueryBuilder object.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceQueryBuilder.rerank","title":"rerank  <code>abstractmethod</code>","text":"<pre><code>rerank(reranker: Reranker) -&gt; Self\n</code></pre> <p>Rerank the results using the specified reranker.</p> <p>Parameters:</p> <ul> <li> <code>reranker</code>               (<code>Reranker</code>)           \u2013            <p>The reranker to use.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>The LanceQueryBuilder object.</code>           \u2013            </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>@abstractmethod\ndef rerank(self, reranker: Reranker) -&gt; Self:\n    \"\"\"Rerank the results using the specified reranker.\n\n    Parameters\n    ----------\n    reranker: Reranker\n        The reranker to use.\n\n    Returns\n    -------\n\n    The LanceQueryBuilder object.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceQueryBuilder.to_query_object","title":"to_query_object  <code>abstractmethod</code>","text":"<pre><code>to_query_object() -&gt; Query\n</code></pre> <p>Return a serializable representation of the query</p> <p>Returns:</p> <ul> <li> <code>Query</code>           \u2013            <p>The serializable representation of the query</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>@abstractmethod\ndef to_query_object(self) -&gt; Query:\n    \"\"\"Return a serializable representation of the query\n\n    Returns\n    -------\n    Query\n        The serializable representation of the query\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceVectorQueryBuilder","title":"lancedb.query.LanceVectorQueryBuilder","text":"<p>               Bases: <code>LanceQueryBuilder</code></p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import lancedb\n&gt;&gt;&gt; data = [{\"vector\": [1.1, 1.2], \"b\": 2},\n...         {\"vector\": [0.5, 1.3], \"b\": 4},\n...         {\"vector\": [0.4, 0.4], \"b\": 6},\n...         {\"vector\": [0.4, 0.4], \"b\": 10}]\n&gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n&gt;&gt;&gt; table = db.create_table(\"my_table\", data=data)\n&gt;&gt;&gt; (table.search([0.4, 0.4])\n...       .distance_type(\"cosine\")\n...       .where(\"b &lt; 10\")\n...       .select([\"b\", \"vector\"])\n...       .limit(2)\n...       .to_pandas())\n   b      vector  _distance\n0  6  [0.4, 0.4]   0.000000\n1  2  [1.1, 1.2]   0.000944\n</code></pre> Source code in <code>lancedb/query.py</code> <pre><code>class LanceVectorQueryBuilder(LanceQueryBuilder):\n    \"\"\"\n    Examples\n    --------\n    &gt;&gt;&gt; import lancedb\n    &gt;&gt;&gt; data = [{\"vector\": [1.1, 1.2], \"b\": 2},\n    ...         {\"vector\": [0.5, 1.3], \"b\": 4},\n    ...         {\"vector\": [0.4, 0.4], \"b\": 6},\n    ...         {\"vector\": [0.4, 0.4], \"b\": 10}]\n    &gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n    &gt;&gt;&gt; table = db.create_table(\"my_table\", data=data)\n    &gt;&gt;&gt; (table.search([0.4, 0.4])\n    ...       .distance_type(\"cosine\")\n    ...       .where(\"b &lt; 10\")\n    ...       .select([\"b\", \"vector\"])\n    ...       .limit(2)\n    ...       .to_pandas())\n       b      vector  _distance\n    0  6  [0.4, 0.4]   0.000000\n    1  2  [1.1, 1.2]   0.000944\n    \"\"\"\n\n    def __init__(\n        self,\n        table: \"Table\",\n        query: Union[np.ndarray, list, \"PIL.Image.Image\"],\n        vector_column: str,\n        str_query: Optional[str] = None,\n        fast_search: bool = None,\n    ):\n        super().__init__(table)\n        self._query = query\n        self._distance_type = None\n        self._nprobes = None\n        self._lower_bound = None\n        self._upper_bound = None\n        self._refine_factor = None\n        self._vector_column = vector_column\n        self._postfilter = None\n        self._reranker = None\n        self._str_query = str_query\n        self._fast_search = fast_search\n\n    def metric(self, metric: Literal[\"l2\", \"cosine\", \"dot\"]) -&gt; LanceVectorQueryBuilder:\n        \"\"\"Set the distance metric to use.\n\n        This is an alias for distance_type() and may be deprecated in the future.\n        Please use distance_type() instead.\n\n        Parameters\n        ----------\n        metric: \"l2\" or \"cosine\" or \"dot\"\n            The distance metric to use. By default \"l2\" is used.\n\n        Returns\n        -------\n        LanceVectorQueryBuilder\n            The LanceQueryBuilder object.\n        \"\"\"\n        return self.distance_type(metric)\n\n    def distance_type(\n        self, distance_type: Literal[\"l2\", \"cosine\", \"dot\"]\n    ) -&gt; \"LanceVectorQueryBuilder\":\n        \"\"\"Set the distance metric to use.\n\n        When performing a vector search we try and find the \"nearest\" vectors according\n        to some kind of distance metric. This parameter controls which distance metric\n        to use.\n\n        Note: if there is a vector index then the distance type used MUST match the\n        distance type used to train the vector index. If this is not done then the\n        results will be invalid.\n\n        Parameters\n        ----------\n        distance_type: \"l2\" or \"cosine\" or \"dot\"\n            The distance metric to use. By default \"l2\" is used.\n\n        Returns\n        -------\n        LanceVectorQueryBuilder\n            The LanceQueryBuilder object.\n        \"\"\"\n        self._distance_type = distance_type.lower()\n        return self\n\n    def nprobes(self, nprobes: int) -&gt; LanceVectorQueryBuilder:\n        \"\"\"Set the number of probes to use.\n\n        Higher values will yield better recall (more likely to find vectors if\n        they exist) at the expense of latency.\n\n        See discussion in [Querying an ANN Index][querying-an-ann-index] for\n        tuning advice.\n\n        Parameters\n        ----------\n        nprobes: int\n            The number of probes to use.\n\n        Returns\n        -------\n        LanceVectorQueryBuilder\n            The LanceQueryBuilder object.\n        \"\"\"\n        self._nprobes = nprobes\n        return self\n\n    def distance_range(\n        self, lower_bound: Optional[float] = None, upper_bound: Optional[float] = None\n    ) -&gt; LanceVectorQueryBuilder:\n        \"\"\"Set the distance range to use.\n\n        Only rows with distances within range [lower_bound, upper_bound)\n        will be returned.\n\n        Parameters\n        ----------\n        lower_bound: Optional[float]\n            The lower bound of the distance range.\n        upper_bound: Optional[float]\n            The upper bound of the distance range.\n\n        Returns\n        -------\n        LanceVectorQueryBuilder\n            The LanceQueryBuilder object.\n        \"\"\"\n        self._lower_bound = lower_bound\n        self._upper_bound = upper_bound\n        return self\n\n    def ef(self, ef: int) -&gt; LanceVectorQueryBuilder:\n        \"\"\"Set the number of candidates to consider during search.\n\n        Higher values will yield better recall (more likely to find vectors if\n        they exist) at the expense of latency.\n\n        This only applies to the HNSW-related index.\n        The default value is 1.5 * limit.\n\n        Parameters\n        ----------\n        ef: int\n            The number of candidates to consider during search.\n\n        Returns\n        -------\n        LanceVectorQueryBuilder\n            The LanceQueryBuilder object.\n        \"\"\"\n        self._ef = ef\n        return self\n\n    def refine_factor(self, refine_factor: int) -&gt; LanceVectorQueryBuilder:\n        \"\"\"Set the refine factor to use, increasing the number of vectors sampled.\n\n        As an example, a refine factor of 2 will sample 2x as many vectors as\n        requested, re-ranks them, and returns the top half most relevant results.\n\n        See discussion in [Querying an ANN Index][querying-an-ann-index] for\n        tuning advice.\n\n        Parameters\n        ----------\n        refine_factor: int\n            The refine factor to use.\n\n        Returns\n        -------\n        LanceVectorQueryBuilder\n            The LanceQueryBuilder object.\n        \"\"\"\n        self._refine_factor = refine_factor\n        return self\n\n    def to_arrow(self, *, timeout: Optional[timedelta] = None) -&gt; pa.Table:\n        \"\"\"\n        Execute the query and return the results as an\n        [Apache Arrow Table](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html#pyarrow.Table).\n\n        In addition to the selected columns, LanceDB also returns a vector\n        and also the \"_distance\" column which is the distance between the query\n        vector and the returned vectors.\n\n        Parameters\n        ----------\n        timeout: Optional[timedelta]\n            The maximum time to wait for the query to complete.\n            If None, wait indefinitely.\n        \"\"\"\n        return self.to_batches(timeout=timeout).read_all()\n\n    def to_query_object(self) -&gt; Query:\n        \"\"\"\n        Build a Query object\n\n        This can be used to serialize a query\n        \"\"\"\n        vector = self._query if isinstance(self._query, list) else self._query.tolist()\n        if isinstance(vector[0], np.ndarray):\n            vector = [v.tolist() for v in vector]\n        return Query(\n            vector=vector,\n            filter=self._where,\n            postfilter=self._postfilter,\n            limit=self._limit,\n            distance_type=self._distance_type,\n            columns=self._columns,\n            nprobes=self._nprobes,\n            lower_bound=self._lower_bound,\n            upper_bound=self._upper_bound,\n            refine_factor=self._refine_factor,\n            vector_column=self._vector_column,\n            with_row_id=self._with_row_id,\n            offset=self._offset,\n            fast_search=self._fast_search,\n            ef=self._ef,\n            bypass_vector_index=self._bypass_vector_index,\n        )\n\n    def to_batches(\n        self,\n        /,\n        batch_size: Optional[int] = None,\n        *,\n        timeout: Optional[timedelta] = None,\n    ) -&gt; pa.RecordBatchReader:\n        \"\"\"\n        Execute the query and return the result as a RecordBatchReader object.\n\n        Parameters\n        ----------\n        batch_size: int\n            The maximum number of selected records in a RecordBatch object.\n        timeout: timedelta, default None\n            The maximum time to wait for the query to complete.\n            If None, wait indefinitely.\n\n        Returns\n        -------\n        pa.RecordBatchReader\n        \"\"\"\n        vector = self._query if isinstance(self._query, list) else self._query.tolist()\n        if isinstance(vector[0], np.ndarray):\n            vector = [v.tolist() for v in vector]\n        query = self.to_query_object()\n        result_set = self._table._execute_query(\n            query, batch_size=batch_size, timeout=timeout\n        )\n        if self._reranker is not None:\n            rs_table = result_set.read_all()\n            result_set = self._reranker.rerank_vector(self._str_query, rs_table)\n            check_reranker_result(result_set)\n            # convert result_set back to RecordBatchReader\n            result_set = pa.RecordBatchReader.from_batches(\n                result_set.schema, result_set.to_batches()\n            )\n\n        return result_set\n\n    def where(self, where: str, prefilter: bool = None) -&gt; LanceVectorQueryBuilder:\n        \"\"\"Set the where clause.\n\n        Parameters\n        ----------\n        where: str\n            The where clause which is a valid SQL where clause. See\n            `Lance filter pushdown &lt;https://lancedb.github.io/lance/read_and_write.html#filter-push-down&gt;`_\n            for valid SQL expressions.\n        prefilter: bool, default True\n            If True, apply the filter before vector search, otherwise the\n            filter is applied on the result of vector search.\n\n        Returns\n        -------\n        LanceQueryBuilder\n            The LanceQueryBuilder object.\n        \"\"\"\n        self._where = where\n        if prefilter is not None:\n            self._postfilter = not prefilter\n        return self\n\n    def rerank(\n        self, reranker: Reranker, query_string: Optional[str] = None\n    ) -&gt; LanceVectorQueryBuilder:\n        \"\"\"Rerank the results using the specified reranker.\n\n        Parameters\n        ----------\n        reranker: Reranker\n            The reranker to use.\n\n        query_string: Optional[str]\n            The query to use for reranking. This needs to be specified explicitly here\n            as the query used for vector search may already be vectorized and the\n            reranker requires a string query.\n            This is only required if the query used for vector search is not a string.\n            Note: This doesn't yet support the case where the query is multimodal or a\n            list of vectors.\n\n        Returns\n        -------\n        LanceVectorQueryBuilder\n            The LanceQueryBuilder object.\n        \"\"\"\n        self._reranker = reranker\n        if self._str_query is None and query_string is None:\n            raise ValueError(\n                \"\"\"\n                The query used for vector search is not a string.\n                In this case, the reranker query needs to be specified explicitly.\n                \"\"\"\n            )\n        if query_string is not None and not isinstance(query_string, str):\n            raise ValueError(\"Reranking currently only supports string queries\")\n        self._str_query = query_string if query_string is not None else self._str_query\n        return self\n\n    def bypass_vector_index(self) -&gt; LanceVectorQueryBuilder:\n        \"\"\"\n        If this is called then any vector index is skipped\n\n        An exhaustive (flat) search will be performed.  The query vector will\n        be compared to every vector in the table.  At high scales this can be\n        expensive.  However, this is often still useful.  For example, skipping\n        the vector index can give you ground truth results which you can use to\n        calculate your recall to select an appropriate value for nprobes.\n\n        Returns\n        -------\n        LanceVectorQueryBuilder\n            The LanceVectorQueryBuilder object.\n        \"\"\"\n        self._bypass_vector_index = True\n        return self\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceVectorQueryBuilder.metric","title":"metric","text":"<pre><code>metric(metric: Literal['l2', 'cosine', 'dot']) -&gt; LanceVectorQueryBuilder\n</code></pre> <p>Set the distance metric to use.</p> <p>This is an alias for distance_type() and may be deprecated in the future. Please use distance_type() instead.</p> <p>Parameters:</p> <ul> <li> <code>metric</code>               (<code>Literal['l2', 'cosine', 'dot']</code>)           \u2013            <p>The distance metric to use. By default \"l2\" is used.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LanceVectorQueryBuilder</code>           \u2013            <p>The LanceQueryBuilder object.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def metric(self, metric: Literal[\"l2\", \"cosine\", \"dot\"]) -&gt; LanceVectorQueryBuilder:\n    \"\"\"Set the distance metric to use.\n\n    This is an alias for distance_type() and may be deprecated in the future.\n    Please use distance_type() instead.\n\n    Parameters\n    ----------\n    metric: \"l2\" or \"cosine\" or \"dot\"\n        The distance metric to use. By default \"l2\" is used.\n\n    Returns\n    -------\n    LanceVectorQueryBuilder\n        The LanceQueryBuilder object.\n    \"\"\"\n    return self.distance_type(metric)\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceVectorQueryBuilder.distance_type","title":"distance_type","text":"<pre><code>distance_type(distance_type: Literal['l2', 'cosine', 'dot']) -&gt; 'LanceVectorQueryBuilder'\n</code></pre> <p>Set the distance metric to use.</p> <p>When performing a vector search we try and find the \"nearest\" vectors according to some kind of distance metric. This parameter controls which distance metric to use.</p> <p>Note: if there is a vector index then the distance type used MUST match the distance type used to train the vector index. If this is not done then the results will be invalid.</p> <p>Parameters:</p> <ul> <li> <code>distance_type</code>               (<code>Literal['l2', 'cosine', 'dot']</code>)           \u2013            <p>The distance metric to use. By default \"l2\" is used.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LanceVectorQueryBuilder</code>           \u2013            <p>The LanceQueryBuilder object.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def distance_type(\n    self, distance_type: Literal[\"l2\", \"cosine\", \"dot\"]\n) -&gt; \"LanceVectorQueryBuilder\":\n    \"\"\"Set the distance metric to use.\n\n    When performing a vector search we try and find the \"nearest\" vectors according\n    to some kind of distance metric. This parameter controls which distance metric\n    to use.\n\n    Note: if there is a vector index then the distance type used MUST match the\n    distance type used to train the vector index. If this is not done then the\n    results will be invalid.\n\n    Parameters\n    ----------\n    distance_type: \"l2\" or \"cosine\" or \"dot\"\n        The distance metric to use. By default \"l2\" is used.\n\n    Returns\n    -------\n    LanceVectorQueryBuilder\n        The LanceQueryBuilder object.\n    \"\"\"\n    self._distance_type = distance_type.lower()\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceVectorQueryBuilder.nprobes","title":"nprobes","text":"<pre><code>nprobes(nprobes: int) -&gt; LanceVectorQueryBuilder\n</code></pre> <p>Set the number of probes to use.</p> <p>Higher values will yield better recall (more likely to find vectors if they exist) at the expense of latency.</p> <p>See discussion in [Querying an ANN Index][querying-an-ann-index] for tuning advice.</p> <p>Parameters:</p> <ul> <li> <code>nprobes</code>               (<code>int</code>)           \u2013            <p>The number of probes to use.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LanceVectorQueryBuilder</code>           \u2013            <p>The LanceQueryBuilder object.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def nprobes(self, nprobes: int) -&gt; LanceVectorQueryBuilder:\n    \"\"\"Set the number of probes to use.\n\n    Higher values will yield better recall (more likely to find vectors if\n    they exist) at the expense of latency.\n\n    See discussion in [Querying an ANN Index][querying-an-ann-index] for\n    tuning advice.\n\n    Parameters\n    ----------\n    nprobes: int\n        The number of probes to use.\n\n    Returns\n    -------\n    LanceVectorQueryBuilder\n        The LanceQueryBuilder object.\n    \"\"\"\n    self._nprobes = nprobes\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceVectorQueryBuilder.distance_range","title":"distance_range","text":"<pre><code>distance_range(lower_bound: Optional[float] = None, upper_bound: Optional[float] = None) -&gt; LanceVectorQueryBuilder\n</code></pre> <p>Set the distance range to use.</p> <p>Only rows with distances within range [lower_bound, upper_bound) will be returned.</p> <p>Parameters:</p> <ul> <li> <code>lower_bound</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>The lower bound of the distance range.</p> </li> <li> <code>upper_bound</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>The upper bound of the distance range.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LanceVectorQueryBuilder</code>           \u2013            <p>The LanceQueryBuilder object.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def distance_range(\n    self, lower_bound: Optional[float] = None, upper_bound: Optional[float] = None\n) -&gt; LanceVectorQueryBuilder:\n    \"\"\"Set the distance range to use.\n\n    Only rows with distances within range [lower_bound, upper_bound)\n    will be returned.\n\n    Parameters\n    ----------\n    lower_bound: Optional[float]\n        The lower bound of the distance range.\n    upper_bound: Optional[float]\n        The upper bound of the distance range.\n\n    Returns\n    -------\n    LanceVectorQueryBuilder\n        The LanceQueryBuilder object.\n    \"\"\"\n    self._lower_bound = lower_bound\n    self._upper_bound = upper_bound\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceVectorQueryBuilder.ef","title":"ef","text":"<pre><code>ef(ef: int) -&gt; LanceVectorQueryBuilder\n</code></pre> <p>Set the number of candidates to consider during search.</p> <p>Higher values will yield better recall (more likely to find vectors if they exist) at the expense of latency.</p> <p>This only applies to the HNSW-related index. The default value is 1.5 * limit.</p> <p>Parameters:</p> <ul> <li> <code>ef</code>               (<code>int</code>)           \u2013            <p>The number of candidates to consider during search.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LanceVectorQueryBuilder</code>           \u2013            <p>The LanceQueryBuilder object.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def ef(self, ef: int) -&gt; LanceVectorQueryBuilder:\n    \"\"\"Set the number of candidates to consider during search.\n\n    Higher values will yield better recall (more likely to find vectors if\n    they exist) at the expense of latency.\n\n    This only applies to the HNSW-related index.\n    The default value is 1.5 * limit.\n\n    Parameters\n    ----------\n    ef: int\n        The number of candidates to consider during search.\n\n    Returns\n    -------\n    LanceVectorQueryBuilder\n        The LanceQueryBuilder object.\n    \"\"\"\n    self._ef = ef\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceVectorQueryBuilder.refine_factor","title":"refine_factor","text":"<pre><code>refine_factor(refine_factor: int) -&gt; LanceVectorQueryBuilder\n</code></pre> <p>Set the refine factor to use, increasing the number of vectors sampled.</p> <p>As an example, a refine factor of 2 will sample 2x as many vectors as requested, re-ranks them, and returns the top half most relevant results.</p> <p>See discussion in [Querying an ANN Index][querying-an-ann-index] for tuning advice.</p> <p>Parameters:</p> <ul> <li> <code>refine_factor</code>               (<code>int</code>)           \u2013            <p>The refine factor to use.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LanceVectorQueryBuilder</code>           \u2013            <p>The LanceQueryBuilder object.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def refine_factor(self, refine_factor: int) -&gt; LanceVectorQueryBuilder:\n    \"\"\"Set the refine factor to use, increasing the number of vectors sampled.\n\n    As an example, a refine factor of 2 will sample 2x as many vectors as\n    requested, re-ranks them, and returns the top half most relevant results.\n\n    See discussion in [Querying an ANN Index][querying-an-ann-index] for\n    tuning advice.\n\n    Parameters\n    ----------\n    refine_factor: int\n        The refine factor to use.\n\n    Returns\n    -------\n    LanceVectorQueryBuilder\n        The LanceQueryBuilder object.\n    \"\"\"\n    self._refine_factor = refine_factor\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceVectorQueryBuilder.to_arrow","title":"to_arrow","text":"<pre><code>to_arrow(*, timeout: Optional[timedelta] = None) -&gt; Table\n</code></pre> <p>Execute the query and return the results as an Apache Arrow Table.</p> <p>In addition to the selected columns, LanceDB also returns a vector and also the \"_distance\" column which is the distance between the query vector and the returned vectors.</p> <p>Parameters:</p> <ul> <li> <code>timeout</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum time to wait for the query to complete. If None, wait indefinitely.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def to_arrow(self, *, timeout: Optional[timedelta] = None) -&gt; pa.Table:\n    \"\"\"\n    Execute the query and return the results as an\n    [Apache Arrow Table](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html#pyarrow.Table).\n\n    In addition to the selected columns, LanceDB also returns a vector\n    and also the \"_distance\" column which is the distance between the query\n    vector and the returned vectors.\n\n    Parameters\n    ----------\n    timeout: Optional[timedelta]\n        The maximum time to wait for the query to complete.\n        If None, wait indefinitely.\n    \"\"\"\n    return self.to_batches(timeout=timeout).read_all()\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceVectorQueryBuilder.to_query_object","title":"to_query_object","text":"<pre><code>to_query_object() -&gt; Query\n</code></pre> <p>Build a Query object</p> <p>This can be used to serialize a query</p> Source code in <code>lancedb/query.py</code> <pre><code>def to_query_object(self) -&gt; Query:\n    \"\"\"\n    Build a Query object\n\n    This can be used to serialize a query\n    \"\"\"\n    vector = self._query if isinstance(self._query, list) else self._query.tolist()\n    if isinstance(vector[0], np.ndarray):\n        vector = [v.tolist() for v in vector]\n    return Query(\n        vector=vector,\n        filter=self._where,\n        postfilter=self._postfilter,\n        limit=self._limit,\n        distance_type=self._distance_type,\n        columns=self._columns,\n        nprobes=self._nprobes,\n        lower_bound=self._lower_bound,\n        upper_bound=self._upper_bound,\n        refine_factor=self._refine_factor,\n        vector_column=self._vector_column,\n        with_row_id=self._with_row_id,\n        offset=self._offset,\n        fast_search=self._fast_search,\n        ef=self._ef,\n        bypass_vector_index=self._bypass_vector_index,\n    )\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceVectorQueryBuilder.to_batches","title":"to_batches","text":"<pre><code>to_batches(batch_size: Optional[int] = None, *, timeout: Optional[timedelta] = None) -&gt; RecordBatchReader\n</code></pre> <p>Execute the query and return the result as a RecordBatchReader object.</p> <p>Parameters:</p> <ul> <li> <code>batch_size</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum number of selected records in a RecordBatch object.</p> </li> <li> <code>timeout</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum time to wait for the query to complete. If None, wait indefinitely.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RecordBatchReader</code>           \u2013            </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def to_batches(\n    self,\n    /,\n    batch_size: Optional[int] = None,\n    *,\n    timeout: Optional[timedelta] = None,\n) -&gt; pa.RecordBatchReader:\n    \"\"\"\n    Execute the query and return the result as a RecordBatchReader object.\n\n    Parameters\n    ----------\n    batch_size: int\n        The maximum number of selected records in a RecordBatch object.\n    timeout: timedelta, default None\n        The maximum time to wait for the query to complete.\n        If None, wait indefinitely.\n\n    Returns\n    -------\n    pa.RecordBatchReader\n    \"\"\"\n    vector = self._query if isinstance(self._query, list) else self._query.tolist()\n    if isinstance(vector[0], np.ndarray):\n        vector = [v.tolist() for v in vector]\n    query = self.to_query_object()\n    result_set = self._table._execute_query(\n        query, batch_size=batch_size, timeout=timeout\n    )\n    if self._reranker is not None:\n        rs_table = result_set.read_all()\n        result_set = self._reranker.rerank_vector(self._str_query, rs_table)\n        check_reranker_result(result_set)\n        # convert result_set back to RecordBatchReader\n        result_set = pa.RecordBatchReader.from_batches(\n            result_set.schema, result_set.to_batches()\n        )\n\n    return result_set\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceVectorQueryBuilder.where","title":"where","text":"<pre><code>where(where: str, prefilter: bool = None) -&gt; LanceVectorQueryBuilder\n</code></pre> <p>Set the where clause.</p> <p>Parameters:</p> <ul> <li> <code>where</code>               (<code>str</code>)           \u2013            <p>The where clause which is a valid SQL where clause. See <code>Lance filter pushdown &lt;https://lancedb.github.io/lance/read_and_write.html#filter-push-down&gt;</code>_ for valid SQL expressions.</p> </li> <li> <code>prefilter</code>               (<code>bool</code>, default:                   <code>None</code> )           \u2013            <p>If True, apply the filter before vector search, otherwise the filter is applied on the result of vector search.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LanceQueryBuilder</code>           \u2013            <p>The LanceQueryBuilder object.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def where(self, where: str, prefilter: bool = None) -&gt; LanceVectorQueryBuilder:\n    \"\"\"Set the where clause.\n\n    Parameters\n    ----------\n    where: str\n        The where clause which is a valid SQL where clause. See\n        `Lance filter pushdown &lt;https://lancedb.github.io/lance/read_and_write.html#filter-push-down&gt;`_\n        for valid SQL expressions.\n    prefilter: bool, default True\n        If True, apply the filter before vector search, otherwise the\n        filter is applied on the result of vector search.\n\n    Returns\n    -------\n    LanceQueryBuilder\n        The LanceQueryBuilder object.\n    \"\"\"\n    self._where = where\n    if prefilter is not None:\n        self._postfilter = not prefilter\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceVectorQueryBuilder.rerank","title":"rerank","text":"<pre><code>rerank(reranker: Reranker, query_string: Optional[str] = None) -&gt; LanceVectorQueryBuilder\n</code></pre> <p>Rerank the results using the specified reranker.</p> <p>Parameters:</p> <ul> <li> <code>reranker</code>               (<code>Reranker</code>)           \u2013            <p>The reranker to use.</p> </li> <li> <code>query_string</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The query to use for reranking. This needs to be specified explicitly here as the query used for vector search may already be vectorized and the reranker requires a string query. This is only required if the query used for vector search is not a string. Note: This doesn't yet support the case where the query is multimodal or a list of vectors.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LanceVectorQueryBuilder</code>           \u2013            <p>The LanceQueryBuilder object.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def rerank(\n    self, reranker: Reranker, query_string: Optional[str] = None\n) -&gt; LanceVectorQueryBuilder:\n    \"\"\"Rerank the results using the specified reranker.\n\n    Parameters\n    ----------\n    reranker: Reranker\n        The reranker to use.\n\n    query_string: Optional[str]\n        The query to use for reranking. This needs to be specified explicitly here\n        as the query used for vector search may already be vectorized and the\n        reranker requires a string query.\n        This is only required if the query used for vector search is not a string.\n        Note: This doesn't yet support the case where the query is multimodal or a\n        list of vectors.\n\n    Returns\n    -------\n    LanceVectorQueryBuilder\n        The LanceQueryBuilder object.\n    \"\"\"\n    self._reranker = reranker\n    if self._str_query is None and query_string is None:\n        raise ValueError(\n            \"\"\"\n            The query used for vector search is not a string.\n            In this case, the reranker query needs to be specified explicitly.\n            \"\"\"\n        )\n    if query_string is not None and not isinstance(query_string, str):\n        raise ValueError(\"Reranking currently only supports string queries\")\n    self._str_query = query_string if query_string is not None else self._str_query\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceVectorQueryBuilder.bypass_vector_index","title":"bypass_vector_index","text":"<pre><code>bypass_vector_index() -&gt; LanceVectorQueryBuilder\n</code></pre> <p>If this is called then any vector index is skipped</p> <p>An exhaustive (flat) search will be performed.  The query vector will be compared to every vector in the table.  At high scales this can be expensive.  However, this is often still useful.  For example, skipping the vector index can give you ground truth results which you can use to calculate your recall to select an appropriate value for nprobes.</p> <p>Returns:</p> <ul> <li> <code>LanceVectorQueryBuilder</code>           \u2013            <p>The LanceVectorQueryBuilder object.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def bypass_vector_index(self) -&gt; LanceVectorQueryBuilder:\n    \"\"\"\n    If this is called then any vector index is skipped\n\n    An exhaustive (flat) search will be performed.  The query vector will\n    be compared to every vector in the table.  At high scales this can be\n    expensive.  However, this is often still useful.  For example, skipping\n    the vector index can give you ground truth results which you can use to\n    calculate your recall to select an appropriate value for nprobes.\n\n    Returns\n    -------\n    LanceVectorQueryBuilder\n        The LanceVectorQueryBuilder object.\n    \"\"\"\n    self._bypass_vector_index = True\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceFtsQueryBuilder","title":"lancedb.query.LanceFtsQueryBuilder","text":"<p>               Bases: <code>LanceQueryBuilder</code></p> <p>A builder for full text search for LanceDB.</p> Source code in <code>lancedb/query.py</code> <pre><code>class LanceFtsQueryBuilder(LanceQueryBuilder):\n    \"\"\"A builder for full text search for LanceDB.\"\"\"\n\n    def __init__(\n        self,\n        table: \"Table\",\n        query: str | FullTextQuery,\n        ordering_field_name: Optional[str] = None,\n        fts_columns: Optional[Union[str, List[str]]] = None,\n    ):\n        super().__init__(table)\n        self._query = query\n        self._phrase_query = False\n        self.ordering_field_name = ordering_field_name\n        self._reranker = None\n        if isinstance(fts_columns, str):\n            fts_columns = [fts_columns]\n        self._fts_columns = fts_columns\n\n    def phrase_query(self, phrase_query: bool = True) -&gt; LanceFtsQueryBuilder:\n        \"\"\"Set whether to use phrase query.\n\n        Parameters\n        ----------\n        phrase_query: bool, default True\n            If True, then the query will be wrapped in quotes and\n            double quotes replaced by single quotes.\n\n        Returns\n        -------\n        LanceFtsQueryBuilder\n            The LanceFtsQueryBuilder object.\n        \"\"\"\n        self._phrase_query = phrase_query\n        return self\n\n    def to_query_object(self) -&gt; Query:\n        return Query(\n            columns=self._columns,\n            filter=self._where,\n            limit=self._limit,\n            postfilter=self._postfilter,\n            with_row_id=self._with_row_id,\n            full_text_query=FullTextSearchQuery(\n                query=self._query, columns=self._fts_columns\n            ),\n            offset=self._offset,\n        )\n\n    def to_arrow(self, *, timeout: Optional[timedelta] = None) -&gt; pa.Table:\n        path, fs, exist = self._table._get_fts_index_path()\n        if exist:\n            return self.tantivy_to_arrow()\n\n        query = self._query\n        if self._phrase_query:\n            raise NotImplementedError(\n                \"Phrase query is not yet supported in Lance FTS. \"\n                \"Use tantivy-based index instead for now.\"\n            )\n        query = self.to_query_object()\n        results = self._table._execute_query(query, timeout=timeout)\n        results = results.read_all()\n        if self._reranker is not None:\n            results = self._reranker.rerank_fts(self._query, results)\n            check_reranker_result(results)\n        return results\n\n    def to_batches(\n        self, /, batch_size: Optional[int] = None, timeout: Optional[timedelta] = None\n    ):\n        raise NotImplementedError(\"to_batches on an FTS query\")\n\n    def tantivy_to_arrow(self) -&gt; pa.Table:\n        try:\n            import tantivy\n        except ImportError:\n            raise ImportError(\n                \"Please install tantivy-py `pip install tantivy` to use the full text search feature.\"  # noqa: E501\n            )\n\n        from .fts import search_index\n\n        # get the index path\n        path, fs, exist = self._table._get_fts_index_path()\n\n        # check if the index exist\n        if not exist:\n            raise FileNotFoundError(\n                \"Fts index does not exist. \"\n                \"Please first call table.create_fts_index(['&lt;field_names&gt;']) to \"\n                \"create the fts index.\"\n            )\n\n        # Check that we are on local filesystem\n        if not isinstance(fs, pa_fs.LocalFileSystem):\n            raise NotImplementedError(\n                \"Tantivy-based full text search \"\n                \"is only supported on the local filesystem\"\n            )\n        # open the index\n        index = tantivy.Index.open(path)\n        # get the scores and doc ids\n        query = self._query\n        if self._phrase_query:\n            query = query.replace('\"', \"'\")\n            query = f'\"{query}\"'\n        limit = self._limit if self._limit is not None else 10\n        row_ids, scores = search_index(\n            index, query, limit, ordering_field=self.ordering_field_name\n        )\n        if len(row_ids) == 0:\n            empty_schema = pa.schema([pa.field(\"_score\", pa.float32())])\n            return pa.Table.from_batches([], schema=empty_schema)\n        scores = pa.array(scores)\n        output_tbl = self._table.to_lance().take(row_ids, columns=self._columns)\n        output_tbl = output_tbl.append_column(\"_score\", scores)\n        # this needs to match vector search results which are uint64\n        row_ids = pa.array(row_ids, type=pa.uint64())\n\n        if self._where is not None:\n            tmp_name = \"__lancedb__duckdb__indexer__\"\n            output_tbl = output_tbl.append_column(\n                tmp_name, pa.array(range(len(output_tbl)))\n            )\n            try:\n                # TODO would be great to have Substrait generate pyarrow compute\n                # expressions or conversely have pyarrow support SQL expressions\n                # using Substrait\n                import duckdb\n\n                indexer = duckdb.sql(\n                    f\"SELECT {tmp_name} FROM output_tbl WHERE {self._where}\"\n                ).to_arrow_table()[tmp_name]\n                output_tbl = output_tbl.take(indexer).drop([tmp_name])\n                row_ids = row_ids.take(indexer)\n\n            except ImportError:\n                import tempfile\n\n                import lance\n\n                # TODO Use \"memory://\" instead once that's supported\n                with tempfile.TemporaryDirectory() as tmp:\n                    ds = lance.write_dataset(output_tbl, tmp)\n                    output_tbl = ds.to_table(filter=self._where)\n                    indexer = output_tbl[tmp_name]\n                    row_ids = row_ids.take(indexer)\n                    output_tbl = output_tbl.drop([tmp_name])\n\n        if self._with_row_id:\n            output_tbl = output_tbl.append_column(\"_rowid\", row_ids)\n\n        if self._reranker is not None:\n            output_tbl = self._reranker.rerank_fts(self._query, output_tbl)\n        return output_tbl\n\n    def rerank(self, reranker: Reranker) -&gt; LanceFtsQueryBuilder:\n        \"\"\"Rerank the results using the specified reranker.\n\n        Parameters\n        ----------\n        reranker: Reranker\n            The reranker to use.\n\n        Returns\n        -------\n        LanceFtsQueryBuilder\n            The LanceQueryBuilder object.\n        \"\"\"\n        self._reranker = reranker\n        return self\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceFtsQueryBuilder.phrase_query","title":"phrase_query","text":"<pre><code>phrase_query(phrase_query: bool = True) -&gt; LanceFtsQueryBuilder\n</code></pre> <p>Set whether to use phrase query.</p> <p>Parameters:</p> <ul> <li> <code>phrase_query</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, then the query will be wrapped in quotes and double quotes replaced by single quotes.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LanceFtsQueryBuilder</code>           \u2013            <p>The LanceFtsQueryBuilder object.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def phrase_query(self, phrase_query: bool = True) -&gt; LanceFtsQueryBuilder:\n    \"\"\"Set whether to use phrase query.\n\n    Parameters\n    ----------\n    phrase_query: bool, default True\n        If True, then the query will be wrapped in quotes and\n        double quotes replaced by single quotes.\n\n    Returns\n    -------\n    LanceFtsQueryBuilder\n        The LanceFtsQueryBuilder object.\n    \"\"\"\n    self._phrase_query = phrase_query\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceFtsQueryBuilder.rerank","title":"rerank","text":"<pre><code>rerank(reranker: Reranker) -&gt; LanceFtsQueryBuilder\n</code></pre> <p>Rerank the results using the specified reranker.</p> <p>Parameters:</p> <ul> <li> <code>reranker</code>               (<code>Reranker</code>)           \u2013            <p>The reranker to use.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LanceFtsQueryBuilder</code>           \u2013            <p>The LanceQueryBuilder object.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def rerank(self, reranker: Reranker) -&gt; LanceFtsQueryBuilder:\n    \"\"\"Rerank the results using the specified reranker.\n\n    Parameters\n    ----------\n    reranker: Reranker\n        The reranker to use.\n\n    Returns\n    -------\n    LanceFtsQueryBuilder\n        The LanceQueryBuilder object.\n    \"\"\"\n    self._reranker = reranker\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceHybridQueryBuilder","title":"lancedb.query.LanceHybridQueryBuilder","text":"<p>               Bases: <code>LanceQueryBuilder</code></p> <p>A query builder that performs hybrid vector and full text search. Results are combined and reranked based on the specified reranker. By default, the results are reranked using the RRFReranker, which uses reciprocal rank fusion score for reranking.</p> <p>To make the vector and fts results comparable, the scores are normalized. Instead of normalizing scores, the <code>normalize</code> parameter can be set to \"rank\" in the <code>rerank</code> method to convert the scores to ranks and then normalize them.</p> Source code in <code>lancedb/query.py</code> <pre><code>class LanceHybridQueryBuilder(LanceQueryBuilder):\n    \"\"\"\n    A query builder that performs hybrid vector and full text search.\n    Results are combined and reranked based on the specified reranker.\n    By default, the results are reranked using the RRFReranker, which\n    uses reciprocal rank fusion score for reranking.\n\n    To make the vector and fts results comparable, the scores are normalized.\n    Instead of normalizing scores, the `normalize` parameter can be set to \"rank\"\n    in the `rerank` method to convert the scores to ranks and then normalize them.\n    \"\"\"\n\n    def __init__(\n        self,\n        table: \"Table\",\n        query: Optional[Union[str, FullTextQuery]] = None,\n        vector_column: Optional[str] = None,\n        fts_columns: Optional[Union[str, List[str]]] = None,\n    ):\n        super().__init__(table)\n        self._query = query\n        self._vector_column = vector_column\n        self._fts_columns = fts_columns\n        self._norm = None\n        self._reranker = None\n        self._nprobes = None\n        self._refine_factor = None\n        self._distance_type = None\n        self._phrase_query = None\n        self._lower_bound = None\n        self._upper_bound = None\n\n    def _validate_query(self, query, vector=None, text=None):\n        if query is not None and (vector is not None or text is not None):\n            raise ValueError(\n                \"You can either provide a string query in search() method\"\n                \"or set `vector()` and `text()` explicitly for hybrid search.\"\n                \"But not both.\"\n            )\n\n        vector_query = vector if vector is not None else query\n        if not isinstance(vector_query, (str, list, np.ndarray)):\n            raise ValueError(\"Vector query must be either a string or a vector\")\n\n        text_query = text or query\n        if text_query is None:\n            raise ValueError(\"Text query must be provided for hybrid search.\")\n        if not isinstance(text_query, (str, FullTextQuery)):\n            raise ValueError(\"Text query must be a string or FullTextQuery\")\n\n        return vector_query, text_query\n\n    def phrase_query(self, phrase_query: bool = None) -&gt; LanceHybridQueryBuilder:\n        \"\"\"Set whether to use phrase query.\n\n        Parameters\n        ----------\n        phrase_query: bool, default True\n            If True, then the query will be wrapped in quotes and\n            double quotes replaced by single quotes.\n\n        Returns\n        -------\n        LanceHybridQueryBuilder\n            The LanceHybridQueryBuilder object.\n        \"\"\"\n        self._phrase_query = phrase_query\n        return self\n\n    def to_query_object(self) -&gt; Query:\n        raise NotImplementedError(\"to_query_object not yet supported on a hybrid query\")\n\n    def to_arrow(self, *, timeout: Optional[timedelta] = None) -&gt; pa.Table:\n        vector_query, fts_query = self._validate_query(\n            self._query, self._vector, self._text\n        )\n        self._fts_query = LanceFtsQueryBuilder(\n            self._table, fts_query, fts_columns=self._fts_columns\n        )\n        vector_query = self._query_to_vector(\n            self._table, vector_query, self._vector_column\n        )\n        self._vector_query = LanceVectorQueryBuilder(\n            self._table, vector_query, self._vector_column\n        )\n\n        if self._limit:\n            self._vector_query.limit(self._limit)\n            self._fts_query.limit(self._limit)\n        if self._columns:\n            self._vector_query.select(self._columns)\n            self._fts_query.select(self._columns)\n        if self._where:\n            self._vector_query.where(self._where, self._postfilter)\n            self._fts_query.where(self._where, self._postfilter)\n        if self._with_row_id:\n            self._vector_query.with_row_id(True)\n            self._fts_query.with_row_id(True)\n        if self._phrase_query:\n            self._fts_query.phrase_query(True)\n        if self._distance_type:\n            self._vector_query.metric(self._distance_type)\n        if self._nprobes:\n            self._vector_query.nprobes(self._nprobes)\n        if self._refine_factor:\n            self._vector_query.refine_factor(self._refine_factor)\n        if self._ef:\n            self._vector_query.ef(self._ef)\n        if self._bypass_vector_index:\n            self._vector_query.bypass_vector_index()\n        if self._lower_bound or self._upper_bound:\n            self._vector_query.distance_range(\n                lower_bound=self._lower_bound, upper_bound=self._upper_bound\n            )\n\n        if self._reranker is None:\n            self._reranker = RRFReranker()\n\n        with ThreadPoolExecutor() as executor:\n            fts_future = executor.submit(\n                self._fts_query.with_row_id(True).to_arrow, timeout=timeout\n            )\n            vector_future = executor.submit(\n                self._vector_query.with_row_id(True).to_arrow, timeout=timeout\n            )\n            fts_results = fts_future.result()\n            vector_results = vector_future.result()\n\n        return self._combine_hybrid_results(\n            fts_results=fts_results,\n            vector_results=vector_results,\n            norm=self._norm,\n            fts_query=self._fts_query._query,\n            reranker=self._reranker,\n            limit=self._limit,\n            with_row_ids=self._with_row_id,\n        )\n\n    @staticmethod\n    def _combine_hybrid_results(\n        fts_results: pa.Table,\n        vector_results: pa.Table,\n        norm: str,\n        fts_query: str,\n        reranker,\n        limit: int,\n        with_row_ids: bool,\n    ) -&gt; pa.Table:\n        if norm == \"rank\":\n            vector_results = LanceHybridQueryBuilder._rank(vector_results, \"_distance\")\n            fts_results = LanceHybridQueryBuilder._rank(fts_results, \"_score\")\n\n        original_distances = None\n        original_scores = None\n        original_distance_row_ids = None\n        original_score_row_ids = None\n        # normalize the scores to be between 0 and 1, 0 being most relevant\n        # We check whether the results (vector and FTS) are empty, because when\n        # they are, they often are missing the _rowid column, which causes an error\n        if vector_results.num_rows &gt; 0:\n            distance_i = vector_results.column_names.index(\"_distance\")\n            original_distances = vector_results.column(distance_i)\n            original_distance_row_ids = vector_results.column(\"_rowid\")\n            vector_results = vector_results.set_column(\n                distance_i,\n                vector_results.field(distance_i),\n                LanceHybridQueryBuilder._normalize_scores(original_distances),\n            )\n\n        # In fts higher scores represent relevance. Not inverting them here as\n        # rerankers might need to preserve this score to support `return_score=\"all\"`\n        if fts_results.num_rows &gt; 0:\n            score_i = fts_results.column_names.index(\"_score\")\n            original_scores = fts_results.column(score_i)\n            original_score_row_ids = fts_results.column(\"_rowid\")\n            fts_results = fts_results.set_column(\n                score_i,\n                fts_results.field(score_i),\n                LanceHybridQueryBuilder._normalize_scores(original_scores),\n            )\n\n        results = reranker.rerank_hybrid(fts_query, vector_results, fts_results)\n\n        check_reranker_result(results)\n\n        if \"_distance\" in results.column_names and original_distances is not None:\n            # restore the original distances\n            indices = pc.index_in(\n                results[\"_rowid\"], original_distance_row_ids, skip_nulls=True\n            )\n            original_distances = pc.take(original_distances, indices)\n            distance_i = results.column_names.index(\"_distance\")\n            results = results.set_column(distance_i, \"_distance\", original_distances)\n\n        if \"_score\" in results.column_names and original_scores is not None:\n            # restore the original scores\n            indices = pc.index_in(\n                results[\"_rowid\"], original_score_row_ids, skip_nulls=True\n            )\n            original_scores = pc.take(original_scores, indices)\n            score_i = results.column_names.index(\"_score\")\n            results = results.set_column(score_i, \"_score\", original_scores)\n\n        results = results.slice(length=limit)\n\n        if not with_row_ids:\n            results = results.drop([\"_rowid\"])\n\n        return results\n\n    def to_batches(\n        self, /, batch_size: Optional[int] = None, timeout: Optional[timedelta] = None\n    ):\n        raise NotImplementedError(\"to_batches not yet supported on a hybrid query\")\n\n    @staticmethod\n    def _rank(results: pa.Table, column: str, ascending: bool = True):\n        if len(results) == 0:\n            return results\n        # Get the _score column from results\n        scores = results.column(column).to_numpy()\n        sort_indices = np.argsort(scores)\n        if not ascending:\n            sort_indices = sort_indices[::-1]\n        ranks = np.empty_like(sort_indices)\n        ranks[sort_indices] = np.arange(len(scores)) + 1\n        # replace the _score column with the ranks\n        _score_idx = results.column_names.index(column)\n        results = results.set_column(\n            _score_idx, column, pa.array(ranks, type=pa.float32())\n        )\n        return results\n\n    @staticmethod\n    def _normalize_scores(scores: pa.Array, invert=False) -&gt; pa.Array:\n        if len(scores) == 0:\n            return scores\n        # normalize the scores by subtracting the min and dividing by the max\n        min, max = pc.min_max(scores).values()\n        rng = pc.subtract(max, min)\n\n        if not pc.equal(rng, pa.scalar(0.0)).as_py():\n            scores = pc.divide(pc.subtract(scores, min), rng)\n        elif not pc.equal(max, pa.scalar(0.0)).as_py():\n            # If rng is 0, then we at least want the scores to be 0\n            scores = pc.subtract(scores, min)\n\n        if invert:\n            scores = pc.subtract(1, scores)\n\n        return scores\n\n    def rerank(\n        self,\n        reranker: Reranker = RRFReranker(),\n        normalize: str = \"score\",\n    ) -&gt; LanceHybridQueryBuilder:\n        \"\"\"\n        Rerank the hybrid search results using the specified reranker. The reranker\n        must be an instance of Reranker class.\n\n        Parameters\n        ----------\n        reranker: Reranker, default RRFReranker()\n            The reranker to use. Must be an instance of Reranker class.\n        normalize: str, default \"score\"\n            The method to normalize the scores. Can be \"rank\" or \"score\". If \"rank\",\n            the scores are converted to ranks and then normalized. If \"score\", the\n            scores are normalized directly.\n        Returns\n        -------\n        LanceHybridQueryBuilder\n            The LanceHybridQueryBuilder object.\n        \"\"\"\n        if normalize not in [\"rank\", \"score\"]:\n            raise ValueError(\"normalize must be 'rank' or 'score'.\")\n        if reranker and not isinstance(reranker, Reranker):\n            raise ValueError(\"reranker must be an instance of Reranker class.\")\n\n        self._norm = normalize\n        self._reranker = reranker\n\n        return self\n\n    def nprobes(self, nprobes: int) -&gt; LanceHybridQueryBuilder:\n        \"\"\"\n        Set the number of probes to use for vector search.\n\n        Higher values will yield better recall (more likely to find vectors if\n        they exist) at the expense of latency.\n\n        Parameters\n        ----------\n        nprobes: int\n            The number of probes to use.\n\n        Returns\n        -------\n        LanceHybridQueryBuilder\n            The LanceHybridQueryBuilder object.\n        \"\"\"\n        self._nprobes = nprobes\n        return self\n\n    def distance_range(\n        self, lower_bound: Optional[float] = None, upper_bound: Optional[float] = None\n    ) -&gt; LanceHybridQueryBuilder:\n        \"\"\"\n        Set the distance range to use.\n\n        Only rows with distances within range [lower_bound, upper_bound)\n        will be returned.\n\n        Parameters\n        ----------\n        lower_bound: Optional[float]\n            The lower bound of the distance range.\n        upper_bound: Optional[float]\n            The upper bound of the distance range.\n\n        Returns\n        -------\n        LanceHybridQueryBuilder\n            The LanceHybridQueryBuilder object.\n        \"\"\"\n        self._lower_bound = lower_bound\n        self._upper_bound = upper_bound\n        return self\n\n    def ef(self, ef: int) -&gt; LanceHybridQueryBuilder:\n        \"\"\"\n        Set the number of candidates to consider during search.\n\n        Higher values will yield better recall (more likely to find vectors if\n        they exist) at the expense of latency.\n\n        This only applies to the HNSW-related index.\n        The default value is 1.5 * limit.\n\n        Parameters\n        ----------\n        ef: int\n            The number of candidates to consider during search.\n\n        Returns\n        -------\n        LanceHybridQueryBuilder\n            The LanceHybridQueryBuilder object.\n        \"\"\"\n        self._ef = ef\n        return self\n\n    def metric(self, metric: Literal[\"l2\", \"cosine\", \"dot\"]) -&gt; LanceHybridQueryBuilder:\n        \"\"\"Set the distance metric to use.\n\n        This is an alias for distance_type() and may be deprecated in the future.\n        Please use distance_type() instead.\n\n        Parameters\n        ----------\n        metric: \"l2\" or \"cosine\" or \"dot\"\n            The distance metric to use. By default \"l2\" is used.\n\n        Returns\n        -------\n        LanceVectorQueryBuilder\n            The LanceQueryBuilder object.\n        \"\"\"\n        return self.distance_type(metric)\n\n    def distance_type(\n        self, distance_type: Literal[\"l2\", \"cosine\", \"dot\"]\n    ) -&gt; \"LanceHybridQueryBuilder\":\n        \"\"\"Set the distance metric to use.\n\n        When performing a vector search we try and find the \"nearest\" vectors according\n        to some kind of distance metric. This parameter controls which distance metric\n        to use.\n\n        Note: if there is a vector index then the distance type used MUST match the\n        distance type used to train the vector index. If this is not done then the\n        results will be invalid.\n\n        Parameters\n        ----------\n        distance_type: \"l2\" or \"cosine\" or \"dot\"\n            The distance metric to use. By default \"l2\" is used.\n\n        Returns\n        -------\n        LanceVectorQueryBuilder\n            The LanceQueryBuilder object.\n        \"\"\"\n        self._distance_type = distance_type.lower()\n        return self\n\n    def refine_factor(self, refine_factor: int) -&gt; LanceHybridQueryBuilder:\n        \"\"\"\n        Refine the vector search results by reading extra elements and\n        re-ranking them in memory.\n\n        Parameters\n        ----------\n        refine_factor: int\n            The refine factor to use.\n\n        Returns\n        -------\n        LanceHybridQueryBuilder\n            The LanceHybridQueryBuilder object.\n        \"\"\"\n        self._refine_factor = refine_factor\n        return self\n\n    def vector(self, vector: Union[np.ndarray, list]) -&gt; LanceHybridQueryBuilder:\n        self._vector = vector\n        return self\n\n    def text(self, text: str | FullTextQuery) -&gt; LanceHybridQueryBuilder:\n        self._text = text\n        return self\n\n    def bypass_vector_index(self) -&gt; LanceHybridQueryBuilder:\n        \"\"\"\n        If this is called then any vector index is skipped\n\n        An exhaustive (flat) search will be performed.  The query vector will\n        be compared to every vector in the table.  At high scales this can be\n        expensive.  However, this is often still useful.  For example, skipping\n        the vector index can give you ground truth results which you can use to\n        calculate your recall to select an appropriate value for nprobes.\n\n        Returns\n        -------\n        LanceHybridQueryBuilder\n            The LanceHybridQueryBuilder object.\n        \"\"\"\n        self._bypass_vector_index = True\n        return self\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceHybridQueryBuilder.phrase_query","title":"phrase_query","text":"<pre><code>phrase_query(phrase_query: bool = None) -&gt; LanceHybridQueryBuilder\n</code></pre> <p>Set whether to use phrase query.</p> <p>Parameters:</p> <ul> <li> <code>phrase_query</code>               (<code>bool</code>, default:                   <code>None</code> )           \u2013            <p>If True, then the query will be wrapped in quotes and double quotes replaced by single quotes.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LanceHybridQueryBuilder</code>           \u2013            <p>The LanceHybridQueryBuilder object.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def phrase_query(self, phrase_query: bool = None) -&gt; LanceHybridQueryBuilder:\n    \"\"\"Set whether to use phrase query.\n\n    Parameters\n    ----------\n    phrase_query: bool, default True\n        If True, then the query will be wrapped in quotes and\n        double quotes replaced by single quotes.\n\n    Returns\n    -------\n    LanceHybridQueryBuilder\n        The LanceHybridQueryBuilder object.\n    \"\"\"\n    self._phrase_query = phrase_query\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceHybridQueryBuilder.rerank","title":"rerank","text":"<pre><code>rerank(reranker: Reranker = RRFReranker(), normalize: str = 'score') -&gt; LanceHybridQueryBuilder\n</code></pre> <p>Rerank the hybrid search results using the specified reranker. The reranker must be an instance of Reranker class.</p> <p>Parameters:</p> <ul> <li> <code>reranker</code>               (<code>Reranker</code>, default:                   <code>RRFReranker()</code> )           \u2013            <p>The reranker to use. Must be an instance of Reranker class.</p> </li> <li> <code>normalize</code>               (<code>str</code>, default:                   <code>'score'</code> )           \u2013            <p>The method to normalize the scores. Can be \"rank\" or \"score\". If \"rank\", the scores are converted to ranks and then normalized. If \"score\", the scores are normalized directly.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LanceHybridQueryBuilder</code>           \u2013            <p>The LanceHybridQueryBuilder object.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def rerank(\n    self,\n    reranker: Reranker = RRFReranker(),\n    normalize: str = \"score\",\n) -&gt; LanceHybridQueryBuilder:\n    \"\"\"\n    Rerank the hybrid search results using the specified reranker. The reranker\n    must be an instance of Reranker class.\n\n    Parameters\n    ----------\n    reranker: Reranker, default RRFReranker()\n        The reranker to use. Must be an instance of Reranker class.\n    normalize: str, default \"score\"\n        The method to normalize the scores. Can be \"rank\" or \"score\". If \"rank\",\n        the scores are converted to ranks and then normalized. If \"score\", the\n        scores are normalized directly.\n    Returns\n    -------\n    LanceHybridQueryBuilder\n        The LanceHybridQueryBuilder object.\n    \"\"\"\n    if normalize not in [\"rank\", \"score\"]:\n        raise ValueError(\"normalize must be 'rank' or 'score'.\")\n    if reranker and not isinstance(reranker, Reranker):\n        raise ValueError(\"reranker must be an instance of Reranker class.\")\n\n    self._norm = normalize\n    self._reranker = reranker\n\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceHybridQueryBuilder.nprobes","title":"nprobes","text":"<pre><code>nprobes(nprobes: int) -&gt; LanceHybridQueryBuilder\n</code></pre> <p>Set the number of probes to use for vector search.</p> <p>Higher values will yield better recall (more likely to find vectors if they exist) at the expense of latency.</p> <p>Parameters:</p> <ul> <li> <code>nprobes</code>               (<code>int</code>)           \u2013            <p>The number of probes to use.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LanceHybridQueryBuilder</code>           \u2013            <p>The LanceHybridQueryBuilder object.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def nprobes(self, nprobes: int) -&gt; LanceHybridQueryBuilder:\n    \"\"\"\n    Set the number of probes to use for vector search.\n\n    Higher values will yield better recall (more likely to find vectors if\n    they exist) at the expense of latency.\n\n    Parameters\n    ----------\n    nprobes: int\n        The number of probes to use.\n\n    Returns\n    -------\n    LanceHybridQueryBuilder\n        The LanceHybridQueryBuilder object.\n    \"\"\"\n    self._nprobes = nprobes\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceHybridQueryBuilder.distance_range","title":"distance_range","text":"<pre><code>distance_range(lower_bound: Optional[float] = None, upper_bound: Optional[float] = None) -&gt; LanceHybridQueryBuilder\n</code></pre> <p>Set the distance range to use.</p> <p>Only rows with distances within range [lower_bound, upper_bound) will be returned.</p> <p>Parameters:</p> <ul> <li> <code>lower_bound</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>The lower bound of the distance range.</p> </li> <li> <code>upper_bound</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>The upper bound of the distance range.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LanceHybridQueryBuilder</code>           \u2013            <p>The LanceHybridQueryBuilder object.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def distance_range(\n    self, lower_bound: Optional[float] = None, upper_bound: Optional[float] = None\n) -&gt; LanceHybridQueryBuilder:\n    \"\"\"\n    Set the distance range to use.\n\n    Only rows with distances within range [lower_bound, upper_bound)\n    will be returned.\n\n    Parameters\n    ----------\n    lower_bound: Optional[float]\n        The lower bound of the distance range.\n    upper_bound: Optional[float]\n        The upper bound of the distance range.\n\n    Returns\n    -------\n    LanceHybridQueryBuilder\n        The LanceHybridQueryBuilder object.\n    \"\"\"\n    self._lower_bound = lower_bound\n    self._upper_bound = upper_bound\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceHybridQueryBuilder.ef","title":"ef","text":"<pre><code>ef(ef: int) -&gt; LanceHybridQueryBuilder\n</code></pre> <p>Set the number of candidates to consider during search.</p> <p>Higher values will yield better recall (more likely to find vectors if they exist) at the expense of latency.</p> <p>This only applies to the HNSW-related index. The default value is 1.5 * limit.</p> <p>Parameters:</p> <ul> <li> <code>ef</code>               (<code>int</code>)           \u2013            <p>The number of candidates to consider during search.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LanceHybridQueryBuilder</code>           \u2013            <p>The LanceHybridQueryBuilder object.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def ef(self, ef: int) -&gt; LanceHybridQueryBuilder:\n    \"\"\"\n    Set the number of candidates to consider during search.\n\n    Higher values will yield better recall (more likely to find vectors if\n    they exist) at the expense of latency.\n\n    This only applies to the HNSW-related index.\n    The default value is 1.5 * limit.\n\n    Parameters\n    ----------\n    ef: int\n        The number of candidates to consider during search.\n\n    Returns\n    -------\n    LanceHybridQueryBuilder\n        The LanceHybridQueryBuilder object.\n    \"\"\"\n    self._ef = ef\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceHybridQueryBuilder.metric","title":"metric","text":"<pre><code>metric(metric: Literal['l2', 'cosine', 'dot']) -&gt; LanceHybridQueryBuilder\n</code></pre> <p>Set the distance metric to use.</p> <p>This is an alias for distance_type() and may be deprecated in the future. Please use distance_type() instead.</p> <p>Parameters:</p> <ul> <li> <code>metric</code>               (<code>Literal['l2', 'cosine', 'dot']</code>)           \u2013            <p>The distance metric to use. By default \"l2\" is used.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LanceVectorQueryBuilder</code>           \u2013            <p>The LanceQueryBuilder object.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def metric(self, metric: Literal[\"l2\", \"cosine\", \"dot\"]) -&gt; LanceHybridQueryBuilder:\n    \"\"\"Set the distance metric to use.\n\n    This is an alias for distance_type() and may be deprecated in the future.\n    Please use distance_type() instead.\n\n    Parameters\n    ----------\n    metric: \"l2\" or \"cosine\" or \"dot\"\n        The distance metric to use. By default \"l2\" is used.\n\n    Returns\n    -------\n    LanceVectorQueryBuilder\n        The LanceQueryBuilder object.\n    \"\"\"\n    return self.distance_type(metric)\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceHybridQueryBuilder.distance_type","title":"distance_type","text":"<pre><code>distance_type(distance_type: Literal['l2', 'cosine', 'dot']) -&gt; 'LanceHybridQueryBuilder'\n</code></pre> <p>Set the distance metric to use.</p> <p>When performing a vector search we try and find the \"nearest\" vectors according to some kind of distance metric. This parameter controls which distance metric to use.</p> <p>Note: if there is a vector index then the distance type used MUST match the distance type used to train the vector index. If this is not done then the results will be invalid.</p> <p>Parameters:</p> <ul> <li> <code>distance_type</code>               (<code>Literal['l2', 'cosine', 'dot']</code>)           \u2013            <p>The distance metric to use. By default \"l2\" is used.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LanceVectorQueryBuilder</code>           \u2013            <p>The LanceQueryBuilder object.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def distance_type(\n    self, distance_type: Literal[\"l2\", \"cosine\", \"dot\"]\n) -&gt; \"LanceHybridQueryBuilder\":\n    \"\"\"Set the distance metric to use.\n\n    When performing a vector search we try and find the \"nearest\" vectors according\n    to some kind of distance metric. This parameter controls which distance metric\n    to use.\n\n    Note: if there is a vector index then the distance type used MUST match the\n    distance type used to train the vector index. If this is not done then the\n    results will be invalid.\n\n    Parameters\n    ----------\n    distance_type: \"l2\" or \"cosine\" or \"dot\"\n        The distance metric to use. By default \"l2\" is used.\n\n    Returns\n    -------\n    LanceVectorQueryBuilder\n        The LanceQueryBuilder object.\n    \"\"\"\n    self._distance_type = distance_type.lower()\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceHybridQueryBuilder.refine_factor","title":"refine_factor","text":"<pre><code>refine_factor(refine_factor: int) -&gt; LanceHybridQueryBuilder\n</code></pre> <p>Refine the vector search results by reading extra elements and re-ranking them in memory.</p> <p>Parameters:</p> <ul> <li> <code>refine_factor</code>               (<code>int</code>)           \u2013            <p>The refine factor to use.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LanceHybridQueryBuilder</code>           \u2013            <p>The LanceHybridQueryBuilder object.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def refine_factor(self, refine_factor: int) -&gt; LanceHybridQueryBuilder:\n    \"\"\"\n    Refine the vector search results by reading extra elements and\n    re-ranking them in memory.\n\n    Parameters\n    ----------\n    refine_factor: int\n        The refine factor to use.\n\n    Returns\n    -------\n    LanceHybridQueryBuilder\n        The LanceHybridQueryBuilder object.\n    \"\"\"\n    self._refine_factor = refine_factor\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.LanceHybridQueryBuilder.bypass_vector_index","title":"bypass_vector_index","text":"<pre><code>bypass_vector_index() -&gt; LanceHybridQueryBuilder\n</code></pre> <p>If this is called then any vector index is skipped</p> <p>An exhaustive (flat) search will be performed.  The query vector will be compared to every vector in the table.  At high scales this can be expensive.  However, this is often still useful.  For example, skipping the vector index can give you ground truth results which you can use to calculate your recall to select an appropriate value for nprobes.</p> <p>Returns:</p> <ul> <li> <code>LanceHybridQueryBuilder</code>           \u2013            <p>The LanceHybridQueryBuilder object.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def bypass_vector_index(self) -&gt; LanceHybridQueryBuilder:\n    \"\"\"\n    If this is called then any vector index is skipped\n\n    An exhaustive (flat) search will be performed.  The query vector will\n    be compared to every vector in the table.  At high scales this can be\n    expensive.  However, this is often still useful.  For example, skipping\n    the vector index can give you ground truth results which you can use to\n    calculate your recall to select an appropriate value for nprobes.\n\n    Returns\n    -------\n    LanceHybridQueryBuilder\n        The LanceHybridQueryBuilder object.\n    \"\"\"\n    self._bypass_vector_index = True\n    return self\n</code></pre>"},{"location":"python/python/#embeddings","title":"Embeddings","text":""},{"location":"python/python/#lancedb.embeddings.registry.EmbeddingFunctionRegistry","title":"lancedb.embeddings.registry.EmbeddingFunctionRegistry","text":"<p>This is a singleton class used to register embedding functions and fetch them by name. It also handles serializing and deserializing. You can implement your own embedding function by subclassing EmbeddingFunction or TextEmbeddingFunction and registering it with the registry.</p> <p>NOTE: Here TEXT is a type alias for Union[str, List[str], pa.Array,       pa.ChunkedArray, np.ndarray]</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; registry = EmbeddingFunctionRegistry.get_instance()\n&gt;&gt;&gt; @registry.register(\"my-embedding-function\")\n... class MyEmbeddingFunction(EmbeddingFunction):\n...     def ndims(self) -&gt; int:\n...         return 128\n...\n...     def compute_query_embeddings(self, query: str, *args, **kwargs):\n...         return self.compute_source_embeddings(query, *args, **kwargs)\n...\n...     def compute_source_embeddings(self, texts, *args, **kwargs):\n...         return [np.random.rand(self.ndims()) for _ in range(len(texts))]\n...\n&gt;&gt;&gt; registry.get(\"my-embedding-function\")\n&lt;class 'lancedb.embeddings.registry.MyEmbeddingFunction'&gt;\n</code></pre> Source code in <code>lancedb/embeddings/registry.py</code> <pre><code>class EmbeddingFunctionRegistry:\n    \"\"\"\n    This is a singleton class used to register embedding functions\n    and fetch them by name. It also handles serializing and deserializing.\n    You can implement your own embedding function by subclassing EmbeddingFunction\n    or TextEmbeddingFunction and registering it with the registry.\n\n    NOTE: Here TEXT is a type alias for Union[str, List[str], pa.Array,\n          pa.ChunkedArray, np.ndarray]\n\n    Examples\n    --------\n    &gt;&gt;&gt; registry = EmbeddingFunctionRegistry.get_instance()\n    &gt;&gt;&gt; @registry.register(\"my-embedding-function\")\n    ... class MyEmbeddingFunction(EmbeddingFunction):\n    ...     def ndims(self) -&gt; int:\n    ...         return 128\n    ...\n    ...     def compute_query_embeddings(self, query: str, *args, **kwargs):\n    ...         return self.compute_source_embeddings(query, *args, **kwargs)\n    ...\n    ...     def compute_source_embeddings(self, texts, *args, **kwargs):\n    ...         return [np.random.rand(self.ndims()) for _ in range(len(texts))]\n    ...\n    &gt;&gt;&gt; registry.get(\"my-embedding-function\")\n    &lt;class 'lancedb.embeddings.registry.MyEmbeddingFunction'&gt;\n    \"\"\"\n\n    @classmethod\n    def get_instance(cls):\n        return __REGISTRY__\n\n    def __init__(self):\n        self._functions = {}\n        self._variables = {}\n\n    def register(self, alias: str = None):\n        \"\"\"\n        This creates a decorator that can be used to register\n        an EmbeddingFunction.\n\n        Parameters\n        ----------\n        alias : Optional[str]\n            a human friendly name for the embedding function. If not\n            provided, the class name will be used.\n        \"\"\"\n\n        # This is a decorator for a class that inherits from BaseModel\n        # It adds the class to the registry\n        def decorator(cls):\n            if not issubclass(cls, EmbeddingFunction):\n                raise TypeError(\"Must be a subclass of EmbeddingFunction\")\n            if cls.__name__ in self._functions:\n                raise KeyError(f\"{cls.__name__} was already registered\")\n            key = alias or cls.__name__\n            self._functions[key] = cls\n            cls.__embedding_function_registry_alias__ = alias\n            return cls\n\n        return decorator\n\n    def reset(self):\n        \"\"\"\n        Reset the registry to its initial state\n        \"\"\"\n        self._functions = {}\n\n    def get(self, name: str):\n        \"\"\"\n        Fetch an embedding function class by name\n\n        Parameters\n        ----------\n        name : str\n            The name of the embedding function to fetch\n            Either the alias or the class name if no alias was provided\n            during registration\n        \"\"\"\n        return self._functions[name]\n\n    def parse_functions(\n        self, metadata: Optional[Dict[bytes, bytes]]\n    ) -&gt; Dict[str, \"EmbeddingFunctionConfig\"]:\n        \"\"\"\n        Parse the metadata from an arrow table and\n        return a mapping of the vector column to the\n        embedding function and source column\n\n        Parameters\n        ----------\n        metadata : Optional[Dict[bytes, bytes]]\n            The metadata from an arrow table. Note that\n            the keys and values are bytes (pyarrow api)\n\n        Returns\n        -------\n        functions : dict\n            A mapping of vector column name to embedding function.\n            An empty dict is returned if input is None or does not\n            contain b\"embedding_functions\".\n        \"\"\"\n        if metadata is None:\n            return {}\n        # Look at both bytes and string keys, since we might use either\n        serialized = metadata.get(\n            b\"embedding_functions\", metadata.get(\"embedding_functions\")\n        )\n        if serialized is None:\n            return {}\n        raw_list = json.loads(serialized.decode(\"utf-8\"))\n        return {\n            obj[\"vector_column\"]: EmbeddingFunctionConfig(\n                vector_column=obj[\"vector_column\"],\n                source_column=obj[\"source_column\"],\n                function=self.get(obj[\"name\"])(**obj[\"model\"]),\n            )\n            for obj in raw_list\n        }\n\n    def function_to_metadata(self, conf: \"EmbeddingFunctionConfig\"):\n        \"\"\"\n        Convert the given embedding function and source / vector column configs\n        into a config dictionary that can be serialized into arrow metadata\n        \"\"\"\n        func = conf.function\n        name = getattr(\n            func, \"__embedding_function_registry_alias__\", func.__class__.__name__\n        )\n        json_data = func.safe_model_dump()\n        return {\n            \"name\": name,\n            \"model\": json_data,\n            \"source_column\": conf.source_column,\n            \"vector_column\": conf.vector_column,\n        }\n\n    def get_table_metadata(self, func_list):\n        \"\"\"\n        Convert a list of embedding functions and source / vector configs\n        into a config dictionary that can be serialized into arrow metadata\n        \"\"\"\n        if func_list is None or len(func_list) == 0:\n            return None\n        json_data = [self.function_to_metadata(func) for func in func_list]\n        # Note that metadata dictionary values must be bytes\n        # so we need to json dump then utf8 encode\n        metadata = json.dumps(json_data, indent=2).encode(\"utf-8\")\n        return {\"embedding_functions\": metadata}\n\n    def set_var(self, name: str, value: str) -&gt; None:\n        \"\"\"\n        Set a variable. These can be accessed in embedding configuration using\n        the syntax `$var:variable_name`. If they are not set, an error will be\n        thrown letting you know which variable is missing. If you want to supply\n        a default value, you can add an additional part in the configuration\n        like so: `$var:variable_name:default_value`. Default values can be\n        used for runtime configurations that are not sensitive, such as\n        whether to use a GPU for inference.\n\n        The name must not contain a colon. Default values can contain colons.\n        \"\"\"\n        if \":\" in name:\n            raise ValueError(\"Variable names cannot contain colons\")\n        self._variables[name] = value\n\n    def get_var(self, name: str) -&gt; str:\n        \"\"\"\n        Get a variable.\n        \"\"\"\n        return self._variables[name]\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.registry.EmbeddingFunctionRegistry.register","title":"register","text":"<pre><code>register(alias: str = None)\n</code></pre> <p>This creates a decorator that can be used to register an EmbeddingFunction.</p> <p>Parameters:</p> <ul> <li> <code>alias</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>a human friendly name for the embedding function. If not provided, the class name will be used.</p> </li> </ul> Source code in <code>lancedb/embeddings/registry.py</code> <pre><code>def register(self, alias: str = None):\n    \"\"\"\n    This creates a decorator that can be used to register\n    an EmbeddingFunction.\n\n    Parameters\n    ----------\n    alias : Optional[str]\n        a human friendly name for the embedding function. If not\n        provided, the class name will be used.\n    \"\"\"\n\n    # This is a decorator for a class that inherits from BaseModel\n    # It adds the class to the registry\n    def decorator(cls):\n        if not issubclass(cls, EmbeddingFunction):\n            raise TypeError(\"Must be a subclass of EmbeddingFunction\")\n        if cls.__name__ in self._functions:\n            raise KeyError(f\"{cls.__name__} was already registered\")\n        key = alias or cls.__name__\n        self._functions[key] = cls\n        cls.__embedding_function_registry_alias__ = alias\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.registry.EmbeddingFunctionRegistry.reset","title":"reset","text":"<pre><code>reset()\n</code></pre> <p>Reset the registry to its initial state</p> Source code in <code>lancedb/embeddings/registry.py</code> <pre><code>def reset(self):\n    \"\"\"\n    Reset the registry to its initial state\n    \"\"\"\n    self._functions = {}\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.registry.EmbeddingFunctionRegistry.get","title":"get","text":"<pre><code>get(name: str)\n</code></pre> <p>Fetch an embedding function class by name</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the embedding function to fetch Either the alias or the class name if no alias was provided during registration</p> </li> </ul> Source code in <code>lancedb/embeddings/registry.py</code> <pre><code>def get(self, name: str):\n    \"\"\"\n    Fetch an embedding function class by name\n\n    Parameters\n    ----------\n    name : str\n        The name of the embedding function to fetch\n        Either the alias or the class name if no alias was provided\n        during registration\n    \"\"\"\n    return self._functions[name]\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.registry.EmbeddingFunctionRegistry.parse_functions","title":"parse_functions","text":"<pre><code>parse_functions(metadata: Optional[Dict[bytes, bytes]]) -&gt; Dict[str, EmbeddingFunctionConfig]\n</code></pre> <p>Parse the metadata from an arrow table and return a mapping of the vector column to the embedding function and source column</p> <p>Parameters:</p> <ul> <li> <code>metadata</code>               (<code>Optional[Dict[bytes, bytes]]</code>)           \u2013            <p>The metadata from an arrow table. Note that the keys and values are bytes (pyarrow api)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>functions</code> (              <code>dict</code> )          \u2013            <p>A mapping of vector column name to embedding function. An empty dict is returned if input is None or does not contain b\"embedding_functions\".</p> </li> </ul> Source code in <code>lancedb/embeddings/registry.py</code> <pre><code>def parse_functions(\n    self, metadata: Optional[Dict[bytes, bytes]]\n) -&gt; Dict[str, \"EmbeddingFunctionConfig\"]:\n    \"\"\"\n    Parse the metadata from an arrow table and\n    return a mapping of the vector column to the\n    embedding function and source column\n\n    Parameters\n    ----------\n    metadata : Optional[Dict[bytes, bytes]]\n        The metadata from an arrow table. Note that\n        the keys and values are bytes (pyarrow api)\n\n    Returns\n    -------\n    functions : dict\n        A mapping of vector column name to embedding function.\n        An empty dict is returned if input is None or does not\n        contain b\"embedding_functions\".\n    \"\"\"\n    if metadata is None:\n        return {}\n    # Look at both bytes and string keys, since we might use either\n    serialized = metadata.get(\n        b\"embedding_functions\", metadata.get(\"embedding_functions\")\n    )\n    if serialized is None:\n        return {}\n    raw_list = json.loads(serialized.decode(\"utf-8\"))\n    return {\n        obj[\"vector_column\"]: EmbeddingFunctionConfig(\n            vector_column=obj[\"vector_column\"],\n            source_column=obj[\"source_column\"],\n            function=self.get(obj[\"name\"])(**obj[\"model\"]),\n        )\n        for obj in raw_list\n    }\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.registry.EmbeddingFunctionRegistry.function_to_metadata","title":"function_to_metadata","text":"<pre><code>function_to_metadata(conf: EmbeddingFunctionConfig)\n</code></pre> <p>Convert the given embedding function and source / vector column configs into a config dictionary that can be serialized into arrow metadata</p> Source code in <code>lancedb/embeddings/registry.py</code> <pre><code>def function_to_metadata(self, conf: \"EmbeddingFunctionConfig\"):\n    \"\"\"\n    Convert the given embedding function and source / vector column configs\n    into a config dictionary that can be serialized into arrow metadata\n    \"\"\"\n    func = conf.function\n    name = getattr(\n        func, \"__embedding_function_registry_alias__\", func.__class__.__name__\n    )\n    json_data = func.safe_model_dump()\n    return {\n        \"name\": name,\n        \"model\": json_data,\n        \"source_column\": conf.source_column,\n        \"vector_column\": conf.vector_column,\n    }\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.registry.EmbeddingFunctionRegistry.get_table_metadata","title":"get_table_metadata","text":"<pre><code>get_table_metadata(func_list)\n</code></pre> <p>Convert a list of embedding functions and source / vector configs into a config dictionary that can be serialized into arrow metadata</p> Source code in <code>lancedb/embeddings/registry.py</code> <pre><code>def get_table_metadata(self, func_list):\n    \"\"\"\n    Convert a list of embedding functions and source / vector configs\n    into a config dictionary that can be serialized into arrow metadata\n    \"\"\"\n    if func_list is None or len(func_list) == 0:\n        return None\n    json_data = [self.function_to_metadata(func) for func in func_list]\n    # Note that metadata dictionary values must be bytes\n    # so we need to json dump then utf8 encode\n    metadata = json.dumps(json_data, indent=2).encode(\"utf-8\")\n    return {\"embedding_functions\": metadata}\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.registry.EmbeddingFunctionRegistry.set_var","title":"set_var","text":"<pre><code>set_var(name: str, value: str) -&gt; None\n</code></pre> <p>Set a variable. These can be accessed in embedding configuration using the syntax <code>$var:variable_name</code>. If they are not set, an error will be thrown letting you know which variable is missing. If you want to supply a default value, you can add an additional part in the configuration like so: <code>$var:variable_name:default_value</code>. Default values can be used for runtime configurations that are not sensitive, such as whether to use a GPU for inference.</p> <p>The name must not contain a colon. Default values can contain colons.</p> Source code in <code>lancedb/embeddings/registry.py</code> <pre><code>def set_var(self, name: str, value: str) -&gt; None:\n    \"\"\"\n    Set a variable. These can be accessed in embedding configuration using\n    the syntax `$var:variable_name`. If they are not set, an error will be\n    thrown letting you know which variable is missing. If you want to supply\n    a default value, you can add an additional part in the configuration\n    like so: `$var:variable_name:default_value`. Default values can be\n    used for runtime configurations that are not sensitive, such as\n    whether to use a GPU for inference.\n\n    The name must not contain a colon. Default values can contain colons.\n    \"\"\"\n    if \":\" in name:\n        raise ValueError(\"Variable names cannot contain colons\")\n    self._variables[name] = value\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.registry.EmbeddingFunctionRegistry.get_var","title":"get_var","text":"<pre><code>get_var(name: str) -&gt; str\n</code></pre> <p>Get a variable.</p> Source code in <code>lancedb/embeddings/registry.py</code> <pre><code>def get_var(self, name: str) -&gt; str:\n    \"\"\"\n    Get a variable.\n    \"\"\"\n    return self._variables[name]\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.base.EmbeddingFunctionConfig","title":"lancedb.embeddings.base.EmbeddingFunctionConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>This model encapsulates the configuration for a embedding function in a lancedb table. It holds the embedding function, the source column, and the vector column</p> Source code in <code>lancedb/embeddings/base.py</code> <pre><code>class EmbeddingFunctionConfig(BaseModel):\n    \"\"\"\n    This model encapsulates the configuration for a embedding function\n    in a lancedb table. It holds the embedding function, the source column,\n    and the vector column\n    \"\"\"\n\n    vector_column: str\n    source_column: str\n    function: EmbeddingFunction\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.base.EmbeddingFunction","title":"lancedb.embeddings.base.EmbeddingFunction","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>An ABC for embedding functions.</p> <p>All concrete embedding functions must implement the following methods: 1. compute_query_embeddings() which takes a query and returns a list of embeddings 2. compute_source_embeddings() which returns a list of embeddings for    the source column For text data, the two will be the same. For multi-modal data, the source column might be images and the vector column might be text. 3. ndims() which returns the number of dimensions of the vector column</p> Source code in <code>lancedb/embeddings/base.py</code> <pre><code>class EmbeddingFunction(BaseModel, ABC):\n    \"\"\"\n    An ABC for embedding functions.\n\n    All concrete embedding functions must implement the following methods:\n    1. compute_query_embeddings() which takes a query and returns a list of embeddings\n    2. compute_source_embeddings() which returns a list of embeddings for\n       the source column\n    For text data, the two will be the same. For multi-modal data, the source column\n    might be images and the vector column might be text.\n    3. ndims() which returns the number of dimensions of the vector column\n    \"\"\"\n\n    __slots__ = (\"__weakref__\",)  # pydantic 1.x compatibility\n    max_retries: int = (\n        7  # Setting 0 disables retires. Maybe this should not be enabled by default,\n    )\n    _ndims: int = PrivateAttr()\n    _original_args: dict = PrivateAttr()\n\n    @classmethod\n    def create(cls, **kwargs):\n        \"\"\"\n        Create an instance of the embedding function\n        \"\"\"\n        resolved_kwargs = cls.__resolveVariables(kwargs)\n        instance = cls(**resolved_kwargs)\n        instance._original_args = kwargs\n        return instance\n\n    @classmethod\n    def __resolveVariables(cls, args: dict) -&gt; dict:\n        \"\"\"\n        Resolve variables in the args\n        \"\"\"\n        from .registry import EmbeddingFunctionRegistry\n\n        new_args = copy.deepcopy(args)\n\n        registry = EmbeddingFunctionRegistry.get_instance()\n        sensitive_keys = cls.sensitive_keys()\n        for k, v in new_args.items():\n            if isinstance(v, str) and not v.startswith(\"$var:\") and k in sensitive_keys:\n                exc = ValueError(\n                    f\"Sensitive key '{k}' cannot be set to a hardcoded value\"\n                )\n                add_note(exc, \"Help: Use $var: to set sensitive keys to variables\")\n                raise exc\n\n            if isinstance(v, str) and v.startswith(\"$var:\"):\n                parts = v[5:].split(\":\", maxsplit=1)\n                if len(parts) == 1:\n                    try:\n                        new_args[k] = registry.get_var(parts[0])\n                    except KeyError:\n                        exc = ValueError(\n                            \"Variable '{}' not found in registry\".format(parts[0])\n                        )\n                        add_note(\n                            exc,\n                            \"Help: Variables are reset in new Python sessions. \"\n                            \"Use `registry.set_var` to set variables.\",\n                        )\n                        raise exc\n                else:\n                    name, default = parts\n                    try:\n                        new_args[k] = registry.get_var(name)\n                    except KeyError:\n                        new_args[k] = default\n        return new_args\n\n    @staticmethod\n    def sensitive_keys() -&gt; List[str]:\n        \"\"\"\n        Return a list of keys that are sensitive and should not be allowed\n        to be set to hardcoded values in the config. For example, API keys.\n        \"\"\"\n        return []\n\n    @abstractmethod\n    def compute_query_embeddings(self, *args, **kwargs) -&gt; list[Union[np.array, None]]:\n        \"\"\"\n        Compute the embeddings for a given user query\n\n        Returns\n        -------\n        A list of embeddings for each input. The embedding of each input can be None\n        when the embedding is not valid.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def compute_source_embeddings(self, *args, **kwargs) -&gt; list[Union[np.array, None]]:\n        \"\"\"Compute the embeddings for the source column in the database\n\n        Returns\n        -------\n        A list of embeddings for each input. The embedding of each input can be None\n        when the embedding is not valid.\n        \"\"\"\n        pass\n\n    def compute_query_embeddings_with_retry(\n        self, *args, **kwargs\n    ) -&gt; list[Union[np.array, None]]:\n        \"\"\"Compute the embeddings for a given user query with retries\n\n        Returns\n        -------\n        A list of embeddings for each input. The embedding of each input can be None\n        when the embedding is not valid.\n        \"\"\"\n        return retry_with_exponential_backoff(\n            self.compute_query_embeddings, max_retries=self.max_retries\n        )(\n            *args,\n            **kwargs,\n        )\n\n    def compute_source_embeddings_with_retry(\n        self, *args, **kwargs\n    ) -&gt; list[Union[np.array, None]]:\n        \"\"\"Compute the embeddings for the source column in the database with retries.\n\n        Returns\n        -------\n        A list of embeddings for each input. The embedding of each input can be None\n        when the embedding is not valid.\n        \"\"\"\n        return retry_with_exponential_backoff(\n            self.compute_source_embeddings, max_retries=self.max_retries\n        )(*args, **kwargs)\n\n    def sanitize_input(self, texts: TEXT) -&gt; Union[List[str], np.ndarray]:\n        \"\"\"\n        Sanitize the input to the embedding function.\n        \"\"\"\n        if isinstance(texts, str):\n            texts = [texts]\n        elif isinstance(texts, pa.Array):\n            texts = texts.to_pylist()\n        elif isinstance(texts, pa.ChunkedArray):\n            texts = texts.combine_chunks().to_pylist()\n        return texts\n\n    def safe_model_dump(self):\n        if not hasattr(self, \"_original_args\"):\n            raise ValueError(\n                \"EmbeddingFunction was not created with EmbeddingFunction.create()\"\n            )\n        return self._original_args\n\n    @abstractmethod\n    def ndims(self) -&gt; int:\n        \"\"\"\n        Return the dimensions of the vector column\n        \"\"\"\n        pass\n\n    def SourceField(self, **kwargs):\n        \"\"\"\n        Creates a pydantic Field that can automatically annotate\n        the source column for this embedding function\n        \"\"\"\n        return Field(json_schema_extra={\"source_column_for\": self}, **kwargs)\n\n    def VectorField(self, **kwargs):\n        \"\"\"\n        Creates a pydantic Field that can automatically annotate\n        the target vector column for this embedding function\n        \"\"\"\n        return Field(json_schema_extra={\"vector_column_for\": self}, **kwargs)\n\n    def __eq__(self, __value: object) -&gt; bool:\n        if not hasattr(__value, \"__dict__\"):\n            return False\n        return vars(self) == vars(__value)\n\n    def __hash__(self) -&gt; int:\n        return hash(frozenset(vars(self).items()))\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.base.EmbeddingFunction.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(**kwargs)\n</code></pre> <p>Create an instance of the embedding function</p> Source code in <code>lancedb/embeddings/base.py</code> <pre><code>@classmethod\ndef create(cls, **kwargs):\n    \"\"\"\n    Create an instance of the embedding function\n    \"\"\"\n    resolved_kwargs = cls.__resolveVariables(kwargs)\n    instance = cls(**resolved_kwargs)\n    instance._original_args = kwargs\n    return instance\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.base.EmbeddingFunction.__resolveVariables","title":"__resolveVariables  <code>classmethod</code>","text":"<pre><code>__resolveVariables(args: dict) -&gt; dict\n</code></pre> <p>Resolve variables in the args</p> Source code in <code>lancedb/embeddings/base.py</code> <pre><code>@classmethod\ndef __resolveVariables(cls, args: dict) -&gt; dict:\n    \"\"\"\n    Resolve variables in the args\n    \"\"\"\n    from .registry import EmbeddingFunctionRegistry\n\n    new_args = copy.deepcopy(args)\n\n    registry = EmbeddingFunctionRegistry.get_instance()\n    sensitive_keys = cls.sensitive_keys()\n    for k, v in new_args.items():\n        if isinstance(v, str) and not v.startswith(\"$var:\") and k in sensitive_keys:\n            exc = ValueError(\n                f\"Sensitive key '{k}' cannot be set to a hardcoded value\"\n            )\n            add_note(exc, \"Help: Use $var: to set sensitive keys to variables\")\n            raise exc\n\n        if isinstance(v, str) and v.startswith(\"$var:\"):\n            parts = v[5:].split(\":\", maxsplit=1)\n            if len(parts) == 1:\n                try:\n                    new_args[k] = registry.get_var(parts[0])\n                except KeyError:\n                    exc = ValueError(\n                        \"Variable '{}' not found in registry\".format(parts[0])\n                    )\n                    add_note(\n                        exc,\n                        \"Help: Variables are reset in new Python sessions. \"\n                        \"Use `registry.set_var` to set variables.\",\n                    )\n                    raise exc\n            else:\n                name, default = parts\n                try:\n                    new_args[k] = registry.get_var(name)\n                except KeyError:\n                    new_args[k] = default\n    return new_args\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.base.EmbeddingFunction.sensitive_keys","title":"sensitive_keys  <code>staticmethod</code>","text":"<pre><code>sensitive_keys() -&gt; List[str]\n</code></pre> <p>Return a list of keys that are sensitive and should not be allowed to be set to hardcoded values in the config. For example, API keys.</p> Source code in <code>lancedb/embeddings/base.py</code> <pre><code>@staticmethod\ndef sensitive_keys() -&gt; List[str]:\n    \"\"\"\n    Return a list of keys that are sensitive and should not be allowed\n    to be set to hardcoded values in the config. For example, API keys.\n    \"\"\"\n    return []\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.base.EmbeddingFunction.compute_query_embeddings","title":"compute_query_embeddings  <code>abstractmethod</code>","text":"<pre><code>compute_query_embeddings(*args, **kwargs) -&gt; list[Union[array, None]]\n</code></pre> <p>Compute the embeddings for a given user query</p> <p>Returns:</p> <ul> <li> <code>A list of embeddings for each input. The embedding of each input can be None</code>           \u2013            </li> <li> <code>when the embedding is not valid.</code>           \u2013            </li> </ul> Source code in <code>lancedb/embeddings/base.py</code> <pre><code>@abstractmethod\ndef compute_query_embeddings(self, *args, **kwargs) -&gt; list[Union[np.array, None]]:\n    \"\"\"\n    Compute the embeddings for a given user query\n\n    Returns\n    -------\n    A list of embeddings for each input. The embedding of each input can be None\n    when the embedding is not valid.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.base.EmbeddingFunction.compute_source_embeddings","title":"compute_source_embeddings  <code>abstractmethod</code>","text":"<pre><code>compute_source_embeddings(*args, **kwargs) -&gt; list[Union[array, None]]\n</code></pre> <p>Compute the embeddings for the source column in the database</p> <p>Returns:</p> <ul> <li> <code>A list of embeddings for each input. The embedding of each input can be None</code>           \u2013            </li> <li> <code>when the embedding is not valid.</code>           \u2013            </li> </ul> Source code in <code>lancedb/embeddings/base.py</code> <pre><code>@abstractmethod\ndef compute_source_embeddings(self, *args, **kwargs) -&gt; list[Union[np.array, None]]:\n    \"\"\"Compute the embeddings for the source column in the database\n\n    Returns\n    -------\n    A list of embeddings for each input. The embedding of each input can be None\n    when the embedding is not valid.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.base.EmbeddingFunction.compute_query_embeddings_with_retry","title":"compute_query_embeddings_with_retry","text":"<pre><code>compute_query_embeddings_with_retry(*args, **kwargs) -&gt; list[Union[array, None]]\n</code></pre> <p>Compute the embeddings for a given user query with retries</p> <p>Returns:</p> <ul> <li> <code>A list of embeddings for each input. The embedding of each input can be None</code>           \u2013            </li> <li> <code>when the embedding is not valid.</code>           \u2013            </li> </ul> Source code in <code>lancedb/embeddings/base.py</code> <pre><code>def compute_query_embeddings_with_retry(\n    self, *args, **kwargs\n) -&gt; list[Union[np.array, None]]:\n    \"\"\"Compute the embeddings for a given user query with retries\n\n    Returns\n    -------\n    A list of embeddings for each input. The embedding of each input can be None\n    when the embedding is not valid.\n    \"\"\"\n    return retry_with_exponential_backoff(\n        self.compute_query_embeddings, max_retries=self.max_retries\n    )(\n        *args,\n        **kwargs,\n    )\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.base.EmbeddingFunction.compute_source_embeddings_with_retry","title":"compute_source_embeddings_with_retry","text":"<pre><code>compute_source_embeddings_with_retry(*args, **kwargs) -&gt; list[Union[array, None]]\n</code></pre> <p>Compute the embeddings for the source column in the database with retries.</p> <p>Returns:</p> <ul> <li> <code>A list of embeddings for each input. The embedding of each input can be None</code>           \u2013            </li> <li> <code>when the embedding is not valid.</code>           \u2013            </li> </ul> Source code in <code>lancedb/embeddings/base.py</code> <pre><code>def compute_source_embeddings_with_retry(\n    self, *args, **kwargs\n) -&gt; list[Union[np.array, None]]:\n    \"\"\"Compute the embeddings for the source column in the database with retries.\n\n    Returns\n    -------\n    A list of embeddings for each input. The embedding of each input can be None\n    when the embedding is not valid.\n    \"\"\"\n    return retry_with_exponential_backoff(\n        self.compute_source_embeddings, max_retries=self.max_retries\n    )(*args, **kwargs)\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.base.EmbeddingFunction.sanitize_input","title":"sanitize_input","text":"<pre><code>sanitize_input(texts: TEXT) -&gt; Union[List[str], ndarray]\n</code></pre> <p>Sanitize the input to the embedding function.</p> Source code in <code>lancedb/embeddings/base.py</code> <pre><code>def sanitize_input(self, texts: TEXT) -&gt; Union[List[str], np.ndarray]:\n    \"\"\"\n    Sanitize the input to the embedding function.\n    \"\"\"\n    if isinstance(texts, str):\n        texts = [texts]\n    elif isinstance(texts, pa.Array):\n        texts = texts.to_pylist()\n    elif isinstance(texts, pa.ChunkedArray):\n        texts = texts.combine_chunks().to_pylist()\n    return texts\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.base.EmbeddingFunction.ndims","title":"ndims  <code>abstractmethod</code>","text":"<pre><code>ndims() -&gt; int\n</code></pre> <p>Return the dimensions of the vector column</p> Source code in <code>lancedb/embeddings/base.py</code> <pre><code>@abstractmethod\ndef ndims(self) -&gt; int:\n    \"\"\"\n    Return the dimensions of the vector column\n    \"\"\"\n    pass\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.base.EmbeddingFunction.SourceField","title":"SourceField","text":"<pre><code>SourceField(**kwargs)\n</code></pre> <p>Creates a pydantic Field that can automatically annotate the source column for this embedding function</p> Source code in <code>lancedb/embeddings/base.py</code> <pre><code>def SourceField(self, **kwargs):\n    \"\"\"\n    Creates a pydantic Field that can automatically annotate\n    the source column for this embedding function\n    \"\"\"\n    return Field(json_schema_extra={\"source_column_for\": self}, **kwargs)\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.base.EmbeddingFunction.VectorField","title":"VectorField","text":"<pre><code>VectorField(**kwargs)\n</code></pre> <p>Creates a pydantic Field that can automatically annotate the target vector column for this embedding function</p> Source code in <code>lancedb/embeddings/base.py</code> <pre><code>def VectorField(self, **kwargs):\n    \"\"\"\n    Creates a pydantic Field that can automatically annotate\n    the target vector column for this embedding function\n    \"\"\"\n    return Field(json_schema_extra={\"vector_column_for\": self}, **kwargs)\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.base.TextEmbeddingFunction","title":"lancedb.embeddings.base.TextEmbeddingFunction","text":"<p>               Bases: <code>EmbeddingFunction</code></p> <p>A callable ABC for embedding functions that take text as input</p> Source code in <code>lancedb/embeddings/base.py</code> <pre><code>class TextEmbeddingFunction(EmbeddingFunction):\n    \"\"\"\n    A callable ABC for embedding functions that take text as input\n    \"\"\"\n\n    def compute_query_embeddings(\n        self, query: str, *args, **kwargs\n    ) -&gt; list[Union[np.array, None]]:\n        return self.compute_source_embeddings(query, *args, **kwargs)\n\n    def compute_source_embeddings(\n        self, texts: TEXT, *args, **kwargs\n    ) -&gt; list[Union[np.array, None]]:\n        texts = self.sanitize_input(texts)\n        return self.generate_embeddings(texts)\n\n    @abstractmethod\n    def generate_embeddings(\n        self, texts: Union[List[str], np.ndarray], *args, **kwargs\n    ) -&gt; list[Union[np.array, None]]:\n        \"\"\"Generate the embeddings for the given texts\"\"\"\n        pass\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.base.TextEmbeddingFunction.generate_embeddings","title":"generate_embeddings  <code>abstractmethod</code>","text":"<pre><code>generate_embeddings(texts: Union[List[str], ndarray], *args, **kwargs) -&gt; list[Union[array, None]]\n</code></pre> <p>Generate the embeddings for the given texts</p> Source code in <code>lancedb/embeddings/base.py</code> <pre><code>@abstractmethod\ndef generate_embeddings(\n    self, texts: Union[List[str], np.ndarray], *args, **kwargs\n) -&gt; list[Union[np.array, None]]:\n    \"\"\"Generate the embeddings for the given texts\"\"\"\n    pass\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.sentence_transformers.SentenceTransformerEmbeddings","title":"lancedb.embeddings.sentence_transformers.SentenceTransformerEmbeddings","text":"<p>               Bases: <code>TextEmbeddingFunction</code></p> <p>An embedding function that uses the sentence-transformers library</p> <p>https://huggingface.co/sentence-transformers</p> <p>Parameters:</p> <ul> <li> <code>name</code>           \u2013            <p>The name of the model to use.</p> </li> <li> <code>device</code>           \u2013            <p>The device to use for the model</p> </li> <li> <code>normalize</code>           \u2013            <p>Whether to normalize the embeddings</p> </li> <li> <code>trust_remote_code</code>           \u2013            <p>Whether to trust the remote code</p> </li> </ul> Source code in <code>lancedb/embeddings/sentence_transformers.py</code> <pre><code>@register(\"sentence-transformers\")\nclass SentenceTransformerEmbeddings(TextEmbeddingFunction):\n    \"\"\"\n    An embedding function that uses the sentence-transformers library\n\n    https://huggingface.co/sentence-transformers\n\n    Parameters\n    ----------\n    name: str, default \"all-MiniLM-L6-v2\"\n        The name of the model to use.\n    device: str, default \"cpu\"\n        The device to use for the model\n    normalize: bool, default True\n        Whether to normalize the embeddings\n    trust_remote_code: bool, default True\n        Whether to trust the remote code\n    \"\"\"\n\n    name: str = \"all-MiniLM-L6-v2\"\n    device: str = \"cpu\"\n    normalize: bool = True\n    trust_remote_code: bool = True\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._ndims = None\n\n    @property\n    def embedding_model(self):\n        \"\"\"\n        Get the sentence-transformers embedding model specified by the\n        name, device, and trust_remote_code. This is cached so that the\n        model is only loaded once per process.\n        \"\"\"\n        return self.get_embedding_model()\n\n    def ndims(self):\n        if self._ndims is None:\n            self._ndims = len(self.generate_embeddings(\"foo\")[0])\n        return self._ndims\n\n    def generate_embeddings(\n        self, texts: Union[List[str], np.ndarray]\n    ) -&gt; List[np.array]:\n        \"\"\"\n        Get the embeddings for the given texts\n\n        Parameters\n        ----------\n        texts: list[str] or np.ndarray (of str)\n            The texts to embed\n        \"\"\"\n        return self.embedding_model.encode(\n            list(texts),\n            convert_to_numpy=True,\n            normalize_embeddings=self.normalize,\n        ).tolist()\n\n    @weak_lru(maxsize=1)\n    def get_embedding_model(self):\n        \"\"\"\n        Get the sentence-transformers embedding model specified by the\n        name, device, and trust_remote_code. This is cached so that the\n        model is only loaded once per process.\n\n        TODO: use lru_cache instead with a reasonable/configurable maxsize\n        \"\"\"\n        sentence_transformers = attempt_import_or_raise(\n            \"sentence_transformers\", \"sentence-transformers\"\n        )\n        return sentence_transformers.SentenceTransformer(\n            self.name, device=self.device, trust_remote_code=self.trust_remote_code\n        )\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.sentence_transformers.SentenceTransformerEmbeddings.embedding_model","title":"embedding_model  <code>property</code>","text":"<pre><code>embedding_model\n</code></pre> <p>Get the sentence-transformers embedding model specified by the name, device, and trust_remote_code. This is cached so that the model is only loaded once per process.</p>"},{"location":"python/python/#lancedb.embeddings.sentence_transformers.SentenceTransformerEmbeddings.generate_embeddings","title":"generate_embeddings","text":"<pre><code>generate_embeddings(texts: Union[List[str], ndarray]) -&gt; List[array]\n</code></pre> <p>Get the embeddings for the given texts</p> <p>Parameters:</p> <ul> <li> <code>texts</code>               (<code>Union[List[str], ndarray]</code>)           \u2013            <p>The texts to embed</p> </li> </ul> Source code in <code>lancedb/embeddings/sentence_transformers.py</code> <pre><code>def generate_embeddings(\n    self, texts: Union[List[str], np.ndarray]\n) -&gt; List[np.array]:\n    \"\"\"\n    Get the embeddings for the given texts\n\n    Parameters\n    ----------\n    texts: list[str] or np.ndarray (of str)\n        The texts to embed\n    \"\"\"\n    return self.embedding_model.encode(\n        list(texts),\n        convert_to_numpy=True,\n        normalize_embeddings=self.normalize,\n    ).tolist()\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.sentence_transformers.SentenceTransformerEmbeddings.get_embedding_model","title":"get_embedding_model","text":"<pre><code>get_embedding_model()\n</code></pre> <p>Get the sentence-transformers embedding model specified by the name, device, and trust_remote_code. This is cached so that the model is only loaded once per process.</p> <p>TODO: use lru_cache instead with a reasonable/configurable maxsize</p> Source code in <code>lancedb/embeddings/sentence_transformers.py</code> <pre><code>@weak_lru(maxsize=1)\ndef get_embedding_model(self):\n    \"\"\"\n    Get the sentence-transformers embedding model specified by the\n    name, device, and trust_remote_code. This is cached so that the\n    model is only loaded once per process.\n\n    TODO: use lru_cache instead with a reasonable/configurable maxsize\n    \"\"\"\n    sentence_transformers = attempt_import_or_raise(\n        \"sentence_transformers\", \"sentence-transformers\"\n    )\n    return sentence_transformers.SentenceTransformer(\n        self.name, device=self.device, trust_remote_code=self.trust_remote_code\n    )\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.openai.OpenAIEmbeddings","title":"lancedb.embeddings.openai.OpenAIEmbeddings","text":"<p>               Bases: <code>TextEmbeddingFunction</code></p> <p>An embedding function that uses the OpenAI API</p> <p>https://platform.openai.com/docs/guides/embeddings</p> <p>This can also be used for open source models that are compatible with the OpenAI API.</p> Notes <p>If you're running an Ollama server locally, you can just override the <code>base_url</code> parameter and provide the Ollama embedding model you want to use (https://ollama.com/library):</p> <pre><code>from lancedb.embeddings import get_registry\nopenai = get_registry().get(\"openai\")\nembedding_function = openai.create(\n    name=\"&lt;ollama-embedding-model-name&gt;\",\n    base_url=\"http://localhost:11434\",\n    )\n</code></pre> Source code in <code>lancedb/embeddings/openai.py</code> <pre><code>@register(\"openai\")\nclass OpenAIEmbeddings(TextEmbeddingFunction):\n    \"\"\"\n    An embedding function that uses the OpenAI API\n\n    https://platform.openai.com/docs/guides/embeddings\n\n    This can also be used for open source models that\n    are compatible with the OpenAI API.\n\n    Notes\n    -----\n    If you're running an Ollama server locally,\n    you can just override the `base_url` parameter\n    and provide the Ollama embedding model you want\n    to use (https://ollama.com/library):\n\n    ```python\n    from lancedb.embeddings import get_registry\n    openai = get_registry().get(\"openai\")\n    embedding_function = openai.create(\n        name=\"&lt;ollama-embedding-model-name&gt;\",\n        base_url=\"http://localhost:11434\",\n        )\n    ```\n\n    \"\"\"\n\n    name: str = \"text-embedding-ada-002\"\n    dim: Optional[int] = None\n    base_url: Optional[str] = None\n    default_headers: Optional[dict] = None\n    organization: Optional[str] = None\n    api_key: Optional[str] = None\n\n    # Set true to use Azure OpenAI API\n    use_azure: bool = False\n\n    def ndims(self):\n        return self._ndims\n\n    @staticmethod\n    def sensitive_keys():\n        return [\"api_key\"]\n\n    @staticmethod\n    def model_names():\n        return [\n            \"text-embedding-ada-002\",\n            \"text-embedding-3-large\",\n            \"text-embedding-3-small\",\n        ]\n\n    @cached_property\n    def _ndims(self):\n        if self.name == \"text-embedding-ada-002\":\n            return 1536\n        elif self.name == \"text-embedding-3-large\":\n            return self.dim or 3072\n        elif self.name == \"text-embedding-3-small\":\n            return self.dim or 1536\n        else:\n            raise ValueError(f\"Unknown model name {self.name}\")\n\n    def generate_embeddings(\n        self, texts: Union[List[str], \"np.ndarray\"]\n    ) -&gt; List[\"np.array\"]:\n        \"\"\"\n        Get the embeddings for the given texts\n\n        Parameters\n        ----------\n        texts: list[str] or np.ndarray (of str)\n            The texts to embed\n        \"\"\"\n        openai = attempt_import_or_raise(\"openai\")\n\n        valid_texts = []\n        valid_indices = []\n        for idx, text in enumerate(texts):\n            if text:\n                valid_texts.append(text)\n                valid_indices.append(idx)\n\n        # TODO retry, rate limit, token limit\n        try:\n            kwargs = {\n                \"input\": valid_texts,\n                \"model\": self.name,\n            }\n            if self.name != \"text-embedding-ada-002\":\n                kwargs[\"dimensions\"] = self.dim\n\n            rs = self._openai_client.embeddings.create(**kwargs)\n            valid_embeddings = {\n                idx: v.embedding for v, idx in zip(rs.data, valid_indices)\n            }\n        except openai.BadRequestError:\n            logging.exception(\"Bad request: %s\", texts)\n            return [None] * len(texts)\n        except Exception:\n            logging.exception(\"OpenAI embeddings error\")\n            raise\n        return [valid_embeddings.get(idx, None) for idx in range(len(texts))]\n\n    @cached_property\n    def _openai_client(self):\n        openai = attempt_import_or_raise(\"openai\")\n        kwargs = {}\n        if self.base_url:\n            kwargs[\"base_url\"] = self.base_url\n        if self.default_headers:\n            kwargs[\"default_headers\"] = self.default_headers\n        if self.organization:\n            kwargs[\"organization\"] = self.organization\n        if self.api_key:\n            kwargs[\"api_key\"] = self.api_key\n\n        if self.use_azure:\n            return openai.AzureOpenAI(**kwargs)\n        else:\n            return openai.OpenAI(**kwargs)\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.openai.OpenAIEmbeddings.generate_embeddings","title":"generate_embeddings","text":"<pre><code>generate_embeddings(texts: Union[List[str], ndarray]) -&gt; List[array]\n</code></pre> <p>Get the embeddings for the given texts</p> <p>Parameters:</p> <ul> <li> <code>texts</code>               (<code>Union[List[str], ndarray]</code>)           \u2013            <p>The texts to embed</p> </li> </ul> Source code in <code>lancedb/embeddings/openai.py</code> <pre><code>def generate_embeddings(\n    self, texts: Union[List[str], \"np.ndarray\"]\n) -&gt; List[\"np.array\"]:\n    \"\"\"\n    Get the embeddings for the given texts\n\n    Parameters\n    ----------\n    texts: list[str] or np.ndarray (of str)\n        The texts to embed\n    \"\"\"\n    openai = attempt_import_or_raise(\"openai\")\n\n    valid_texts = []\n    valid_indices = []\n    for idx, text in enumerate(texts):\n        if text:\n            valid_texts.append(text)\n            valid_indices.append(idx)\n\n    # TODO retry, rate limit, token limit\n    try:\n        kwargs = {\n            \"input\": valid_texts,\n            \"model\": self.name,\n        }\n        if self.name != \"text-embedding-ada-002\":\n            kwargs[\"dimensions\"] = self.dim\n\n        rs = self._openai_client.embeddings.create(**kwargs)\n        valid_embeddings = {\n            idx: v.embedding for v, idx in zip(rs.data, valid_indices)\n        }\n    except openai.BadRequestError:\n        logging.exception(\"Bad request: %s\", texts)\n        return [None] * len(texts)\n    except Exception:\n        logging.exception(\"OpenAI embeddings error\")\n        raise\n    return [valid_embeddings.get(idx, None) for idx in range(len(texts))]\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.open_clip.OpenClipEmbeddings","title":"lancedb.embeddings.open_clip.OpenClipEmbeddings","text":"<p>               Bases: <code>EmbeddingFunction</code></p> <p>An embedding function that uses the OpenClip API For multi-modal text-to-image search</p> <p>https://github.com/mlfoundations/open_clip</p> Source code in <code>lancedb/embeddings/open_clip.py</code> <pre><code>@register(\"open-clip\")\nclass OpenClipEmbeddings(EmbeddingFunction):\n    \"\"\"\n    An embedding function that uses the OpenClip API\n    For multi-modal text-to-image search\n\n    https://github.com/mlfoundations/open_clip\n    \"\"\"\n\n    name: str = \"ViT-B-32\"\n    pretrained: str = \"laion2b_s34b_b79k\"\n    device: str = \"cpu\"\n    batch_size: int = 64\n    normalize: bool = True\n    _model = PrivateAttr()\n    _preprocess = PrivateAttr()\n    _tokenizer = PrivateAttr()\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        open_clip = attempt_import_or_raise(\"open_clip\", \"open-clip\")\n        model, _, preprocess = open_clip.create_model_and_transforms(\n            self.name, pretrained=self.pretrained\n        )\n        model.to(self.device)\n        self._model, self._preprocess = model, preprocess\n        self._tokenizer = open_clip.get_tokenizer(self.name)\n        self._ndims = None\n\n    def ndims(self):\n        if self._ndims is None:\n            self._ndims = self.generate_text_embeddings(\"foo\").shape[0]\n        return self._ndims\n\n    def compute_query_embeddings(\n        self, query: Union[str, \"PIL.Image.Image\"], *args, **kwargs\n    ) -&gt; List[np.ndarray]:\n        \"\"\"\n        Compute the embeddings for a given user query\n\n        Parameters\n        ----------\n        query : Union[str, PIL.Image.Image]\n            The query to embed. A query can be either text or an image.\n        \"\"\"\n        if isinstance(query, str):\n            return [self.generate_text_embeddings(query)]\n        else:\n            PIL = attempt_import_or_raise(\"PIL\", \"pillow\")\n            if isinstance(query, PIL.Image.Image):\n                return [self.generate_image_embedding(query)]\n            else:\n                raise TypeError(\"OpenClip supports str or PIL Image as query\")\n\n    def generate_text_embeddings(self, text: str) -&gt; np.ndarray:\n        torch = attempt_import_or_raise(\"torch\")\n        text = self.sanitize_input(text)\n        text = self._tokenizer(text)\n        text.to(self.device)\n        with torch.no_grad():\n            text_features = self._model.encode_text(text.to(self.device))\n            if self.normalize:\n                text_features /= text_features.norm(dim=-1, keepdim=True)\n            return text_features.cpu().numpy().squeeze()\n\n    def sanitize_input(self, images: IMAGES) -&gt; Union[List[bytes], np.ndarray]:\n        \"\"\"\n        Sanitize the input to the embedding function.\n        \"\"\"\n        if isinstance(images, (str, bytes)):\n            images = [images]\n        elif isinstance(images, pa.Array):\n            images = images.to_pylist()\n        elif isinstance(images, pa.ChunkedArray):\n            images = images.combine_chunks().to_pylist()\n        return images\n\n    def compute_source_embeddings(\n        self, images: IMAGES, *args, **kwargs\n    ) -&gt; List[np.array]:\n        \"\"\"\n        Get the embeddings for the given images\n        \"\"\"\n        images = self.sanitize_input(images)\n        embeddings = []\n        for i in range(0, len(images), self.batch_size):\n            j = min(i + self.batch_size, len(images))\n            batch = images[i:j]\n            embeddings.extend(self._parallel_get(batch))\n        return embeddings\n\n    def _parallel_get(self, images: Union[List[str], List[bytes]]) -&gt; List[np.ndarray]:\n        \"\"\"\n        Issue concurrent requests to retrieve the image data\n        \"\"\"\n        with concurrent.futures.ThreadPoolExecutor() as executor:\n            futures = [\n                executor.submit(self.generate_image_embedding, image)\n                for image in images\n            ]\n            return [future.result() for future in tqdm(futures)]\n\n    def generate_image_embedding(\n        self, image: Union[str, bytes, \"PIL.Image.Image\"]\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Generate the embedding for a single image\n\n        Parameters\n        ----------\n        image : Union[str, bytes, PIL.Image.Image]\n            The image to embed. If the image is a str, it is treated as a uri.\n            If the image is bytes, it is treated as the raw image bytes.\n        \"\"\"\n        torch = attempt_import_or_raise(\"torch\")\n        # TODO handle retry and errors for https\n        image = self._to_pil(image)\n        image = self._preprocess(image).unsqueeze(0)\n        with torch.no_grad():\n            return self._encode_and_normalize_image(image)\n\n    def _to_pil(self, image: Union[str, bytes]):\n        PIL = attempt_import_or_raise(\"PIL\", \"pillow\")\n        if isinstance(image, bytes):\n            return PIL.Image.open(io.BytesIO(image))\n        if isinstance(image, PIL.Image.Image):\n            return image\n        elif isinstance(image, str):\n            parsed = urlparse.urlparse(image)\n            # TODO handle drive letter on windows.\n            if parsed.scheme == \"file\":\n                return PIL.Image.open(parsed.path)\n            elif parsed.scheme == \"\":\n                return PIL.Image.open(image if os.name == \"nt\" else parsed.path)\n            elif parsed.scheme.startswith(\"http\"):\n                return PIL.Image.open(io.BytesIO(url_retrieve(image)))\n            else:\n                raise NotImplementedError(\"Only local and http(s) urls are supported\")\n\n    def _encode_and_normalize_image(self, image_tensor: \"torch.Tensor\"):\n        \"\"\"\n        encode a single image tensor and optionally normalize the output\n        \"\"\"\n        image_features = self._model.encode_image(image_tensor.to(self.device))\n        if self.normalize:\n            image_features /= image_features.norm(dim=-1, keepdim=True)\n        return image_features.cpu().numpy().squeeze()\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.open_clip.OpenClipEmbeddings.compute_query_embeddings","title":"compute_query_embeddings","text":"<pre><code>compute_query_embeddings(query: Union[str, Image], *args, **kwargs) -&gt; List[ndarray]\n</code></pre> <p>Compute the embeddings for a given user query</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>Union[str, Image]</code>)           \u2013            <p>The query to embed. A query can be either text or an image.</p> </li> </ul> Source code in <code>lancedb/embeddings/open_clip.py</code> <pre><code>def compute_query_embeddings(\n    self, query: Union[str, \"PIL.Image.Image\"], *args, **kwargs\n) -&gt; List[np.ndarray]:\n    \"\"\"\n    Compute the embeddings for a given user query\n\n    Parameters\n    ----------\n    query : Union[str, PIL.Image.Image]\n        The query to embed. A query can be either text or an image.\n    \"\"\"\n    if isinstance(query, str):\n        return [self.generate_text_embeddings(query)]\n    else:\n        PIL = attempt_import_or_raise(\"PIL\", \"pillow\")\n        if isinstance(query, PIL.Image.Image):\n            return [self.generate_image_embedding(query)]\n        else:\n            raise TypeError(\"OpenClip supports str or PIL Image as query\")\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.open_clip.OpenClipEmbeddings.sanitize_input","title":"sanitize_input","text":"<pre><code>sanitize_input(images: IMAGES) -&gt; Union[List[bytes], ndarray]\n</code></pre> <p>Sanitize the input to the embedding function.</p> Source code in <code>lancedb/embeddings/open_clip.py</code> <pre><code>def sanitize_input(self, images: IMAGES) -&gt; Union[List[bytes], np.ndarray]:\n    \"\"\"\n    Sanitize the input to the embedding function.\n    \"\"\"\n    if isinstance(images, (str, bytes)):\n        images = [images]\n    elif isinstance(images, pa.Array):\n        images = images.to_pylist()\n    elif isinstance(images, pa.ChunkedArray):\n        images = images.combine_chunks().to_pylist()\n    return images\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.open_clip.OpenClipEmbeddings.compute_source_embeddings","title":"compute_source_embeddings","text":"<pre><code>compute_source_embeddings(images: IMAGES, *args, **kwargs) -&gt; List[array]\n</code></pre> <p>Get the embeddings for the given images</p> Source code in <code>lancedb/embeddings/open_clip.py</code> <pre><code>def compute_source_embeddings(\n    self, images: IMAGES, *args, **kwargs\n) -&gt; List[np.array]:\n    \"\"\"\n    Get the embeddings for the given images\n    \"\"\"\n    images = self.sanitize_input(images)\n    embeddings = []\n    for i in range(0, len(images), self.batch_size):\n        j = min(i + self.batch_size, len(images))\n        batch = images[i:j]\n        embeddings.extend(self._parallel_get(batch))\n    return embeddings\n</code></pre>"},{"location":"python/python/#lancedb.embeddings.open_clip.OpenClipEmbeddings.generate_image_embedding","title":"generate_image_embedding","text":"<pre><code>generate_image_embedding(image: Union[str, bytes, Image]) -&gt; ndarray\n</code></pre> <p>Generate the embedding for a single image</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>Union[str, bytes, Image]</code>)           \u2013            <p>The image to embed. If the image is a str, it is treated as a uri. If the image is bytes, it is treated as the raw image bytes.</p> </li> </ul> Source code in <code>lancedb/embeddings/open_clip.py</code> <pre><code>def generate_image_embedding(\n    self, image: Union[str, bytes, \"PIL.Image.Image\"]\n) -&gt; np.ndarray:\n    \"\"\"\n    Generate the embedding for a single image\n\n    Parameters\n    ----------\n    image : Union[str, bytes, PIL.Image.Image]\n        The image to embed. If the image is a str, it is treated as a uri.\n        If the image is bytes, it is treated as the raw image bytes.\n    \"\"\"\n    torch = attempt_import_or_raise(\"torch\")\n    # TODO handle retry and errors for https\n    image = self._to_pil(image)\n    image = self._preprocess(image).unsqueeze(0)\n    with torch.no_grad():\n        return self._encode_and_normalize_image(image)\n</code></pre>"},{"location":"python/python/#context","title":"Context","text":""},{"location":"python/python/#lancedb.context.contextualize","title":"lancedb.context.contextualize","text":"<pre><code>contextualize(raw_df: 'pd.DataFrame') -&gt; Contextualizer\n</code></pre> <p>Create a Contextualizer object for the given DataFrame.</p> <p>Used to create context windows. Context windows are rolling subsets of text data.</p> <p>The input text column should already be separated into rows that will be the unit of the window. So to create a context window over tokens, start with a DataFrame with one token per row. To create a context window over sentences, start with a DataFrame with one sentence per row.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from lancedb.context import contextualize\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = pd.DataFrame({\n...    'token': ['The', 'quick', 'brown', 'fox', 'jumped', 'over',\n...              'the', 'lazy', 'dog', 'I', 'love', 'sandwiches'],\n...    'document_id': [1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2]\n... })\n</code></pre> <p><code>window</code> determines how many rows to include in each window. In our case this how many tokens, but depending on the input data, it could be sentences, paragraphs, messages, etc.</p> <pre><code>&gt;&gt;&gt; contextualize(data).window(3).stride(1).text_col('token').to_pandas()\n                token  document_id\n0     The quick brown            1\n1     quick brown fox            1\n2    brown fox jumped            1\n3     fox jumped over            1\n4     jumped over the            1\n5       over the lazy            1\n6        the lazy dog            1\n7          lazy dog I            1\n8          dog I love            1\n9   I love sandwiches            2\n10    love sandwiches            2\n&gt;&gt;&gt; (contextualize(data).window(7).stride(1).min_window_size(7)\n...   .text_col('token').to_pandas())\n                                  token  document_id\n0   The quick brown fox jumped over the            1\n1  quick brown fox jumped over the lazy            1\n2    brown fox jumped over the lazy dog            1\n3        fox jumped over the lazy dog I            1\n4       jumped over the lazy dog I love            1\n5   over the lazy dog I love sandwiches            1\n</code></pre> <p><code>stride</code> determines how many rows to skip between each window start. This can be used to reduce the total number of windows generated.</p> <pre><code>&gt;&gt;&gt; contextualize(data).window(4).stride(2).text_col('token').to_pandas()\n                    token  document_id\n0     The quick brown fox            1\n2   brown fox jumped over            1\n4    jumped over the lazy            1\n6          the lazy dog I            1\n8   dog I love sandwiches            1\n10        love sandwiches            2\n</code></pre> <p><code>groupby</code> determines how to group the rows. For example, we would like to have context windows that don't cross document boundaries. In this case, we can pass <code>document_id</code> as the group by.</p> <pre><code>&gt;&gt;&gt; (contextualize(data)\n...     .window(4).stride(2).text_col('token').groupby('document_id')\n...     .to_pandas())\n                   token  document_id\n0    The quick brown fox            1\n2  brown fox jumped over            1\n4   jumped over the lazy            1\n6           the lazy dog            1\n9      I love sandwiches            2\n</code></pre> <p><code>min_window_size</code> determines the minimum size of the context windows that are generated.This can be used to trim the last few context windows which have size less than <code>min_window_size</code>. By default context windows of size 1 are skipped.</p> <pre><code>&gt;&gt;&gt; (contextualize(data)\n...     .window(6).stride(3).text_col('token').groupby('document_id')\n...     .to_pandas())\n                             token  document_id\n0  The quick brown fox jumped over            1\n3     fox jumped over the lazy dog            1\n6                     the lazy dog            1\n9                I love sandwiches            2\n</code></pre> <pre><code>&gt;&gt;&gt; (contextualize(data)\n...     .window(6).stride(3).min_window_size(4).text_col('token')\n...     .groupby('document_id')\n...     .to_pandas())\n                             token  document_id\n0  The quick brown fox jumped over            1\n3     fox jumped over the lazy dog            1\n</code></pre> Source code in <code>lancedb/context.py</code> <pre><code>def contextualize(raw_df: \"pd.DataFrame\") -&gt; Contextualizer:\n    \"\"\"Create a Contextualizer object for the given DataFrame.\n\n    Used to create context windows. Context windows are rolling subsets of text\n    data.\n\n    The input text column should already be separated into rows that will be the\n    unit of the window. So to create a context window over tokens, start with\n    a DataFrame with one token per row. To create a context window over sentences,\n    start with a DataFrame with one sentence per row.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from lancedb.context import contextualize\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; data = pd.DataFrame({\n    ...    'token': ['The', 'quick', 'brown', 'fox', 'jumped', 'over',\n    ...              'the', 'lazy', 'dog', 'I', 'love', 'sandwiches'],\n    ...    'document_id': [1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2]\n    ... })\n\n    ``window`` determines how many rows to include in each window. In our case\n    this how many tokens, but depending on the input data, it could be sentences,\n    paragraphs, messages, etc.\n\n    &gt;&gt;&gt; contextualize(data).window(3).stride(1).text_col('token').to_pandas()\n                    token  document_id\n    0     The quick brown            1\n    1     quick brown fox            1\n    2    brown fox jumped            1\n    3     fox jumped over            1\n    4     jumped over the            1\n    5       over the lazy            1\n    6        the lazy dog            1\n    7          lazy dog I            1\n    8          dog I love            1\n    9   I love sandwiches            2\n    10    love sandwiches            2\n    &gt;&gt;&gt; (contextualize(data).window(7).stride(1).min_window_size(7)\n    ...   .text_col('token').to_pandas())\n                                      token  document_id\n    0   The quick brown fox jumped over the            1\n    1  quick brown fox jumped over the lazy            1\n    2    brown fox jumped over the lazy dog            1\n    3        fox jumped over the lazy dog I            1\n    4       jumped over the lazy dog I love            1\n    5   over the lazy dog I love sandwiches            1\n\n    ``stride`` determines how many rows to skip between each window start. This can\n    be used to reduce the total number of windows generated.\n\n    &gt;&gt;&gt; contextualize(data).window(4).stride(2).text_col('token').to_pandas()\n                        token  document_id\n    0     The quick brown fox            1\n    2   brown fox jumped over            1\n    4    jumped over the lazy            1\n    6          the lazy dog I            1\n    8   dog I love sandwiches            1\n    10        love sandwiches            2\n\n    ``groupby`` determines how to group the rows. For example, we would like to have\n    context windows that don't cross document boundaries. In this case, we can\n    pass ``document_id`` as the group by.\n\n    &gt;&gt;&gt; (contextualize(data)\n    ...     .window(4).stride(2).text_col('token').groupby('document_id')\n    ...     .to_pandas())\n                       token  document_id\n    0    The quick brown fox            1\n    2  brown fox jumped over            1\n    4   jumped over the lazy            1\n    6           the lazy dog            1\n    9      I love sandwiches            2\n\n    ``min_window_size`` determines the minimum size of the context windows\n    that are generated.This can be used to trim the last few context windows\n    which have size less than ``min_window_size``.\n    By default context windows of size 1 are skipped.\n\n    &gt;&gt;&gt; (contextualize(data)\n    ...     .window(6).stride(3).text_col('token').groupby('document_id')\n    ...     .to_pandas())\n                                 token  document_id\n    0  The quick brown fox jumped over            1\n    3     fox jumped over the lazy dog            1\n    6                     the lazy dog            1\n    9                I love sandwiches            2\n\n    &gt;&gt;&gt; (contextualize(data)\n    ...     .window(6).stride(3).min_window_size(4).text_col('token')\n    ...     .groupby('document_id')\n    ...     .to_pandas())\n                                 token  document_id\n    0  The quick brown fox jumped over            1\n    3     fox jumped over the lazy dog            1\n\n    \"\"\"\n    return Contextualizer(raw_df)\n</code></pre>"},{"location":"python/python/#lancedb.context.Contextualizer","title":"lancedb.context.Contextualizer","text":"<p>Create context windows from a DataFrame. See lancedb.context.contextualize.</p> Source code in <code>lancedb/context.py</code> <pre><code>class Contextualizer:\n    \"\"\"Create context windows from a DataFrame.\n    See [lancedb.context.contextualize][].\n    \"\"\"\n\n    def __init__(self, raw_df):\n        self._text_col = None\n        self._groupby = None\n        self._stride = None\n        self._window = None\n        self._min_window_size = 2\n        self._raw_df = raw_df\n\n    def window(self, window: int) -&gt; Contextualizer:\n        \"\"\"Set the window size. i.e., how many rows to include in each window.\n\n        Parameters\n        ----------\n        window: int\n            The window size.\n        \"\"\"\n        self._window = window\n        return self\n\n    def stride(self, stride: int) -&gt; Contextualizer:\n        \"\"\"Set the stride. i.e., how many rows to skip between each window.\n\n        Parameters\n        ----------\n        stride: int\n            The stride.\n        \"\"\"\n        self._stride = stride\n        return self\n\n    def groupby(self, groupby: str) -&gt; Contextualizer:\n        \"\"\"Set the groupby column. i.e., how to group the rows.\n        Windows don't cross groups\n\n        Parameters\n        ----------\n        groupby: str\n            The groupby column.\n        \"\"\"\n        self._groupby = groupby\n        return self\n\n    def text_col(self, text_col: str) -&gt; Contextualizer:\n        \"\"\"Set the text column used to make the context window.\n\n        Parameters\n        ----------\n        text_col: str\n            The text column.\n        \"\"\"\n        self._text_col = text_col\n        return self\n\n    def min_window_size(self, min_window_size: int) -&gt; Contextualizer:\n        \"\"\"Set the (optional) min_window_size size for the context window.\n\n        Parameters\n        ----------\n        min_window_size: int\n            The min_window_size.\n        \"\"\"\n        self._min_window_size = min_window_size\n        return self\n\n    @deprecation.deprecated(\n        deprecated_in=\"0.3.1\",\n        removed_in=\"0.4.0\",\n        current_version=__version__,\n        details=\"Use to_pandas() instead\",\n    )\n    def to_df(self) -&gt; \"pd.DataFrame\":\n        return self.to_pandas()\n\n    def to_pandas(self) -&gt; \"pd.DataFrame\":\n        \"\"\"Create the context windows and return a DataFrame.\"\"\"\n        if pd is None:\n            raise ImportError(\n                \"pandas is required to create context windows using lancedb\"\n            )\n\n        if self._text_col not in self._raw_df.columns.tolist():\n            raise MissingColumnError(self._text_col)\n\n        if self._window is None or self._window &lt; 1:\n            raise MissingValueError(\n                \"The value of window is None or less than 1. Specify the \"\n                \"window size (number of rows to include in each window)\"\n            )\n\n        if self._stride is None or self._stride &lt; 1:\n            raise MissingValueError(\n                \"The value of stride is None or less than 1. Specify the \"\n                \"stride (number of rows to skip between each window)\"\n            )\n\n        def process_group(grp):\n            # For each group, create the text rolling window\n            # with values of size &gt;= min_window_size\n            text = grp[self._text_col].values\n            contexts = grp.iloc[:: self._stride, :].copy()\n            windows = [\n                \" \".join(text[start_i : min(start_i + self._window, len(grp))])\n                for start_i in range(0, len(grp), self._stride)\n                if start_i + self._window &lt;= len(grp)\n                or len(grp) - start_i &gt;= self._min_window_size\n            ]\n            # if last few rows dropped\n            if len(windows) &lt; len(contexts):\n                contexts = contexts.iloc[: len(windows)]\n            contexts[self._text_col] = windows\n            return contexts\n\n        if self._groupby is None:\n            return process_group(self._raw_df)\n        # concat result from all groups\n        return pd.concat(\n            [process_group(grp) for _, grp in self._raw_df.groupby(self._groupby)]\n        )\n</code></pre>"},{"location":"python/python/#lancedb.context.Contextualizer.window","title":"window","text":"<pre><code>window(window: int) -&gt; Contextualizer\n</code></pre> <p>Set the window size. i.e., how many rows to include in each window.</p> <p>Parameters:</p> <ul> <li> <code>window</code>               (<code>int</code>)           \u2013            <p>The window size.</p> </li> </ul> Source code in <code>lancedb/context.py</code> <pre><code>def window(self, window: int) -&gt; Contextualizer:\n    \"\"\"Set the window size. i.e., how many rows to include in each window.\n\n    Parameters\n    ----------\n    window: int\n        The window size.\n    \"\"\"\n    self._window = window\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.context.Contextualizer.stride","title":"stride","text":"<pre><code>stride(stride: int) -&gt; Contextualizer\n</code></pre> <p>Set the stride. i.e., how many rows to skip between each window.</p> <p>Parameters:</p> <ul> <li> <code>stride</code>               (<code>int</code>)           \u2013            <p>The stride.</p> </li> </ul> Source code in <code>lancedb/context.py</code> <pre><code>def stride(self, stride: int) -&gt; Contextualizer:\n    \"\"\"Set the stride. i.e., how many rows to skip between each window.\n\n    Parameters\n    ----------\n    stride: int\n        The stride.\n    \"\"\"\n    self._stride = stride\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.context.Contextualizer.groupby","title":"groupby","text":"<pre><code>groupby(groupby: str) -&gt; Contextualizer\n</code></pre> <p>Set the groupby column. i.e., how to group the rows. Windows don't cross groups</p> <p>Parameters:</p> <ul> <li> <code>groupby</code>               (<code>str</code>)           \u2013            <p>The groupby column.</p> </li> </ul> Source code in <code>lancedb/context.py</code> <pre><code>def groupby(self, groupby: str) -&gt; Contextualizer:\n    \"\"\"Set the groupby column. i.e., how to group the rows.\n    Windows don't cross groups\n\n    Parameters\n    ----------\n    groupby: str\n        The groupby column.\n    \"\"\"\n    self._groupby = groupby\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.context.Contextualizer.text_col","title":"text_col","text":"<pre><code>text_col(text_col: str) -&gt; Contextualizer\n</code></pre> <p>Set the text column used to make the context window.</p> <p>Parameters:</p> <ul> <li> <code>text_col</code>               (<code>str</code>)           \u2013            <p>The text column.</p> </li> </ul> Source code in <code>lancedb/context.py</code> <pre><code>def text_col(self, text_col: str) -&gt; Contextualizer:\n    \"\"\"Set the text column used to make the context window.\n\n    Parameters\n    ----------\n    text_col: str\n        The text column.\n    \"\"\"\n    self._text_col = text_col\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.context.Contextualizer.min_window_size","title":"min_window_size","text":"<pre><code>min_window_size(min_window_size: int) -&gt; Contextualizer\n</code></pre> <p>Set the (optional) min_window_size size for the context window.</p> <p>Parameters:</p> <ul> <li> <code>min_window_size</code>               (<code>int</code>)           \u2013            <p>The min_window_size.</p> </li> </ul> Source code in <code>lancedb/context.py</code> <pre><code>def min_window_size(self, min_window_size: int) -&gt; Contextualizer:\n    \"\"\"Set the (optional) min_window_size size for the context window.\n\n    Parameters\n    ----------\n    min_window_size: int\n        The min_window_size.\n    \"\"\"\n    self._min_window_size = min_window_size\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.context.Contextualizer.to_pandas","title":"to_pandas","text":"<pre><code>to_pandas() -&gt; 'pd.DataFrame'\n</code></pre> <p>Create the context windows and return a DataFrame.</p> Source code in <code>lancedb/context.py</code> <pre><code>def to_pandas(self) -&gt; \"pd.DataFrame\":\n    \"\"\"Create the context windows and return a DataFrame.\"\"\"\n    if pd is None:\n        raise ImportError(\n            \"pandas is required to create context windows using lancedb\"\n        )\n\n    if self._text_col not in self._raw_df.columns.tolist():\n        raise MissingColumnError(self._text_col)\n\n    if self._window is None or self._window &lt; 1:\n        raise MissingValueError(\n            \"The value of window is None or less than 1. Specify the \"\n            \"window size (number of rows to include in each window)\"\n        )\n\n    if self._stride is None or self._stride &lt; 1:\n        raise MissingValueError(\n            \"The value of stride is None or less than 1. Specify the \"\n            \"stride (number of rows to skip between each window)\"\n        )\n\n    def process_group(grp):\n        # For each group, create the text rolling window\n        # with values of size &gt;= min_window_size\n        text = grp[self._text_col].values\n        contexts = grp.iloc[:: self._stride, :].copy()\n        windows = [\n            \" \".join(text[start_i : min(start_i + self._window, len(grp))])\n            for start_i in range(0, len(grp), self._stride)\n            if start_i + self._window &lt;= len(grp)\n            or len(grp) - start_i &gt;= self._min_window_size\n        ]\n        # if last few rows dropped\n        if len(windows) &lt; len(contexts):\n            contexts = contexts.iloc[: len(windows)]\n        contexts[self._text_col] = windows\n        return contexts\n\n    if self._groupby is None:\n        return process_group(self._raw_df)\n    # concat result from all groups\n    return pd.concat(\n        [process_group(grp) for _, grp in self._raw_df.groupby(self._groupby)]\n    )\n</code></pre>"},{"location":"python/python/#full-text-search","title":"Full text search","text":""},{"location":"python/python/#lancedb.fts.create_index","title":"lancedb.fts.create_index","text":"<pre><code>create_index(index_path: str, text_fields: List[str], ordering_fields: Optional[List[str]] = None, tokenizer_name: str = 'default') -&gt; Index\n</code></pre> <p>Create a new Index (not populated)</p> <p>Parameters:</p> <ul> <li> <code>index_path</code>               (<code>str</code>)           \u2013            <p>Path to the index directory</p> </li> <li> <code>text_fields</code>               (<code>List[str]</code>)           \u2013            <p>List of text fields to index</p> </li> <li> <code>ordering_fields</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of unsigned type fields to order by at search time</p> </li> <li> <code>tokenizer_name</code>               (<code>str</code>, default:                   <code>\"default\"</code> )           \u2013            <p>The tokenizer to use</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>index</code> (              <code>Index</code> )          \u2013            <p>The index object (not yet populated)</p> </li> </ul> Source code in <code>lancedb/fts.py</code> <pre><code>def create_index(\n    index_path: str,\n    text_fields: List[str],\n    ordering_fields: Optional[List[str]] = None,\n    tokenizer_name: str = \"default\",\n) -&gt; tantivy.Index:\n    \"\"\"\n    Create a new Index (not populated)\n\n    Parameters\n    ----------\n    index_path : str\n        Path to the index directory\n    text_fields : List[str]\n        List of text fields to index\n    ordering_fields: List[str]\n        List of unsigned type fields to order by at search time\n    tokenizer_name : str, default \"default\"\n        The tokenizer to use\n\n    Returns\n    -------\n    index : tantivy.Index\n        The index object (not yet populated)\n    \"\"\"\n    if ordering_fields is None:\n        ordering_fields = []\n    # Declaring our schema.\n    schema_builder = tantivy.SchemaBuilder()\n    # special field that we'll populate with row_id\n    schema_builder.add_integer_field(\"doc_id\", stored=True)\n    # data fields\n    for name in text_fields:\n        schema_builder.add_text_field(name, stored=True, tokenizer_name=tokenizer_name)\n    if ordering_fields:\n        for name in ordering_fields:\n            schema_builder.add_unsigned_field(name, fast=True)\n    schema = schema_builder.build()\n    os.makedirs(index_path, exist_ok=True)\n    index = tantivy.Index(schema, path=index_path)\n    return index\n</code></pre>"},{"location":"python/python/#lancedb.fts.populate_index","title":"lancedb.fts.populate_index","text":"<pre><code>populate_index(index: Index, table: LanceTable, fields: List[str], writer_heap_size: Optional[int] = None, ordering_fields: Optional[List[str]] = None) -&gt; int\n</code></pre> <p>Populate an index with data from a LanceTable</p> <p>Parameters:</p> <ul> <li> <code>index</code>               (<code>Index</code>)           \u2013            <p>The index object</p> </li> <li> <code>table</code>               (<code>LanceTable</code>)           \u2013            <p>The table to index</p> </li> <li> <code>fields</code>               (<code>List[str]</code>)           \u2013            <p>List of fields to index</p> </li> <li> <code>writer_heap_size</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>The writer heap size in bytes, defaults to 1GB</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>The number of rows indexed</p> </li> </ul> Source code in <code>lancedb/fts.py</code> <pre><code>def populate_index(\n    index: tantivy.Index,\n    table: LanceTable,\n    fields: List[str],\n    writer_heap_size: Optional[int] = None,\n    ordering_fields: Optional[List[str]] = None,\n) -&gt; int:\n    \"\"\"\n    Populate an index with data from a LanceTable\n\n    Parameters\n    ----------\n    index : tantivy.Index\n        The index object\n    table : LanceTable\n        The table to index\n    fields : List[str]\n        List of fields to index\n    writer_heap_size : int\n        The writer heap size in bytes, defaults to 1GB\n\n    Returns\n    -------\n    int\n        The number of rows indexed\n    \"\"\"\n    if ordering_fields is None:\n        ordering_fields = []\n    writer_heap_size = writer_heap_size or 1024 * 1024 * 1024\n    # first check the fields exist and are string or large string type\n    nested = []\n\n    for name in fields:\n        try:\n            f = table.schema.field(name)  # raises KeyError if not found\n        except KeyError:\n            f = resolve_path(table.schema, name)\n            nested.append(name)\n\n        if not pa.types.is_string(f.type) and not pa.types.is_large_string(f.type):\n            raise TypeError(f\"Field {name} is not a string type\")\n\n    # create a tantivy writer\n    writer = index.writer(heap_size=writer_heap_size)\n    # write data into index\n    dataset = table.to_lance()\n    row_id = 0\n\n    max_nested_level = 0\n    if len(nested) &gt; 0:\n        max_nested_level = max([len(name.split(\".\")) for name in nested])\n\n    for b in dataset.to_batches(columns=fields + ordering_fields):\n        if max_nested_level &gt; 0:\n            b = pa.Table.from_batches([b])\n            for _ in range(max_nested_level - 1):\n                b = b.flatten()\n        for i in range(b.num_rows):\n            doc = tantivy.Document()\n            for name in fields:\n                value = b[name][i].as_py()\n                if value is not None:\n                    doc.add_text(name, value)\n            for name in ordering_fields:\n                value = b[name][i].as_py()\n                if value is not None:\n                    doc.add_unsigned(name, value)\n            if not doc.is_empty:\n                doc.add_integer(\"doc_id\", row_id)\n                writer.add_document(doc)\n            row_id += 1\n    # commit changes\n    writer.commit()\n    return row_id\n</code></pre>"},{"location":"python/python/#lancedb.fts.search_index","title":"lancedb.fts.search_index","text":"<pre><code>search_index(index: Index, query: str, limit: int = 10, ordering_field=None) -&gt; Tuple[Tuple[int], Tuple[float]]\n</code></pre> <p>Search an index for a query</p> <p>Parameters:</p> <ul> <li> <code>index</code>               (<code>Index</code>)           \u2013            <p>The index object</p> </li> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>The query string</p> </li> <li> <code>limit</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The maximum number of results to return</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ids_and_score</code> (              <code>list[tuple[int], tuple[float]]</code> )          \u2013            <p>A tuple of two tuples, the first containing the document ids and the second containing the scores</p> </li> </ul> Source code in <code>lancedb/fts.py</code> <pre><code>def search_index(\n    index: tantivy.Index, query: str, limit: int = 10, ordering_field=None\n) -&gt; Tuple[Tuple[int], Tuple[float]]:\n    \"\"\"\n    Search an index for a query\n\n    Parameters\n    ----------\n    index : tantivy.Index\n        The index object\n    query : str\n        The query string\n    limit : int\n        The maximum number of results to return\n\n    Returns\n    -------\n    ids_and_score: list[tuple[int], tuple[float]]\n        A tuple of two tuples, the first containing the document ids\n        and the second containing the scores\n    \"\"\"\n    searcher = index.searcher()\n    query = index.parse_query(query)\n    # get top results\n    if ordering_field:\n        results = searcher.search(query, limit, order_by_field=ordering_field)\n    else:\n        results = searcher.search(query, limit)\n    if results.count == 0:\n        return tuple(), tuple()\n    return tuple(\n        zip(\n            *[\n                (searcher.doc(doc_address)[\"doc_id\"][0], score)\n                for score, doc_address in results.hits\n            ]\n        )\n    )\n</code></pre>"},{"location":"python/python/#utilities","title":"Utilities","text":""},{"location":"python/python/#lancedb.schema.vector","title":"lancedb.schema.vector","text":"<pre><code>vector(dimension: int, value_type: DataType = pa.float32()) -&gt; DataType\n</code></pre> <p>A help function to create a vector type.</p> <p>Parameters:</p> <ul> <li> <code>dimension</code>               (<code>int</code>)           \u2013            </li> <li> <code>value_type</code>               (<code>DataType</code>, default:                   <code>float32()</code> )           \u2013            <p>The type of the value in the vector.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>A PyArrow DataType for vectors.</code>           \u2013            </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pyarrow as pa\n&gt;&gt;&gt; import lancedb\n&gt;&gt;&gt; schema = pa.schema([\n...     pa.field(\"id\", pa.int64()),\n...     pa.field(\"vector\", lancedb.vector(756)),\n... ])\n</code></pre> Source code in <code>lancedb/schema.py</code> <pre><code>def vector(dimension: int, value_type: pa.DataType = pa.float32()) -&gt; pa.DataType:\n    \"\"\"A help function to create a vector type.\n\n    Parameters\n    ----------\n    dimension: The dimension of the vector.\n    value_type: pa.DataType, optional\n        The type of the value in the vector.\n\n    Returns\n    -------\n    A PyArrow DataType for vectors.\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; import pyarrow as pa\n    &gt;&gt;&gt; import lancedb\n    &gt;&gt;&gt; schema = pa.schema([\n    ...     pa.field(\"id\", pa.int64()),\n    ...     pa.field(\"vector\", lancedb.vector(756)),\n    ... ])\n    \"\"\"\n    return pa.list_(value_type, dimension)\n</code></pre>"},{"location":"python/python/#lancedb.merge.LanceMergeInsertBuilder","title":"lancedb.merge.LanceMergeInsertBuilder","text":"<p>               Bases: <code>object</code></p> <p>Builder for a LanceDB merge insert operation</p> <p>See <code>merge_insert</code> for more context</p> Source code in <code>lancedb/merge.py</code> <pre><code>class LanceMergeInsertBuilder(object):\n    \"\"\"Builder for a LanceDB merge insert operation\n\n    See [`merge_insert`][lancedb.table.Table.merge_insert] for\n    more context\n    \"\"\"\n\n    def __init__(self, table: \"Table\", on: List[str]):  # noqa: F821\n        # Do not put a docstring here.  This method should be hidden\n        # from API docs.  Users should use merge_insert to create\n        # this object.\n        self._table = table\n        self._on = on\n        self._when_matched_update_all = False\n        self._when_matched_update_all_condition = None\n        self._when_not_matched_insert_all = False\n        self._when_not_matched_by_source_delete = False\n        self._when_not_matched_by_source_condition = None\n\n    def when_matched_update_all(\n        self, *, where: Optional[str] = None\n    ) -&gt; LanceMergeInsertBuilder:\n        \"\"\"\n        Rows that exist in both the source table (new data) and\n        the target table (old data) will be updated, replacing\n        the old row with the corresponding matching row.\n\n        If there are multiple matches then the behavior is undefined.\n        Currently this causes multiple copies of the row to be created\n        but that behavior is subject to change.\n        \"\"\"\n        self._when_matched_update_all = True\n        self._when_matched_update_all_condition = where\n        return self\n\n    def when_not_matched_insert_all(self) -&gt; LanceMergeInsertBuilder:\n        \"\"\"\n        Rows that exist only in the source table (new data) should\n        be inserted into the target table.\n        \"\"\"\n        self._when_not_matched_insert_all = True\n        return self\n\n    def when_not_matched_by_source_delete(\n        self, condition: Optional[str] = None\n    ) -&gt; LanceMergeInsertBuilder:\n        \"\"\"\n        Rows that exist only in the target table (old data) will be\n        deleted.  An optional condition can be provided to limit what\n        data is deleted.\n\n        Parameters\n        ----------\n        condition: Optional[str], default None\n            If None then all such rows will be deleted.  Otherwise the\n            condition will be used as an SQL filter to limit what rows\n            are deleted.\n        \"\"\"\n        self._when_not_matched_by_source_delete = True\n        if condition is not None:\n            self._when_not_matched_by_source_condition = condition\n        return self\n\n    def execute(\n        self,\n        new_data: DATA,\n        on_bad_vectors: str = \"error\",\n        fill_value: float = 0.0,\n    ):\n        \"\"\"\n        Executes the merge insert operation\n\n        Nothing is returned but the [`Table`][lancedb.table.Table] is updated\n\n        Parameters\n        ----------\n        new_data: DATA\n            New records which will be matched against the existing records\n            to potentially insert or update into the table.  This parameter\n            can be anything you use for [`add`][lancedb.table.Table.add]\n        on_bad_vectors: str, default \"error\"\n            What to do if any of the vectors are not the same size or contains NaNs.\n            One of \"error\", \"drop\", \"fill\".\n        fill_value: float, default 0.\n            The value to use when filling vectors. Only used if on_bad_vectors=\"fill\".\n        \"\"\"\n        return self._table._do_merge(self, new_data, on_bad_vectors, fill_value)\n</code></pre>"},{"location":"python/python/#lancedb.merge.LanceMergeInsertBuilder.when_matched_update_all","title":"when_matched_update_all","text":"<pre><code>when_matched_update_all(*, where: Optional[str] = None) -&gt; LanceMergeInsertBuilder\n</code></pre> <p>Rows that exist in both the source table (new data) and the target table (old data) will be updated, replacing the old row with the corresponding matching row.</p> <p>If there are multiple matches then the behavior is undefined. Currently this causes multiple copies of the row to be created but that behavior is subject to change.</p> Source code in <code>lancedb/merge.py</code> <pre><code>def when_matched_update_all(\n    self, *, where: Optional[str] = None\n) -&gt; LanceMergeInsertBuilder:\n    \"\"\"\n    Rows that exist in both the source table (new data) and\n    the target table (old data) will be updated, replacing\n    the old row with the corresponding matching row.\n\n    If there are multiple matches then the behavior is undefined.\n    Currently this causes multiple copies of the row to be created\n    but that behavior is subject to change.\n    \"\"\"\n    self._when_matched_update_all = True\n    self._when_matched_update_all_condition = where\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.merge.LanceMergeInsertBuilder.when_not_matched_insert_all","title":"when_not_matched_insert_all","text":"<pre><code>when_not_matched_insert_all() -&gt; LanceMergeInsertBuilder\n</code></pre> <p>Rows that exist only in the source table (new data) should be inserted into the target table.</p> Source code in <code>lancedb/merge.py</code> <pre><code>def when_not_matched_insert_all(self) -&gt; LanceMergeInsertBuilder:\n    \"\"\"\n    Rows that exist only in the source table (new data) should\n    be inserted into the target table.\n    \"\"\"\n    self._when_not_matched_insert_all = True\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.merge.LanceMergeInsertBuilder.when_not_matched_by_source_delete","title":"when_not_matched_by_source_delete","text":"<pre><code>when_not_matched_by_source_delete(condition: Optional[str] = None) -&gt; LanceMergeInsertBuilder\n</code></pre> <p>Rows that exist only in the target table (old data) will be deleted.  An optional condition can be provided to limit what data is deleted.</p> <p>Parameters:</p> <ul> <li> <code>condition</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>If None then all such rows will be deleted.  Otherwise the condition will be used as an SQL filter to limit what rows are deleted.</p> </li> </ul> Source code in <code>lancedb/merge.py</code> <pre><code>def when_not_matched_by_source_delete(\n    self, condition: Optional[str] = None\n) -&gt; LanceMergeInsertBuilder:\n    \"\"\"\n    Rows that exist only in the target table (old data) will be\n    deleted.  An optional condition can be provided to limit what\n    data is deleted.\n\n    Parameters\n    ----------\n    condition: Optional[str], default None\n        If None then all such rows will be deleted.  Otherwise the\n        condition will be used as an SQL filter to limit what rows\n        are deleted.\n    \"\"\"\n    self._when_not_matched_by_source_delete = True\n    if condition is not None:\n        self._when_not_matched_by_source_condition = condition\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.merge.LanceMergeInsertBuilder.execute","title":"execute","text":"<pre><code>execute(new_data: DATA, on_bad_vectors: str = 'error', fill_value: float = 0.0)\n</code></pre> <p>Executes the merge insert operation</p> <p>Nothing is returned but the <code>Table</code> is updated</p> <p>Parameters:</p> <ul> <li> <code>new_data</code>               (<code>DATA</code>)           \u2013            <p>New records which will be matched against the existing records to potentially insert or update into the table.  This parameter can be anything you use for <code>add</code></p> </li> <li> <code>on_bad_vectors</code>               (<code>str</code>, default:                   <code>'error'</code> )           \u2013            <p>What to do if any of the vectors are not the same size or contains NaNs. One of \"error\", \"drop\", \"fill\".</p> </li> <li> <code>fill_value</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>The value to use when filling vectors. Only used if on_bad_vectors=\"fill\".</p> </li> </ul> Source code in <code>lancedb/merge.py</code> <pre><code>def execute(\n    self,\n    new_data: DATA,\n    on_bad_vectors: str = \"error\",\n    fill_value: float = 0.0,\n):\n    \"\"\"\n    Executes the merge insert operation\n\n    Nothing is returned but the [`Table`][lancedb.table.Table] is updated\n\n    Parameters\n    ----------\n    new_data: DATA\n        New records which will be matched against the existing records\n        to potentially insert or update into the table.  This parameter\n        can be anything you use for [`add`][lancedb.table.Table.add]\n    on_bad_vectors: str, default \"error\"\n        What to do if any of the vectors are not the same size or contains NaNs.\n        One of \"error\", \"drop\", \"fill\".\n    fill_value: float, default 0.\n        The value to use when filling vectors. Only used if on_bad_vectors=\"fill\".\n    \"\"\"\n    return self._table._do_merge(self, new_data, on_bad_vectors, fill_value)\n</code></pre>"},{"location":"python/python/#integrations","title":"Integrations","text":""},{"location":"python/python/#pydantic","title":"Pydantic","text":""},{"location":"python/python/#lancedb.pydantic.pydantic_to_schema","title":"lancedb.pydantic.pydantic_to_schema","text":"<pre><code>pydantic_to_schema(model: Type[BaseModel]) -&gt; Schema\n</code></pre> <p>Convert a Pydantic Model to a    PyArrow Schema.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Type[BaseModel]</code>)           \u2013            <p>The Pydantic BaseModel to convert to Arrow Schema.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Schema</code>           \u2013            <p>The Arrow Schema</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from typing import List, Optional\n&gt;&gt;&gt; import pydantic\n&gt;&gt;&gt; from lancedb.pydantic import pydantic_to_schema, Vector\n&gt;&gt;&gt; class FooModel(pydantic.BaseModel):\n...     id: int\n...     s: str\n...     vec: Vector(1536)  # fixed_size_list&lt;item: float32&gt;[1536]\n...     li: List[int]\n...\n&gt;&gt;&gt; schema = pydantic_to_schema(FooModel)\n&gt;&gt;&gt; assert schema == pa.schema([\n...     pa.field(\"id\", pa.int64(), False),\n...     pa.field(\"s\", pa.utf8(), False),\n...     pa.field(\"vec\", pa.list_(pa.float32(), 1536)),\n...     pa.field(\"li\", pa.list_(pa.int64()), False),\n... ])\n</code></pre> Source code in <code>lancedb/pydantic.py</code> <pre><code>def pydantic_to_schema(model: Type[pydantic.BaseModel]) -&gt; pa.Schema:\n    \"\"\"Convert a [Pydantic Model][pydantic.BaseModel] to a\n       [PyArrow Schema][pyarrow.Schema].\n\n    Parameters\n    ----------\n    model : Type[pydantic.BaseModel]\n        The Pydantic BaseModel to convert to Arrow Schema.\n\n    Returns\n    -------\n    pyarrow.Schema\n        The Arrow Schema\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; from typing import List, Optional\n    &gt;&gt;&gt; import pydantic\n    &gt;&gt;&gt; from lancedb.pydantic import pydantic_to_schema, Vector\n    &gt;&gt;&gt; class FooModel(pydantic.BaseModel):\n    ...     id: int\n    ...     s: str\n    ...     vec: Vector(1536)  # fixed_size_list&lt;item: float32&gt;[1536]\n    ...     li: List[int]\n    ...\n    &gt;&gt;&gt; schema = pydantic_to_schema(FooModel)\n    &gt;&gt;&gt; assert schema == pa.schema([\n    ...     pa.field(\"id\", pa.int64(), False),\n    ...     pa.field(\"s\", pa.utf8(), False),\n    ...     pa.field(\"vec\", pa.list_(pa.float32(), 1536)),\n    ...     pa.field(\"li\", pa.list_(pa.int64()), False),\n    ... ])\n    \"\"\"\n    fields = _pydantic_model_to_fields(model)\n    return pa.schema(fields)\n</code></pre>"},{"location":"python/python/#lancedb.pydantic.vector","title":"lancedb.pydantic.vector","text":"<pre><code>vector(dim: int, value_type: DataType = pa.float32())\n</code></pre> Source code in <code>lancedb/pydantic.py</code> <pre><code>def vector(dim: int, value_type: pa.DataType = pa.float32()):\n    # TODO: remove in future release\n    from warnings import warn\n\n    warn(\n        \"lancedb.pydantic.vector() is deprecated, use lancedb.pydantic.Vector instead.\"\n        \"This function will be removed in future release\",\n        DeprecationWarning,\n    )\n    return Vector(dim, value_type)\n</code></pre>"},{"location":"python/python/#lancedb.pydantic.LanceModel","title":"lancedb.pydantic.LanceModel","text":"<p>               Bases: <code>BaseModel</code></p> <p>A Pydantic Model base class that can be converted to a LanceDB Table.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import lancedb\n&gt;&gt;&gt; from lancedb.pydantic import LanceModel, Vector\n&gt;&gt;&gt;\n&gt;&gt;&gt; class TestModel(LanceModel):\n...     name: str\n...     vector: Vector(2)\n...\n&gt;&gt;&gt; db = lancedb.connect(\"./example\")\n&gt;&gt;&gt; table = db.create_table(\"test\", schema=TestModel)\n&gt;&gt;&gt; table.add([\n...     TestModel(name=\"test\", vector=[1.0, 2.0])\n... ])\n&gt;&gt;&gt; table.search([0., 0.]).limit(1).to_pydantic(TestModel)\n[TestModel(name='test', vector=FixedSizeList(dim=2))]\n</code></pre> Source code in <code>lancedb/pydantic.py</code> <pre><code>class LanceModel(pydantic.BaseModel):\n    \"\"\"\n    A Pydantic Model base class that can be converted to a LanceDB Table.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import lancedb\n    &gt;&gt;&gt; from lancedb.pydantic import LanceModel, Vector\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; class TestModel(LanceModel):\n    ...     name: str\n    ...     vector: Vector(2)\n    ...\n    &gt;&gt;&gt; db = lancedb.connect(\"./example\")\n    &gt;&gt;&gt; table = db.create_table(\"test\", schema=TestModel)\n    &gt;&gt;&gt; table.add([\n    ...     TestModel(name=\"test\", vector=[1.0, 2.0])\n    ... ])\n    &gt;&gt;&gt; table.search([0., 0.]).limit(1).to_pydantic(TestModel)\n    [TestModel(name='test', vector=FixedSizeList(dim=2))]\n    \"\"\"\n\n    @classmethod\n    def to_arrow_schema(cls):\n        \"\"\"\n        Get the Arrow Schema for this model.\n        \"\"\"\n        schema = pydantic_to_schema(cls)\n        functions = cls.parse_embedding_functions()\n        if len(functions) &gt; 0:\n            # Prevent circular import\n            from .embeddings import EmbeddingFunctionRegistry\n\n            metadata = EmbeddingFunctionRegistry.get_instance().get_table_metadata(\n                functions\n            )\n            schema = schema.with_metadata(metadata)\n        return schema\n\n    @classmethod\n    def field_names(cls) -&gt; List[str]:\n        \"\"\"\n        Get the field names of this model.\n        \"\"\"\n        return list(cls.safe_get_fields().keys())\n\n    @classmethod\n    def safe_get_fields(cls):\n        if PYDANTIC_VERSION.major &lt; 2:\n            return cls.__fields__\n        return cls.model_fields\n\n    @classmethod\n    def parse_embedding_functions(cls) -&gt; List[\"EmbeddingFunctionConfig\"]:\n        \"\"\"\n        Parse the embedding functions from this model.\n        \"\"\"\n        from .embeddings import EmbeddingFunctionConfig\n\n        vec_and_function = []\n        for name, field_info in cls.safe_get_fields().items():\n            func = get_extras(field_info, \"vector_column_for\")\n            if func is not None:\n                vec_and_function.append([name, func])\n\n        configs = []\n        for vec, func in vec_and_function:\n            for source, field_info in cls.safe_get_fields().items():\n                src_func = get_extras(field_info, \"source_column_for\")\n                if src_func is func:\n                    # note we can't use == here since the function is a pydantic\n                    # model so two instances of the same function are ==, so if you\n                    # have multiple vector columns from multiple sources, both will\n                    # be mapped to the same source column\n                    # GH594\n                    configs.append(\n                        EmbeddingFunctionConfig(\n                            source_column=source, vector_column=vec, function=func\n                        )\n                    )\n        return configs\n</code></pre>"},{"location":"python/python/#lancedb.pydantic.LanceModel.to_arrow_schema","title":"to_arrow_schema  <code>classmethod</code>","text":"<pre><code>to_arrow_schema()\n</code></pre> <p>Get the Arrow Schema for this model.</p> Source code in <code>lancedb/pydantic.py</code> <pre><code>@classmethod\ndef to_arrow_schema(cls):\n    \"\"\"\n    Get the Arrow Schema for this model.\n    \"\"\"\n    schema = pydantic_to_schema(cls)\n    functions = cls.parse_embedding_functions()\n    if len(functions) &gt; 0:\n        # Prevent circular import\n        from .embeddings import EmbeddingFunctionRegistry\n\n        metadata = EmbeddingFunctionRegistry.get_instance().get_table_metadata(\n            functions\n        )\n        schema = schema.with_metadata(metadata)\n    return schema\n</code></pre>"},{"location":"python/python/#lancedb.pydantic.LanceModel.field_names","title":"field_names  <code>classmethod</code>","text":"<pre><code>field_names() -&gt; List[str]\n</code></pre> <p>Get the field names of this model.</p> Source code in <code>lancedb/pydantic.py</code> <pre><code>@classmethod\ndef field_names(cls) -&gt; List[str]:\n    \"\"\"\n    Get the field names of this model.\n    \"\"\"\n    return list(cls.safe_get_fields().keys())\n</code></pre>"},{"location":"python/python/#lancedb.pydantic.LanceModel.parse_embedding_functions","title":"parse_embedding_functions  <code>classmethod</code>","text":"<pre><code>parse_embedding_functions() -&gt; List['EmbeddingFunctionConfig']\n</code></pre> <p>Parse the embedding functions from this model.</p> Source code in <code>lancedb/pydantic.py</code> <pre><code>@classmethod\ndef parse_embedding_functions(cls) -&gt; List[\"EmbeddingFunctionConfig\"]:\n    \"\"\"\n    Parse the embedding functions from this model.\n    \"\"\"\n    from .embeddings import EmbeddingFunctionConfig\n\n    vec_and_function = []\n    for name, field_info in cls.safe_get_fields().items():\n        func = get_extras(field_info, \"vector_column_for\")\n        if func is not None:\n            vec_and_function.append([name, func])\n\n    configs = []\n    for vec, func in vec_and_function:\n        for source, field_info in cls.safe_get_fields().items():\n            src_func = get_extras(field_info, \"source_column_for\")\n            if src_func is func:\n                # note we can't use == here since the function is a pydantic\n                # model so two instances of the same function are ==, so if you\n                # have multiple vector columns from multiple sources, both will\n                # be mapped to the same source column\n                # GH594\n                configs.append(\n                    EmbeddingFunctionConfig(\n                        source_column=source, vector_column=vec, function=func\n                    )\n                )\n    return configs\n</code></pre>"},{"location":"python/python/#reranking","title":"Reranking","text":""},{"location":"python/python/#lancedb.rerankers.linear_combination.LinearCombinationReranker","title":"lancedb.rerankers.linear_combination.LinearCombinationReranker","text":"<p>               Bases: <code>Reranker</code></p> <p>Reranks the results using a linear combination of the scores from the vector and FTS search. For missing scores, fill with <code>fill</code> value.</p> <p>Parameters:</p> <ul> <li> <code>weight</code>               (<code>float</code>, default:                   <code>0.7</code> )           \u2013            <p>The weight to give to the vector score. Must be between 0 and 1.</p> </li> <li> <code>fill</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>The score to give to results that are only in one of the two result sets. This is treated as penalty, so a higher value means a lower score. TODO: We should just hardcode this-- its pretty confusing as we invert scores to calculate final score</p> </li> <li> <code>return_score</code>               (<code>str</code>, default:                   <code>\"relevance\"</code> )           \u2013            <p>opntions are \"relevance\" or \"all\" The type of score to return. If \"relevance\", will return only the relevance score. If \"all\", will return all scores from the vector and FTS search along with the relevance score.</p> </li> </ul> Source code in <code>lancedb/rerankers/linear_combination.py</code> <pre><code>class LinearCombinationReranker(Reranker):\n    \"\"\"\n    Reranks the results using a linear combination of the scores from the\n    vector and FTS search. For missing scores, fill with `fill` value.\n    Parameters\n    ----------\n    weight : float, default 0.7\n        The weight to give to the vector score. Must be between 0 and 1.\n    fill : float, default 1.0\n        The score to give to results that are only in one of the two result sets.\n        This is treated as penalty, so a higher value means a lower score.\n        TODO: We should just hardcode this--\n        its pretty confusing as we invert scores to calculate final score\n    return_score : str, default \"relevance\"\n        opntions are \"relevance\" or \"all\"\n        The type of score to return. If \"relevance\", will return only the relevance\n        score. If \"all\", will return all scores from the vector and FTS search along\n        with the relevance score.\n    \"\"\"\n\n    def __init__(\n        self, weight: float = 0.7, fill: float = 1.0, return_score=\"relevance\"\n    ):\n        if weight &lt; 0 or weight &gt; 1:\n            raise ValueError(\"weight must be between 0 and 1.\")\n        super().__init__(return_score)\n        self.weight = weight\n        self.fill = fill\n\n    def rerank_hybrid(\n        self,\n        query: str,  # noqa: F821\n        vector_results: pa.Table,\n        fts_results: pa.Table,\n    ):\n        combined_results = self.merge_results(vector_results, fts_results, self.fill)\n\n        return combined_results\n\n    def merge_results(\n        self, vector_results: pa.Table, fts_results: pa.Table, fill: float\n    ):\n        # If one is empty then return the other and add _relevance_score\n        # column equal the existing vector or fts score\n        if len(vector_results) == 0:\n            results = fts_results.append_column(\n                \"_relevance_score\",\n                pa.array(fts_results[\"_score\"], type=pa.float32()),\n            )\n            if self.score == \"relevance\":\n                results = self._keep_relevance_score(results)\n            elif self.score == \"all\":\n                results = results.append_column(\n                    \"_distance\",\n                    pa.array([nan] * len(fts_results), type=pa.float32()),\n                )\n            return results\n\n        if len(fts_results) == 0:\n            # invert the distance to relevance score\n            results = vector_results.append_column(\n                \"_relevance_score\",\n                pa.array(\n                    [\n                        self._invert_score(distance)\n                        for distance in vector_results[\"_distance\"].to_pylist()\n                    ],\n                    type=pa.float32(),\n                ),\n            )\n            if self.score == \"relevance\":\n                results = self._keep_relevance_score(results)\n            elif self.score == \"all\":\n                results = results.append_column(\n                    \"_score\",\n                    pa.array([nan] * len(vector_results), type=pa.float32()),\n                )\n            return results\n        results = defaultdict()\n        for vector_result in vector_results.to_pylist():\n            results[vector_result[\"_rowid\"]] = vector_result\n        for fts_result in fts_results.to_pylist():\n            row_id = fts_result[\"_rowid\"]\n            if row_id in results:\n                results[row_id][\"_score\"] = fts_result[\"_score\"]\n            else:\n                results[row_id] = fts_result\n\n        combined_list = []\n        for row_id, result in results.items():\n            vector_score = self._invert_score(result.get(\"_distance\", fill))\n            fts_score = result.get(\"_score\", fill)\n            result[\"_relevance_score\"] = self._combine_score(vector_score, fts_score)\n            combined_list.append(result)\n\n        relevance_score_schema = pa.schema(\n            [\n                pa.field(\"_relevance_score\", pa.float32()),\n            ]\n        )\n        combined_schema = pa.unify_schemas(\n            [vector_results.schema, fts_results.schema, relevance_score_schema]\n        )\n        tbl = pa.Table.from_pylist(combined_list, schema=combined_schema).sort_by(\n            [(\"_relevance_score\", \"descending\")]\n        )\n        if self.score == \"relevance\":\n            tbl = self._keep_relevance_score(tbl)\n        return tbl\n\n    def _combine_score(self, vector_score, fts_score):\n        # these scores represent distance\n        return 1 - (self.weight * vector_score + (1 - self.weight) * fts_score)\n\n    def _invert_score(self, dist: float):\n        # Invert the score between relevance and distance\n        return 1 - dist\n</code></pre>"},{"location":"python/python/#lancedb.rerankers.cohere.CohereReranker","title":"lancedb.rerankers.cohere.CohereReranker","text":"<p>               Bases: <code>Reranker</code></p> <p>Reranks the results using the Cohere Rerank API. https://docs.cohere.com/docs/rerank-guide</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>str</code>, default:                   <code>\"rerank-english-v2.0\"</code> )           \u2013            <p>The name of the cross encoder model to use. Available cohere models are: - rerank-english-v2.0 - rerank-multilingual-v2.0</p> </li> <li> <code>column</code>               (<code>str</code>, default:                   <code>\"text\"</code> )           \u2013            <p>The name of the column to use as input to the cross encoder model.</p> </li> <li> <code>top_n</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The number of results to return. If None, will return all results.</p> </li> </ul> Source code in <code>lancedb/rerankers/cohere.py</code> <pre><code>class CohereReranker(Reranker):\n    \"\"\"\n    Reranks the results using the Cohere Rerank API.\n    https://docs.cohere.com/docs/rerank-guide\n\n    Parameters\n    ----------\n    model_name : str, default \"rerank-english-v2.0\"\n        The name of the cross encoder model to use. Available cohere models are:\n        - rerank-english-v2.0\n        - rerank-multilingual-v2.0\n    column : str, default \"text\"\n        The name of the column to use as input to the cross encoder model.\n    top_n : str, default None\n        The number of results to return. If None, will return all results.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"rerank-english-v3.0\",\n        column: str = \"text\",\n        top_n: Union[int, None] = None,\n        return_score=\"relevance\",\n        api_key: Union[str, None] = None,\n    ):\n        super().__init__(return_score)\n        self.model_name = model_name\n        self.column = column\n        self.top_n = top_n\n        self.api_key = api_key\n\n    @cached_property\n    def _client(self):\n        cohere = attempt_import_or_raise(\"cohere\")\n        # ensure version is at least 0.5.0\n        if hasattr(cohere, \"__version__\") and Version(cohere.__version__) &lt; Version(\n            \"0.5.0\"\n        ):\n            raise ValueError(\n                f\"cohere version must be at least 0.5.0, found {cohere.__version__}\"\n            )\n        if os.environ.get(\"COHERE_API_KEY\") is None and self.api_key is None:\n            raise ValueError(\n                \"COHERE_API_KEY not set. Either set it in your environment or \\\n                pass it as `api_key` argument to the CohereReranker.\"\n            )\n        return cohere.Client(os.environ.get(\"COHERE_API_KEY\") or self.api_key)\n\n    def _rerank(self, result_set: pa.Table, query: str):\n        result_set = self._handle_empty_results(result_set)\n        if len(result_set) == 0:\n            return result_set\n        docs = result_set[self.column].to_pylist()\n        response = self._client.rerank(\n            query=query,\n            documents=docs,\n            top_n=self.top_n,\n            model=self.model_name,\n        )\n        results = (\n            response.results\n        )  # returns list (text, idx, relevance) attributes sorted descending by score\n        indices, scores = list(\n            zip(*[(result.index, result.relevance_score) for result in results])\n        )  # tuples\n        result_set = result_set.take(list(indices))\n        # add the scores\n        result_set = result_set.append_column(\n            \"_relevance_score\", pa.array(scores, type=pa.float32())\n        )\n\n        return result_set\n\n    def rerank_hybrid(\n        self,\n        query: str,\n        vector_results: pa.Table,\n        fts_results: pa.Table,\n    ):\n        combined_results = self.merge_results(vector_results, fts_results)\n        combined_results = self._rerank(combined_results, query)\n        if self.score == \"relevance\":\n            combined_results = self._keep_relevance_score(combined_results)\n        elif self.score == \"all\":\n            raise NotImplementedError(\n                \"return_score='all' not implemented for cohere reranker\"\n            )\n        return combined_results\n\n    def rerank_vector(self, query: str, vector_results: pa.Table):\n        vector_results = self._rerank(vector_results, query)\n        if self.score == \"relevance\":\n            vector_results = vector_results.drop_columns([\"_distance\"])\n        return vector_results\n\n    def rerank_fts(self, query: str, fts_results: pa.Table):\n        fts_results = self._rerank(fts_results, query)\n        if self.score == \"relevance\":\n            fts_results = fts_results.drop_columns([\"_score\"])\n        return fts_results\n</code></pre>"},{"location":"python/python/#lancedb.rerankers.colbert.ColbertReranker","title":"lancedb.rerankers.colbert.ColbertReranker","text":"<p>               Bases: <code>AnswerdotaiRerankers</code></p> <p>Reranks the results using the ColBERT model.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>str</code>, default:                   <code>\"colbert\" (colbert-ir/colbert-v2.0)</code> )           \u2013            <p>The name of the cross encoder model to use.</p> </li> <li> <code>column</code>               (<code>str</code>, default:                   <code>\"text\"</code> )           \u2013            <p>The name of the column to use as input to the cross encoder model.</p> </li> <li> <code>return_score</code>               (<code>str</code>, default:                   <code>\"relevance\"</code> )           \u2013            <p>options are \"relevance\" or \"all\". Only \"relevance\" is supported for now.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional keyword arguments to pass to the model, for example, 'device'. See AnswerDotAI/rerankers for more information.</p> </li> </ul> Source code in <code>lancedb/rerankers/colbert.py</code> <pre><code>class ColbertReranker(AnswerdotaiRerankers):\n    \"\"\"\n    Reranks the results using the ColBERT model.\n\n    Parameters\n    ----------\n    model_name : str, default \"colbert\" (colbert-ir/colbert-v2.0)\n        The name of the cross encoder model to use.\n    column : str, default \"text\"\n        The name of the column to use as input to the cross encoder model.\n    return_score : str, default \"relevance\"\n        options are \"relevance\" or \"all\". Only \"relevance\" is supported for now.\n    **kwargs\n        Additional keyword arguments to pass to the model, for example, 'device'.\n        See AnswerDotAI/rerankers for more information.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"colbert-ir/colbertv2.0\",\n        column: str = \"text\",\n        return_score=\"relevance\",\n        **kwargs,\n    ):\n        super().__init__(\n            model_type=\"colbert\",\n            model_name=model_name,\n            column=column,\n            return_score=return_score,\n            **kwargs,\n        )\n</code></pre>"},{"location":"python/python/#lancedb.rerankers.cross_encoder.CrossEncoderReranker","title":"lancedb.rerankers.cross_encoder.CrossEncoderReranker","text":"<p>               Bases: <code>Reranker</code></p> <p>Reranks the results using a cross encoder model. The cross encoder model is used to score the query and each result. The results are then sorted by the score.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>str</code>, default:                   <code>\"cross-encoder/ms-marco-TinyBERT-L-6\"</code> )           \u2013            <p>The name of the cross encoder model to use. See the sentence transformers documentation for a list of available models.</p> </li> <li> <code>column</code>               (<code>str</code>, default:                   <code>\"text\"</code> )           \u2013            <p>The name of the column to use as input to the cross encoder model.</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The device to use for the cross encoder model. If None, will use \"cuda\" if available, otherwise \"cpu\".</p> </li> <li> <code>return_score</code>               (<code>str</code>, default:                   <code>\"relevance\"</code> )           \u2013            <p>options are \"relevance\" or \"all\". Only \"relevance\" is supported for now.</p> </li> <li> <code>trust_remote_code</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, will trust the remote code to be safe. If False, will not trust the remote code and will not run it</p> </li> </ul> Source code in <code>lancedb/rerankers/cross_encoder.py</code> <pre><code>class CrossEncoderReranker(Reranker):\n    \"\"\"\n    Reranks the results using a cross encoder model. The cross encoder model is\n    used to score the query and each result. The results are then sorted by the score.\n\n    Parameters\n    ----------\n    model_name : str, default \"cross-encoder/ms-marco-TinyBERT-L-6\"\n        The name of the cross encoder model to use. See the sentence transformers\n        documentation for a list of available models.\n    column : str, default \"text\"\n        The name of the column to use as input to the cross encoder model.\n    device : str, default None\n        The device to use for the cross encoder model. If None, will use \"cuda\"\n        if available, otherwise \"cpu\".\n    return_score : str, default \"relevance\"\n        options are \"relevance\" or \"all\". Only \"relevance\" is supported for now.\n    trust_remote_code : bool, default True\n        If True, will trust the remote code to be safe. If False, will not trust\n        the remote code and will not run it\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"cross-encoder/ms-marco-TinyBERT-L-6\",\n        column: str = \"text\",\n        device: Union[str, None] = None,\n        return_score=\"relevance\",\n        trust_remote_code: bool = True,\n    ):\n        super().__init__(return_score)\n        torch = attempt_import_or_raise(\"torch\")\n        self.model_name = model_name\n        self.column = column\n        self.device = device\n        self.trust_remote_code = trust_remote_code\n        if self.device is None:\n            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    @cached_property\n    def model(self):\n        sbert = attempt_import_or_raise(\"sentence_transformers\")\n        # Allows overriding the automatically selected device\n        cross_encoder = sbert.CrossEncoder(\n            self.model_name,\n            device=self.device,\n            trust_remote_code=self.trust_remote_code,\n        )\n\n        return cross_encoder\n\n    def _rerank(self, result_set: pa.Table, query: str):\n        result_set = self._handle_empty_results(result_set)\n        if len(result_set) == 0:\n            return result_set\n        passages = result_set[self.column].to_pylist()\n        cross_inp = [[query, passage] for passage in passages]\n        cross_scores = self.model.predict(cross_inp)\n        result_set = result_set.append_column(\n            \"_relevance_score\", pa.array(cross_scores, type=pa.float32())\n        )\n\n        return result_set\n\n    def rerank_hybrid(\n        self,\n        query: str,\n        vector_results: pa.Table,\n        fts_results: pa.Table,\n    ):\n        combined_results = self.merge_results(vector_results, fts_results)\n        combined_results = self._rerank(combined_results, query)\n        # sort the results by _score\n        if self.score == \"relevance\":\n            combined_results = self._keep_relevance_score(combined_results)\n        elif self.score == \"all\":\n            raise NotImplementedError(\n                \"return_score='all' not implemented for CrossEncoderReranker\"\n            )\n        combined_results = combined_results.sort_by(\n            [(\"_relevance_score\", \"descending\")]\n        )\n\n        return combined_results\n\n    def rerank_vector(self, query: str, vector_results: pa.Table):\n        vector_results = self._rerank(vector_results, query)\n        if self.score == \"relevance\":\n            vector_results = vector_results.drop_columns([\"_distance\"])\n\n        vector_results = vector_results.sort_by([(\"_relevance_score\", \"descending\")])\n        return vector_results\n\n    def rerank_fts(self, query: str, fts_results: pa.Table):\n        fts_results = self._rerank(fts_results, query)\n        if self.score == \"relevance\":\n            fts_results = fts_results.drop_columns([\"_score\"])\n\n        fts_results = fts_results.sort_by([(\"_relevance_score\", \"descending\")])\n        return fts_results\n</code></pre>"},{"location":"python/python/#lancedb.rerankers.openai.OpenaiReranker","title":"lancedb.rerankers.openai.OpenaiReranker","text":"<p>               Bases: <code>Reranker</code></p> <p>Reranks the results using the OpenAI API. WARNING: This is a prompt based reranker that uses chat model that is not a dedicated reranker API. This should be treated as experimental.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>str</code>, default:                   <code>\"gpt-4-turbo-preview\"</code> )           \u2013            <p>The name of the cross encoder model to use.</p> </li> <li> <code>column</code>               (<code>str</code>, default:                   <code>\"text\"</code> )           \u2013            <p>The name of the column to use as input to the cross encoder model.</p> </li> <li> <code>return_score</code>               (<code>str</code>, default:                   <code>\"relevance\"</code> )           \u2013            <p>options are \"relevance\" or \"all\". Only \"relevance\" is supported for now.</p> </li> <li> <code>api_key</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The API key to use. If None, will use the OPENAI_API_KEY environment variable.</p> </li> </ul> Source code in <code>lancedb/rerankers/openai.py</code> <pre><code>class OpenaiReranker(Reranker):\n    \"\"\"\n    Reranks the results using the OpenAI API.\n    WARNING: This is a prompt based reranker that uses chat model that is\n    not a dedicated reranker API. This should be treated as experimental.\n\n    Parameters\n    ----------\n    model_name : str, default \"gpt-4-turbo-preview\"\n        The name of the cross encoder model to use.\n    column : str, default \"text\"\n        The name of the column to use as input to the cross encoder model.\n    return_score : str, default \"relevance\"\n        options are \"relevance\" or \"all\". Only \"relevance\" is supported for now.\n    api_key : str, default None\n        The API key to use. If None, will use the OPENAI_API_KEY environment variable.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"gpt-4-turbo-preview\",\n        column: str = \"text\",\n        return_score=\"relevance\",\n        api_key: Optional[str] = None,\n    ):\n        super().__init__(return_score)\n        self.model_name = model_name\n        self.column = column\n        self.api_key = api_key\n\n    def _rerank(self, result_set: pa.Table, query: str):\n        result_set = self._handle_empty_results(result_set)\n        if len(result_set) == 0:\n            return result_set\n        docs = result_set[self.column].to_pylist()\n        response = self._client.chat.completions.create(\n            model=self.model_name,\n            response_format={\"type\": \"json_object\"},\n            temperature=0,\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are an expert relevance ranker. Given a list of\\\n                        documents and a query, your job is to determine the relevance\\\n                        each document is for answering the query. Your output is JSON,\\\n                        which is a list of documents. Each document has two fields,\\\n                        content and relevance_score.  relevance_score is from 0.0 to\\\n                        1.0 indicating the relevance of the text to the given query.\\\n                        Make sure to include all documents in the response.\",\n                },\n                {\"role\": \"user\", \"content\": f\"Query: {query} Docs: {docs}\"},\n            ],\n        )\n        results = json.loads(response.choices[0].message.content)[\"documents\"]\n        docs, scores = list(\n            zip(*[(result[\"content\"], result[\"relevance_score\"]) for result in results])\n        )  # tuples\n        # replace the self.column column with the docs\n        result_set = result_set.drop(self.column)\n        result_set = result_set.append_column(\n            self.column, pa.array(docs, type=pa.string())\n        )\n        # add the scores\n        result_set = result_set.append_column(\n            \"_relevance_score\", pa.array(scores, type=pa.float32())\n        )\n\n        return result_set\n\n    def rerank_hybrid(\n        self,\n        query: str,\n        vector_results: pa.Table,\n        fts_results: pa.Table,\n    ):\n        combined_results = self.merge_results(vector_results, fts_results)\n        combined_results = self._rerank(combined_results, query)\n        if self.score == \"relevance\":\n            combined_results = self._keep_relevance_score(combined_results)\n        elif self.score == \"all\":\n            raise NotImplementedError(\n                \"OpenAI Reranker does not support score='all' yet\"\n            )\n\n        combined_results = combined_results.sort_by(\n            [(\"_relevance_score\", \"descending\")]\n        )\n\n        return combined_results\n\n    def rerank_vector(self, query: str, vector_results: pa.Table):\n        vector_results = self._rerank(vector_results, query)\n        if self.score == \"relevance\":\n            vector_results = vector_results.drop_columns([\"_distance\"])\n        vector_results = vector_results.sort_by([(\"_relevance_score\", \"descending\")])\n        return vector_results\n\n    def rerank_fts(self, query: str, fts_results: pa.Table):\n        fts_results = self._rerank(fts_results, query)\n        if self.score == \"relevance\":\n            fts_results = fts_results.drop_columns([\"_score\"])\n        fts_results = fts_results.sort_by([(\"_relevance_score\", \"descending\")])\n        return fts_results\n\n    @cached_property\n    def _client(self):\n        openai = attempt_import_or_raise(\n            \"openai\"\n        )  # TODO: force version or handle versions &lt; 1.0\n        if os.environ.get(\"OPENAI_API_KEY\") is None and self.api_key is None:\n            raise ValueError(\n                \"OPENAI_API_KEY not set. Either set it in your environment or \\\n                pass it as `api_key` argument to the CohereReranker.\"\n            )\n        return openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\") or self.api_key)\n</code></pre>"},{"location":"python/python/#connections-asynchronous","title":"Connections (Asynchronous)","text":"<p>Connections represent a connection to a LanceDb database and can be used to create, list, or open tables.</p>"},{"location":"python/python/#lancedb.connect_async","title":"lancedb.connect_async  <code>async</code>","text":"<pre><code>connect_async(uri: URI, *, api_key: Optional[str] = None, region: str = 'us-east-1', host_override: Optional[str] = None, read_consistency_interval: Optional[timedelta] = None, client_config: Optional[Union[ClientConfig, Dict[str, Any]]] = None, storage_options: Optional[Dict[str, str]] = None) -&gt; AsyncConnection\n</code></pre> <p>Connect to a LanceDB database.</p> <p>Parameters:</p> <ul> <li> <code>uri</code>               (<code>URI</code>)           \u2013            <p>The uri of the database.</p> </li> <li> <code>api_key</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>If present, connect to LanceDB cloud. Otherwise, connect to a database on file system or cloud storage. Can be set via environment variable <code>LANCEDB_API_KEY</code>.</p> </li> <li> <code>region</code>               (<code>str</code>, default:                   <code>'us-east-1'</code> )           \u2013            <p>The region to use for LanceDB Cloud.</p> </li> <li> <code>host_override</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The override url for LanceDB Cloud.</p> </li> <li> <code>read_consistency_interval</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>(For LanceDB OSS only) The interval at which to check for updates to the table from other processes. If None, then consistency is not checked. For performance reasons, this is the default. For strong consistency, set this to zero seconds. Then every read will check for updates from other processes. As a compromise, you can set this to a non-zero timedelta for eventual consistency. If more than that interval has passed since the last check, then the table will be checked for updates. Note: this consistency only applies to read operations. Write operations are always consistent.</p> </li> <li> <code>client_config</code>               (<code>Optional[Union[ClientConfig, Dict[str, Any]]]</code>, default:                   <code>None</code> )           \u2013            <p>Configuration options for the LanceDB Cloud HTTP client. If a dict, then the keys are the attributes of the ClientConfig class. If None, then the default configuration is used.</p> </li> <li> <code>storage_options</code>               (<code>Optional[Dict[str, str]]</code>, default:                   <code>None</code> )           \u2013            <p>Additional options for the storage backend. See available options at https://lancedb.github.io/lancedb/guides/storage/</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import lancedb\n&gt;&gt;&gt; async def doctest_example():\n...     # For a local directory, provide a path to the database\n...     db = await lancedb.connect_async(\"~/.lancedb\")\n...     # For object storage, use a URI prefix\n...     db = await lancedb.connect_async(\"s3://my-bucket/lancedb\",\n...                                      storage_options={\n...                                          \"aws_access_key_id\": \"***\"})\n...     # Connect to LanceDB cloud\n...     db = await lancedb.connect_async(\"db://my_database\", api_key=\"ldb_...\",\n...                                      client_config={\n...                                          \"retry_config\": {\"retries\": 5}})\n</code></pre> <p>Returns:</p> <ul> <li> <code>conn</code> (              <code>AsyncConnection</code> )          \u2013            <p>A connection to a LanceDB database.</p> </li> </ul> Source code in <code>lancedb/__init__.py</code> <pre><code>async def connect_async(\n    uri: URI,\n    *,\n    api_key: Optional[str] = None,\n    region: str = \"us-east-1\",\n    host_override: Optional[str] = None,\n    read_consistency_interval: Optional[timedelta] = None,\n    client_config: Optional[Union[ClientConfig, Dict[str, Any]]] = None,\n    storage_options: Optional[Dict[str, str]] = None,\n) -&gt; AsyncConnection:\n    \"\"\"Connect to a LanceDB database.\n\n    Parameters\n    ----------\n    uri: str or Path\n        The uri of the database.\n    api_key: str, optional\n        If present, connect to LanceDB cloud.\n        Otherwise, connect to a database on file system or cloud storage.\n        Can be set via environment variable `LANCEDB_API_KEY`.\n    region: str, default \"us-east-1\"\n        The region to use for LanceDB Cloud.\n    host_override: str, optional\n        The override url for LanceDB Cloud.\n    read_consistency_interval: timedelta, default None\n        (For LanceDB OSS only)\n        The interval at which to check for updates to the table from other\n        processes. If None, then consistency is not checked. For performance\n        reasons, this is the default. For strong consistency, set this to\n        zero seconds. Then every read will check for updates from other\n        processes. As a compromise, you can set this to a non-zero timedelta\n        for eventual consistency. If more than that interval has passed since\n        the last check, then the table will be checked for updates. Note: this\n        consistency only applies to read operations. Write operations are\n        always consistent.\n    client_config: ClientConfig or dict, optional\n        Configuration options for the LanceDB Cloud HTTP client. If a dict, then\n        the keys are the attributes of the ClientConfig class. If None, then the\n        default configuration is used.\n    storage_options: dict, optional\n        Additional options for the storage backend. See available options at\n        &lt;https://lancedb.github.io/lancedb/guides/storage/&gt;\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; import lancedb\n    &gt;&gt;&gt; async def doctest_example():\n    ...     # For a local directory, provide a path to the database\n    ...     db = await lancedb.connect_async(\"~/.lancedb\")\n    ...     # For object storage, use a URI prefix\n    ...     db = await lancedb.connect_async(\"s3://my-bucket/lancedb\",\n    ...                                      storage_options={\n    ...                                          \"aws_access_key_id\": \"***\"})\n    ...     # Connect to LanceDB cloud\n    ...     db = await lancedb.connect_async(\"db://my_database\", api_key=\"ldb_...\",\n    ...                                      client_config={\n    ...                                          \"retry_config\": {\"retries\": 5}})\n\n    Returns\n    -------\n    conn : AsyncConnection\n        A connection to a LanceDB database.\n    \"\"\"\n    if read_consistency_interval is not None:\n        read_consistency_interval_secs = read_consistency_interval.total_seconds()\n    else:\n        read_consistency_interval_secs = None\n\n    if isinstance(client_config, dict):\n        client_config = ClientConfig(**client_config)\n\n    return AsyncConnection(\n        await lancedb_connect(\n            sanitize_uri(uri),\n            api_key,\n            region,\n            host_override,\n            read_consistency_interval_secs,\n            client_config,\n            storage_options,\n        )\n    )\n</code></pre>"},{"location":"python/python/#lancedb.db.AsyncConnection","title":"lancedb.db.AsyncConnection","text":"<p>               Bases: <code>object</code></p> <p>An active LanceDB connection</p> <p>To obtain a connection you can use the connect_async function.</p> <p>This could be a native connection (using lance) or a remote connection (e.g. for connecting to LanceDb Cloud)</p> <p>Local connections do not currently hold any open resources but they may do so in the future (for example, for shared cache or connections to catalog services) Remote connections represent an open connection to the remote server.  The close method can be used to release any underlying resources eagerly.  The connection can also be used as a context manager.</p> <p>Connections can be shared on multiple threads and are expected to be long lived. Connections can also be used as a context manager, however, in many cases a single connection can be used for the lifetime of the application and so this is often not needed.  Closing a connection is optional.  If it is not closed then it will be automatically closed when the connection object is deleted.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import lancedb\n&gt;&gt;&gt; async def doctest_example():\n...   with await lancedb.connect_async(\"/tmp/my_dataset\") as conn:\n...     # do something with the connection\n...     pass\n...   # conn is closed here\n</code></pre> Source code in <code>lancedb/db.py</code> <pre><code>class AsyncConnection(object):\n    \"\"\"An active LanceDB connection\n\n    To obtain a connection you can use the [connect_async][lancedb.connect_async]\n    function.\n\n    This could be a native connection (using lance) or a remote connection (e.g. for\n    connecting to LanceDb Cloud)\n\n    Local connections do not currently hold any open resources but they may do so in the\n    future (for example, for shared cache or connections to catalog services) Remote\n    connections represent an open connection to the remote server.  The\n    [close][lancedb.db.AsyncConnection.close] method can be used to release any\n    underlying resources eagerly.  The connection can also be used as a context manager.\n\n    Connections can be shared on multiple threads and are expected to be long lived.\n    Connections can also be used as a context manager, however, in many cases a single\n    connection can be used for the lifetime of the application and so this is often\n    not needed.  Closing a connection is optional.  If it is not closed then it will\n    be automatically closed when the connection object is deleted.\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; import lancedb\n    &gt;&gt;&gt; async def doctest_example():\n    ...   with await lancedb.connect_async(\"/tmp/my_dataset\") as conn:\n    ...     # do something with the connection\n    ...     pass\n    ...   # conn is closed here\n    \"\"\"\n\n    def __init__(self, connection: LanceDbConnection):\n        self._inner = connection\n\n    def __repr__(self):\n        return self._inner.__repr__()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *_):\n        self.close()\n\n    def is_open(self):\n        \"\"\"Return True if the connection is open.\"\"\"\n        return self._inner.is_open()\n\n    def close(self):\n        \"\"\"Close the connection, releasing any underlying resources.\n\n        It is safe to call this method multiple times.\n\n        Any attempt to use the connection after it is closed will result in an error.\"\"\"\n        self._inner.close()\n\n    @property\n    def uri(self) -&gt; str:\n        return self._inner.uri\n\n    async def table_names(\n        self, *, start_after: Optional[str] = None, limit: Optional[int] = None\n    ) -&gt; Iterable[str]:\n        \"\"\"List all tables in this database, in sorted order\n\n        Parameters\n        ----------\n        start_after: str, optional\n            If present, only return names that come lexicographically after the supplied\n            value.\n\n            This can be combined with limit to implement pagination by setting this to\n            the last table name from the previous page.\n        limit: int, default 10\n            The number of results to return.\n\n        Returns\n        -------\n        Iterable of str\n        \"\"\"\n        return await self._inner.table_names(start_after=start_after, limit=limit)\n\n    async def create_table(\n        self,\n        name: str,\n        data: Optional[DATA] = None,\n        schema: Optional[Union[pa.Schema, LanceModel]] = None,\n        mode: Optional[Literal[\"create\", \"overwrite\"]] = None,\n        exist_ok: Optional[bool] = None,\n        on_bad_vectors: Optional[str] = None,\n        fill_value: Optional[float] = None,\n        storage_options: Optional[Dict[str, str]] = None,\n        *,\n        embedding_functions: Optional[List[EmbeddingFunctionConfig]] = None,\n    ) -&gt; AsyncTable:\n        \"\"\"Create an [AsyncTable][lancedb.table.AsyncTable] in the database.\n\n        Parameters\n        ----------\n        name: str\n            The name of the table.\n        data: The data to initialize the table, *optional*\n            User must provide at least one of `data` or `schema`.\n            Acceptable types are:\n\n            - list-of-dict\n\n            - pandas.DataFrame\n\n            - pyarrow.Table or pyarrow.RecordBatch\n        schema: The schema of the table, *optional*\n            Acceptable types are:\n\n            - pyarrow.Schema\n\n            - [LanceModel][lancedb.pydantic.LanceModel]\n        mode: Literal[\"create\", \"overwrite\"]; default \"create\"\n            The mode to use when creating the table.\n            Can be either \"create\" or \"overwrite\".\n            By default, if the table already exists, an exception is raised.\n            If you want to overwrite the table, use mode=\"overwrite\".\n        exist_ok: bool, default False\n            If a table by the same name already exists, then raise an exception\n            if exist_ok=False. If exist_ok=True, then open the existing table;\n            it will not add the provided data but will validate against any\n            schema that's specified.\n        on_bad_vectors: str, default \"error\"\n            What to do if any of the vectors are not the same size or contains NaNs.\n            One of \"error\", \"drop\", \"fill\".\n        fill_value: float\n            The value to use when filling vectors. Only used if on_bad_vectors=\"fill\".\n        storage_options: dict, optional\n            Additional options for the storage backend. Options already set on the\n            connection will be inherited by the table, but can be overridden here.\n            See available options at\n            &lt;https://lancedb.github.io/lancedb/guides/storage/&gt;\n\n        Returns\n        -------\n        AsyncTable\n            A reference to the newly created table.\n\n        !!! note\n\n            The vector index won't be created by default.\n            To create the index, call the `create_index` method on the table.\n\n        Examples\n        --------\n\n        Can create with list of tuples or dictionaries:\n\n        &gt;&gt;&gt; import lancedb\n        &gt;&gt;&gt; async def doctest_example():\n        ...     db = await lancedb.connect_async(\"./.lancedb\")\n        ...     data = [{\"vector\": [1.1, 1.2], \"lat\": 45.5, \"long\": -122.7},\n        ...             {\"vector\": [0.2, 1.8], \"lat\": 40.1, \"long\":  -74.1}]\n        ...     my_table = await db.create_table(\"my_table\", data)\n        ...     print(await my_table.query().limit(5).to_arrow())\n        &gt;&gt;&gt; import asyncio\n        &gt;&gt;&gt; asyncio.run(doctest_example())\n        pyarrow.Table\n        vector: fixed_size_list&lt;item: float&gt;[2]\n          child 0, item: float\n        lat: double\n        long: double\n        ----\n        vector: [[[1.1,1.2],[0.2,1.8]]]\n        lat: [[45.5,40.1]]\n        long: [[-122.7,-74.1]]\n\n        You can also pass a pandas DataFrame:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; data = pd.DataFrame({\n        ...    \"vector\": [[1.1, 1.2], [0.2, 1.8]],\n        ...    \"lat\": [45.5, 40.1],\n        ...    \"long\": [-122.7, -74.1]\n        ... })\n        &gt;&gt;&gt; async def pandas_example():\n        ...     db = await lancedb.connect_async(\"./.lancedb\")\n        ...     my_table = await db.create_table(\"table2\", data)\n        ...     print(await my_table.query().limit(5).to_arrow())\n        &gt;&gt;&gt; asyncio.run(pandas_example())\n        pyarrow.Table\n        vector: fixed_size_list&lt;item: float&gt;[2]\n          child 0, item: float\n        lat: double\n        long: double\n        ----\n        vector: [[[1.1,1.2],[0.2,1.8]]]\n        lat: [[45.5,40.1]]\n        long: [[-122.7,-74.1]]\n\n        Data is converted to Arrow before being written to disk. For maximum\n        control over how data is saved, either provide the PyArrow schema to\n        convert to or else provide a [PyArrow Table](pyarrow.Table) directly.\n\n        &gt;&gt;&gt; import pyarrow as pa\n        &gt;&gt;&gt; custom_schema = pa.schema([\n        ...   pa.field(\"vector\", pa.list_(pa.float32(), 2)),\n        ...   pa.field(\"lat\", pa.float32()),\n        ...   pa.field(\"long\", pa.float32())\n        ... ])\n        &gt;&gt;&gt; async def with_schema():\n        ...     db = await lancedb.connect_async(\"./.lancedb\")\n        ...     my_table = await db.create_table(\"table3\", data, schema = custom_schema)\n        ...     print(await my_table.query().limit(5).to_arrow())\n        &gt;&gt;&gt; asyncio.run(with_schema())\n        pyarrow.Table\n        vector: fixed_size_list&lt;item: float&gt;[2]\n          child 0, item: float\n        lat: float\n        long: float\n        ----\n        vector: [[[1.1,1.2],[0.2,1.8]]]\n        lat: [[45.5,40.1]]\n        long: [[-122.7,-74.1]]\n\n\n        It is also possible to create an table from `[Iterable[pa.RecordBatch]]`:\n\n\n        &gt;&gt;&gt; import pyarrow as pa\n        &gt;&gt;&gt; def make_batches():\n        ...     for i in range(5):\n        ...         yield pa.RecordBatch.from_arrays(\n        ...             [\n        ...                 pa.array([[3.1, 4.1], [5.9, 26.5]],\n        ...                     pa.list_(pa.float32(), 2)),\n        ...                 pa.array([\"foo\", \"bar\"]),\n        ...                 pa.array([10.0, 20.0]),\n        ...             ],\n        ...             [\"vector\", \"item\", \"price\"],\n        ...         )\n        &gt;&gt;&gt; schema=pa.schema([\n        ...     pa.field(\"vector\", pa.list_(pa.float32(), 2)),\n        ...     pa.field(\"item\", pa.utf8()),\n        ...     pa.field(\"price\", pa.float32()),\n        ... ])\n        &gt;&gt;&gt; async def iterable_example():\n        ...     db = await lancedb.connect_async(\"./.lancedb\")\n        ...     await db.create_table(\"table4\", make_batches(), schema=schema)\n        &gt;&gt;&gt; asyncio.run(iterable_example())\n        \"\"\"\n        metadata = None\n\n        if embedding_functions is not None:\n            # If we passed in embedding functions explicitly\n            # then we'll override any schema metadata that\n            # may was implicitly specified by the LanceModel schema\n            registry = EmbeddingFunctionRegistry.get_instance()\n            metadata = registry.get_table_metadata(embedding_functions)\n\n        # Defining defaults here and not in function prototype.  In the future\n        # these defaults will move into rust so better to keep them as None.\n        if on_bad_vectors is None:\n            on_bad_vectors = \"error\"\n\n        if fill_value is None:\n            fill_value = 0.0\n\n        data, schema = sanitize_create_table(\n            data, schema, metadata, on_bad_vectors, fill_value\n        )\n        validate_schema(schema)\n\n        if exist_ok is None:\n            exist_ok = False\n        if mode is None:\n            mode = \"create\"\n        if mode == \"create\" and exist_ok:\n            mode = \"exist_ok\"\n\n        if data is None:\n            new_table = await self._inner.create_empty_table(\n                name,\n                mode,\n                schema,\n                storage_options=storage_options,\n            )\n        else:\n            data = data_to_reader(data, schema)\n            new_table = await self._inner.create_table(\n                name,\n                mode,\n                data,\n                storage_options=storage_options,\n            )\n\n        return AsyncTable(new_table)\n\n    async def open_table(\n        self,\n        name: str,\n        storage_options: Optional[Dict[str, str]] = None,\n        index_cache_size: Optional[int] = None,\n    ) -&gt; AsyncTable:\n        \"\"\"Open a Lance Table in the database.\n\n        Parameters\n        ----------\n        name: str\n            The name of the table.\n        storage_options: dict, optional\n            Additional options for the storage backend. Options already set on the\n            connection will be inherited by the table, but can be overridden here.\n            See available options at\n            &lt;https://lancedb.github.io/lancedb/guides/storage/&gt;\n        index_cache_size: int, default 256\n            Set the size of the index cache, specified as a number of entries\n\n            The exact meaning of an \"entry\" will depend on the type of index:\n            * IVF - there is one entry for each IVF partition\n            * BTREE - there is one entry for the entire index\n\n            This cache applies to the entire opened table, across all indices.\n            Setting this value higher will increase performance on larger datasets\n            at the expense of more RAM\n\n        Returns\n        -------\n        A LanceTable object representing the table.\n        \"\"\"\n        table = await self._inner.open_table(name, storage_options, index_cache_size)\n        return AsyncTable(table)\n\n    async def rename_table(self, old_name: str, new_name: str):\n        \"\"\"Rename a table in the database.\n\n        Parameters\n        ----------\n        old_name: str\n            The current name of the table.\n        new_name: str\n            The new name of the table.\n        \"\"\"\n        await self._inner.rename_table(old_name, new_name)\n\n    async def drop_table(self, name: str, *, ignore_missing: bool = False):\n        \"\"\"Drop a table from the database.\n\n        Parameters\n        ----------\n        name: str\n            The name of the table.\n        ignore_missing: bool, default False\n            If True, ignore if the table does not exist.\n        \"\"\"\n        try:\n            await self._inner.drop_table(name)\n        except ValueError as e:\n            if not ignore_missing:\n                raise e\n            if f\"Table '{name}' was not found\" not in str(e):\n                raise e\n\n    async def drop_all_tables(self):\n        \"\"\"Drop all tables from the database.\"\"\"\n        await self._inner.drop_all_tables()\n\n    @deprecation.deprecated(\n        deprecated_in=\"0.15.1\",\n        removed_in=\"0.17\",\n        current_version=__version__,\n        details=\"Use drop_all_tables() instead\",\n    )\n    async def drop_database(self):\n        \"\"\"\n        Drop database\n        This is the same thing as dropping all the tables\n        \"\"\"\n        await self._inner.drop_all_tables()\n</code></pre>"},{"location":"python/python/#lancedb.db.AsyncConnection.is_open","title":"is_open","text":"<pre><code>is_open()\n</code></pre> <p>Return True if the connection is open.</p> Source code in <code>lancedb/db.py</code> <pre><code>def is_open(self):\n    \"\"\"Return True if the connection is open.\"\"\"\n    return self._inner.is_open()\n</code></pre>"},{"location":"python/python/#lancedb.db.AsyncConnection.close","title":"close","text":"<pre><code>close()\n</code></pre> <p>Close the connection, releasing any underlying resources.</p> <p>It is safe to call this method multiple times.</p> <p>Any attempt to use the connection after it is closed will result in an error.</p> Source code in <code>lancedb/db.py</code> <pre><code>def close(self):\n    \"\"\"Close the connection, releasing any underlying resources.\n\n    It is safe to call this method multiple times.\n\n    Any attempt to use the connection after it is closed will result in an error.\"\"\"\n    self._inner.close()\n</code></pre>"},{"location":"python/python/#lancedb.db.AsyncConnection.table_names","title":"table_names  <code>async</code>","text":"<pre><code>table_names(*, start_after: Optional[str] = None, limit: Optional[int] = None) -&gt; Iterable[str]\n</code></pre> <p>List all tables in this database, in sorted order</p> <p>Parameters:</p> <ul> <li> <code>start_after</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>If present, only return names that come lexicographically after the supplied value.</p> <p>This can be combined with limit to implement pagination by setting this to the last table name from the previous page.</p> </li> <li> <code>limit</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The number of results to return.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Iterable of str</code>           \u2013            </li> </ul> Source code in <code>lancedb/db.py</code> <pre><code>async def table_names(\n    self, *, start_after: Optional[str] = None, limit: Optional[int] = None\n) -&gt; Iterable[str]:\n    \"\"\"List all tables in this database, in sorted order\n\n    Parameters\n    ----------\n    start_after: str, optional\n        If present, only return names that come lexicographically after the supplied\n        value.\n\n        This can be combined with limit to implement pagination by setting this to\n        the last table name from the previous page.\n    limit: int, default 10\n        The number of results to return.\n\n    Returns\n    -------\n    Iterable of str\n    \"\"\"\n    return await self._inner.table_names(start_after=start_after, limit=limit)\n</code></pre>"},{"location":"python/python/#lancedb.db.AsyncConnection.create_table","title":"create_table  <code>async</code>","text":"<pre><code>create_table(name: str, data: Optional[DATA] = None, schema: Optional[Union[Schema, LanceModel]] = None, mode: Optional[Literal['create', 'overwrite']] = None, exist_ok: Optional[bool] = None, on_bad_vectors: Optional[str] = None, fill_value: Optional[float] = None, storage_options: Optional[Dict[str, str]] = None, *, embedding_functions: Optional[List[EmbeddingFunctionConfig]] = None) -&gt; AsyncTable\n</code></pre> <p>Create an AsyncTable in the database.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the table.</p> </li> <li> <code>data</code>               (<code>Optional[DATA]</code>, default:                   <code>None</code> )           \u2013            <p>User must provide at least one of <code>data</code> or <code>schema</code>. Acceptable types are:</p> <ul> <li> <p>list-of-dict</p> </li> <li> <p>pandas.DataFrame</p> </li> <li> <p>pyarrow.Table or pyarrow.RecordBatch</p> </li> </ul> </li> <li> <code>schema</code>               (<code>Optional[Union[Schema, LanceModel]]</code>, default:                   <code>None</code> )           \u2013            <p>Acceptable types are:</p> <ul> <li> <p>pyarrow.Schema</p> </li> <li> <p>LanceModel</p> </li> </ul> </li> <li> <code>mode</code>               (<code>Optional[Literal['create', 'overwrite']]</code>, default:                   <code>None</code> )           \u2013            <p>The mode to use when creating the table. Can be either \"create\" or \"overwrite\". By default, if the table already exists, an exception is raised. If you want to overwrite the table, use mode=\"overwrite\".</p> </li> <li> <code>exist_ok</code>               (<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>If a table by the same name already exists, then raise an exception if exist_ok=False. If exist_ok=True, then open the existing table; it will not add the provided data but will validate against any schema that's specified.</p> </li> <li> <code>on_bad_vectors</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>What to do if any of the vectors are not the same size or contains NaNs. One of \"error\", \"drop\", \"fill\".</p> </li> <li> <code>fill_value</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>The value to use when filling vectors. Only used if on_bad_vectors=\"fill\".</p> </li> <li> <code>storage_options</code>               (<code>Optional[Dict[str, str]]</code>, default:                   <code>None</code> )           \u2013            <p>Additional options for the storage backend. Options already set on the connection will be inherited by the table, but can be overridden here. See available options at https://lancedb.github.io/lancedb/guides/storage/</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AsyncTable</code>           \u2013            <p>A reference to the newly created table.</p> </li> <li> <code>!!! note</code>           \u2013            <p>The vector index won't be created by default. To create the index, call the <code>create_index</code> method on the table.</p> </li> </ul> <p>Examples:</p> <p>Can create with list of tuples or dictionaries:</p> <pre><code>&gt;&gt;&gt; import lancedb\n&gt;&gt;&gt; async def doctest_example():\n...     db = await lancedb.connect_async(\"./.lancedb\")\n...     data = [{\"vector\": [1.1, 1.2], \"lat\": 45.5, \"long\": -122.7},\n...             {\"vector\": [0.2, 1.8], \"lat\": 40.1, \"long\":  -74.1}]\n...     my_table = await db.create_table(\"my_table\", data)\n...     print(await my_table.query().limit(5).to_arrow())\n&gt;&gt;&gt; import asyncio\n&gt;&gt;&gt; asyncio.run(doctest_example())\npyarrow.Table\nvector: fixed_size_list&lt;item: float&gt;[2]\n  child 0, item: float\nlat: double\nlong: double\n----\nvector: [[[1.1,1.2],[0.2,1.8]]]\nlat: [[45.5,40.1]]\nlong: [[-122.7,-74.1]]\n</code></pre> <p>You can also pass a pandas DataFrame:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = pd.DataFrame({\n...    \"vector\": [[1.1, 1.2], [0.2, 1.8]],\n...    \"lat\": [45.5, 40.1],\n...    \"long\": [-122.7, -74.1]\n... })\n&gt;&gt;&gt; async def pandas_example():\n...     db = await lancedb.connect_async(\"./.lancedb\")\n...     my_table = await db.create_table(\"table2\", data)\n...     print(await my_table.query().limit(5).to_arrow())\n&gt;&gt;&gt; asyncio.run(pandas_example())\npyarrow.Table\nvector: fixed_size_list&lt;item: float&gt;[2]\n  child 0, item: float\nlat: double\nlong: double\n----\nvector: [[[1.1,1.2],[0.2,1.8]]]\nlat: [[45.5,40.1]]\nlong: [[-122.7,-74.1]]\n</code></pre> <p>Data is converted to Arrow before being written to disk. For maximum control over how data is saved, either provide the PyArrow schema to convert to or else provide a PyArrow Table directly.</p> <pre><code>&gt;&gt;&gt; import pyarrow as pa\n&gt;&gt;&gt; custom_schema = pa.schema([\n...   pa.field(\"vector\", pa.list_(pa.float32(), 2)),\n...   pa.field(\"lat\", pa.float32()),\n...   pa.field(\"long\", pa.float32())\n... ])\n&gt;&gt;&gt; async def with_schema():\n...     db = await lancedb.connect_async(\"./.lancedb\")\n...     my_table = await db.create_table(\"table3\", data, schema = custom_schema)\n...     print(await my_table.query().limit(5).to_arrow())\n&gt;&gt;&gt; asyncio.run(with_schema())\npyarrow.Table\nvector: fixed_size_list&lt;item: float&gt;[2]\n  child 0, item: float\nlat: float\nlong: float\n----\nvector: [[[1.1,1.2],[0.2,1.8]]]\nlat: [[45.5,40.1]]\nlong: [[-122.7,-74.1]]\n</code></pre> <p>It is also possible to create an table from <code>[Iterable[pa.RecordBatch]]</code>:</p> <pre><code>&gt;&gt;&gt; import pyarrow as pa\n&gt;&gt;&gt; def make_batches():\n...     for i in range(5):\n...         yield pa.RecordBatch.from_arrays(\n...             [\n...                 pa.array([[3.1, 4.1], [5.9, 26.5]],\n...                     pa.list_(pa.float32(), 2)),\n...                 pa.array([\"foo\", \"bar\"]),\n...                 pa.array([10.0, 20.0]),\n...             ],\n...             [\"vector\", \"item\", \"price\"],\n...         )\n&gt;&gt;&gt; schema=pa.schema([\n...     pa.field(\"vector\", pa.list_(pa.float32(), 2)),\n...     pa.field(\"item\", pa.utf8()),\n...     pa.field(\"price\", pa.float32()),\n... ])\n&gt;&gt;&gt; async def iterable_example():\n...     db = await lancedb.connect_async(\"./.lancedb\")\n...     await db.create_table(\"table4\", make_batches(), schema=schema)\n&gt;&gt;&gt; asyncio.run(iterable_example())\n</code></pre> Source code in <code>lancedb/db.py</code> <pre><code>async def create_table(\n    self,\n    name: str,\n    data: Optional[DATA] = None,\n    schema: Optional[Union[pa.Schema, LanceModel]] = None,\n    mode: Optional[Literal[\"create\", \"overwrite\"]] = None,\n    exist_ok: Optional[bool] = None,\n    on_bad_vectors: Optional[str] = None,\n    fill_value: Optional[float] = None,\n    storage_options: Optional[Dict[str, str]] = None,\n    *,\n    embedding_functions: Optional[List[EmbeddingFunctionConfig]] = None,\n) -&gt; AsyncTable:\n    \"\"\"Create an [AsyncTable][lancedb.table.AsyncTable] in the database.\n\n    Parameters\n    ----------\n    name: str\n        The name of the table.\n    data: The data to initialize the table, *optional*\n        User must provide at least one of `data` or `schema`.\n        Acceptable types are:\n\n        - list-of-dict\n\n        - pandas.DataFrame\n\n        - pyarrow.Table or pyarrow.RecordBatch\n    schema: The schema of the table, *optional*\n        Acceptable types are:\n\n        - pyarrow.Schema\n\n        - [LanceModel][lancedb.pydantic.LanceModel]\n    mode: Literal[\"create\", \"overwrite\"]; default \"create\"\n        The mode to use when creating the table.\n        Can be either \"create\" or \"overwrite\".\n        By default, if the table already exists, an exception is raised.\n        If you want to overwrite the table, use mode=\"overwrite\".\n    exist_ok: bool, default False\n        If a table by the same name already exists, then raise an exception\n        if exist_ok=False. If exist_ok=True, then open the existing table;\n        it will not add the provided data but will validate against any\n        schema that's specified.\n    on_bad_vectors: str, default \"error\"\n        What to do if any of the vectors are not the same size or contains NaNs.\n        One of \"error\", \"drop\", \"fill\".\n    fill_value: float\n        The value to use when filling vectors. Only used if on_bad_vectors=\"fill\".\n    storage_options: dict, optional\n        Additional options for the storage backend. Options already set on the\n        connection will be inherited by the table, but can be overridden here.\n        See available options at\n        &lt;https://lancedb.github.io/lancedb/guides/storage/&gt;\n\n    Returns\n    -------\n    AsyncTable\n        A reference to the newly created table.\n\n    !!! note\n\n        The vector index won't be created by default.\n        To create the index, call the `create_index` method on the table.\n\n    Examples\n    --------\n\n    Can create with list of tuples or dictionaries:\n\n    &gt;&gt;&gt; import lancedb\n    &gt;&gt;&gt; async def doctest_example():\n    ...     db = await lancedb.connect_async(\"./.lancedb\")\n    ...     data = [{\"vector\": [1.1, 1.2], \"lat\": 45.5, \"long\": -122.7},\n    ...             {\"vector\": [0.2, 1.8], \"lat\": 40.1, \"long\":  -74.1}]\n    ...     my_table = await db.create_table(\"my_table\", data)\n    ...     print(await my_table.query().limit(5).to_arrow())\n    &gt;&gt;&gt; import asyncio\n    &gt;&gt;&gt; asyncio.run(doctest_example())\n    pyarrow.Table\n    vector: fixed_size_list&lt;item: float&gt;[2]\n      child 0, item: float\n    lat: double\n    long: double\n    ----\n    vector: [[[1.1,1.2],[0.2,1.8]]]\n    lat: [[45.5,40.1]]\n    long: [[-122.7,-74.1]]\n\n    You can also pass a pandas DataFrame:\n\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; data = pd.DataFrame({\n    ...    \"vector\": [[1.1, 1.2], [0.2, 1.8]],\n    ...    \"lat\": [45.5, 40.1],\n    ...    \"long\": [-122.7, -74.1]\n    ... })\n    &gt;&gt;&gt; async def pandas_example():\n    ...     db = await lancedb.connect_async(\"./.lancedb\")\n    ...     my_table = await db.create_table(\"table2\", data)\n    ...     print(await my_table.query().limit(5).to_arrow())\n    &gt;&gt;&gt; asyncio.run(pandas_example())\n    pyarrow.Table\n    vector: fixed_size_list&lt;item: float&gt;[2]\n      child 0, item: float\n    lat: double\n    long: double\n    ----\n    vector: [[[1.1,1.2],[0.2,1.8]]]\n    lat: [[45.5,40.1]]\n    long: [[-122.7,-74.1]]\n\n    Data is converted to Arrow before being written to disk. For maximum\n    control over how data is saved, either provide the PyArrow schema to\n    convert to or else provide a [PyArrow Table](pyarrow.Table) directly.\n\n    &gt;&gt;&gt; import pyarrow as pa\n    &gt;&gt;&gt; custom_schema = pa.schema([\n    ...   pa.field(\"vector\", pa.list_(pa.float32(), 2)),\n    ...   pa.field(\"lat\", pa.float32()),\n    ...   pa.field(\"long\", pa.float32())\n    ... ])\n    &gt;&gt;&gt; async def with_schema():\n    ...     db = await lancedb.connect_async(\"./.lancedb\")\n    ...     my_table = await db.create_table(\"table3\", data, schema = custom_schema)\n    ...     print(await my_table.query().limit(5).to_arrow())\n    &gt;&gt;&gt; asyncio.run(with_schema())\n    pyarrow.Table\n    vector: fixed_size_list&lt;item: float&gt;[2]\n      child 0, item: float\n    lat: float\n    long: float\n    ----\n    vector: [[[1.1,1.2],[0.2,1.8]]]\n    lat: [[45.5,40.1]]\n    long: [[-122.7,-74.1]]\n\n\n    It is also possible to create an table from `[Iterable[pa.RecordBatch]]`:\n\n\n    &gt;&gt;&gt; import pyarrow as pa\n    &gt;&gt;&gt; def make_batches():\n    ...     for i in range(5):\n    ...         yield pa.RecordBatch.from_arrays(\n    ...             [\n    ...                 pa.array([[3.1, 4.1], [5.9, 26.5]],\n    ...                     pa.list_(pa.float32(), 2)),\n    ...                 pa.array([\"foo\", \"bar\"]),\n    ...                 pa.array([10.0, 20.0]),\n    ...             ],\n    ...             [\"vector\", \"item\", \"price\"],\n    ...         )\n    &gt;&gt;&gt; schema=pa.schema([\n    ...     pa.field(\"vector\", pa.list_(pa.float32(), 2)),\n    ...     pa.field(\"item\", pa.utf8()),\n    ...     pa.field(\"price\", pa.float32()),\n    ... ])\n    &gt;&gt;&gt; async def iterable_example():\n    ...     db = await lancedb.connect_async(\"./.lancedb\")\n    ...     await db.create_table(\"table4\", make_batches(), schema=schema)\n    &gt;&gt;&gt; asyncio.run(iterable_example())\n    \"\"\"\n    metadata = None\n\n    if embedding_functions is not None:\n        # If we passed in embedding functions explicitly\n        # then we'll override any schema metadata that\n        # may was implicitly specified by the LanceModel schema\n        registry = EmbeddingFunctionRegistry.get_instance()\n        metadata = registry.get_table_metadata(embedding_functions)\n\n    # Defining defaults here and not in function prototype.  In the future\n    # these defaults will move into rust so better to keep them as None.\n    if on_bad_vectors is None:\n        on_bad_vectors = \"error\"\n\n    if fill_value is None:\n        fill_value = 0.0\n\n    data, schema = sanitize_create_table(\n        data, schema, metadata, on_bad_vectors, fill_value\n    )\n    validate_schema(schema)\n\n    if exist_ok is None:\n        exist_ok = False\n    if mode is None:\n        mode = \"create\"\n    if mode == \"create\" and exist_ok:\n        mode = \"exist_ok\"\n\n    if data is None:\n        new_table = await self._inner.create_empty_table(\n            name,\n            mode,\n            schema,\n            storage_options=storage_options,\n        )\n    else:\n        data = data_to_reader(data, schema)\n        new_table = await self._inner.create_table(\n            name,\n            mode,\n            data,\n            storage_options=storage_options,\n        )\n\n    return AsyncTable(new_table)\n</code></pre>"},{"location":"python/python/#lancedb.db.AsyncConnection.open_table","title":"open_table  <code>async</code>","text":"<pre><code>open_table(name: str, storage_options: Optional[Dict[str, str]] = None, index_cache_size: Optional[int] = None) -&gt; AsyncTable\n</code></pre> <p>Open a Lance Table in the database.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the table.</p> </li> <li> <code>storage_options</code>               (<code>Optional[Dict[str, str]]</code>, default:                   <code>None</code> )           \u2013            <p>Additional options for the storage backend. Options already set on the connection will be inherited by the table, but can be overridden here. See available options at https://lancedb.github.io/lancedb/guides/storage/</p> </li> <li> <code>index_cache_size</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Set the size of the index cache, specified as a number of entries</p> <p>The exact meaning of an \"entry\" will depend on the type of index: * IVF - there is one entry for each IVF partition * BTREE - there is one entry for the entire index</p> <p>This cache applies to the entire opened table, across all indices. Setting this value higher will increase performance on larger datasets at the expense of more RAM</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>A LanceTable object representing the table.</code>           \u2013            </li> </ul> Source code in <code>lancedb/db.py</code> <pre><code>async def open_table(\n    self,\n    name: str,\n    storage_options: Optional[Dict[str, str]] = None,\n    index_cache_size: Optional[int] = None,\n) -&gt; AsyncTable:\n    \"\"\"Open a Lance Table in the database.\n\n    Parameters\n    ----------\n    name: str\n        The name of the table.\n    storage_options: dict, optional\n        Additional options for the storage backend. Options already set on the\n        connection will be inherited by the table, but can be overridden here.\n        See available options at\n        &lt;https://lancedb.github.io/lancedb/guides/storage/&gt;\n    index_cache_size: int, default 256\n        Set the size of the index cache, specified as a number of entries\n\n        The exact meaning of an \"entry\" will depend on the type of index:\n        * IVF - there is one entry for each IVF partition\n        * BTREE - there is one entry for the entire index\n\n        This cache applies to the entire opened table, across all indices.\n        Setting this value higher will increase performance on larger datasets\n        at the expense of more RAM\n\n    Returns\n    -------\n    A LanceTable object representing the table.\n    \"\"\"\n    table = await self._inner.open_table(name, storage_options, index_cache_size)\n    return AsyncTable(table)\n</code></pre>"},{"location":"python/python/#lancedb.db.AsyncConnection.rename_table","title":"rename_table  <code>async</code>","text":"<pre><code>rename_table(old_name: str, new_name: str)\n</code></pre> <p>Rename a table in the database.</p> <p>Parameters:</p> <ul> <li> <code>old_name</code>               (<code>str</code>)           \u2013            <p>The current name of the table.</p> </li> <li> <code>new_name</code>               (<code>str</code>)           \u2013            <p>The new name of the table.</p> </li> </ul> Source code in <code>lancedb/db.py</code> <pre><code>async def rename_table(self, old_name: str, new_name: str):\n    \"\"\"Rename a table in the database.\n\n    Parameters\n    ----------\n    old_name: str\n        The current name of the table.\n    new_name: str\n        The new name of the table.\n    \"\"\"\n    await self._inner.rename_table(old_name, new_name)\n</code></pre>"},{"location":"python/python/#lancedb.db.AsyncConnection.drop_table","title":"drop_table  <code>async</code>","text":"<pre><code>drop_table(name: str, *, ignore_missing: bool = False)\n</code></pre> <p>Drop a table from the database.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the table.</p> </li> <li> <code>ignore_missing</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, ignore if the table does not exist.</p> </li> </ul> Source code in <code>lancedb/db.py</code> <pre><code>async def drop_table(self, name: str, *, ignore_missing: bool = False):\n    \"\"\"Drop a table from the database.\n\n    Parameters\n    ----------\n    name: str\n        The name of the table.\n    ignore_missing: bool, default False\n        If True, ignore if the table does not exist.\n    \"\"\"\n    try:\n        await self._inner.drop_table(name)\n    except ValueError as e:\n        if not ignore_missing:\n            raise e\n        if f\"Table '{name}' was not found\" not in str(e):\n            raise e\n</code></pre>"},{"location":"python/python/#lancedb.db.AsyncConnection.drop_all_tables","title":"drop_all_tables  <code>async</code>","text":"<pre><code>drop_all_tables()\n</code></pre> <p>Drop all tables from the database.</p> Source code in <code>lancedb/db.py</code> <pre><code>async def drop_all_tables(self):\n    \"\"\"Drop all tables from the database.\"\"\"\n    await self._inner.drop_all_tables()\n</code></pre>"},{"location":"python/python/#lancedb.db.AsyncConnection.drop_database","title":"drop_database  <code>async</code>","text":"<pre><code>drop_database()\n</code></pre> <p>Drop database This is the same thing as dropping all the tables</p> Source code in <code>lancedb/db.py</code> <pre><code>@deprecation.deprecated(\n    deprecated_in=\"0.15.1\",\n    removed_in=\"0.17\",\n    current_version=__version__,\n    details=\"Use drop_all_tables() instead\",\n)\nasync def drop_database(self):\n    \"\"\"\n    Drop database\n    This is the same thing as dropping all the tables\n    \"\"\"\n    await self._inner.drop_all_tables()\n</code></pre>"},{"location":"python/python/#tables-asynchronous","title":"Tables (Asynchronous)","text":"<p>Table hold your actual data as a collection of records / rows.</p>"},{"location":"python/python/#lancedb.table.AsyncTable","title":"lancedb.table.AsyncTable","text":"<p>An AsyncTable is a collection of Records in a LanceDB Database.</p> <p>An AsyncTable can be obtained from the AsyncConnection.create_table and AsyncConnection.open_table methods.</p> <p>An AsyncTable object is expected to be long lived and reused for multiple operations. AsyncTable objects will cache a certain amount of index data in memory. This cache will be freed when the Table is garbage collected.  To eagerly free the cache you can call the close method.  Once the AsyncTable is closed, it cannot be used for any further operations.</p> <p>An AsyncTable can also be used as a context manager, and will automatically close when the context is exited.  Closing a table is optional.  If you do not close the table, it will be closed when the AsyncTable object is garbage collected.</p> <p>Examples:</p> <p>Create using AsyncConnection.create_table (more examples in that method's documentation).</p> <pre><code>&gt;&gt;&gt; import lancedb\n&gt;&gt;&gt; async def create_a_table():\n...     db = await lancedb.connect_async(\"./.lancedb\")\n...     data = [{\"vector\": [1.1, 1.2], \"b\": 2}]\n...     table = await db.create_table(\"my_table\", data=data)\n...     print(await table.query().limit(5).to_arrow())\n&gt;&gt;&gt; import asyncio\n&gt;&gt;&gt; asyncio.run(create_a_table())\npyarrow.Table\nvector: fixed_size_list&lt;item: float&gt;[2]\n  child 0, item: float\nb: int64\n----\nvector: [[[1.1,1.2]]]\nb: [[2]]\n</code></pre> <p>Can append new data with AsyncTable.add().</p> <pre><code>&gt;&gt;&gt; async def add_to_table():\n...     db = await lancedb.connect_async(\"./.lancedb\")\n...     table = await db.open_table(\"my_table\")\n...     await table.add([{\"vector\": [0.5, 1.3], \"b\": 4}])\n&gt;&gt;&gt; asyncio.run(add_to_table())\n</code></pre> <p>Can query the table with AsyncTable.vector_search.</p> <pre><code>&gt;&gt;&gt; async def search_table_for_vector():\n...     db = await lancedb.connect_async(\"./.lancedb\")\n...     table = await db.open_table(\"my_table\")\n...     results = (\n...       await table.vector_search([0.4, 0.4]).select([\"b\", \"vector\"]).to_pandas()\n...     )\n...     print(results)\n&gt;&gt;&gt; asyncio.run(search_table_for_vector())\n   b      vector  _distance\n0  4  [0.5, 1.3]       0.82\n1  2  [1.1, 1.2]       1.13\n</code></pre> <p>Search queries are much faster when an index is created. See AsyncTable.create_index.</p> Source code in <code>lancedb/table.py</code> <pre><code>class AsyncTable:\n    \"\"\"\n    An AsyncTable is a collection of Records in a LanceDB Database.\n\n    An AsyncTable can be obtained from the\n    [AsyncConnection.create_table][lancedb.AsyncConnection.create_table] and\n    [AsyncConnection.open_table][lancedb.AsyncConnection.open_table] methods.\n\n    An AsyncTable object is expected to be long lived and reused for multiple\n    operations. AsyncTable objects will cache a certain amount of index data in memory.\n    This cache will be freed when the Table is garbage collected.  To eagerly free the\n    cache you can call the [close][lancedb.AsyncTable.close] method.  Once the\n    AsyncTable is closed, it cannot be used for any further operations.\n\n    An AsyncTable can also be used as a context manager, and will automatically close\n    when the context is exited.  Closing a table is optional.  If you do not close the\n    table, it will be closed when the AsyncTable object is garbage collected.\n\n    Examples\n    --------\n\n    Create using [AsyncConnection.create_table][lancedb.AsyncConnection.create_table]\n    (more examples in that method's documentation).\n\n    &gt;&gt;&gt; import lancedb\n    &gt;&gt;&gt; async def create_a_table():\n    ...     db = await lancedb.connect_async(\"./.lancedb\")\n    ...     data = [{\"vector\": [1.1, 1.2], \"b\": 2}]\n    ...     table = await db.create_table(\"my_table\", data=data)\n    ...     print(await table.query().limit(5).to_arrow())\n    &gt;&gt;&gt; import asyncio\n    &gt;&gt;&gt; asyncio.run(create_a_table())\n    pyarrow.Table\n    vector: fixed_size_list&lt;item: float&gt;[2]\n      child 0, item: float\n    b: int64\n    ----\n    vector: [[[1.1,1.2]]]\n    b: [[2]]\n\n    Can append new data with [AsyncTable.add()][lancedb.table.AsyncTable.add].\n\n    &gt;&gt;&gt; async def add_to_table():\n    ...     db = await lancedb.connect_async(\"./.lancedb\")\n    ...     table = await db.open_table(\"my_table\")\n    ...     await table.add([{\"vector\": [0.5, 1.3], \"b\": 4}])\n    &gt;&gt;&gt; asyncio.run(add_to_table())\n\n    Can query the table with\n    [AsyncTable.vector_search][lancedb.table.AsyncTable.vector_search].\n\n    &gt;&gt;&gt; async def search_table_for_vector():\n    ...     db = await lancedb.connect_async(\"./.lancedb\")\n    ...     table = await db.open_table(\"my_table\")\n    ...     results = (\n    ...       await table.vector_search([0.4, 0.4]).select([\"b\", \"vector\"]).to_pandas()\n    ...     )\n    ...     print(results)\n    &gt;&gt;&gt; asyncio.run(search_table_for_vector())\n       b      vector  _distance\n    0  4  [0.5, 1.3]       0.82\n    1  2  [1.1, 1.2]       1.13\n\n    Search queries are much faster when an index is created. See\n    [AsyncTable.create_index][lancedb.table.AsyncTable.create_index].\n    \"\"\"\n\n    def __init__(self, table: LanceDBTable):\n        \"\"\"Create a new AsyncTable object.\n\n        You should not create AsyncTable objects directly.\n\n        Use [AsyncConnection.create_table][lancedb.AsyncConnection.create_table] and\n        [AsyncConnection.open_table][lancedb.AsyncConnection.open_table] to obtain\n        Table objects.\"\"\"\n        self._inner = table\n\n    def __repr__(self):\n        return self._inner.__repr__()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *_):\n        self.close()\n\n    def is_open(self) -&gt; bool:\n        \"\"\"Return True if the table is open.\"\"\"\n        return self._inner.is_open()\n\n    def close(self):\n        \"\"\"Close the table and free any resources associated with it.\n\n        It is safe to call this method multiple times.\n\n        Any attempt to use the table after it has been closed will raise an error.\"\"\"\n        return self._inner.close()\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"The name of the table.\"\"\"\n        return self._inner.name()\n\n    async def schema(self) -&gt; pa.Schema:\n        \"\"\"The [Arrow Schema](https://arrow.apache.org/docs/python/api/datatypes.html#)\n        of this Table\n\n        \"\"\"\n        return await self._inner.schema()\n\n    async def embedding_functions(self) -&gt; Dict[str, EmbeddingFunctionConfig]:\n        \"\"\"\n        Get the embedding functions for the table\n\n        Returns\n        -------\n        funcs: Dict[str, EmbeddingFunctionConfig]\n            A mapping of the vector column to the embedding function\n            or empty dict if not configured.\n        \"\"\"\n        schema = await self.schema()\n        return EmbeddingFunctionRegistry.get_instance().parse_functions(schema.metadata)\n\n    async def count_rows(self, filter: Optional[str] = None) -&gt; int:\n        \"\"\"\n        Count the number of rows in the table.\n\n        Parameters\n        ----------\n        filter: str, optional\n            A SQL where clause to filter the rows to count.\n        \"\"\"\n        return await self._inner.count_rows(filter)\n\n    async def head(self, n=5) -&gt; pa.Table:\n        \"\"\"\n        Return the first `n` rows of the table.\n\n        Parameters\n        ----------\n        n: int, default 5\n            The number of rows to return.\n        \"\"\"\n        return await self.query().limit(n).to_arrow()\n\n    def query(self) -&gt; AsyncQuery:\n        \"\"\"\n        Returns an [AsyncQuery][lancedb.query.AsyncQuery] that can be used\n        to search the table.\n\n        Use methods on the returned query to control query behavior.  The query\n        can be executed with methods like [to_arrow][lancedb.query.AsyncQuery.to_arrow],\n        [to_pandas][lancedb.query.AsyncQuery.to_pandas] and more.\n        \"\"\"\n        return AsyncQuery(self._inner.query())\n\n    async def to_pandas(self) -&gt; \"pd.DataFrame\":\n        \"\"\"Return the table as a pandas DataFrame.\n\n        Returns\n        -------\n        pd.DataFrame\n        \"\"\"\n        return (await self.to_arrow()).to_pandas()\n\n    async def to_arrow(self) -&gt; pa.Table:\n        \"\"\"Return the table as a pyarrow Table.\n\n        Returns\n        -------\n        pa.Table\n        \"\"\"\n        return await self.query().to_arrow()\n\n    async def create_index(\n        self,\n        column: str,\n        *,\n        replace: Optional[bool] = None,\n        config: Optional[\n            Union[IvfFlat, IvfPq, HnswPq, HnswSq, BTree, Bitmap, LabelList, FTS]\n        ] = None,\n        wait_timeout: Optional[timedelta] = None,\n    ):\n        \"\"\"Create an index to speed up queries\n\n        Indices can be created on vector columns or scalar columns.\n        Indices on vector columns will speed up vector searches.\n        Indices on scalar columns will speed up filtering (in both\n        vector and non-vector searches)\n\n        Parameters\n        ----------\n        column: str\n            The column to index.\n        replace: bool, default True\n            Whether to replace the existing index\n\n            If this is false, and another index already exists on the same columns\n            and the same name, then an error will be returned.  This is true even if\n            that index is out of date.\n\n            The default is True\n        config: default None\n            For advanced configuration you can specify the type of index you would\n            like to create.   You can also specify index-specific parameters when\n            creating an index object.\n        wait_timeout: timedelta, optional\n            The timeout to wait if indexing is asynchronous.\n        \"\"\"\n        if config is not None:\n            if not isinstance(\n                config, (IvfFlat, IvfPq, HnswPq, HnswSq, BTree, Bitmap, LabelList, FTS)\n            ):\n                raise TypeError(\n                    \"config must be an instance of IvfPq, HnswPq, HnswSq, BTree,\"\n                    \" Bitmap, LabelList, or FTS\"\n                )\n        try:\n            await self._inner.create_index(\n                column, index=config, replace=replace, wait_timeout=wait_timeout\n            )\n        except ValueError as e:\n            if \"not support the requested language\" in str(e):\n                supported_langs = \", \".join(lang_mapping.values())\n                help_msg = f\"Supported languages: {supported_langs}\"\n                add_note(e, help_msg)\n            raise e\n\n    async def drop_index(self, name: str) -&gt; None:\n        \"\"\"\n        Drop an index from the table.\n\n        Parameters\n        ----------\n        name: str\n            The name of the index to drop.\n\n        Notes\n        -----\n        This does not delete the index from disk, it just removes it from the table.\n        To delete the index, run [optimize][lancedb.table.AsyncTable.optimize]\n        after dropping the index.\n\n        Use [list_indices][lancedb.table.AsyncTable.list_indices] to find the names\n        of the indices.\n        \"\"\"\n        await self._inner.drop_index(name)\n\n    async def prewarm_index(self, name: str) -&gt; None:\n        \"\"\"\n        Prewarm an index in the table.\n\n        Parameters\n        ----------\n        name: str\n            The name of the index to prewarm\n\n        Notes\n        -----\n        This will load the index into memory.  This may reduce the cold-start time for\n        future queries.  If the index does not fit in the cache then this call may be\n        wasteful.\n        \"\"\"\n        await self._inner.prewarm_index(name)\n\n    async def wait_for_index(\n        self, index_names: Iterable[str], timeout: timedelta = timedelta(seconds=300)\n    ) -&gt; None:\n        \"\"\"\n        Wait for indexing to complete for the given index names.\n        This will poll the table until all the indices are fully indexed,\n        or raise a timeout exception if the timeout is reached.\n\n        Parameters\n        ----------\n        index_names: str\n            The name of the indices to poll\n        timeout: timedelta\n            Timeout to wait for asynchronous indexing. The default is 5 minutes.\n        \"\"\"\n        await self._inner.wait_for_index(index_names, timeout)\n\n    async def add(\n        self,\n        data: DATA,\n        *,\n        mode: Optional[Literal[\"append\", \"overwrite\"]] = \"append\",\n        on_bad_vectors: Optional[OnBadVectorsType] = None,\n        fill_value: Optional[float] = None,\n    ):\n        \"\"\"Add more data to the [Table](Table).\n\n        Parameters\n        ----------\n        data: DATA\n            The data to insert into the table. Acceptable types are:\n\n            - list-of-dict\n\n            - pandas.DataFrame\n\n            - pyarrow.Table or pyarrow.RecordBatch\n        mode: str\n            The mode to use when writing the data. Valid values are\n            \"append\" and \"overwrite\".\n        on_bad_vectors: str, default \"error\"\n            What to do if any of the vectors are not the same size or contains NaNs.\n            One of \"error\", \"drop\", \"fill\", \"null\".\n        fill_value: float, default 0.\n            The value to use when filling vectors. Only used if on_bad_vectors=\"fill\".\n\n        \"\"\"\n        schema = await self.schema()\n        if on_bad_vectors is None:\n            on_bad_vectors = \"error\"\n        if fill_value is None:\n            fill_value = 0.0\n        data = _sanitize_data(\n            data,\n            schema,\n            metadata=schema.metadata,\n            on_bad_vectors=on_bad_vectors,\n            fill_value=fill_value,\n            allow_subschema=True,\n        )\n        if isinstance(data, pa.Table):\n            data = data.to_reader()\n\n        await self._inner.add(data, mode or \"append\")\n\n    def merge_insert(self, on: Union[str, Iterable[str]]) -&gt; LanceMergeInsertBuilder:\n        \"\"\"\n        Returns a [`LanceMergeInsertBuilder`][lancedb.merge.LanceMergeInsertBuilder]\n        that can be used to create a \"merge insert\" operation\n\n        This operation can add rows, update rows, and remove rows all in a single\n        transaction. It is a very generic tool that can be used to create\n        behaviors like \"insert if not exists\", \"update or insert (i.e. upsert)\",\n        or even replace a portion of existing data with new data (e.g. replace\n        all data where month=\"january\")\n\n        The merge insert operation works by combining new data from a\n        **source table** with existing data in a **target table** by using a\n        join.  There are three categories of records.\n\n        \"Matched\" records are records that exist in both the source table and\n        the target table. \"Not matched\" records exist only in the source table\n        (e.g. these are new data) \"Not matched by source\" records exist only\n        in the target table (this is old data)\n\n        The builder returned by this method can be used to customize what\n        should happen for each category of data.\n\n        Please note that the data may appear to be reordered as part of this\n        operation.  This is because updated rows will be deleted from the\n        dataset and then reinserted at the end with the new values.\n\n        Parameters\n        ----------\n\n        on: Union[str, Iterable[str]]\n            A column (or columns) to join on.  This is how records from the\n            source table and target table are matched.  Typically this is some\n            kind of key or id column.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import lancedb\n        &gt;&gt;&gt; data = pa.table({\"a\": [2, 1, 3], \"b\": [\"a\", \"b\", \"c\"]})\n        &gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n        &gt;&gt;&gt; table = db.create_table(\"my_table\", data)\n        &gt;&gt;&gt; new_data = pa.table({\"a\": [2, 3, 4], \"b\": [\"x\", \"y\", \"z\"]})\n        &gt;&gt;&gt; # Perform a \"upsert\" operation\n        &gt;&gt;&gt; table.merge_insert(\"a\")             \\\\\n        ...      .when_matched_update_all()     \\\\\n        ...      .when_not_matched_insert_all() \\\\\n        ...      .execute(new_data)\n        &gt;&gt;&gt; # The order of new rows is non-deterministic since we use\n        &gt;&gt;&gt; # a hash-join as part of this operation and so we sort here\n        &gt;&gt;&gt; table.to_arrow().sort_by(\"a\").to_pandas()\n           a  b\n        0  1  b\n        1  2  x\n        2  3  y\n        3  4  z\n        \"\"\"\n        on = [on] if isinstance(on, str) else list(iter(on))\n\n        return LanceMergeInsertBuilder(self, on)\n\n    @overload\n    async def search(\n        self,\n        query: Optional[str] = None,\n        vector_column_name: Optional[str] = None,\n        query_type: Literal[\"auto\"] = ...,\n        ordering_field_name: Optional[str] = None,\n        fts_columns: Optional[Union[str, List[str]]] = None,\n    ) -&gt; Union[AsyncHybridQuery, AsyncFTSQuery, AsyncVectorQuery]: ...\n\n    @overload\n    async def search(\n        self,\n        query: Optional[str] = None,\n        vector_column_name: Optional[str] = None,\n        query_type: Literal[\"hybrid\"] = ...,\n        ordering_field_name: Optional[str] = None,\n        fts_columns: Optional[Union[str, List[str]]] = None,\n    ) -&gt; AsyncHybridQuery: ...\n\n    @overload\n    async def search(\n        self,\n        query: Optional[Union[VEC, \"PIL.Image.Image\", Tuple]] = None,\n        vector_column_name: Optional[str] = None,\n        query_type: Literal[\"auto\"] = ...,\n        ordering_field_name: Optional[str] = None,\n        fts_columns: Optional[Union[str, List[str]]] = None,\n    ) -&gt; AsyncVectorQuery: ...\n\n    @overload\n    async def search(\n        self,\n        query: Optional[str] = None,\n        vector_column_name: Optional[str] = None,\n        query_type: Literal[\"fts\"] = ...,\n        ordering_field_name: Optional[str] = None,\n        fts_columns: Optional[Union[str, List[str]]] = None,\n    ) -&gt; AsyncFTSQuery: ...\n\n    @overload\n    async def search(\n        self,\n        query: Optional[\n            Union[VEC, str, \"PIL.Image.Image\", Tuple, FullTextQuery]\n        ] = None,\n        vector_column_name: Optional[str] = None,\n        query_type: Literal[\"vector\"] = ...,\n        ordering_field_name: Optional[str] = None,\n        fts_columns: Optional[Union[str, List[str]]] = None,\n    ) -&gt; AsyncVectorQuery: ...\n\n    async def search(\n        self,\n        query: Optional[\n            Union[VEC, str, \"PIL.Image.Image\", Tuple, FullTextQuery]\n        ] = None,\n        vector_column_name: Optional[str] = None,\n        query_type: QueryType = \"auto\",\n        ordering_field_name: Optional[str] = None,\n        fts_columns: Optional[Union[str, List[str]]] = None,\n    ) -&gt; Union[AsyncHybridQuery, AsyncFTSQuery, AsyncVectorQuery]:\n        \"\"\"Create a search query to find the nearest neighbors\n        of the given query vector. We currently support [vector search][search]\n        and [full-text search][experimental-full-text-search].\n\n        All query options are defined in [AsyncQuery][lancedb.query.AsyncQuery].\n\n        Parameters\n        ----------\n        query: list/np.ndarray/str/PIL.Image.Image, default None\n            The targetted vector to search for.\n\n            - *default None*.\n            Acceptable types are: list, np.ndarray, PIL.Image.Image\n\n            - If None then the select/where/limit clauses are applied to filter\n            the table\n        vector_column_name: str, optional\n            The name of the vector column to search.\n\n            The vector column needs to be a pyarrow fixed size list type\n\n            - If not specified then the vector column is inferred from\n            the table schema\n\n            - If the table has multiple vector columns then the *vector_column_name*\n            needs to be specified. Otherwise, an error is raised.\n        query_type: str\n            *default \"auto\"*.\n            Acceptable types are: \"vector\", \"fts\", \"hybrid\", or \"auto\"\n\n            - If \"auto\" then the query type is inferred from the query;\n\n                - If `query` is a list/np.ndarray then the query type is\n                \"vector\";\n\n                - If `query` is a PIL.Image.Image then either do vector search,\n                or raise an error if no corresponding embedding function is found.\n\n            - If `query` is a string, then the query type is \"vector\" if the\n              table has embedding functions else the query type is \"fts\"\n\n        Returns\n        -------\n        LanceQueryBuilder\n            A query builder object representing the query.\n        \"\"\"\n\n        def is_embedding(query):\n            return isinstance(query, (list, np.ndarray, pa.Array, pa.ChunkedArray))\n\n        async def get_embedding_func(\n            vector_column_name: Optional[str],\n            query_type: QueryType,\n            query: Optional[Union[VEC, str, \"PIL.Image.Image\", Tuple, FullTextQuery]],\n        ) -&gt; Tuple[str, EmbeddingFunctionConfig]:\n            if isinstance(query, FullTextQuery):\n                query_type = \"fts\"\n            schema = await self.schema()\n            vector_column_name = infer_vector_column_name(\n                schema=schema,\n                query_type=query_type,\n                query=query,\n                vector_column_name=vector_column_name,\n            )\n            funcs = EmbeddingFunctionRegistry.get_instance().parse_functions(\n                schema.metadata\n            )\n            func = funcs.get(vector_column_name)\n            if func is None:\n                error = ValueError(\n                    f\"Column '{vector_column_name}' has no registered \"\n                    \"embedding function.\"\n                )\n                if len(funcs) &gt; 0:\n                    add_note(\n                        error,\n                        \"Embedding functions are registered for columns: \"\n                        f\"{list(funcs.keys())}\",\n                    )\n                else:\n                    add_note(\n                        error, \"No embedding functions are registered for any columns.\"\n                    )\n                raise error\n            return vector_column_name, func\n\n        async def make_embedding(embedding, query):\n            if embedding is not None:\n                loop = asyncio.get_running_loop()\n                # This function is likely to block, since it either calls an expensive\n                # function or makes an HTTP request to an embeddings REST API.\n                return (\n                    await loop.run_in_executor(\n                        None,\n                        embedding.function.compute_query_embeddings_with_retry,\n                        query,\n                    )\n                )[0]\n            else:\n                return None\n\n        if query_type == \"auto\":\n            # Infer the query type.\n            if is_embedding(query):\n                vector_query = query\n                query_type = \"vector\"\n            elif isinstance(query, FullTextQuery):\n                query_type = \"fts\"\n            elif isinstance(query, str):\n                try:\n                    (\n                        indices,\n                        (vector_column_name, embedding_conf),\n                    ) = await asyncio.gather(\n                        self.list_indices(),\n                        get_embedding_func(vector_column_name, \"auto\", query),\n                    )\n                except ValueError as e:\n                    if \"Column\" in str(\n                        e\n                    ) and \"has no registered embedding function\" in str(e):\n                        # If the column has no registered embedding function,\n                        # then it's an FTS query.\n                        query_type = \"fts\"\n                    else:\n                        raise e\n                else:\n                    if embedding_conf is not None:\n                        vector_query = await make_embedding(embedding_conf, query)\n                        if any(\n                            i.columns[0] == embedding_conf.source_column\n                            and i.index_type == \"FTS\"\n                            for i in indices\n                        ):\n                            query_type = \"hybrid\"\n                        else:\n                            query_type = \"vector\"\n                    else:\n                        query_type = \"fts\"\n            else:\n                # it's an image or something else embeddable.\n                query_type = \"vector\"\n        elif query_type == \"vector\":\n            if is_embedding(query):\n                vector_query = query\n            else:\n                vector_column_name, embedding_conf = await get_embedding_func(\n                    vector_column_name, query_type, query\n                )\n                vector_query = await make_embedding(embedding_conf, query)\n        elif query_type == \"hybrid\":\n            if is_embedding(query):\n                raise ValueError(\"Hybrid search requires a text query\")\n            else:\n                vector_column_name, embedding_conf = await get_embedding_func(\n                    vector_column_name, query_type, query\n                )\n                vector_query = await make_embedding(embedding_conf, query)\n\n        if query_type == \"vector\":\n            builder = self.query().nearest_to(vector_query)\n            if vector_column_name:\n                builder = builder.column(vector_column_name)\n            return builder\n        elif query_type == \"fts\":\n            return self.query().nearest_to_text(query, columns=fts_columns)\n        elif query_type == \"hybrid\":\n            builder = self.query().nearest_to(vector_query)\n            if vector_column_name:\n                builder = builder.column(vector_column_name)\n            return builder.nearest_to_text(query, columns=fts_columns)\n        else:\n            raise ValueError(f\"Unknown query type: '{query_type}'\")\n\n    def vector_search(\n        self,\n        query_vector: Union[VEC, Tuple],\n    ) -&gt; AsyncVectorQuery:\n        \"\"\"\n        Search the table with a given query vector.\n        This is a convenience method for preparing a vector query and\n        is the same thing as calling `nearestTo` on the builder returned\n        by `query`.  Seer [nearest_to][lancedb.query.AsyncQuery.nearest_to] for more\n        details.\n        \"\"\"\n        return self.query().nearest_to(query_vector)\n\n    def _sync_query_to_async(\n        self, query: Query\n    ) -&gt; AsyncHybridQuery | AsyncFTSQuery | AsyncVectorQuery | AsyncQuery:\n        async_query = self.query()\n        if query.limit is not None:\n            async_query = async_query.limit(query.limit)\n        if query.offset is not None:\n            async_query = async_query.offset(query.offset)\n        if query.columns:\n            async_query = async_query.select(query.columns)\n        if query.filter:\n            async_query = async_query.where(query.filter)\n        if query.fast_search:\n            async_query = async_query.fast_search()\n        if query.with_row_id:\n            async_query = async_query.with_row_id()\n\n        if query.vector:\n            async_query = async_query.nearest_to(query.vector).distance_range(\n                query.lower_bound, query.upper_bound\n            )\n            if query.distance_type is not None:\n                async_query = async_query.distance_type(query.distance_type)\n            if query.nprobes is not None:\n                async_query = async_query.nprobes(query.nprobes)\n            if query.refine_factor is not None:\n                async_query = async_query.refine_factor(query.refine_factor)\n            if query.vector_column:\n                async_query = async_query.column(query.vector_column)\n            if query.ef:\n                async_query = async_query.ef(query.ef)\n            if query.bypass_vector_index:\n                async_query = async_query.bypass_vector_index()\n\n        if query.postfilter:\n            async_query = async_query.postfilter()\n\n        if query.full_text_query:\n            async_query = async_query.nearest_to_text(\n                query.full_text_query.query, query.full_text_query.columns\n            )\n\n        return async_query\n\n    async def _execute_query(\n        self,\n        query: Query,\n        *,\n        batch_size: Optional[int] = None,\n        timeout: Optional[timedelta] = None,\n    ) -&gt; pa.RecordBatchReader:\n        # The sync table calls into this method, so we need to map the\n        # query to the async version of the query and run that here. This is only\n        # used for that code path right now.\n\n        async_query = self._sync_query_to_async(query)\n\n        return await async_query.to_batches(\n            max_batch_length=batch_size, timeout=timeout\n        )\n\n    async def _explain_plan(self, query: Query, verbose: Optional[bool]) -&gt; str:\n        # This method is used by the sync table\n        async_query = self._sync_query_to_async(query)\n        return await async_query.explain_plan(verbose)\n\n    async def _analyze_plan(self, query: Query) -&gt; str:\n        # This method is used by the sync table\n        async_query = self._sync_query_to_async(query)\n        return await async_query.analyze_plan()\n\n    async def _do_merge(\n        self,\n        merge: LanceMergeInsertBuilder,\n        new_data: DATA,\n        on_bad_vectors: OnBadVectorsType,\n        fill_value: float,\n    ):\n        schema = await self.schema()\n        if on_bad_vectors is None:\n            on_bad_vectors = \"error\"\n        if fill_value is None:\n            fill_value = 0.0\n        data = _sanitize_data(\n            new_data,\n            schema,\n            metadata=schema.metadata,\n            on_bad_vectors=on_bad_vectors,\n            fill_value=fill_value,\n            allow_subschema=True,\n        )\n        if isinstance(data, pa.Table):\n            data = pa.RecordBatchReader.from_batches(data.schema, data.to_batches())\n        await self._inner.execute_merge_insert(\n            data,\n            dict(\n                on=merge._on,\n                when_matched_update_all=merge._when_matched_update_all,\n                when_matched_update_all_condition=merge._when_matched_update_all_condition,\n                when_not_matched_insert_all=merge._when_not_matched_insert_all,\n                when_not_matched_by_source_delete=merge._when_not_matched_by_source_delete,\n                when_not_matched_by_source_condition=merge._when_not_matched_by_source_condition,\n            ),\n        )\n\n    async def delete(self, where: str):\n        \"\"\"Delete rows from the table.\n\n        This can be used to delete a single row, many rows, all rows, or\n        sometimes no rows (if your predicate matches nothing).\n\n        Parameters\n        ----------\n        where: str\n            The SQL where clause to use when deleting rows.\n\n            - For example, 'x = 2' or 'x IN (1, 2, 3)'.\n\n            The filter must not be empty, or it will error.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import lancedb\n        &gt;&gt;&gt; data = [\n        ...    {\"x\": 1, \"vector\": [1.0, 2]},\n        ...    {\"x\": 2, \"vector\": [3.0, 4]},\n        ...    {\"x\": 3, \"vector\": [5.0, 6]}\n        ... ]\n        &gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n        &gt;&gt;&gt; table = db.create_table(\"my_table\", data)\n        &gt;&gt;&gt; table.to_pandas()\n           x      vector\n        0  1  [1.0, 2.0]\n        1  2  [3.0, 4.0]\n        2  3  [5.0, 6.0]\n        &gt;&gt;&gt; table.delete(\"x = 2\")\n        &gt;&gt;&gt; table.to_pandas()\n           x      vector\n        0  1  [1.0, 2.0]\n        1  3  [5.0, 6.0]\n\n        If you have a list of values to delete, you can combine them into a\n        stringified list and use the `IN` operator:\n\n        &gt;&gt;&gt; to_remove = [1, 5]\n        &gt;&gt;&gt; to_remove = \", \".join([str(v) for v in to_remove])\n        &gt;&gt;&gt; to_remove\n        '1, 5'\n        &gt;&gt;&gt; table.delete(f\"x IN ({to_remove})\")\n        &gt;&gt;&gt; table.to_pandas()\n           x      vector\n        0  3  [5.0, 6.0]\n        \"\"\"\n        return await self._inner.delete(where)\n\n    async def update(\n        self,\n        updates: Optional[Dict[str, Any]] = None,\n        *,\n        where: Optional[str] = None,\n        updates_sql: Optional[Dict[str, str]] = None,\n    ):\n        \"\"\"\n        This can be used to update zero to all rows in the table.\n\n        If a filter is provided with `where` then only rows matching the\n        filter will be updated.  Otherwise all rows will be updated.\n\n        Parameters\n        ----------\n        updates: dict, optional\n            The updates to apply.  The keys should be the name of the column to\n            update.  The values should be the new values to assign.  This is\n            required unless updates_sql is supplied.\n        where: str, optional\n            An SQL filter that controls which rows are updated. For example, 'x = 2'\n            or 'x IN (1, 2, 3)'.  Only rows that satisfy this filter will be udpated.\n        updates_sql: dict, optional\n            The updates to apply, expressed as SQL expression strings.  The keys should\n            be column names. The values should be SQL expressions.  These can be SQL\n            literals (e.g. \"7\" or \"'foo'\") or they can be expressions based on the\n            previous value of the row (e.g. \"x + 1\" to increment the x column by 1)\n\n        Examples\n        --------\n        &gt;&gt;&gt; import asyncio\n        &gt;&gt;&gt; import lancedb\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; async def demo_update():\n        ...     data = pd.DataFrame({\"x\": [1, 2], \"vector\": [[1, 2], [3, 4]]})\n        ...     db = await lancedb.connect_async(\"./.lancedb\")\n        ...     table = await db.create_table(\"my_table\", data)\n        ...     # x is [1, 2], vector is [[1, 2], [3, 4]]\n        ...     await table.update({\"vector\": [10, 10]}, where=\"x = 2\")\n        ...     # x is [1, 2], vector is [[1, 2], [10, 10]]\n        ...     await table.update(updates_sql={\"x\": \"x + 1\"})\n        ...     # x is [2, 3], vector is [[1, 2], [10, 10]]\n        &gt;&gt;&gt; asyncio.run(demo_update())\n        \"\"\"\n        if updates is not None and updates_sql is not None:\n            raise ValueError(\"Only one of updates or updates_sql can be provided\")\n        if updates is None and updates_sql is None:\n            raise ValueError(\"Either updates or updates_sql must be provided\")\n\n        if updates is not None:\n            updates_sql = {k: value_to_sql(v) for k, v in updates.items()}\n\n        return await self._inner.update(updates_sql, where)\n\n    async def add_columns(\n        self, transforms: dict[str, str] | pa.field | List[pa.field] | pa.Schema\n    ):\n        \"\"\"\n        Add new columns with defined values.\n\n        Parameters\n        ----------\n        transforms: Dict[str, str]\n            A map of column name to a SQL expression to use to calculate the\n            value of the new column. These expressions will be evaluated for\n            each row in the table, and can reference existing columns.\n            Alternatively, you can pass a pyarrow field or schema to add\n            new columns with NULLs.\n        \"\"\"\n        if isinstance(transforms, pa.Field):\n            transforms = [transforms]\n        if isinstance(transforms, list) and all(\n            {isinstance(f, pa.Field) for f in transforms}\n        ):\n            transforms = pa.schema(transforms)\n        if isinstance(transforms, pa.Schema):\n            await self._inner.add_columns_with_schema(transforms)\n        else:\n            await self._inner.add_columns(list(transforms.items()))\n\n    async def alter_columns(self, *alterations: Iterable[dict[str, Any]]):\n        \"\"\"\n        Alter column names and nullability.\n\n        alterations : Iterable[Dict[str, Any]]\n            A sequence of dictionaries, each with the following keys:\n            - \"path\": str\n                The column path to alter. For a top-level column, this is the name.\n                For a nested column, this is the dot-separated path, e.g. \"a.b.c\".\n            - \"rename\": str, optional\n                The new name of the column. If not specified, the column name is\n                not changed.\n            - \"data_type\": pyarrow.DataType, optional\n               The new data type of the column. Existing values will be casted\n               to this type. If not specified, the column data type is not changed.\n            - \"nullable\": bool, optional\n                Whether the column should be nullable. If not specified, the column\n                nullability is not changed. Only non-nullable columns can be changed\n                to nullable. Currently, you cannot change a nullable column to\n                non-nullable.\n        \"\"\"\n        await self._inner.alter_columns(alterations)\n\n    async def drop_columns(self, columns: Iterable[str]):\n        \"\"\"\n        Drop columns from the table.\n\n        Parameters\n        ----------\n        columns : Iterable[str]\n            The names of the columns to drop.\n        \"\"\"\n        await self._inner.drop_columns(columns)\n\n    async def version(self) -&gt; int:\n        \"\"\"\n        Retrieve the version of the table\n\n        LanceDb supports versioning.  Every operation that modifies the table increases\n        version.  As long as a version hasn't been deleted you can `[Self::checkout]`\n        that version to view the data at that point.  In addition, you can\n        `[Self::restore]` the version to replace the current table with a previous\n        version.\n        \"\"\"\n        return await self._inner.version()\n\n    async def list_versions(self):\n        \"\"\"\n        List all versions of the table\n        \"\"\"\n        versions = await self._inner.list_versions()\n        for v in versions:\n            ts_nanos = v[\"timestamp\"]\n            v[\"timestamp\"] = datetime.fromtimestamp(ts_nanos // 1e9) + timedelta(\n                microseconds=(ts_nanos % 1e9) // 1e3\n            )\n\n        return versions\n\n    async def checkout(self, version: int):\n        \"\"\"\n        Checks out a specific version of the Table\n\n        Any read operation on the table will now access the data at the checked out\n        version. As a consequence, calling this method will disable any read consistency\n        interval that was previously set.\n\n        This is a read-only operation that turns the table into a sort of \"view\"\n        or \"detached head\".  Other table instances will not be affected.  To make the\n        change permanent you can use the `[Self::restore]` method.\n\n        Any operation that modifies the table will fail while the table is in a checked\n        out state.\n\n        To return the table to a normal state use `[Self::checkout_latest]`\n        \"\"\"\n        try:\n            await self._inner.checkout(version)\n        except RuntimeError as e:\n            if \"not found\" in str(e):\n                raise ValueError(\n                    f\"Version {version} no longer exists. Was it cleaned up?\"\n                )\n            else:\n                raise\n\n    async def checkout_latest(self):\n        \"\"\"\n        Ensures the table is pointing at the latest version\n\n        This can be used to manually update a table when the read_consistency_interval\n        is None\n        It can also be used to undo a `[Self::checkout]` operation\n        \"\"\"\n        await self._inner.checkout_latest()\n\n    async def restore(self, version: Optional[int] = None):\n        \"\"\"\n        Restore the table to the currently checked out version\n\n        This operation will fail if checkout has not been called previously\n\n        This operation will overwrite the latest version of the table with a\n        previous version.  Any changes made since the checked out version will\n        no longer be visible.\n\n        Once the operation concludes the table will no longer be in a checked\n        out state and the read_consistency_interval, if any, will apply.\n        \"\"\"\n        await self._inner.restore(version)\n\n    async def optimize(\n        self,\n        *,\n        cleanup_older_than: Optional[timedelta] = None,\n        delete_unverified: bool = False,\n        retrain=False,\n    ) -&gt; OptimizeStats:\n        \"\"\"\n        Optimize the on-disk data and indices for better performance.\n\n        Modeled after ``VACUUM`` in PostgreSQL.\n\n        Optimization covers three operations:\n\n         * Compaction: Merges small files into larger ones\n         * Prune: Removes old versions of the dataset\n         * Index: Optimizes the indices, adding new data to existing indices\n\n        Parameters\n        ----------\n        cleanup_older_than: timedelta, optional default 7 days\n            All files belonging to versions older than this will be removed.  Set\n            to 0 days to remove all versions except the latest.  The latest version\n            is never removed.\n        delete_unverified: bool, default False\n            Files leftover from a failed transaction may appear to be part of an\n            in-progress operation (e.g. appending new data) and these files will not\n            be deleted unless they are at least 7 days old. If delete_unverified is True\n            then these files will be deleted regardless of their age.\n        retrain: bool, default False\n            If True, retrain the vector indices, this would refine the IVF clustering\n            and quantization, which may improve the search accuracy. It's faster than\n            re-creating the index from scratch, so it's recommended to try this first,\n            when the data distribution has changed significantly.\n\n        Experimental API\n        ----------------\n\n        The optimization process is undergoing active development and may change.\n        Our goal with these changes is to improve the performance of optimization and\n        reduce the complexity.\n\n        That being said, it is essential today to run optimize if you want the best\n        performance.  It should be stable and safe to use in production, but it our\n        hope that the API may be simplified (or not even need to be called) in the\n        future.\n\n        The frequency an application shoudl call optimize is based on the frequency of\n        data modifications.  If data is frequently added, deleted, or updated then\n        optimize should be run frequently.  A good rule of thumb is to run optimize if\n        you have added or modified 100,000 or more records or run more than 20 data\n        modification operations.\n        \"\"\"\n        cleanup_since_ms: Optional[int] = None\n        if cleanup_older_than is not None:\n            cleanup_since_ms = round(cleanup_older_than.total_seconds() * 1000)\n        return await self._inner.optimize(\n            cleanup_since_ms=cleanup_since_ms,\n            delete_unverified=delete_unverified,\n            retrain=retrain,\n        )\n\n    async def list_indices(self) -&gt; Iterable[IndexConfig]:\n        \"\"\"\n        List all indices that have been created with Self::create_index\n        \"\"\"\n        return await self._inner.list_indices()\n\n    async def index_stats(self, index_name: str) -&gt; Optional[IndexStatistics]:\n        \"\"\"\n        Retrieve statistics about an index\n\n        Parameters\n        ----------\n        index_name: str\n            The name of the index to retrieve statistics for\n\n        Returns\n        -------\n        IndexStatistics or None\n            The statistics about the index. Returns None if the index does not exist.\n        \"\"\"\n        stats = await self._inner.index_stats(index_name)\n        if stats is None:\n            return None\n        else:\n            return IndexStatistics(**stats)\n\n    async def uses_v2_manifest_paths(self) -&gt; bool:\n        \"\"\"\n        Check if the table is using the new v2 manifest paths.\n\n        Returns\n        -------\n        bool\n            True if the table is using the new v2 manifest paths, False otherwise.\n        \"\"\"\n        return await self._inner.uses_v2_manifest_paths()\n\n    async def migrate_manifest_paths_v2(self):\n        \"\"\"\n        Migrate the manifest paths to the new format.\n\n        This will update the manifest to use the new v2 format for paths.\n\n        This function is idempotent, and can be run multiple times without\n        changing the state of the object store.\n\n        !!! danger\n\n            This should not be run while other concurrent operations are happening.\n            And it should also run until completion before resuming other operations.\n\n        You can use\n        [AsyncTable.uses_v2_manifest_paths][lancedb.table.AsyncTable.uses_v2_manifest_paths]\n        to check if the table is already using the new path style.\n        \"\"\"\n        await self._inner.migrate_manifest_paths_v2()\n\n    async def replace_field_metadata(\n        self, field_name: str, new_metadata: dict[str, str]\n    ):\n        \"\"\"\n        Replace the metadata of a field in the schema\n\n        Parameters\n        ----------\n        field_name: str\n            The name of the field to replace the metadata for\n        new_metadata: dict\n            The new metadata to set\n        \"\"\"\n        await self._inner.replace_field_metadata(field_name, new_metadata)\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the table.</p>"},{"location":"python/python/#lancedb.table.AsyncTable.__init__","title":"__init__","text":"<pre><code>__init__(table: Table)\n</code></pre> <p>Create a new AsyncTable object.</p> <p>You should not create AsyncTable objects directly.</p> <p>Use AsyncConnection.create_table and AsyncConnection.open_table to obtain Table objects.</p> Source code in <code>lancedb/table.py</code> <pre><code>def __init__(self, table: LanceDBTable):\n    \"\"\"Create a new AsyncTable object.\n\n    You should not create AsyncTable objects directly.\n\n    Use [AsyncConnection.create_table][lancedb.AsyncConnection.create_table] and\n    [AsyncConnection.open_table][lancedb.AsyncConnection.open_table] to obtain\n    Table objects.\"\"\"\n    self._inner = table\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.is_open","title":"is_open","text":"<pre><code>is_open() -&gt; bool\n</code></pre> <p>Return True if the table is open.</p> Source code in <code>lancedb/table.py</code> <pre><code>def is_open(self) -&gt; bool:\n    \"\"\"Return True if the table is open.\"\"\"\n    return self._inner.is_open()\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.close","title":"close","text":"<pre><code>close()\n</code></pre> <p>Close the table and free any resources associated with it.</p> <p>It is safe to call this method multiple times.</p> <p>Any attempt to use the table after it has been closed will raise an error.</p> Source code in <code>lancedb/table.py</code> <pre><code>def close(self):\n    \"\"\"Close the table and free any resources associated with it.\n\n    It is safe to call this method multiple times.\n\n    Any attempt to use the table after it has been closed will raise an error.\"\"\"\n    return self._inner.close()\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.schema","title":"schema  <code>async</code>","text":"<pre><code>schema() -&gt; Schema\n</code></pre> <p>The Arrow Schema of this Table</p> Source code in <code>lancedb/table.py</code> <pre><code>async def schema(self) -&gt; pa.Schema:\n    \"\"\"The [Arrow Schema](https://arrow.apache.org/docs/python/api/datatypes.html#)\n    of this Table\n\n    \"\"\"\n    return await self._inner.schema()\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.embedding_functions","title":"embedding_functions  <code>async</code>","text":"<pre><code>embedding_functions() -&gt; Dict[str, EmbeddingFunctionConfig]\n</code></pre> <p>Get the embedding functions for the table</p> <p>Returns:</p> <ul> <li> <code>funcs</code> (              <code>Dict[str, EmbeddingFunctionConfig]</code> )          \u2013            <p>A mapping of the vector column to the embedding function or empty dict if not configured.</p> </li> </ul> Source code in <code>lancedb/table.py</code> <pre><code>async def embedding_functions(self) -&gt; Dict[str, EmbeddingFunctionConfig]:\n    \"\"\"\n    Get the embedding functions for the table\n\n    Returns\n    -------\n    funcs: Dict[str, EmbeddingFunctionConfig]\n        A mapping of the vector column to the embedding function\n        or empty dict if not configured.\n    \"\"\"\n    schema = await self.schema()\n    return EmbeddingFunctionRegistry.get_instance().parse_functions(schema.metadata)\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.count_rows","title":"count_rows  <code>async</code>","text":"<pre><code>count_rows(filter: Optional[str] = None) -&gt; int\n</code></pre> <p>Count the number of rows in the table.</p> <p>Parameters:</p> <ul> <li> <code>filter</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>A SQL where clause to filter the rows to count.</p> </li> </ul> Source code in <code>lancedb/table.py</code> <pre><code>async def count_rows(self, filter: Optional[str] = None) -&gt; int:\n    \"\"\"\n    Count the number of rows in the table.\n\n    Parameters\n    ----------\n    filter: str, optional\n        A SQL where clause to filter the rows to count.\n    \"\"\"\n    return await self._inner.count_rows(filter)\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.head","title":"head  <code>async</code>","text":"<pre><code>head(n=5) -&gt; Table\n</code></pre> <p>Return the first <code>n</code> rows of the table.</p> <p>Parameters:</p> <ul> <li> <code>n</code>           \u2013            <p>The number of rows to return.</p> </li> </ul> Source code in <code>lancedb/table.py</code> <pre><code>async def head(self, n=5) -&gt; pa.Table:\n    \"\"\"\n    Return the first `n` rows of the table.\n\n    Parameters\n    ----------\n    n: int, default 5\n        The number of rows to return.\n    \"\"\"\n    return await self.query().limit(n).to_arrow()\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.query","title":"query","text":"<pre><code>query() -&gt; AsyncQuery\n</code></pre> <p>Returns an AsyncQuery that can be used to search the table.</p> <p>Use methods on the returned query to control query behavior.  The query can be executed with methods like to_arrow, to_pandas and more.</p> Source code in <code>lancedb/table.py</code> <pre><code>def query(self) -&gt; AsyncQuery:\n    \"\"\"\n    Returns an [AsyncQuery][lancedb.query.AsyncQuery] that can be used\n    to search the table.\n\n    Use methods on the returned query to control query behavior.  The query\n    can be executed with methods like [to_arrow][lancedb.query.AsyncQuery.to_arrow],\n    [to_pandas][lancedb.query.AsyncQuery.to_pandas] and more.\n    \"\"\"\n    return AsyncQuery(self._inner.query())\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.to_pandas","title":"to_pandas  <code>async</code>","text":"<pre><code>to_pandas() -&gt; 'pd.DataFrame'\n</code></pre> <p>Return the table as a pandas DataFrame.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            </li> </ul> Source code in <code>lancedb/table.py</code> <pre><code>async def to_pandas(self) -&gt; \"pd.DataFrame\":\n    \"\"\"Return the table as a pandas DataFrame.\n\n    Returns\n    -------\n    pd.DataFrame\n    \"\"\"\n    return (await self.to_arrow()).to_pandas()\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.to_arrow","title":"to_arrow  <code>async</code>","text":"<pre><code>to_arrow() -&gt; Table\n</code></pre> <p>Return the table as a pyarrow Table.</p> <p>Returns:</p> <ul> <li> <code>Table</code>           \u2013            </li> </ul> Source code in <code>lancedb/table.py</code> <pre><code>async def to_arrow(self) -&gt; pa.Table:\n    \"\"\"Return the table as a pyarrow Table.\n\n    Returns\n    -------\n    pa.Table\n    \"\"\"\n    return await self.query().to_arrow()\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.create_index","title":"create_index  <code>async</code>","text":"<pre><code>create_index(column: str, *, replace: Optional[bool] = None, config: Optional[Union[IvfFlat, IvfPq, HnswPq, HnswSq, BTree, Bitmap, LabelList, FTS]] = None, wait_timeout: Optional[timedelta] = None)\n</code></pre> <p>Create an index to speed up queries</p> <p>Indices can be created on vector columns or scalar columns. Indices on vector columns will speed up vector searches. Indices on scalar columns will speed up filtering (in both vector and non-vector searches)</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>str</code>)           \u2013            <p>The column to index.</p> </li> <li> <code>replace</code>               (<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>Whether to replace the existing index</p> <p>If this is false, and another index already exists on the same columns and the same name, then an error will be returned.  This is true even if that index is out of date.</p> <p>The default is True</p> </li> <li> <code>config</code>               (<code>Optional[Union[IvfFlat, IvfPq, HnswPq, HnswSq, BTree, Bitmap, LabelList, FTS]]</code>, default:                   <code>None</code> )           \u2013            <p>For advanced configuration you can specify the type of index you would like to create.   You can also specify index-specific parameters when creating an index object.</p> </li> <li> <code>wait_timeout</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>The timeout to wait if indexing is asynchronous.</p> </li> </ul> Source code in <code>lancedb/table.py</code> <pre><code>async def create_index(\n    self,\n    column: str,\n    *,\n    replace: Optional[bool] = None,\n    config: Optional[\n        Union[IvfFlat, IvfPq, HnswPq, HnswSq, BTree, Bitmap, LabelList, FTS]\n    ] = None,\n    wait_timeout: Optional[timedelta] = None,\n):\n    \"\"\"Create an index to speed up queries\n\n    Indices can be created on vector columns or scalar columns.\n    Indices on vector columns will speed up vector searches.\n    Indices on scalar columns will speed up filtering (in both\n    vector and non-vector searches)\n\n    Parameters\n    ----------\n    column: str\n        The column to index.\n    replace: bool, default True\n        Whether to replace the existing index\n\n        If this is false, and another index already exists on the same columns\n        and the same name, then an error will be returned.  This is true even if\n        that index is out of date.\n\n        The default is True\n    config: default None\n        For advanced configuration you can specify the type of index you would\n        like to create.   You can also specify index-specific parameters when\n        creating an index object.\n    wait_timeout: timedelta, optional\n        The timeout to wait if indexing is asynchronous.\n    \"\"\"\n    if config is not None:\n        if not isinstance(\n            config, (IvfFlat, IvfPq, HnswPq, HnswSq, BTree, Bitmap, LabelList, FTS)\n        ):\n            raise TypeError(\n                \"config must be an instance of IvfPq, HnswPq, HnswSq, BTree,\"\n                \" Bitmap, LabelList, or FTS\"\n            )\n    try:\n        await self._inner.create_index(\n            column, index=config, replace=replace, wait_timeout=wait_timeout\n        )\n    except ValueError as e:\n        if \"not support the requested language\" in str(e):\n            supported_langs = \", \".join(lang_mapping.values())\n            help_msg = f\"Supported languages: {supported_langs}\"\n            add_note(e, help_msg)\n        raise e\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.drop_index","title":"drop_index  <code>async</code>","text":"<pre><code>drop_index(name: str) -&gt; None\n</code></pre> <p>Drop an index from the table.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the index to drop.</p> </li> </ul> Notes <p>This does not delete the index from disk, it just removes it from the table. To delete the index, run optimize after dropping the index.</p> <p>Use list_indices to find the names of the indices.</p> Source code in <code>lancedb/table.py</code> <pre><code>async def drop_index(self, name: str) -&gt; None:\n    \"\"\"\n    Drop an index from the table.\n\n    Parameters\n    ----------\n    name: str\n        The name of the index to drop.\n\n    Notes\n    -----\n    This does not delete the index from disk, it just removes it from the table.\n    To delete the index, run [optimize][lancedb.table.AsyncTable.optimize]\n    after dropping the index.\n\n    Use [list_indices][lancedb.table.AsyncTable.list_indices] to find the names\n    of the indices.\n    \"\"\"\n    await self._inner.drop_index(name)\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.prewarm_index","title":"prewarm_index  <code>async</code>","text":"<pre><code>prewarm_index(name: str) -&gt; None\n</code></pre> <p>Prewarm an index in the table.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the index to prewarm</p> </li> </ul> Notes <p>This will load the index into memory.  This may reduce the cold-start time for future queries.  If the index does not fit in the cache then this call may be wasteful.</p> Source code in <code>lancedb/table.py</code> <pre><code>async def prewarm_index(self, name: str) -&gt; None:\n    \"\"\"\n    Prewarm an index in the table.\n\n    Parameters\n    ----------\n    name: str\n        The name of the index to prewarm\n\n    Notes\n    -----\n    This will load the index into memory.  This may reduce the cold-start time for\n    future queries.  If the index does not fit in the cache then this call may be\n    wasteful.\n    \"\"\"\n    await self._inner.prewarm_index(name)\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.wait_for_index","title":"wait_for_index  <code>async</code>","text":"<pre><code>wait_for_index(index_names: Iterable[str], timeout: timedelta = timedelta(seconds=300)) -&gt; None\n</code></pre> <p>Wait for indexing to complete for the given index names. This will poll the table until all the indices are fully indexed, or raise a timeout exception if the timeout is reached.</p> <p>Parameters:</p> <ul> <li> <code>index_names</code>               (<code>Iterable[str]</code>)           \u2013            <p>The name of the indices to poll</p> </li> <li> <code>timeout</code>               (<code>timedelta</code>, default:                   <code>timedelta(seconds=300)</code> )           \u2013            <p>Timeout to wait for asynchronous indexing. The default is 5 minutes.</p> </li> </ul> Source code in <code>lancedb/table.py</code> <pre><code>async def wait_for_index(\n    self, index_names: Iterable[str], timeout: timedelta = timedelta(seconds=300)\n) -&gt; None:\n    \"\"\"\n    Wait for indexing to complete for the given index names.\n    This will poll the table until all the indices are fully indexed,\n    or raise a timeout exception if the timeout is reached.\n\n    Parameters\n    ----------\n    index_names: str\n        The name of the indices to poll\n    timeout: timedelta\n        Timeout to wait for asynchronous indexing. The default is 5 minutes.\n    \"\"\"\n    await self._inner.wait_for_index(index_names, timeout)\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.add","title":"add  <code>async</code>","text":"<pre><code>add(data: DATA, *, mode: Optional[Literal['append', 'overwrite']] = 'append', on_bad_vectors: Optional[OnBadVectorsType] = None, fill_value: Optional[float] = None)\n</code></pre> <p>Add more data to the Table.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>DATA</code>)           \u2013            <p>The data to insert into the table. Acceptable types are:</p> <ul> <li> <p>list-of-dict</p> </li> <li> <p>pandas.DataFrame</p> </li> <li> <p>pyarrow.Table or pyarrow.RecordBatch</p> </li> </ul> </li> <li> <code>mode</code>               (<code>Optional[Literal['append', 'overwrite']]</code>, default:                   <code>'append'</code> )           \u2013            <p>The mode to use when writing the data. Valid values are \"append\" and \"overwrite\".</p> </li> <li> <code>on_bad_vectors</code>               (<code>Optional[OnBadVectorsType]</code>, default:                   <code>None</code> )           \u2013            <p>What to do if any of the vectors are not the same size or contains NaNs. One of \"error\", \"drop\", \"fill\", \"null\".</p> </li> <li> <code>fill_value</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>The value to use when filling vectors. Only used if on_bad_vectors=\"fill\".</p> </li> </ul> Source code in <code>lancedb/table.py</code> <pre><code>async def add(\n    self,\n    data: DATA,\n    *,\n    mode: Optional[Literal[\"append\", \"overwrite\"]] = \"append\",\n    on_bad_vectors: Optional[OnBadVectorsType] = None,\n    fill_value: Optional[float] = None,\n):\n    \"\"\"Add more data to the [Table](Table).\n\n    Parameters\n    ----------\n    data: DATA\n        The data to insert into the table. Acceptable types are:\n\n        - list-of-dict\n\n        - pandas.DataFrame\n\n        - pyarrow.Table or pyarrow.RecordBatch\n    mode: str\n        The mode to use when writing the data. Valid values are\n        \"append\" and \"overwrite\".\n    on_bad_vectors: str, default \"error\"\n        What to do if any of the vectors are not the same size or contains NaNs.\n        One of \"error\", \"drop\", \"fill\", \"null\".\n    fill_value: float, default 0.\n        The value to use when filling vectors. Only used if on_bad_vectors=\"fill\".\n\n    \"\"\"\n    schema = await self.schema()\n    if on_bad_vectors is None:\n        on_bad_vectors = \"error\"\n    if fill_value is None:\n        fill_value = 0.0\n    data = _sanitize_data(\n        data,\n        schema,\n        metadata=schema.metadata,\n        on_bad_vectors=on_bad_vectors,\n        fill_value=fill_value,\n        allow_subschema=True,\n    )\n    if isinstance(data, pa.Table):\n        data = data.to_reader()\n\n    await self._inner.add(data, mode or \"append\")\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.merge_insert","title":"merge_insert","text":"<pre><code>merge_insert(on: Union[str, Iterable[str]]) -&gt; LanceMergeInsertBuilder\n</code></pre> <p>Returns a <code>LanceMergeInsertBuilder</code> that can be used to create a \"merge insert\" operation</p> <p>This operation can add rows, update rows, and remove rows all in a single transaction. It is a very generic tool that can be used to create behaviors like \"insert if not exists\", \"update or insert (i.e. upsert)\", or even replace a portion of existing data with new data (e.g. replace all data where month=\"january\")</p> <p>The merge insert operation works by combining new data from a source table with existing data in a target table by using a join.  There are three categories of records.</p> <p>\"Matched\" records are records that exist in both the source table and the target table. \"Not matched\" records exist only in the source table (e.g. these are new data) \"Not matched by source\" records exist only in the target table (this is old data)</p> <p>The builder returned by this method can be used to customize what should happen for each category of data.</p> <p>Please note that the data may appear to be reordered as part of this operation.  This is because updated rows will be deleted from the dataset and then reinserted at the end with the new values.</p> <p>Parameters:</p> <ul> <li> <code>on</code>               (<code>Union[str, Iterable[str]]</code>)           \u2013            <p>A column (or columns) to join on.  This is how records from the source table and target table are matched.  Typically this is some kind of key or id column.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import lancedb\n&gt;&gt;&gt; data = pa.table({\"a\": [2, 1, 3], \"b\": [\"a\", \"b\", \"c\"]})\n&gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n&gt;&gt;&gt; table = db.create_table(\"my_table\", data)\n&gt;&gt;&gt; new_data = pa.table({\"a\": [2, 3, 4], \"b\": [\"x\", \"y\", \"z\"]})\n&gt;&gt;&gt; # Perform a \"upsert\" operation\n&gt;&gt;&gt; table.merge_insert(\"a\")             \\\n...      .when_matched_update_all()     \\\n...      .when_not_matched_insert_all() \\\n...      .execute(new_data)\n&gt;&gt;&gt; # The order of new rows is non-deterministic since we use\n&gt;&gt;&gt; # a hash-join as part of this operation and so we sort here\n&gt;&gt;&gt; table.to_arrow().sort_by(\"a\").to_pandas()\n   a  b\n0  1  b\n1  2  x\n2  3  y\n3  4  z\n</code></pre> Source code in <code>lancedb/table.py</code> <pre><code>def merge_insert(self, on: Union[str, Iterable[str]]) -&gt; LanceMergeInsertBuilder:\n    \"\"\"\n    Returns a [`LanceMergeInsertBuilder`][lancedb.merge.LanceMergeInsertBuilder]\n    that can be used to create a \"merge insert\" operation\n\n    This operation can add rows, update rows, and remove rows all in a single\n    transaction. It is a very generic tool that can be used to create\n    behaviors like \"insert if not exists\", \"update or insert (i.e. upsert)\",\n    or even replace a portion of existing data with new data (e.g. replace\n    all data where month=\"january\")\n\n    The merge insert operation works by combining new data from a\n    **source table** with existing data in a **target table** by using a\n    join.  There are three categories of records.\n\n    \"Matched\" records are records that exist in both the source table and\n    the target table. \"Not matched\" records exist only in the source table\n    (e.g. these are new data) \"Not matched by source\" records exist only\n    in the target table (this is old data)\n\n    The builder returned by this method can be used to customize what\n    should happen for each category of data.\n\n    Please note that the data may appear to be reordered as part of this\n    operation.  This is because updated rows will be deleted from the\n    dataset and then reinserted at the end with the new values.\n\n    Parameters\n    ----------\n\n    on: Union[str, Iterable[str]]\n        A column (or columns) to join on.  This is how records from the\n        source table and target table are matched.  Typically this is some\n        kind of key or id column.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import lancedb\n    &gt;&gt;&gt; data = pa.table({\"a\": [2, 1, 3], \"b\": [\"a\", \"b\", \"c\"]})\n    &gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n    &gt;&gt;&gt; table = db.create_table(\"my_table\", data)\n    &gt;&gt;&gt; new_data = pa.table({\"a\": [2, 3, 4], \"b\": [\"x\", \"y\", \"z\"]})\n    &gt;&gt;&gt; # Perform a \"upsert\" operation\n    &gt;&gt;&gt; table.merge_insert(\"a\")             \\\\\n    ...      .when_matched_update_all()     \\\\\n    ...      .when_not_matched_insert_all() \\\\\n    ...      .execute(new_data)\n    &gt;&gt;&gt; # The order of new rows is non-deterministic since we use\n    &gt;&gt;&gt; # a hash-join as part of this operation and so we sort here\n    &gt;&gt;&gt; table.to_arrow().sort_by(\"a\").to_pandas()\n       a  b\n    0  1  b\n    1  2  x\n    2  3  y\n    3  4  z\n    \"\"\"\n    on = [on] if isinstance(on, str) else list(iter(on))\n\n    return LanceMergeInsertBuilder(self, on)\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.search","title":"search  <code>async</code>","text":"<pre><code>search(query: Optional[Union[VEC, str, 'PIL.Image.Image', Tuple, FullTextQuery]] = None, vector_column_name: Optional[str] = None, query_type: QueryType = 'auto', ordering_field_name: Optional[str] = None, fts_columns: Optional[Union[str, List[str]]] = None) -&gt; Union[AsyncHybridQuery, AsyncFTSQuery, AsyncVectorQuery]\n</code></pre> <p>Create a search query to find the nearest neighbors of the given query vector. We currently support vector search and [full-text search][experimental-full-text-search].</p> <p>All query options are defined in AsyncQuery.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>Optional[Union[VEC, str, 'PIL.Image.Image', Tuple, FullTextQuery]]</code>, default:                   <code>None</code> )           \u2013            <p>The targetted vector to search for.</p> <ul> <li> <p>default None. Acceptable types are: list, np.ndarray, PIL.Image.Image</p> </li> <li> <p>If None then the select/where/limit clauses are applied to filter the table</p> </li> </ul> </li> <li> <code>vector_column_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The name of the vector column to search.</p> <p>The vector column needs to be a pyarrow fixed size list type</p> <ul> <li> <p>If not specified then the vector column is inferred from the table schema</p> </li> <li> <p>If the table has multiple vector columns then the vector_column_name needs to be specified. Otherwise, an error is raised.</p> </li> </ul> </li> <li> <code>query_type</code>               (<code>QueryType</code>, default:                   <code>'auto'</code> )           \u2013            <p>default \"auto\". Acceptable types are: \"vector\", \"fts\", \"hybrid\", or \"auto\"</p> <ul> <li> <p>If \"auto\" then the query type is inferred from the query;</p> <ul> <li> <p>If <code>query</code> is a list/np.ndarray then the query type is \"vector\";</p> </li> <li> <p>If <code>query</code> is a PIL.Image.Image then either do vector search, or raise an error if no corresponding embedding function is found.</p> </li> </ul> </li> <li> <p>If <code>query</code> is a string, then the query type is \"vector\" if the   table has embedding functions else the query type is \"fts\"</p> </li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> <code>LanceQueryBuilder</code>           \u2013            <p>A query builder object representing the query.</p> </li> </ul> Source code in <code>lancedb/table.py</code> <pre><code>async def search(\n    self,\n    query: Optional[\n        Union[VEC, str, \"PIL.Image.Image\", Tuple, FullTextQuery]\n    ] = None,\n    vector_column_name: Optional[str] = None,\n    query_type: QueryType = \"auto\",\n    ordering_field_name: Optional[str] = None,\n    fts_columns: Optional[Union[str, List[str]]] = None,\n) -&gt; Union[AsyncHybridQuery, AsyncFTSQuery, AsyncVectorQuery]:\n    \"\"\"Create a search query to find the nearest neighbors\n    of the given query vector. We currently support [vector search][search]\n    and [full-text search][experimental-full-text-search].\n\n    All query options are defined in [AsyncQuery][lancedb.query.AsyncQuery].\n\n    Parameters\n    ----------\n    query: list/np.ndarray/str/PIL.Image.Image, default None\n        The targetted vector to search for.\n\n        - *default None*.\n        Acceptable types are: list, np.ndarray, PIL.Image.Image\n\n        - If None then the select/where/limit clauses are applied to filter\n        the table\n    vector_column_name: str, optional\n        The name of the vector column to search.\n\n        The vector column needs to be a pyarrow fixed size list type\n\n        - If not specified then the vector column is inferred from\n        the table schema\n\n        - If the table has multiple vector columns then the *vector_column_name*\n        needs to be specified. Otherwise, an error is raised.\n    query_type: str\n        *default \"auto\"*.\n        Acceptable types are: \"vector\", \"fts\", \"hybrid\", or \"auto\"\n\n        - If \"auto\" then the query type is inferred from the query;\n\n            - If `query` is a list/np.ndarray then the query type is\n            \"vector\";\n\n            - If `query` is a PIL.Image.Image then either do vector search,\n            or raise an error if no corresponding embedding function is found.\n\n        - If `query` is a string, then the query type is \"vector\" if the\n          table has embedding functions else the query type is \"fts\"\n\n    Returns\n    -------\n    LanceQueryBuilder\n        A query builder object representing the query.\n    \"\"\"\n\n    def is_embedding(query):\n        return isinstance(query, (list, np.ndarray, pa.Array, pa.ChunkedArray))\n\n    async def get_embedding_func(\n        vector_column_name: Optional[str],\n        query_type: QueryType,\n        query: Optional[Union[VEC, str, \"PIL.Image.Image\", Tuple, FullTextQuery]],\n    ) -&gt; Tuple[str, EmbeddingFunctionConfig]:\n        if isinstance(query, FullTextQuery):\n            query_type = \"fts\"\n        schema = await self.schema()\n        vector_column_name = infer_vector_column_name(\n            schema=schema,\n            query_type=query_type,\n            query=query,\n            vector_column_name=vector_column_name,\n        )\n        funcs = EmbeddingFunctionRegistry.get_instance().parse_functions(\n            schema.metadata\n        )\n        func = funcs.get(vector_column_name)\n        if func is None:\n            error = ValueError(\n                f\"Column '{vector_column_name}' has no registered \"\n                \"embedding function.\"\n            )\n            if len(funcs) &gt; 0:\n                add_note(\n                    error,\n                    \"Embedding functions are registered for columns: \"\n                    f\"{list(funcs.keys())}\",\n                )\n            else:\n                add_note(\n                    error, \"No embedding functions are registered for any columns.\"\n                )\n            raise error\n        return vector_column_name, func\n\n    async def make_embedding(embedding, query):\n        if embedding is not None:\n            loop = asyncio.get_running_loop()\n            # This function is likely to block, since it either calls an expensive\n            # function or makes an HTTP request to an embeddings REST API.\n            return (\n                await loop.run_in_executor(\n                    None,\n                    embedding.function.compute_query_embeddings_with_retry,\n                    query,\n                )\n            )[0]\n        else:\n            return None\n\n    if query_type == \"auto\":\n        # Infer the query type.\n        if is_embedding(query):\n            vector_query = query\n            query_type = \"vector\"\n        elif isinstance(query, FullTextQuery):\n            query_type = \"fts\"\n        elif isinstance(query, str):\n            try:\n                (\n                    indices,\n                    (vector_column_name, embedding_conf),\n                ) = await asyncio.gather(\n                    self.list_indices(),\n                    get_embedding_func(vector_column_name, \"auto\", query),\n                )\n            except ValueError as e:\n                if \"Column\" in str(\n                    e\n                ) and \"has no registered embedding function\" in str(e):\n                    # If the column has no registered embedding function,\n                    # then it's an FTS query.\n                    query_type = \"fts\"\n                else:\n                    raise e\n            else:\n                if embedding_conf is not None:\n                    vector_query = await make_embedding(embedding_conf, query)\n                    if any(\n                        i.columns[0] == embedding_conf.source_column\n                        and i.index_type == \"FTS\"\n                        for i in indices\n                    ):\n                        query_type = \"hybrid\"\n                    else:\n                        query_type = \"vector\"\n                else:\n                    query_type = \"fts\"\n        else:\n            # it's an image or something else embeddable.\n            query_type = \"vector\"\n    elif query_type == \"vector\":\n        if is_embedding(query):\n            vector_query = query\n        else:\n            vector_column_name, embedding_conf = await get_embedding_func(\n                vector_column_name, query_type, query\n            )\n            vector_query = await make_embedding(embedding_conf, query)\n    elif query_type == \"hybrid\":\n        if is_embedding(query):\n            raise ValueError(\"Hybrid search requires a text query\")\n        else:\n            vector_column_name, embedding_conf = await get_embedding_func(\n                vector_column_name, query_type, query\n            )\n            vector_query = await make_embedding(embedding_conf, query)\n\n    if query_type == \"vector\":\n        builder = self.query().nearest_to(vector_query)\n        if vector_column_name:\n            builder = builder.column(vector_column_name)\n        return builder\n    elif query_type == \"fts\":\n        return self.query().nearest_to_text(query, columns=fts_columns)\n    elif query_type == \"hybrid\":\n        builder = self.query().nearest_to(vector_query)\n        if vector_column_name:\n            builder = builder.column(vector_column_name)\n        return builder.nearest_to_text(query, columns=fts_columns)\n    else:\n        raise ValueError(f\"Unknown query type: '{query_type}'\")\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.vector_search","title":"vector_search","text":"<pre><code>vector_search(query_vector: Union[VEC, Tuple]) -&gt; AsyncVectorQuery\n</code></pre> <p>Search the table with a given query vector. This is a convenience method for preparing a vector query and is the same thing as calling <code>nearestTo</code> on the builder returned by <code>query</code>.  Seer nearest_to for more details.</p> Source code in <code>lancedb/table.py</code> <pre><code>def vector_search(\n    self,\n    query_vector: Union[VEC, Tuple],\n) -&gt; AsyncVectorQuery:\n    \"\"\"\n    Search the table with a given query vector.\n    This is a convenience method for preparing a vector query and\n    is the same thing as calling `nearestTo` on the builder returned\n    by `query`.  Seer [nearest_to][lancedb.query.AsyncQuery.nearest_to] for more\n    details.\n    \"\"\"\n    return self.query().nearest_to(query_vector)\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.delete","title":"delete  <code>async</code>","text":"<pre><code>delete(where: str)\n</code></pre> <p>Delete rows from the table.</p> <p>This can be used to delete a single row, many rows, all rows, or sometimes no rows (if your predicate matches nothing).</p> <p>Parameters:</p> <ul> <li> <code>where</code>               (<code>str</code>)           \u2013            <p>The SQL where clause to use when deleting rows.</p> <ul> <li>For example, 'x = 2' or 'x IN (1, 2, 3)'.</li> </ul> <p>The filter must not be empty, or it will error.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import lancedb\n&gt;&gt;&gt; data = [\n...    {\"x\": 1, \"vector\": [1.0, 2]},\n...    {\"x\": 2, \"vector\": [3.0, 4]},\n...    {\"x\": 3, \"vector\": [5.0, 6]}\n... ]\n&gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n&gt;&gt;&gt; table = db.create_table(\"my_table\", data)\n&gt;&gt;&gt; table.to_pandas()\n   x      vector\n0  1  [1.0, 2.0]\n1  2  [3.0, 4.0]\n2  3  [5.0, 6.0]\n&gt;&gt;&gt; table.delete(\"x = 2\")\n&gt;&gt;&gt; table.to_pandas()\n   x      vector\n0  1  [1.0, 2.0]\n1  3  [5.0, 6.0]\n</code></pre> <p>If you have a list of values to delete, you can combine them into a stringified list and use the <code>IN</code> operator:</p> <pre><code>&gt;&gt;&gt; to_remove = [1, 5]\n&gt;&gt;&gt; to_remove = \", \".join([str(v) for v in to_remove])\n&gt;&gt;&gt; to_remove\n'1, 5'\n&gt;&gt;&gt; table.delete(f\"x IN ({to_remove})\")\n&gt;&gt;&gt; table.to_pandas()\n   x      vector\n0  3  [5.0, 6.0]\n</code></pre> Source code in <code>lancedb/table.py</code> <pre><code>async def delete(self, where: str):\n    \"\"\"Delete rows from the table.\n\n    This can be used to delete a single row, many rows, all rows, or\n    sometimes no rows (if your predicate matches nothing).\n\n    Parameters\n    ----------\n    where: str\n        The SQL where clause to use when deleting rows.\n\n        - For example, 'x = 2' or 'x IN (1, 2, 3)'.\n\n        The filter must not be empty, or it will error.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import lancedb\n    &gt;&gt;&gt; data = [\n    ...    {\"x\": 1, \"vector\": [1.0, 2]},\n    ...    {\"x\": 2, \"vector\": [3.0, 4]},\n    ...    {\"x\": 3, \"vector\": [5.0, 6]}\n    ... ]\n    &gt;&gt;&gt; db = lancedb.connect(\"./.lancedb\")\n    &gt;&gt;&gt; table = db.create_table(\"my_table\", data)\n    &gt;&gt;&gt; table.to_pandas()\n       x      vector\n    0  1  [1.0, 2.0]\n    1  2  [3.0, 4.0]\n    2  3  [5.0, 6.0]\n    &gt;&gt;&gt; table.delete(\"x = 2\")\n    &gt;&gt;&gt; table.to_pandas()\n       x      vector\n    0  1  [1.0, 2.0]\n    1  3  [5.0, 6.0]\n\n    If you have a list of values to delete, you can combine them into a\n    stringified list and use the `IN` operator:\n\n    &gt;&gt;&gt; to_remove = [1, 5]\n    &gt;&gt;&gt; to_remove = \", \".join([str(v) for v in to_remove])\n    &gt;&gt;&gt; to_remove\n    '1, 5'\n    &gt;&gt;&gt; table.delete(f\"x IN ({to_remove})\")\n    &gt;&gt;&gt; table.to_pandas()\n       x      vector\n    0  3  [5.0, 6.0]\n    \"\"\"\n    return await self._inner.delete(where)\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.update","title":"update  <code>async</code>","text":"<pre><code>update(updates: Optional[Dict[str, Any]] = None, *, where: Optional[str] = None, updates_sql: Optional[Dict[str, str]] = None)\n</code></pre> <p>This can be used to update zero to all rows in the table.</p> <p>If a filter is provided with <code>where</code> then only rows matching the filter will be updated.  Otherwise all rows will be updated.</p> <p>Parameters:</p> <ul> <li> <code>updates</code>               (<code>Optional[Dict[str, Any]]</code>, default:                   <code>None</code> )           \u2013            <p>The updates to apply.  The keys should be the name of the column to update.  The values should be the new values to assign.  This is required unless updates_sql is supplied.</p> </li> <li> <code>where</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>An SQL filter that controls which rows are updated. For example, 'x = 2' or 'x IN (1, 2, 3)'.  Only rows that satisfy this filter will be udpated.</p> </li> <li> <code>updates_sql</code>               (<code>Optional[Dict[str, str]]</code>, default:                   <code>None</code> )           \u2013            <p>The updates to apply, expressed as SQL expression strings.  The keys should be column names. The values should be SQL expressions.  These can be SQL literals (e.g. \"7\" or \"'foo'\") or they can be expressions based on the previous value of the row (e.g. \"x + 1\" to increment the x column by 1)</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import asyncio\n&gt;&gt;&gt; import lancedb\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; async def demo_update():\n...     data = pd.DataFrame({\"x\": [1, 2], \"vector\": [[1, 2], [3, 4]]})\n...     db = await lancedb.connect_async(\"./.lancedb\")\n...     table = await db.create_table(\"my_table\", data)\n...     # x is [1, 2], vector is [[1, 2], [3, 4]]\n...     await table.update({\"vector\": [10, 10]}, where=\"x = 2\")\n...     # x is [1, 2], vector is [[1, 2], [10, 10]]\n...     await table.update(updates_sql={\"x\": \"x + 1\"})\n...     # x is [2, 3], vector is [[1, 2], [10, 10]]\n&gt;&gt;&gt; asyncio.run(demo_update())\n</code></pre> Source code in <code>lancedb/table.py</code> <pre><code>async def update(\n    self,\n    updates: Optional[Dict[str, Any]] = None,\n    *,\n    where: Optional[str] = None,\n    updates_sql: Optional[Dict[str, str]] = None,\n):\n    \"\"\"\n    This can be used to update zero to all rows in the table.\n\n    If a filter is provided with `where` then only rows matching the\n    filter will be updated.  Otherwise all rows will be updated.\n\n    Parameters\n    ----------\n    updates: dict, optional\n        The updates to apply.  The keys should be the name of the column to\n        update.  The values should be the new values to assign.  This is\n        required unless updates_sql is supplied.\n    where: str, optional\n        An SQL filter that controls which rows are updated. For example, 'x = 2'\n        or 'x IN (1, 2, 3)'.  Only rows that satisfy this filter will be udpated.\n    updates_sql: dict, optional\n        The updates to apply, expressed as SQL expression strings.  The keys should\n        be column names. The values should be SQL expressions.  These can be SQL\n        literals (e.g. \"7\" or \"'foo'\") or they can be expressions based on the\n        previous value of the row (e.g. \"x + 1\" to increment the x column by 1)\n\n    Examples\n    --------\n    &gt;&gt;&gt; import asyncio\n    &gt;&gt;&gt; import lancedb\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; async def demo_update():\n    ...     data = pd.DataFrame({\"x\": [1, 2], \"vector\": [[1, 2], [3, 4]]})\n    ...     db = await lancedb.connect_async(\"./.lancedb\")\n    ...     table = await db.create_table(\"my_table\", data)\n    ...     # x is [1, 2], vector is [[1, 2], [3, 4]]\n    ...     await table.update({\"vector\": [10, 10]}, where=\"x = 2\")\n    ...     # x is [1, 2], vector is [[1, 2], [10, 10]]\n    ...     await table.update(updates_sql={\"x\": \"x + 1\"})\n    ...     # x is [2, 3], vector is [[1, 2], [10, 10]]\n    &gt;&gt;&gt; asyncio.run(demo_update())\n    \"\"\"\n    if updates is not None and updates_sql is not None:\n        raise ValueError(\"Only one of updates or updates_sql can be provided\")\n    if updates is None and updates_sql is None:\n        raise ValueError(\"Either updates or updates_sql must be provided\")\n\n    if updates is not None:\n        updates_sql = {k: value_to_sql(v) for k, v in updates.items()}\n\n    return await self._inner.update(updates_sql, where)\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.add_columns","title":"add_columns  <code>async</code>","text":"<pre><code>add_columns(transforms: dict[str, str] | field | List[field] | Schema)\n</code></pre> <p>Add new columns with defined values.</p> <p>Parameters:</p> <ul> <li> <code>transforms</code>               (<code>dict[str, str] | field | List[field] | Schema</code>)           \u2013            <p>A map of column name to a SQL expression to use to calculate the value of the new column. These expressions will be evaluated for each row in the table, and can reference existing columns. Alternatively, you can pass a pyarrow field or schema to add new columns with NULLs.</p> </li> </ul> Source code in <code>lancedb/table.py</code> <pre><code>async def add_columns(\n    self, transforms: dict[str, str] | pa.field | List[pa.field] | pa.Schema\n):\n    \"\"\"\n    Add new columns with defined values.\n\n    Parameters\n    ----------\n    transforms: Dict[str, str]\n        A map of column name to a SQL expression to use to calculate the\n        value of the new column. These expressions will be evaluated for\n        each row in the table, and can reference existing columns.\n        Alternatively, you can pass a pyarrow field or schema to add\n        new columns with NULLs.\n    \"\"\"\n    if isinstance(transforms, pa.Field):\n        transforms = [transforms]\n    if isinstance(transforms, list) and all(\n        {isinstance(f, pa.Field) for f in transforms}\n    ):\n        transforms = pa.schema(transforms)\n    if isinstance(transforms, pa.Schema):\n        await self._inner.add_columns_with_schema(transforms)\n    else:\n        await self._inner.add_columns(list(transforms.items()))\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.alter_columns","title":"alter_columns  <code>async</code>","text":"<pre><code>alter_columns(*alterations: Iterable[dict[str, Any]])\n</code></pre> <p>Alter column names and nullability.</p> <p>alterations : Iterable[Dict[str, Any]]     A sequence of dictionaries, each with the following keys:     - \"path\": str         The column path to alter. For a top-level column, this is the name.         For a nested column, this is the dot-separated path, e.g. \"a.b.c\".     - \"rename\": str, optional         The new name of the column. If not specified, the column name is         not changed.     - \"data_type\": pyarrow.DataType, optional        The new data type of the column. Existing values will be casted        to this type. If not specified, the column data type is not changed.     - \"nullable\": bool, optional         Whether the column should be nullable. If not specified, the column         nullability is not changed. Only non-nullable columns can be changed         to nullable. Currently, you cannot change a nullable column to         non-nullable.</p> Source code in <code>lancedb/table.py</code> <pre><code>async def alter_columns(self, *alterations: Iterable[dict[str, Any]]):\n    \"\"\"\n    Alter column names and nullability.\n\n    alterations : Iterable[Dict[str, Any]]\n        A sequence of dictionaries, each with the following keys:\n        - \"path\": str\n            The column path to alter. For a top-level column, this is the name.\n            For a nested column, this is the dot-separated path, e.g. \"a.b.c\".\n        - \"rename\": str, optional\n            The new name of the column. If not specified, the column name is\n            not changed.\n        - \"data_type\": pyarrow.DataType, optional\n           The new data type of the column. Existing values will be casted\n           to this type. If not specified, the column data type is not changed.\n        - \"nullable\": bool, optional\n            Whether the column should be nullable. If not specified, the column\n            nullability is not changed. Only non-nullable columns can be changed\n            to nullable. Currently, you cannot change a nullable column to\n            non-nullable.\n    \"\"\"\n    await self._inner.alter_columns(alterations)\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.drop_columns","title":"drop_columns  <code>async</code>","text":"<pre><code>drop_columns(columns: Iterable[str])\n</code></pre> <p>Drop columns from the table.</p> <p>Parameters:</p> <ul> <li> <code>columns</code>               (<code>Iterable[str]</code>)           \u2013            <p>The names of the columns to drop.</p> </li> </ul> Source code in <code>lancedb/table.py</code> <pre><code>async def drop_columns(self, columns: Iterable[str]):\n    \"\"\"\n    Drop columns from the table.\n\n    Parameters\n    ----------\n    columns : Iterable[str]\n        The names of the columns to drop.\n    \"\"\"\n    await self._inner.drop_columns(columns)\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.version","title":"version  <code>async</code>","text":"<pre><code>version() -&gt; int\n</code></pre> <p>Retrieve the version of the table</p> <p>LanceDb supports versioning.  Every operation that modifies the table increases version.  As long as a version hasn't been deleted you can <code>[Self::checkout]</code> that version to view the data at that point.  In addition, you can <code>[Self::restore]</code> the version to replace the current table with a previous version.</p> Source code in <code>lancedb/table.py</code> <pre><code>async def version(self) -&gt; int:\n    \"\"\"\n    Retrieve the version of the table\n\n    LanceDb supports versioning.  Every operation that modifies the table increases\n    version.  As long as a version hasn't been deleted you can `[Self::checkout]`\n    that version to view the data at that point.  In addition, you can\n    `[Self::restore]` the version to replace the current table with a previous\n    version.\n    \"\"\"\n    return await self._inner.version()\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.list_versions","title":"list_versions  <code>async</code>","text":"<pre><code>list_versions()\n</code></pre> <p>List all versions of the table</p> Source code in <code>lancedb/table.py</code> <pre><code>async def list_versions(self):\n    \"\"\"\n    List all versions of the table\n    \"\"\"\n    versions = await self._inner.list_versions()\n    for v in versions:\n        ts_nanos = v[\"timestamp\"]\n        v[\"timestamp\"] = datetime.fromtimestamp(ts_nanos // 1e9) + timedelta(\n            microseconds=(ts_nanos % 1e9) // 1e3\n        )\n\n    return versions\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.checkout","title":"checkout  <code>async</code>","text":"<pre><code>checkout(version: int)\n</code></pre> <p>Checks out a specific version of the Table</p> <p>Any read operation on the table will now access the data at the checked out version. As a consequence, calling this method will disable any read consistency interval that was previously set.</p> <p>This is a read-only operation that turns the table into a sort of \"view\" or \"detached head\".  Other table instances will not be affected.  To make the change permanent you can use the <code>[Self::restore]</code> method.</p> <p>Any operation that modifies the table will fail while the table is in a checked out state.</p> <p>To return the table to a normal state use <code>[Self::checkout_latest]</code></p> Source code in <code>lancedb/table.py</code> <pre><code>async def checkout(self, version: int):\n    \"\"\"\n    Checks out a specific version of the Table\n\n    Any read operation on the table will now access the data at the checked out\n    version. As a consequence, calling this method will disable any read consistency\n    interval that was previously set.\n\n    This is a read-only operation that turns the table into a sort of \"view\"\n    or \"detached head\".  Other table instances will not be affected.  To make the\n    change permanent you can use the `[Self::restore]` method.\n\n    Any operation that modifies the table will fail while the table is in a checked\n    out state.\n\n    To return the table to a normal state use `[Self::checkout_latest]`\n    \"\"\"\n    try:\n        await self._inner.checkout(version)\n    except RuntimeError as e:\n        if \"not found\" in str(e):\n            raise ValueError(\n                f\"Version {version} no longer exists. Was it cleaned up?\"\n            )\n        else:\n            raise\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.checkout_latest","title":"checkout_latest  <code>async</code>","text":"<pre><code>checkout_latest()\n</code></pre> <p>Ensures the table is pointing at the latest version</p> <p>This can be used to manually update a table when the read_consistency_interval is None It can also be used to undo a <code>[Self::checkout]</code> operation</p> Source code in <code>lancedb/table.py</code> <pre><code>async def checkout_latest(self):\n    \"\"\"\n    Ensures the table is pointing at the latest version\n\n    This can be used to manually update a table when the read_consistency_interval\n    is None\n    It can also be used to undo a `[Self::checkout]` operation\n    \"\"\"\n    await self._inner.checkout_latest()\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.restore","title":"restore  <code>async</code>","text":"<pre><code>restore(version: Optional[int] = None)\n</code></pre> <p>Restore the table to the currently checked out version</p> <p>This operation will fail if checkout has not been called previously</p> <p>This operation will overwrite the latest version of the table with a previous version.  Any changes made since the checked out version will no longer be visible.</p> <p>Once the operation concludes the table will no longer be in a checked out state and the read_consistency_interval, if any, will apply.</p> Source code in <code>lancedb/table.py</code> <pre><code>async def restore(self, version: Optional[int] = None):\n    \"\"\"\n    Restore the table to the currently checked out version\n\n    This operation will fail if checkout has not been called previously\n\n    This operation will overwrite the latest version of the table with a\n    previous version.  Any changes made since the checked out version will\n    no longer be visible.\n\n    Once the operation concludes the table will no longer be in a checked\n    out state and the read_consistency_interval, if any, will apply.\n    \"\"\"\n    await self._inner.restore(version)\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.optimize","title":"optimize  <code>async</code>","text":"<pre><code>optimize(*, cleanup_older_than: Optional[timedelta] = None, delete_unverified: bool = False, retrain=False) -&gt; OptimizeStats\n</code></pre> <p>Optimize the on-disk data and indices for better performance.</p> <p>Modeled after <code>VACUUM</code> in PostgreSQL.</p> <p>Optimization covers three operations:</p> <ul> <li>Compaction: Merges small files into larger ones</li> <li>Prune: Removes old versions of the dataset</li> <li>Index: Optimizes the indices, adding new data to existing indices</li> </ul> <p>Parameters:</p> <ul> <li> <code>cleanup_older_than</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>All files belonging to versions older than this will be removed.  Set to 0 days to remove all versions except the latest.  The latest version is never removed.</p> </li> <li> <code>delete_unverified</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Files leftover from a failed transaction may appear to be part of an in-progress operation (e.g. appending new data) and these files will not be deleted unless they are at least 7 days old. If delete_unverified is True then these files will be deleted regardless of their age.</p> </li> <li> <code>retrain</code>           \u2013            <p>If True, retrain the vector indices, this would refine the IVF clustering and quantization, which may improve the search accuracy. It's faster than re-creating the index from scratch, so it's recommended to try this first, when the data distribution has changed significantly.</p> </li> </ul> Experimental API <p>The optimization process is undergoing active development and may change. Our goal with these changes is to improve the performance of optimization and reduce the complexity.</p> <p>That being said, it is essential today to run optimize if you want the best performance.  It should be stable and safe to use in production, but it our hope that the API may be simplified (or not even need to be called) in the future.</p> <p>The frequency an application shoudl call optimize is based on the frequency of data modifications.  If data is frequently added, deleted, or updated then optimize should be run frequently.  A good rule of thumb is to run optimize if you have added or modified 100,000 or more records or run more than 20 data modification operations.</p> Source code in <code>lancedb/table.py</code> <pre><code>async def optimize(\n    self,\n    *,\n    cleanup_older_than: Optional[timedelta] = None,\n    delete_unverified: bool = False,\n    retrain=False,\n) -&gt; OptimizeStats:\n    \"\"\"\n    Optimize the on-disk data and indices for better performance.\n\n    Modeled after ``VACUUM`` in PostgreSQL.\n\n    Optimization covers three operations:\n\n     * Compaction: Merges small files into larger ones\n     * Prune: Removes old versions of the dataset\n     * Index: Optimizes the indices, adding new data to existing indices\n\n    Parameters\n    ----------\n    cleanup_older_than: timedelta, optional default 7 days\n        All files belonging to versions older than this will be removed.  Set\n        to 0 days to remove all versions except the latest.  The latest version\n        is never removed.\n    delete_unverified: bool, default False\n        Files leftover from a failed transaction may appear to be part of an\n        in-progress operation (e.g. appending new data) and these files will not\n        be deleted unless they are at least 7 days old. If delete_unverified is True\n        then these files will be deleted regardless of their age.\n    retrain: bool, default False\n        If True, retrain the vector indices, this would refine the IVF clustering\n        and quantization, which may improve the search accuracy. It's faster than\n        re-creating the index from scratch, so it's recommended to try this first,\n        when the data distribution has changed significantly.\n\n    Experimental API\n    ----------------\n\n    The optimization process is undergoing active development and may change.\n    Our goal with these changes is to improve the performance of optimization and\n    reduce the complexity.\n\n    That being said, it is essential today to run optimize if you want the best\n    performance.  It should be stable and safe to use in production, but it our\n    hope that the API may be simplified (or not even need to be called) in the\n    future.\n\n    The frequency an application shoudl call optimize is based on the frequency of\n    data modifications.  If data is frequently added, deleted, or updated then\n    optimize should be run frequently.  A good rule of thumb is to run optimize if\n    you have added or modified 100,000 or more records or run more than 20 data\n    modification operations.\n    \"\"\"\n    cleanup_since_ms: Optional[int] = None\n    if cleanup_older_than is not None:\n        cleanup_since_ms = round(cleanup_older_than.total_seconds() * 1000)\n    return await self._inner.optimize(\n        cleanup_since_ms=cleanup_since_ms,\n        delete_unverified=delete_unverified,\n        retrain=retrain,\n    )\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.list_indices","title":"list_indices  <code>async</code>","text":"<pre><code>list_indices() -&gt; Iterable[IndexConfig]\n</code></pre> <p>List all indices that have been created with Self::create_index</p> Source code in <code>lancedb/table.py</code> <pre><code>async def list_indices(self) -&gt; Iterable[IndexConfig]:\n    \"\"\"\n    List all indices that have been created with Self::create_index\n    \"\"\"\n    return await self._inner.list_indices()\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.index_stats","title":"index_stats  <code>async</code>","text":"<pre><code>index_stats(index_name: str) -&gt; Optional[IndexStatistics]\n</code></pre> <p>Retrieve statistics about an index</p> <p>Parameters:</p> <ul> <li> <code>index_name</code>               (<code>str</code>)           \u2013            <p>The name of the index to retrieve statistics for</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>IndexStatistics or None</code>           \u2013            <p>The statistics about the index. Returns None if the index does not exist.</p> </li> </ul> Source code in <code>lancedb/table.py</code> <pre><code>async def index_stats(self, index_name: str) -&gt; Optional[IndexStatistics]:\n    \"\"\"\n    Retrieve statistics about an index\n\n    Parameters\n    ----------\n    index_name: str\n        The name of the index to retrieve statistics for\n\n    Returns\n    -------\n    IndexStatistics or None\n        The statistics about the index. Returns None if the index does not exist.\n    \"\"\"\n    stats = await self._inner.index_stats(index_name)\n    if stats is None:\n        return None\n    else:\n        return IndexStatistics(**stats)\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.uses_v2_manifest_paths","title":"uses_v2_manifest_paths  <code>async</code>","text":"<pre><code>uses_v2_manifest_paths() -&gt; bool\n</code></pre> <p>Check if the table is using the new v2 manifest paths.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if the table is using the new v2 manifest paths, False otherwise.</p> </li> </ul> Source code in <code>lancedb/table.py</code> <pre><code>async def uses_v2_manifest_paths(self) -&gt; bool:\n    \"\"\"\n    Check if the table is using the new v2 manifest paths.\n\n    Returns\n    -------\n    bool\n        True if the table is using the new v2 manifest paths, False otherwise.\n    \"\"\"\n    return await self._inner.uses_v2_manifest_paths()\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.migrate_manifest_paths_v2","title":"migrate_manifest_paths_v2  <code>async</code>","text":"<pre><code>migrate_manifest_paths_v2()\n</code></pre> <p>Migrate the manifest paths to the new format.</p> <p>This will update the manifest to use the new v2 format for paths.</p> <p>This function is idempotent, and can be run multiple times without changing the state of the object store.</p> <p>Danger</p> <p>This should not be run while other concurrent operations are happening. And it should also run until completion before resuming other operations.</p> <p>You can use AsyncTable.uses_v2_manifest_paths to check if the table is already using the new path style.</p> Source code in <code>lancedb/table.py</code> <pre><code>async def migrate_manifest_paths_v2(self):\n    \"\"\"\n    Migrate the manifest paths to the new format.\n\n    This will update the manifest to use the new v2 format for paths.\n\n    This function is idempotent, and can be run multiple times without\n    changing the state of the object store.\n\n    !!! danger\n\n        This should not be run while other concurrent operations are happening.\n        And it should also run until completion before resuming other operations.\n\n    You can use\n    [AsyncTable.uses_v2_manifest_paths][lancedb.table.AsyncTable.uses_v2_manifest_paths]\n    to check if the table is already using the new path style.\n    \"\"\"\n    await self._inner.migrate_manifest_paths_v2()\n</code></pre>"},{"location":"python/python/#lancedb.table.AsyncTable.replace_field_metadata","title":"replace_field_metadata  <code>async</code>","text":"<pre><code>replace_field_metadata(field_name: str, new_metadata: dict[str, str])\n</code></pre> <p>Replace the metadata of a field in the schema</p> <p>Parameters:</p> <ul> <li> <code>field_name</code>               (<code>str</code>)           \u2013            <p>The name of the field to replace the metadata for</p> </li> <li> <code>new_metadata</code>               (<code>dict[str, str]</code>)           \u2013            <p>The new metadata to set</p> </li> </ul> Source code in <code>lancedb/table.py</code> <pre><code>async def replace_field_metadata(\n    self, field_name: str, new_metadata: dict[str, str]\n):\n    \"\"\"\n    Replace the metadata of a field in the schema\n\n    Parameters\n    ----------\n    field_name: str\n        The name of the field to replace the metadata for\n    new_metadata: dict\n        The new metadata to set\n    \"\"\"\n    await self._inner.replace_field_metadata(field_name, new_metadata)\n</code></pre>"},{"location":"python/python/#indices-asynchronous","title":"Indices (Asynchronous)","text":"<p>Indices can be created on a table to speed up queries. This section lists the indices that LanceDb supports.</p>"},{"location":"python/python/#lancedb.index.BTree","title":"lancedb.index.BTree  <code>dataclass</code>","text":"<p>Describes a btree index configuration</p> <p>A btree index is an index on scalar columns.  The index stores a copy of the column in sorted order.  A header entry is created for each block of rows (currently the block size is fixed at 4096).  These header entries are stored in a separate cacheable structure (a btree).  To search for data the header is used to determine which blocks need to be read from disk.</p> <p>For example, a btree index in a table with 1Bi rows requires sizeof(Scalar) * 256Ki bytes of memory and will generally need to read sizeof(Scalar) * 4096 bytes to find the correct row ids.</p> <p>This index is good for scalar columns with mostly distinct values and does best when the query is highly selective. It works with numeric, temporal, and string columns.</p> <p>The btree index does not currently have any parameters though parameters such as the block size may be added in the future.</p> Source code in <code>lancedb/index.py</code> <pre><code>@dataclass\nclass BTree:\n    \"\"\"Describes a btree index configuration\n\n    A btree index is an index on scalar columns.  The index stores a copy of the\n    column in sorted order.  A header entry is created for each block of rows\n    (currently the block size is fixed at 4096).  These header entries are stored\n    in a separate cacheable structure (a btree).  To search for data the header is\n    used to determine which blocks need to be read from disk.\n\n    For example, a btree index in a table with 1Bi rows requires\n    sizeof(Scalar) * 256Ki bytes of memory and will generally need to read\n    sizeof(Scalar) * 4096 bytes to find the correct row ids.\n\n    This index is good for scalar columns with mostly distinct values and does best\n    when the query is highly selective. It works with numeric, temporal, and string\n    columns.\n\n    The btree index does not currently have any parameters though parameters such as\n    the block size may be added in the future.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"python/python/#lancedb.index.Bitmap","title":"lancedb.index.Bitmap  <code>dataclass</code>","text":"<p>Describe a Bitmap index configuration.</p> <p>A <code>Bitmap</code> index stores a bitmap for each distinct value in the column for every row.</p> <p>This index works best for low-cardinality numeric or string columns, where the number of unique values is small (i.e., less than a few thousands). <code>Bitmap</code> index can accelerate the following filters:</p> <ul> <li><code>&lt;</code>, <code>&lt;=</code>, <code>=</code>, <code>&gt;</code>, <code>&gt;=</code></li> <li><code>IN (value1, value2, ...)</code></li> <li><code>between (value1, value2)</code></li> <li><code>is null</code></li> </ul> <p>For example, a bitmap index with a table with 1Bi rows, and 128 distinct values, requires 128 / 8 * 1Bi bytes on disk.</p> Source code in <code>lancedb/index.py</code> <pre><code>@dataclass\nclass Bitmap:\n    \"\"\"Describe a Bitmap index configuration.\n\n    A `Bitmap` index stores a bitmap for each distinct value in the column for\n    every row.\n\n    This index works best for low-cardinality numeric or string columns,\n    where the number of unique values is small (i.e., less than a few thousands).\n    `Bitmap` index can accelerate the following filters:\n\n    - `&lt;`, `&lt;=`, `=`, `&gt;`, `&gt;=`\n    - `IN (value1, value2, ...)`\n    - `between (value1, value2)`\n    - `is null`\n\n    For example, a bitmap index with a table with 1Bi rows, and 128 distinct values,\n    requires 128 / 8 * 1Bi bytes on disk.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"python/python/#lancedb.index.LabelList","title":"lancedb.index.LabelList  <code>dataclass</code>","text":"<p>Describe a LabelList index configuration.</p> <p><code>LabelList</code> is a scalar index that can be used on <code>List&lt;T&gt;</code> columns to support queries with <code>array_contains_all</code> and <code>array_contains_any</code> using an underlying bitmap index.</p> <p>For example, it works with <code>tags</code>, <code>categories</code>, <code>keywords</code>, etc.</p> Source code in <code>lancedb/index.py</code> <pre><code>@dataclass\nclass LabelList:\n    \"\"\"Describe a LabelList index configuration.\n\n    `LabelList` is a scalar index that can be used on `List&lt;T&gt;` columns to\n    support queries with `array_contains_all` and `array_contains_any`\n    using an underlying bitmap index.\n\n    For example, it works with `tags`, `categories`, `keywords`, etc.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"python/python/#lancedb.index.FTS","title":"lancedb.index.FTS  <code>dataclass</code>","text":"<p>Describe a FTS index configuration.</p> <p><code>FTS</code> is a full-text search index that can be used on <code>String</code> columns</p> <p>For example, it works with <code>title</code>, <code>description</code>, <code>content</code>, etc.</p> <p>Attributes:</p> <ul> <li> <code>with_position</code>               (<code>bool, default True</code>)           \u2013            <p>Whether to store the position of the token in the document. Setting this to False can reduce the size of the index and improve indexing speed, but it will disable support for phrase queries.</p> </li> <li> <code>base_tokenizer</code>               (<code>str, default \"simple\"</code>)           \u2013            <p>The base tokenizer to use for tokenization. Options are: - \"simple\": Splits text by whitespace and punctuation. - \"whitespace\": Split text by whitespace, but not punctuation. - \"raw\": No tokenization. The entire text is treated as a single token.</p> </li> <li> <code>language</code>               (<code>str, default \"English\"</code>)           \u2013            <p>The language to use for tokenization.</p> </li> <li> <code>max_token_length</code>               (<code>int, default 40</code>)           \u2013            <p>The maximum token length to index. Tokens longer than this length will be ignored.</p> </li> <li> <code>lower_case</code>               (<code>bool, default True</code>)           \u2013            <p>Whether to convert the token to lower case. This makes queries case-insensitive.</p> </li> <li> <code>stem</code>               (<code>bool, default False</code>)           \u2013            <p>Whether to stem the token. Stemming reduces words to their root form. For example, in English \"running\" and \"runs\" would both be reduced to \"run\".</p> </li> <li> <code>remove_stop_words</code>               (<code>bool, default False</code>)           \u2013            <p>Whether to remove stop words. Stop words are common words that are often removed from text before indexing. For example, in English \"the\" and \"and\".</p> </li> <li> <code>ascii_folding</code>               (<code>bool, default False</code>)           \u2013            <p>Whether to fold ASCII characters. This converts accented characters to their ASCII equivalent. For example, \"caf\u00e9\" would be converted to \"cafe\".</p> </li> </ul> Source code in <code>lancedb/index.py</code> <pre><code>@dataclass\nclass FTS:\n    \"\"\"Describe a FTS index configuration.\n\n    `FTS` is a full-text search index that can be used on `String` columns\n\n    For example, it works with `title`, `description`, `content`, etc.\n\n    Attributes\n    ----------\n    with_position : bool, default True\n        Whether to store the position of the token in the document. Setting this\n        to False can reduce the size of the index and improve indexing speed,\n        but it will disable support for phrase queries.\n    base_tokenizer : str, default \"simple\"\n        The base tokenizer to use for tokenization. Options are:\n        - \"simple\": Splits text by whitespace and punctuation.\n        - \"whitespace\": Split text by whitespace, but not punctuation.\n        - \"raw\": No tokenization. The entire text is treated as a single token.\n    language : str, default \"English\"\n        The language to use for tokenization.\n    max_token_length : int, default 40\n        The maximum token length to index. Tokens longer than this length will be\n        ignored.\n    lower_case : bool, default True\n        Whether to convert the token to lower case. This makes queries case-insensitive.\n    stem : bool, default False\n        Whether to stem the token. Stemming reduces words to their root form.\n        For example, in English \"running\" and \"runs\" would both be reduced to \"run\".\n    remove_stop_words : bool, default False\n        Whether to remove stop words. Stop words are common words that are often\n        removed from text before indexing. For example, in English \"the\" and \"and\".\n    ascii_folding : bool, default False\n        Whether to fold ASCII characters. This converts accented characters to\n        their ASCII equivalent. For example, \"caf\u00e9\" would be converted to \"cafe\".\n    \"\"\"\n\n    with_position: bool = True\n    base_tokenizer: Literal[\"simple\", \"raw\", \"whitespace\"] = \"simple\"\n    language: str = \"English\"\n    max_token_length: Optional[int] = 40\n    lower_case: bool = True\n    stem: bool = False\n    remove_stop_words: bool = False\n    ascii_folding: bool = False\n</code></pre>"},{"location":"python/python/#lancedb.index.IvfPq","title":"lancedb.index.IvfPq  <code>dataclass</code>","text":"<p>Describes an IVF PQ Index</p> <p>This index stores a compressed (quantized) copy of every vector.  These vectors are grouped into partitions of similar vectors.  Each partition keeps track of a centroid which is the average value of all vectors in the group.</p> <p>During a query the centroids are compared with the query vector to find the closest partitions.  The compressed vectors in these partitions are then searched to find the closest vectors.</p> <p>The compression scheme is called product quantization.  Each vector is divide into subvectors and then each subvector is quantized into a small number of bits.  the parameters <code>num_bits</code> and <code>num_subvectors</code> control this process, providing a tradeoff between index size (and thus search speed) and index accuracy.</p> <p>The partitioning process is called IVF and the <code>num_partitions</code> parameter controls how many groups to create.</p> <p>Note that training an IVF PQ index on a large dataset is a slow operation and currently is also a memory intensive operation.</p> <p>Attributes:</p> <ul> <li> <code>distance_type</code>               (<code>str, default \"l2\"</code>)           \u2013            <p>The distance metric used to train the index</p> <p>This is used when training the index to calculate the IVF partitions (vectors are grouped in partitions with similar vectors according to this distance type) and to calculate a subvector's code during quantization.</p> <p>The distance type used to train an index MUST match the distance type used to search the index.  Failure to do so will yield inaccurate results.</p> <p>The following distance types are available:</p> <p>\"l2\" - Euclidean distance. This is a very common distance metric that accounts for both magnitude and direction when determining the distance between vectors. l2 distance has a range of [0, \u221e).</p> <p>\"cosine\" - Cosine distance.  Cosine distance is a distance metric calculated from the cosine similarity between two vectors. Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space. It is defined to equal the cosine of the angle between them.  Unlike l2, the cosine distance is not affected by the magnitude of the vectors.  Cosine distance has a range of [0, 2].</p> <p>Note: the cosine distance is undefined when one (or both) of the vectors are all zeros (there is no direction).  These vectors are invalid and may never be returned from a vector search.</p> <p>\"dot\" - Dot product. Dot distance is the dot product of two vectors. Dot distance has a range of (-\u221e, \u221e). If the vectors are normalized (i.e. their l2 norm is 1), then dot distance is equivalent to the cosine distance.</p> </li> <li> <code>num_partitions</code>               (<code>int, default sqrt(num_rows)</code>)           \u2013            <p>The number of IVF partitions to create.</p> <p>This value should generally scale with the number of rows in the dataset. By default the number of partitions is the square root of the number of rows.</p> <p>If this value is too large then the first part of the search (picking the right partition) will be slow.  If this value is too small then the second part of the search (searching within a partition) will be slow.</p> </li> <li> <code>num_sub_vectors</code>               (<code>int, default is vector dimension / 16</code>)           \u2013            <p>Number of sub-vectors of PQ.</p> <p>This value controls how much the vector is compressed during the quantization step.  The more sub vectors there are the less the vector is compressed.  The default is the dimension of the vector divided by 16.  If the dimension is not evenly divisible by 16 we use the dimension divded by 8.</p> <p>The above two cases are highly preferred.  Having 8 or 16 values per subvector allows us to use efficient SIMD instructions.</p> <p>If the dimension is not visible by 8 then we use 1 subvector.  This is not ideal and will likely result in poor performance.</p> </li> <li> <code>num_bits</code>               (<code>int, default 8</code>)           \u2013            <p>Number of bits to encode each sub-vector.</p> <p>This value controls how much the sub-vectors are compressed.  The more bits the more accurate the index but the slower search.  The default is 8 bits.  Only 4 and 8 are supported.</p> </li> <li> <code>max_iterations</code>               (<code>int, default 50</code>)           \u2013            <p>Max iteration to train kmeans.</p> <p>When training an IVF PQ index we use kmeans to calculate the partitions. This parameter controls how many iterations of kmeans to run.</p> <p>Increasing this might improve the quality of the index but in most cases these extra iterations have diminishing returns.</p> <p>The default value is 50.</p> </li> <li> <code>sample_rate</code>               (<code>int, default 256</code>)           \u2013            <p>The rate used to calculate the number of training vectors for kmeans.</p> <p>When an IVF PQ index is trained, we need to calculate partitions.  These are groups of vectors that are similar to each other.  To do this we use an algorithm called kmeans.</p> <p>Running kmeans on a large dataset can be slow.  To speed this up we run kmeans on a random sample of the data.  This parameter controls the size of the sample.  The total number of vectors used to train the index is <code>sample_rate * num_partitions</code>.</p> <p>Increasing this value might improve the quality of the index but in most cases the default should be sufficient.</p> <p>The default value is 256.</p> </li> </ul> Source code in <code>lancedb/index.py</code> <pre><code>@dataclass\nclass IvfPq:\n    \"\"\"Describes an IVF PQ Index\n\n    This index stores a compressed (quantized) copy of every vector.  These vectors\n    are grouped into partitions of similar vectors.  Each partition keeps track of\n    a centroid which is the average value of all vectors in the group.\n\n    During a query the centroids are compared with the query vector to find the\n    closest partitions.  The compressed vectors in these partitions are then\n    searched to find the closest vectors.\n\n    The compression scheme is called product quantization.  Each vector is divide\n    into subvectors and then each subvector is quantized into a small number of\n    bits.  the parameters `num_bits` and `num_subvectors` control this process,\n    providing a tradeoff between index size (and thus search speed) and index\n    accuracy.\n\n    The partitioning process is called IVF and the `num_partitions` parameter\n    controls how many groups to create.\n\n    Note that training an IVF PQ index on a large dataset is a slow operation and\n    currently is also a memory intensive operation.\n\n    Attributes\n    ----------\n    distance_type: str, default \"l2\"\n        The distance metric used to train the index\n\n        This is used when training the index to calculate the IVF partitions\n        (vectors are grouped in partitions with similar vectors according to this\n        distance type) and to calculate a subvector's code during quantization.\n\n        The distance type used to train an index MUST match the distance type used\n        to search the index.  Failure to do so will yield inaccurate results.\n\n        The following distance types are available:\n\n        \"l2\" - Euclidean distance. This is a very common distance metric that\n        accounts for both magnitude and direction when determining the distance\n        between vectors. l2 distance has a range of [0, \u221e).\n\n        \"cosine\" - Cosine distance.  Cosine distance is a distance metric\n        calculated from the cosine similarity between two vectors. Cosine\n        similarity is a measure of similarity between two non-zero vectors of an\n        inner product space. It is defined to equal the cosine of the angle\n        between them.  Unlike l2, the cosine distance is not affected by the\n        magnitude of the vectors.  Cosine distance has a range of [0, 2].\n\n        Note: the cosine distance is undefined when one (or both) of the vectors\n        are all zeros (there is no direction).  These vectors are invalid and may\n        never be returned from a vector search.\n\n        \"dot\" - Dot product. Dot distance is the dot product of two vectors. Dot\n        distance has a range of (-\u221e, \u221e). If the vectors are normalized (i.e. their\n        l2 norm is 1), then dot distance is equivalent to the cosine distance.\n    num_partitions: int, default sqrt(num_rows)\n        The number of IVF partitions to create.\n\n        This value should generally scale with the number of rows in the dataset.\n        By default the number of partitions is the square root of the number of\n        rows.\n\n        If this value is too large then the first part of the search (picking the\n        right partition) will be slow.  If this value is too small then the second\n        part of the search (searching within a partition) will be slow.\n    num_sub_vectors: int, default is vector dimension / 16\n        Number of sub-vectors of PQ.\n\n        This value controls how much the vector is compressed during the\n        quantization step.  The more sub vectors there are the less the vector is\n        compressed.  The default is the dimension of the vector divided by 16.  If\n        the dimension is not evenly divisible by 16 we use the dimension divded by\n        8.\n\n        The above two cases are highly preferred.  Having 8 or 16 values per\n        subvector allows us to use efficient SIMD instructions.\n\n        If the dimension is not visible by 8 then we use 1 subvector.  This is not\n        ideal and will likely result in poor performance.\n    num_bits: int, default 8\n        Number of bits to encode each sub-vector.\n\n        This value controls how much the sub-vectors are compressed.  The more bits\n        the more accurate the index but the slower search.  The default is 8\n        bits.  Only 4 and 8 are supported.\n    max_iterations: int, default 50\n        Max iteration to train kmeans.\n\n        When training an IVF PQ index we use kmeans to calculate the partitions.\n        This parameter controls how many iterations of kmeans to run.\n\n        Increasing this might improve the quality of the index but in most cases\n        these extra iterations have diminishing returns.\n\n        The default value is 50.\n    sample_rate: int, default 256\n        The rate used to calculate the number of training vectors for kmeans.\n\n        When an IVF PQ index is trained, we need to calculate partitions.  These\n        are groups of vectors that are similar to each other.  To do this we use an\n        algorithm called kmeans.\n\n        Running kmeans on a large dataset can be slow.  To speed this up we run\n        kmeans on a random sample of the data.  This parameter controls the size of\n        the sample.  The total number of vectors used to train the index is\n        `sample_rate * num_partitions`.\n\n        Increasing this value might improve the quality of the index but in most\n        cases the default should be sufficient.\n\n        The default value is 256.\n    \"\"\"\n\n    distance_type: Literal[\"l2\", \"cosine\", \"dot\"] = \"l2\"\n    num_partitions: Optional[int] = None\n    num_sub_vectors: Optional[int] = None\n    num_bits: int = 8\n    max_iterations: int = 50\n    sample_rate: int = 256\n</code></pre>"},{"location":"python/python/#lancedb.index.HnswPq","title":"lancedb.index.HnswPq  <code>dataclass</code>","text":"<p>Describe a HNSW-PQ index configuration.</p> <p>HNSW-PQ stands for Hierarchical Navigable Small World - Product Quantization. It is a variant of the HNSW algorithm that uses product quantization to compress the vectors. To create an HNSW-PQ index, you can specify the following parameters:</p> <p>Parameters:</p> <ul> <li> <code>distance_type</code>               (<code>Literal['l2', 'cosine', 'dot']</code>, default:                   <code>'l2'</code> )           \u2013            <p>The distance metric used to train the index.</p> <p>The following distance types are available:</p> <p>\"l2\" - Euclidean distance. This is a very common distance metric that accounts for both magnitude and direction when determining the distance between vectors. l2 distance has a range of [0, \u221e).</p> <p>\"cosine\" - Cosine distance.  Cosine distance is a distance metric calculated from the cosine similarity between two vectors. Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space. It is defined to equal the cosine of the angle between them.  Unlike l2, the cosine distance is not affected by the magnitude of the vectors.  Cosine distance has a range of [0, 2].</p> <p>\"dot\" - Dot product. Dot distance is the dot product of two vectors. Dot distance has a range of (-\u221e, \u221e). If the vectors are normalized (i.e. their l2 norm is 1), then dot distance is equivalent to the cosine distance.</p> </li> <li> <code>num_partitions</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The number of IVF partitions to create.</p> <p>For HNSW, we recommend a small number of partitions. Setting this to 1 works well for most tables. For very large tables, training just one HNSW graph will require too much memory. Each partition becomes its own HNSW graph, so setting this value higher reduces the peak memory use of training.</p> </li> <li> <code>default</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The number of IVF partitions to create.</p> <p>For HNSW, we recommend a small number of partitions. Setting this to 1 works well for most tables. For very large tables, training just one HNSW graph will require too much memory. Each partition becomes its own HNSW graph, so setting this value higher reduces the peak memory use of training.</p> </li> <li> <code>num_sub_vectors</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Number of sub-vectors of PQ.</p> <p>This value controls how much the vector is compressed during the quantization step. The more sub vectors there are the less the vector is compressed.  The default is the dimension of the vector divided by 16. If the dimension is not evenly divisible by 16 we use the dimension divided by 8.</p> <p>The above two cases are highly preferred.  Having 8 or 16 values per subvector allows us to use efficient SIMD instructions.</p> <p>If the dimension is not visible by 8 then we use 1 subvector.  This is not ideal and will likely result in poor performance.</p> <p>num_bits: int, default 8 Number of bits to encode each sub-vector.</p> <p>This value controls how much the sub-vectors are compressed.  The more bits the more accurate the index but the slower search. Only 4 and 8 are supported.</p> </li> <li> <code>default</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Number of sub-vectors of PQ.</p> <p>This value controls how much the vector is compressed during the quantization step. The more sub vectors there are the less the vector is compressed.  The default is the dimension of the vector divided by 16. If the dimension is not evenly divisible by 16 we use the dimension divided by 8.</p> <p>The above two cases are highly preferred.  Having 8 or 16 values per subvector allows us to use efficient SIMD instructions.</p> <p>If the dimension is not visible by 8 then we use 1 subvector.  This is not ideal and will likely result in poor performance.</p> <p>num_bits: int, default 8 Number of bits to encode each sub-vector.</p> <p>This value controls how much the sub-vectors are compressed.  The more bits the more accurate the index but the slower search. Only 4 and 8 are supported.</p> </li> <li> <code>max_iterations</code>               (<code>int</code>, default:                   <code>50</code> )           \u2013            <p>Max iterations to train kmeans.</p> <p>When training an IVF index we use kmeans to calculate the partitions.  This parameter controls how many iterations of kmeans to run.</p> <p>Increasing this might improve the quality of the index but in most cases the parameter is unused because kmeans will converge with fewer iterations.  The parameter is only used in cases where kmeans does not appear to converge.  In those cases it is unlikely that setting this larger will lead to the index converging anyways.</p> </li> <li> <code>default</code>               (<code>int</code>, default:                   <code>50</code> )           \u2013            <p>Max iterations to train kmeans.</p> <p>When training an IVF index we use kmeans to calculate the partitions.  This parameter controls how many iterations of kmeans to run.</p> <p>Increasing this might improve the quality of the index but in most cases the parameter is unused because kmeans will converge with fewer iterations.  The parameter is only used in cases where kmeans does not appear to converge.  In those cases it is unlikely that setting this larger will lead to the index converging anyways.</p> </li> <li> <code>sample_rate</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The rate used to calculate the number of training vectors for kmeans.</p> <p>When an IVF index is trained, we need to calculate partitions.  These are groups of vectors that are similar to each other.  To do this we use an algorithm called kmeans.</p> <p>Running kmeans on a large dataset can be slow.  To speed this up we run kmeans on a random sample of the data.  This parameter controls the size of the sample.  The total number of vectors used to train the index is <code>sample_rate * num_partitions</code>.</p> <p>Increasing this value might improve the quality of the index but in most cases the default should be sufficient.</p> </li> <li> <code>default</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The rate used to calculate the number of training vectors for kmeans.</p> <p>When an IVF index is trained, we need to calculate partitions.  These are groups of vectors that are similar to each other.  To do this we use an algorithm called kmeans.</p> <p>Running kmeans on a large dataset can be slow.  To speed this up we run kmeans on a random sample of the data.  This parameter controls the size of the sample.  The total number of vectors used to train the index is <code>sample_rate * num_partitions</code>.</p> <p>Increasing this value might improve the quality of the index but in most cases the default should be sufficient.</p> </li> <li> <code>m</code>               (<code>int</code>, default:                   <code>20</code> )           \u2013            <p>The number of neighbors to select for each vector in the HNSW graph.</p> <p>This value controls the tradeoff between search speed and accuracy. The higher the value the more accurate the search but the slower it will be.</p> </li> <li> <code>default</code>               (<code>int</code>, default:                   <code>20</code> )           \u2013            <p>The number of neighbors to select for each vector in the HNSW graph.</p> <p>This value controls the tradeoff between search speed and accuracy. The higher the value the more accurate the search but the slower it will be.</p> </li> <li> <code>ef_construction</code>               (<code>int</code>, default:                   <code>300</code> )           \u2013            <p>The number of candidates to evaluate during the construction of the HNSW graph.</p> <p>This value controls the tradeoff between build speed and accuracy. The higher the value the more accurate the build but the slower it will be. 150 to 300 is the typical range. 100 is a minimum for good quality search results. In most cases, there is no benefit to setting this higher than 500. This value should be set to a value that is not less than <code>ef</code> in the search phase.</p> </li> <li> <code>default</code>               (<code>int</code>, default:                   <code>300</code> )           \u2013            <p>The number of candidates to evaluate during the construction of the HNSW graph.</p> <p>This value controls the tradeoff between build speed and accuracy. The higher the value the more accurate the build but the slower it will be. 150 to 300 is the typical range. 100 is a minimum for good quality search results. In most cases, there is no benefit to setting this higher than 500. This value should be set to a value that is not less than <code>ef</code> in the search phase.</p> </li> </ul> Source code in <code>lancedb/index.py</code> <pre><code>@dataclass\nclass HnswPq:\n    \"\"\"Describe a HNSW-PQ index configuration.\n\n    HNSW-PQ stands for Hierarchical Navigable Small World - Product Quantization.\n    It is a variant of the HNSW algorithm that uses product quantization to compress\n    the vectors. To create an HNSW-PQ index, you can specify the following parameters:\n\n    Parameters\n    ----------\n\n    distance_type: str, default \"l2\"\n\n        The distance metric used to train the index.\n\n        The following distance types are available:\n\n        \"l2\" - Euclidean distance. This is a very common distance metric that\n        accounts for both magnitude and direction when determining the distance\n        between vectors. l2 distance has a range of [0, \u221e).\n\n        \"cosine\" - Cosine distance.  Cosine distance is a distance metric\n        calculated from the cosine similarity between two vectors. Cosine\n        similarity is a measure of similarity between two non-zero vectors of an\n        inner product space. It is defined to equal the cosine of the angle\n        between them.  Unlike l2, the cosine distance is not affected by the\n        magnitude of the vectors.  Cosine distance has a range of [0, 2].\n\n        \"dot\" - Dot product. Dot distance is the dot product of two vectors. Dot\n        distance has a range of (-\u221e, \u221e). If the vectors are normalized (i.e. their\n        l2 norm is 1), then dot distance is equivalent to the cosine distance.\n\n    num_partitions, default sqrt(num_rows)\n\n        The number of IVF partitions to create.\n\n        For HNSW, we recommend a small number of partitions. Setting this to 1 works\n        well for most tables. For very large tables, training just one HNSW graph\n        will require too much memory. Each partition becomes its own HNSW graph, so\n        setting this value higher reduces the peak memory use of training.\n\n    num_sub_vectors, default is vector dimension / 16\n\n        Number of sub-vectors of PQ.\n\n        This value controls how much the vector is compressed during the\n        quantization step. The more sub vectors there are the less the vector is\n        compressed.  The default is the dimension of the vector divided by 16.\n        If the dimension is not evenly divisible by 16 we use the dimension\n        divided by 8.\n\n        The above two cases are highly preferred.  Having 8 or 16 values per\n        subvector allows us to use efficient SIMD instructions.\n\n        If the dimension is not visible by 8 then we use 1 subvector.  This is not\n        ideal and will likely result in poor performance.\n\n     num_bits: int, default 8\n        Number of bits to encode each sub-vector.\n\n        This value controls how much the sub-vectors are compressed.  The more bits\n        the more accurate the index but the slower search. Only 4 and 8 are supported.\n\n    max_iterations, default 50\n\n        Max iterations to train kmeans.\n\n        When training an IVF index we use kmeans to calculate the partitions.  This\n        parameter controls how many iterations of kmeans to run.\n\n        Increasing this might improve the quality of the index but in most cases the\n        parameter is unused because kmeans will converge with fewer iterations.  The\n        parameter is only used in cases where kmeans does not appear to converge.  In\n        those cases it is unlikely that setting this larger will lead to the index\n        converging anyways.\n\n    sample_rate, default 256\n\n        The rate used to calculate the number of training vectors for kmeans.\n\n        When an IVF index is trained, we need to calculate partitions.  These are\n        groups of vectors that are similar to each other.  To do this we use an\n        algorithm called kmeans.\n\n        Running kmeans on a large dataset can be slow.  To speed this up we\n        run kmeans on a random sample of the data.  This parameter controls the\n        size of the sample.  The total number of vectors used to train the index\n        is `sample_rate * num_partitions`.\n\n        Increasing this value might improve the quality of the index but in\n        most cases the default should be sufficient.\n\n    m, default 20\n\n        The number of neighbors to select for each vector in the HNSW graph.\n\n        This value controls the tradeoff between search speed and accuracy.\n        The higher the value the more accurate the search but the slower it will be.\n\n    ef_construction, default 300\n\n        The number of candidates to evaluate during the construction of the HNSW graph.\n\n        This value controls the tradeoff between build speed and accuracy.\n        The higher the value the more accurate the build but the slower it will be.\n        150 to 300 is the typical range. 100 is a minimum for good quality search\n        results. In most cases, there is no benefit to setting this higher than 500.\n        This value should be set to a value that is not less than `ef` in the\n        search phase.\n    \"\"\"\n\n    distance_type: Literal[\"l2\", \"cosine\", \"dot\"] = \"l2\"\n    num_partitions: Optional[int] = None\n    num_sub_vectors: Optional[int] = None\n    num_bits: int = 8\n    max_iterations: int = 50\n    sample_rate: int = 256\n    m: int = 20\n    ef_construction: int = 300\n</code></pre>"},{"location":"python/python/#lancedb.index.HnswSq","title":"lancedb.index.HnswSq  <code>dataclass</code>","text":"<p>Describe a HNSW-SQ index configuration.</p> <p>HNSW-SQ stands for Hierarchical Navigable Small World - Scalar Quantization. It is a variant of the HNSW algorithm that uses scalar quantization to compress the vectors.</p> <p>Parameters:</p> <ul> <li> <code>distance_type</code>               (<code>Literal['l2', 'cosine', 'dot']</code>, default:                   <code>'l2'</code> )           \u2013            <p>The distance metric used to train the index.</p> <p>The following distance types are available:</p> <p>\"l2\" - Euclidean distance. This is a very common distance metric that accounts for both magnitude and direction when determining the distance between vectors. l2 distance has a range of [0, \u221e).</p> <p>\"cosine\" - Cosine distance.  Cosine distance is a distance metric calculated from the cosine similarity between two vectors. Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space. It is defined to equal the cosine of the angle between them.  Unlike l2, the cosine distance is not affected by the magnitude of the vectors.  Cosine distance has a range of [0, 2].</p> <p>\"dot\" - Dot product. Dot distance is the dot product of two vectors. Dot distance has a range of (-\u221e, \u221e). If the vectors are normalized (i.e. their l2 norm is 1), then dot distance is equivalent to the cosine distance.</p> </li> <li> <code>num_partitions</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The number of IVF partitions to create.</p> <p>For HNSW, we recommend a small number of partitions. Setting this to 1 works well for most tables. For very large tables, training just one HNSW graph will require too much memory. Each partition becomes its own HNSW graph, so setting this value higher reduces the peak memory use of training.</p> </li> <li> <code>default</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The number of IVF partitions to create.</p> <p>For HNSW, we recommend a small number of partitions. Setting this to 1 works well for most tables. For very large tables, training just one HNSW graph will require too much memory. Each partition becomes its own HNSW graph, so setting this value higher reduces the peak memory use of training.</p> </li> <li> <code>max_iterations</code>               (<code>int</code>, default:                   <code>50</code> )           \u2013            <p>Max iterations to train kmeans.</p> <p>When training an IVF index we use kmeans to calculate the partitions. This parameter controls how many iterations of kmeans to run.</p> <p>Increasing this might improve the quality of the index but in most cases the parameter is unused because kmeans will converge with fewer iterations. The parameter is only used in cases where kmeans does not appear to converge. In those cases it is unlikely that setting this larger will lead to the index converging anyways.</p> </li> <li> <code>default</code>               (<code>int</code>, default:                   <code>50</code> )           \u2013            <p>Max iterations to train kmeans.</p> <p>When training an IVF index we use kmeans to calculate the partitions. This parameter controls how many iterations of kmeans to run.</p> <p>Increasing this might improve the quality of the index but in most cases the parameter is unused because kmeans will converge with fewer iterations. The parameter is only used in cases where kmeans does not appear to converge. In those cases it is unlikely that setting this larger will lead to the index converging anyways.</p> </li> <li> <code>sample_rate</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The rate used to calculate the number of training vectors for kmeans.</p> <p>When an IVF index is trained, we need to calculate partitions.  These are groups of vectors that are similar to each other.  To do this we use an algorithm called kmeans.</p> <p>Running kmeans on a large dataset can be slow.  To speed this up we run kmeans on a random sample of the data.  This parameter controls the size of the sample.  The total number of vectors used to train the index is <code>sample_rate * num_partitions</code>.</p> <p>Increasing this value might improve the quality of the index but in most cases the default should be sufficient.</p> </li> <li> <code>default</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The rate used to calculate the number of training vectors for kmeans.</p> <p>When an IVF index is trained, we need to calculate partitions.  These are groups of vectors that are similar to each other.  To do this we use an algorithm called kmeans.</p> <p>Running kmeans on a large dataset can be slow.  To speed this up we run kmeans on a random sample of the data.  This parameter controls the size of the sample.  The total number of vectors used to train the index is <code>sample_rate * num_partitions</code>.</p> <p>Increasing this value might improve the quality of the index but in most cases the default should be sufficient.</p> </li> <li> <code>m</code>               (<code>int</code>, default:                   <code>20</code> )           \u2013            <p>The number of neighbors to select for each vector in the HNSW graph.</p> <p>This value controls the tradeoff between search speed and accuracy. The higher the value the more accurate the search but the slower it will be.</p> </li> <li> <code>default</code>               (<code>int</code>, default:                   <code>20</code> )           \u2013            <p>The number of neighbors to select for each vector in the HNSW graph.</p> <p>This value controls the tradeoff between search speed and accuracy. The higher the value the more accurate the search but the slower it will be.</p> </li> <li> <code>ef_construction</code>               (<code>int</code>, default:                   <code>300</code> )           \u2013            <p>The number of candidates to evaluate during the construction of the HNSW graph.</p> <p>This value controls the tradeoff between build speed and accuracy. The higher the value the more accurate the build but the slower it will be. 150 to 300 is the typical range. 100 is a minimum for good quality search results. In most cases, there is no benefit to setting this higher than 500. This value should be set to a value that is not less than <code>ef</code> in the search phase.</p> </li> <li> <code>default</code>               (<code>int</code>, default:                   <code>300</code> )           \u2013            <p>The number of candidates to evaluate during the construction of the HNSW graph.</p> <p>This value controls the tradeoff between build speed and accuracy. The higher the value the more accurate the build but the slower it will be. 150 to 300 is the typical range. 100 is a minimum for good quality search results. In most cases, there is no benefit to setting this higher than 500. This value should be set to a value that is not less than <code>ef</code> in the search phase.</p> </li> </ul> Source code in <code>lancedb/index.py</code> <pre><code>@dataclass\nclass HnswSq:\n    \"\"\"Describe a HNSW-SQ index configuration.\n\n    HNSW-SQ stands for Hierarchical Navigable Small World - Scalar Quantization.\n    It is a variant of the HNSW algorithm that uses scalar quantization to compress\n    the vectors.\n\n    Parameters\n    ----------\n\n    distance_type: str, default \"l2\"\n\n        The distance metric used to train the index.\n\n        The following distance types are available:\n\n        \"l2\" - Euclidean distance. This is a very common distance metric that\n        accounts for both magnitude and direction when determining the distance\n        between vectors. l2 distance has a range of [0, \u221e).\n\n        \"cosine\" - Cosine distance.  Cosine distance is a distance metric\n        calculated from the cosine similarity between two vectors. Cosine\n        similarity is a measure of similarity between two non-zero vectors of an\n        inner product space. It is defined to equal the cosine of the angle\n        between them.  Unlike l2, the cosine distance is not affected by the\n        magnitude of the vectors.  Cosine distance has a range of [0, 2].\n\n        \"dot\" - Dot product. Dot distance is the dot product of two vectors. Dot\n        distance has a range of (-\u221e, \u221e). If the vectors are normalized (i.e. their\n        l2 norm is 1), then dot distance is equivalent to the cosine distance.\n\n    num_partitions, default sqrt(num_rows)\n\n        The number of IVF partitions to create.\n\n        For HNSW, we recommend a small number of partitions. Setting this to 1 works\n        well for most tables. For very large tables, training just one HNSW graph\n        will require too much memory. Each partition becomes its own HNSW graph, so\n        setting this value higher reduces the peak memory use of training.\n\n    max_iterations, default 50\n\n        Max iterations to train kmeans.\n\n        When training an IVF index we use kmeans to calculate the partitions.\n        This parameter controls how many iterations of kmeans to run.\n\n        Increasing this might improve the quality of the index but in most cases\n        the parameter is unused because kmeans will converge with fewer iterations.\n        The parameter is only used in cases where kmeans does not appear to converge.\n        In those cases it is unlikely that setting this larger will lead to\n        the index converging anyways.\n\n    sample_rate, default 256\n\n        The rate used to calculate the number of training vectors for kmeans.\n\n        When an IVF index is trained, we need to calculate partitions.  These\n        are groups of vectors that are similar to each other.  To do this\n        we use an algorithm called kmeans.\n\n        Running kmeans on a large dataset can be slow.  To speed this up we\n        run kmeans on a random sample of the data.  This parameter controls the\n        size of the sample.  The total number of vectors used to train the index\n        is `sample_rate * num_partitions`.\n\n        Increasing this value might improve the quality of the index but in\n        most cases the default should be sufficient.\n\n    m, default 20\n\n        The number of neighbors to select for each vector in the HNSW graph.\n\n        This value controls the tradeoff between search speed and accuracy.\n        The higher the value the more accurate the search but the slower it will be.\n\n    ef_construction, default 300\n\n        The number of candidates to evaluate during the construction of the HNSW graph.\n\n        This value controls the tradeoff between build speed and accuracy.\n        The higher the value the more accurate the build but the slower it will be.\n        150 to 300 is the typical range. 100 is a minimum for good quality search\n        results. In most cases, there is no benefit to setting this higher than 500.\n        This value should be set to a value that is not less than `ef` in the search\n        phase.\n\n    \"\"\"\n\n    distance_type: Literal[\"l2\", \"cosine\", \"dot\"] = \"l2\"\n    num_partitions: Optional[int] = None\n    max_iterations: int = 50\n    sample_rate: int = 256\n    m: int = 20\n    ef_construction: int = 300\n</code></pre>"},{"location":"python/python/#lancedb.index.IvfFlat","title":"lancedb.index.IvfFlat  <code>dataclass</code>","text":"<p>Describes an IVF Flat Index</p> <p>This index stores raw vectors. These vectors are grouped into partitions of similar vectors. Each partition keeps track of a centroid which is the average value of all vectors in the group.</p> <p>Attributes:</p> <ul> <li> <code>distance_type</code>               (<code>str, default \"l2\"</code>)           \u2013            <p>The distance metric used to train the index</p> <p>This is used when training the index to calculate the IVF partitions (vectors are grouped in partitions with similar vectors according to this distance type) and to calculate a subvector's code during quantization.</p> <p>The distance type used to train an index MUST match the distance type used to search the index.  Failure to do so will yield inaccurate results.</p> <p>The following distance types are available:</p> <p>\"l2\" - Euclidean distance. This is a very common distance metric that accounts for both magnitude and direction when determining the distance between vectors. l2 distance has a range of [0, \u221e).</p> <p>\"cosine\" - Cosine distance.  Cosine distance is a distance metric calculated from the cosine similarity between two vectors. Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space. It is defined to equal the cosine of the angle between them.  Unlike l2, the cosine distance is not affected by the magnitude of the vectors.  Cosine distance has a range of [0, 2].</p> <p>Note: the cosine distance is undefined when one (or both) of the vectors are all zeros (there is no direction).  These vectors are invalid and may never be returned from a vector search.</p> <p>\"dot\" - Dot product. Dot distance is the dot product of two vectors. Dot distance has a range of (-\u221e, \u221e). If the vectors are normalized (i.e. their l2 norm is 1), then dot distance is equivalent to the cosine distance.</p> <p>\"hamming\" - Hamming distance. Hamming distance is a distance metric calculated as the number of positions at which the corresponding bits are different. Hamming distance has a range of [0, vector dimension].</p> </li> <li> <code>num_partitions</code>               (<code>int, default sqrt(num_rows)</code>)           \u2013            <p>The number of IVF partitions to create.</p> <p>This value should generally scale with the number of rows in the dataset. By default the number of partitions is the square root of the number of rows.</p> <p>If this value is too large then the first part of the search (picking the right partition) will be slow.  If this value is too small then the second part of the search (searching within a partition) will be slow.</p> </li> <li> <code>max_iterations</code>               (<code>int, default 50</code>)           \u2013            <p>Max iteration to train kmeans.</p> <p>When training an IVF PQ index we use kmeans to calculate the partitions. This parameter controls how many iterations of kmeans to run.</p> <p>Increasing this might improve the quality of the index but in most cases these extra iterations have diminishing returns.</p> <p>The default value is 50.</p> </li> <li> <code>sample_rate</code>               (<code>int, default 256</code>)           \u2013            <p>The rate used to calculate the number of training vectors for kmeans.</p> <p>When an IVF PQ index is trained, we need to calculate partitions.  These are groups of vectors that are similar to each other.  To do this we use an algorithm called kmeans.</p> <p>Running kmeans on a large dataset can be slow.  To speed this up we run kmeans on a random sample of the data.  This parameter controls the size of the sample.  The total number of vectors used to train the index is <code>sample_rate * num_partitions</code>.</p> <p>Increasing this value might improve the quality of the index but in most cases the default should be sufficient.</p> <p>The default value is 256.</p> </li> </ul> Source code in <code>lancedb/index.py</code> <pre><code>@dataclass\nclass IvfFlat:\n    \"\"\"Describes an IVF Flat Index\n\n    This index stores raw vectors.\n    These vectors are grouped into partitions of similar vectors.\n    Each partition keeps track of a centroid which is\n    the average value of all vectors in the group.\n\n    Attributes\n    ----------\n    distance_type: str, default \"l2\"\n        The distance metric used to train the index\n\n        This is used when training the index to calculate the IVF partitions\n        (vectors are grouped in partitions with similar vectors according to this\n        distance type) and to calculate a subvector's code during quantization.\n\n        The distance type used to train an index MUST match the distance type used\n        to search the index.  Failure to do so will yield inaccurate results.\n\n        The following distance types are available:\n\n        \"l2\" - Euclidean distance. This is a very common distance metric that\n        accounts for both magnitude and direction when determining the distance\n        between vectors. l2 distance has a range of [0, \u221e).\n\n        \"cosine\" - Cosine distance.  Cosine distance is a distance metric\n        calculated from the cosine similarity between two vectors. Cosine\n        similarity is a measure of similarity between two non-zero vectors of an\n        inner product space. It is defined to equal the cosine of the angle\n        between them.  Unlike l2, the cosine distance is not affected by the\n        magnitude of the vectors.  Cosine distance has a range of [0, 2].\n\n        Note: the cosine distance is undefined when one (or both) of the vectors\n        are all zeros (there is no direction).  These vectors are invalid and may\n        never be returned from a vector search.\n\n        \"dot\" - Dot product. Dot distance is the dot product of two vectors. Dot\n        distance has a range of (-\u221e, \u221e). If the vectors are normalized (i.e. their\n        l2 norm is 1), then dot distance is equivalent to the cosine distance.\n\n        \"hamming\" - Hamming distance. Hamming distance is a distance metric\n        calculated as the number of positions at which the corresponding bits are\n        different. Hamming distance has a range of [0, vector dimension].\n\n    num_partitions: int, default sqrt(num_rows)\n        The number of IVF partitions to create.\n\n        This value should generally scale with the number of rows in the dataset.\n        By default the number of partitions is the square root of the number of\n        rows.\n\n        If this value is too large then the first part of the search (picking the\n        right partition) will be slow.  If this value is too small then the second\n        part of the search (searching within a partition) will be slow.\n\n    max_iterations: int, default 50\n        Max iteration to train kmeans.\n\n        When training an IVF PQ index we use kmeans to calculate the partitions.\n        This parameter controls how many iterations of kmeans to run.\n\n        Increasing this might improve the quality of the index but in most cases\n        these extra iterations have diminishing returns.\n\n        The default value is 50.\n    sample_rate: int, default 256\n        The rate used to calculate the number of training vectors for kmeans.\n\n        When an IVF PQ index is trained, we need to calculate partitions.  These\n        are groups of vectors that are similar to each other.  To do this we use an\n        algorithm called kmeans.\n\n        Running kmeans on a large dataset can be slow.  To speed this up we run\n        kmeans on a random sample of the data.  This parameter controls the size of\n        the sample.  The total number of vectors used to train the index is\n        `sample_rate * num_partitions`.\n\n        Increasing this value might improve the quality of the index but in most\n        cases the default should be sufficient.\n\n        The default value is 256.\n    \"\"\"\n\n    distance_type: Literal[\"l2\", \"cosine\", \"dot\", \"hamming\"] = \"l2\"\n    num_partitions: Optional[int] = None\n    max_iterations: int = 50\n    sample_rate: int = 256\n</code></pre>"},{"location":"python/python/#querying-asynchronous","title":"Querying (Asynchronous)","text":"<p>Queries allow you to return data from your database. Basic queries can be created with the AsyncTable.query method to return the entire (typically filtered) table. Vector searches return the rows nearest to a query vector and can be created with the AsyncTable.vector_search method.</p>"},{"location":"python/python/#lancedb.query.AsyncQuery","title":"lancedb.query.AsyncQuery","text":"<p>               Bases: <code>AsyncQueryBase</code></p> Source code in <code>lancedb/query.py</code> <pre><code>class AsyncQuery(AsyncQueryBase):\n    def __init__(self, inner: LanceQuery):\n        \"\"\"\n        Construct an AsyncQuery\n\n        This method is not intended to be called directly.  Instead, use the\n        [AsyncTable.query][lancedb.table.AsyncTable.query] method to create a query.\n        \"\"\"\n        super().__init__(inner)\n        self._inner = inner\n\n    @classmethod\n    def _query_vec_to_array(self, vec: Union[VEC, Tuple]):\n        if isinstance(vec, list):\n            return pa.array(vec)\n        if isinstance(vec, np.ndarray):\n            return pa.array(vec)\n        if isinstance(vec, pa.Array):\n            return vec\n        if isinstance(vec, pa.ChunkedArray):\n            return vec.combine_chunks()\n        if isinstance(vec, tuple):\n            return pa.array(vec)\n        # We've checked everything we formally support in our typings\n        # but, as a fallback, let pyarrow try and convert it anyway.\n        # This can allow for some more exotic things like iterables\n        return pa.array(vec)\n\n    def nearest_to(\n        self,\n        query_vector: Union[VEC, Tuple, List[VEC]],\n    ) -&gt; AsyncVectorQuery:\n        \"\"\"\n        Find the nearest vectors to the given query vector.\n\n        This converts the query from a plain query to a vector query.\n\n        This method will attempt to convert the input to the query vector\n        expected by the embedding model.  If the input cannot be converted\n        then an error will be thrown.\n\n        By default, there is no embedding model, and the input should be\n        something that can be converted to a pyarrow array of floats.  This\n        includes lists, numpy arrays, and tuples.\n\n        If there is only one vector column (a column whose data type is a\n        fixed size list of floats) then the column does not need to be specified.\n        If there is more than one vector column you must use\n        [AsyncVectorQuery.column][lancedb.query.AsyncVectorQuery.column] to specify\n        which column you would like to compare with.\n\n        If no index has been created on the vector column then a vector query\n        will perform a distance comparison between the query vector and every\n        vector in the database and then sort the results.  This is sometimes\n        called a \"flat search\"\n\n        For small databases, with tens of thousands of vectors or less, this can\n        be reasonably fast.  In larger databases you should create a vector index\n        on the column.  If there is a vector index then an \"approximate\" nearest\n        neighbor search (frequently called an ANN search) will be performed.  This\n        search is much faster, but the results will be approximate.\n\n        The query can be further parameterized using the returned builder.  There\n        are various ANN search parameters that will let you fine tune your recall\n        accuracy vs search latency.\n\n        Vector searches always have a [limit][].  If `limit` has not been called then\n        a default `limit` of 10 will be used.\n\n        Typically, a single vector is passed in as the query. However, you can also\n        pass in multiple vectors. When multiple vectors are passed in, if the vector\n        column is with multivector type, then the vectors will be treated as a single\n        query. Or the vectors will be treated as multiple queries, this can be useful\n        if you want to find the nearest vectors to multiple query vectors.\n        This is not expected to be faster than making multiple queries concurrently;\n        it is just a convenience method. If multiple vectors are passed in then\n        an additional column `query_index` will be added to the results. This column\n        will contain the index of the query vector that the result is nearest to.\n        \"\"\"\n        if query_vector is None:\n            raise ValueError(\"query_vector can not be None\")\n\n        if (\n            isinstance(query_vector, (list, np.ndarray, pa.Array))\n            and len(query_vector) &gt; 0\n            and isinstance(query_vector[0], (list, np.ndarray, pa.Array))\n        ):\n            # multiple have been passed\n            query_vectors = [AsyncQuery._query_vec_to_array(v) for v in query_vector]\n            new_self = self._inner.nearest_to(query_vectors[0])\n            for v in query_vectors[1:]:\n                new_self.add_query_vector(v)\n            return AsyncVectorQuery(new_self)\n        else:\n            return AsyncVectorQuery(\n                self._inner.nearest_to(AsyncQuery._query_vec_to_array(query_vector))\n            )\n\n    def nearest_to_text(\n        self, query: str | FullTextQuery, columns: Union[str, List[str], None] = None\n    ) -&gt; AsyncFTSQuery:\n        \"\"\"\n        Find the documents that are most relevant to the given text query.\n\n        This method will perform a full text search on the table and return\n        the most relevant documents.  The relevance is determined by BM25.\n\n        The columns to search must be with native FTS index\n        (Tantivy-based can't work with this method).\n\n        By default, all indexed columns are searched,\n        now only one column can be searched at a time.\n\n        Parameters\n        ----------\n        query: str\n            The text query to search for.\n        columns: str or list of str, default None\n            The columns to search in. If None, all indexed columns are searched.\n            For now only one column can be searched at a time.\n        \"\"\"\n        if isinstance(columns, str):\n            columns = [columns]\n        if columns is None:\n            columns = []\n\n        if isinstance(query, str):\n            return AsyncFTSQuery(\n                self._inner.nearest_to_text({\"query\": query, \"columns\": columns})\n            )\n        # FullTextQuery object\n        return AsyncFTSQuery(self._inner.nearest_to_text({\"query\": query.to_dict()}))\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncQuery.where","title":"where","text":"<pre><code>where(predicate: str) -&gt; Self\n</code></pre> <p>Only return rows matching the given predicate</p> <p>The predicate should be supplied as an SQL query string.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; predicate = \"x &gt; 10\"\n&gt;&gt;&gt; predicate = \"y &gt; 0 AND y &lt; 100\"\n&gt;&gt;&gt; predicate = \"x &gt; 5 OR y = 'test'\"\n</code></pre> <p>Filtering performance can often be improved by creating a scalar index on the filter column(s).</p> Source code in <code>lancedb/query.py</code> <pre><code>def where(self, predicate: str) -&gt; Self:\n    \"\"\"\n    Only return rows matching the given predicate\n\n    The predicate should be supplied as an SQL query string.\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; predicate = \"x &gt; 10\"\n    &gt;&gt;&gt; predicate = \"y &gt; 0 AND y &lt; 100\"\n    &gt;&gt;&gt; predicate = \"x &gt; 5 OR y = 'test'\"\n\n    Filtering performance can often be improved by creating a scalar index\n    on the filter column(s).\n    \"\"\"\n    self._inner.where(predicate)\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncQuery.select","title":"select","text":"<pre><code>select(columns: Union[List[str], dict[str, str]]) -&gt; Self\n</code></pre> <p>Return only the specified columns.</p> <p>By default a query will return all columns from the table.  However, this can have a very significant impact on latency.  LanceDb stores data in a columnar fashion.  This means we can finely tune our I/O to select exactly the columns we need.</p> <p>As a best practice you should always limit queries to the columns that you need. If you pass in a list of column names then only those columns will be returned.</p> <p>You can also use this method to create new \"dynamic\" columns based on your existing columns. For example, you may not care about \"a\" or \"b\" but instead simply want \"a + b\".  This is often seen in the SELECT clause of an SQL query (e.g. <code>SELECT a+b FROM my_table</code>).</p> <p>To create dynamic columns you can pass in a dict[str, str].  A column will be returned for each entry in the map.  The key provides the name of the column. The value is an SQL string used to specify how the column is calculated.</p> <p>For example, an SQL query might state <code>SELECT a + b AS combined, c</code>.  The equivalent input to this method would be <code>{\"combined\": \"a + b\", \"c\": \"c\"}</code>.</p> <p>Columns will always be returned in the order given, even if that order is different than the order used when adding the data.</p> Source code in <code>lancedb/query.py</code> <pre><code>def select(self, columns: Union[List[str], dict[str, str]]) -&gt; Self:\n    \"\"\"\n    Return only the specified columns.\n\n    By default a query will return all columns from the table.  However, this can\n    have a very significant impact on latency.  LanceDb stores data in a columnar\n    fashion.  This\n    means we can finely tune our I/O to select exactly the columns we need.\n\n    As a best practice you should always limit queries to the columns that you need.\n    If you pass in a list of column names then only those columns will be\n    returned.\n\n    You can also use this method to create new \"dynamic\" columns based on your\n    existing columns. For example, you may not care about \"a\" or \"b\" but instead\n    simply want \"a + b\".  This is often seen in the SELECT clause of an SQL query\n    (e.g. `SELECT a+b FROM my_table`).\n\n    To create dynamic columns you can pass in a dict[str, str].  A column will be\n    returned for each entry in the map.  The key provides the name of the column.\n    The value is an SQL string used to specify how the column is calculated.\n\n    For example, an SQL query might state `SELECT a + b AS combined, c`.  The\n    equivalent input to this method would be `{\"combined\": \"a + b\", \"c\": \"c\"}`.\n\n    Columns will always be returned in the order given, even if that order is\n    different than the order used when adding the data.\n    \"\"\"\n    if isinstance(columns, list) and all(isinstance(c, str) for c in columns):\n        self._inner.select_columns(columns)\n    elif isinstance(columns, dict) and all(\n        isinstance(k, str) and isinstance(v, str) for k, v in columns.items()\n    ):\n        self._inner.select(list(columns.items()))\n    else:\n        raise TypeError(\"columns must be a list of column names or a dict\")\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncQuery.limit","title":"limit","text":"<pre><code>limit(limit: int) -&gt; Self\n</code></pre> <p>Set the maximum number of results to return.</p> <p>By default, a plain search has no limit.  If this method is not called then every valid row from the table will be returned.</p> Source code in <code>lancedb/query.py</code> <pre><code>def limit(self, limit: int) -&gt; Self:\n    \"\"\"\n    Set the maximum number of results to return.\n\n    By default, a plain search has no limit.  If this method is not\n    called then every valid row from the table will be returned.\n    \"\"\"\n    self._inner.limit(limit)\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncQuery.offset","title":"offset","text":"<pre><code>offset(offset: int) -&gt; Self\n</code></pre> <p>Set the offset for the results.</p> <p>Parameters:</p> <ul> <li> <code>offset</code>               (<code>int</code>)           \u2013            <p>The offset to start fetching results from.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def offset(self, offset: int) -&gt; Self:\n    \"\"\"\n    Set the offset for the results.\n\n    Parameters\n    ----------\n    offset: int\n        The offset to start fetching results from.\n    \"\"\"\n    self._inner.offset(offset)\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncQuery.fast_search","title":"fast_search","text":"<pre><code>fast_search() -&gt; Self\n</code></pre> <p>Skip searching un-indexed data.</p> <p>This can make queries faster, but will miss any data that has not been indexed.</p> <p>Tip</p> <p>You can add new data into an existing index by calling AsyncTable.optimize.</p> Source code in <code>lancedb/query.py</code> <pre><code>def fast_search(self) -&gt; Self:\n    \"\"\"\n    Skip searching un-indexed data.\n\n    This can make queries faster, but will miss any data that has not been\n    indexed.\n\n    !!! tip\n        You can add new data into an existing index by calling\n        [AsyncTable.optimize][lancedb.table.AsyncTable.optimize].\n    \"\"\"\n    self._inner.fast_search()\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncQuery.with_row_id","title":"with_row_id","text":"<pre><code>with_row_id() -&gt; Self\n</code></pre> <p>Include the _rowid column in the results.</p> Source code in <code>lancedb/query.py</code> <pre><code>def with_row_id(self) -&gt; Self:\n    \"\"\"\n    Include the _rowid column in the results.\n    \"\"\"\n    self._inner.with_row_id()\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncQuery.postfilter","title":"postfilter","text":"<pre><code>postfilter() -&gt; Self\n</code></pre> <p>If this is called then filtering will happen after the search instead of before. By default filtering will be performed before the search.  This is how filtering is typically understood to work.  This prefilter step does add some additional latency.  Creating a scalar index on the filter column(s) can often improve this latency.  However, sometimes a filter is too complex or scalar indices cannot be applied to the column.  In these cases postfiltering can be used instead of prefiltering to improve latency. Post filtering applies the filter to the results of the search.  This means we only run the filter on a much smaller set of data.  However, it can cause the query to return fewer than <code>limit</code> results (or even no results) if none of the nearest results match the filter. Post filtering happens during the \"refine stage\" (described in more detail in @see {@link VectorQuery#refineFactor}).  This means that setting a higher refine factor can often help restore some of the results lost by post filtering.</p> Source code in <code>lancedb/query.py</code> <pre><code>def postfilter(self) -&gt; Self:\n    \"\"\"\n    If this is called then filtering will happen after the search instead of\n    before.\n    By default filtering will be performed before the search.  This is how\n    filtering is typically understood to work.  This prefilter step does add some\n    additional latency.  Creating a scalar index on the filter column(s) can\n    often improve this latency.  However, sometimes a filter is too complex or\n    scalar indices cannot be applied to the column.  In these cases postfiltering\n    can be used instead of prefiltering to improve latency.\n    Post filtering applies the filter to the results of the search.  This\n    means we only run the filter on a much smaller set of data.  However, it can\n    cause the query to return fewer than `limit` results (or even no results) if\n    none of the nearest results match the filter.\n    Post filtering happens during the \"refine stage\" (described in more detail in\n    @see {@link VectorQuery#refineFactor}).  This means that setting a higher refine\n    factor can often help restore some of the results lost by post filtering.\n    \"\"\"\n    self._inner.postfilter()\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncQuery.to_batches","title":"to_batches  <code>async</code>","text":"<pre><code>to_batches(*, max_batch_length: Optional[int] = None, timeout: Optional[timedelta] = None) -&gt; AsyncRecordBatchReader\n</code></pre> <p>Execute the query and return the results as an Apache Arrow RecordBatchReader.</p> <p>Parameters:</p> <ul> <li> <code>max_batch_length</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum number of selected records in a single RecordBatch object. If not specified, a default batch length is used. It is possible for batches to be smaller than the provided length if the underlying data is stored in smaller chunks.</p> </li> <li> <code>timeout</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum time to wait for the query to complete. If not specified, no timeout is applied. If the query does not complete within the specified time, an error will be raised.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>async def to_batches(\n    self,\n    *,\n    max_batch_length: Optional[int] = None,\n    timeout: Optional[timedelta] = None,\n) -&gt; AsyncRecordBatchReader:\n    \"\"\"\n    Execute the query and return the results as an Apache Arrow RecordBatchReader.\n\n    Parameters\n    ----------\n\n    max_batch_length: Optional[int]\n        The maximum number of selected records in a single RecordBatch object.\n        If not specified, a default batch length is used.\n        It is possible for batches to be smaller than the provided length if the\n        underlying data is stored in smaller chunks.\n    timeout: Optional[timedelta]\n        The maximum time to wait for the query to complete.\n        If not specified, no timeout is applied. If the query does not\n        complete within the specified time, an error will be raised.\n    \"\"\"\n    return AsyncRecordBatchReader(\n        await self._inner.execute(max_batch_length, timeout)\n    )\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncQuery.to_arrow","title":"to_arrow  <code>async</code>","text":"<pre><code>to_arrow(timeout: Optional[timedelta] = None) -&gt; Table\n</code></pre> <p>Execute the query and collect the results into an Apache Arrow Table.</p> <p>This method will collect all results into memory before returning.  If you expect a large number of results, you may want to use to_batches</p> <p>Parameters:</p> <ul> <li> <code>timeout</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum time to wait for the query to complete. If not specified, no timeout is applied. If the query does not complete within the specified time, an error will be raised.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>async def to_arrow(self, timeout: Optional[timedelta] = None) -&gt; pa.Table:\n    \"\"\"\n    Execute the query and collect the results into an Apache Arrow Table.\n\n    This method will collect all results into memory before returning.  If\n    you expect a large number of results, you may want to use\n    [to_batches][lancedb.query.AsyncQueryBase.to_batches]\n\n    Parameters\n    ----------\n    timeout: Optional[timedelta]\n        The maximum time to wait for the query to complete.\n        If not specified, no timeout is applied. If the query does not\n        complete within the specified time, an error will be raised.\n    \"\"\"\n    batch_iter = await self.to_batches(timeout=timeout)\n    return pa.Table.from_batches(\n        await batch_iter.read_all(), schema=batch_iter.schema\n    )\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncQuery.to_list","title":"to_list  <code>async</code>","text":"<pre><code>to_list(timeout: Optional[timedelta] = None) -&gt; List[dict]\n</code></pre> <p>Execute the query and return the results as a list of dictionaries.</p> <p>Each list entry is a dictionary with the selected column names as keys, or all table columns if <code>select</code> is not called. The vector and the \"_distance\" fields are returned whether or not they're explicitly selected.</p> <p>Parameters:</p> <ul> <li> <code>timeout</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum time to wait for the query to complete. If not specified, no timeout is applied. If the query does not complete within the specified time, an error will be raised.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>async def to_list(self, timeout: Optional[timedelta] = None) -&gt; List[dict]:\n    \"\"\"\n    Execute the query and return the results as a list of dictionaries.\n\n    Each list entry is a dictionary with the selected column names as keys,\n    or all table columns if `select` is not called. The vector and the \"_distance\"\n    fields are returned whether or not they're explicitly selected.\n\n    Parameters\n    ----------\n    timeout: Optional[timedelta]\n        The maximum time to wait for the query to complete.\n        If not specified, no timeout is applied. If the query does not\n        complete within the specified time, an error will be raised.\n    \"\"\"\n    return (await self.to_arrow(timeout=timeout)).to_pylist()\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncQuery.to_pandas","title":"to_pandas  <code>async</code>","text":"<pre><code>to_pandas(flatten: Optional[Union[int, bool]] = None, timeout: Optional[timedelta] = None) -&gt; 'pd.DataFrame'\n</code></pre> <p>Execute the query and collect the results into a pandas DataFrame.</p> <p>This method will collect all results into memory before returning.  If you expect a large number of results, you may want to use to_batches and convert each batch to pandas separately.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import asyncio\n&gt;&gt;&gt; from lancedb import connect_async\n&gt;&gt;&gt; async def doctest_example():\n...     conn = await connect_async(\"./.lancedb\")\n...     table = await conn.create_table(\"my_table\", data=[{\"a\": 1, \"b\": 2}])\n...     async for batch in await table.query().to_batches():\n...         batch_df = batch.to_pandas()\n&gt;&gt;&gt; asyncio.run(doctest_example())\n</code></pre> <p>Parameters:</p> <ul> <li> <code>flatten</code>               (<code>Optional[Union[int, bool]]</code>, default:                   <code>None</code> )           \u2013            <p>If flatten is True, flatten all nested columns. If flatten is an integer, flatten the nested columns up to the specified depth. If unspecified, do not flatten the nested columns.</p> </li> <li> <code>timeout</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum time to wait for the query to complete. If not specified, no timeout is applied. If the query does not complete within the specified time, an error will be raised.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>async def to_pandas(\n    self,\n    flatten: Optional[Union[int, bool]] = None,\n    timeout: Optional[timedelta] = None,\n) -&gt; \"pd.DataFrame\":\n    \"\"\"\n    Execute the query and collect the results into a pandas DataFrame.\n\n    This method will collect all results into memory before returning.  If you\n    expect a large number of results, you may want to use\n    [to_batches][lancedb.query.AsyncQueryBase.to_batches] and convert each batch to\n    pandas separately.\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; import asyncio\n    &gt;&gt;&gt; from lancedb import connect_async\n    &gt;&gt;&gt; async def doctest_example():\n    ...     conn = await connect_async(\"./.lancedb\")\n    ...     table = await conn.create_table(\"my_table\", data=[{\"a\": 1, \"b\": 2}])\n    ...     async for batch in await table.query().to_batches():\n    ...         batch_df = batch.to_pandas()\n    &gt;&gt;&gt; asyncio.run(doctest_example())\n\n    Parameters\n    ----------\n    flatten: Optional[Union[int, bool]]\n        If flatten is True, flatten all nested columns.\n        If flatten is an integer, flatten the nested columns up to the\n        specified depth.\n        If unspecified, do not flatten the nested columns.\n    timeout: Optional[timedelta]\n        The maximum time to wait for the query to complete.\n        If not specified, no timeout is applied. If the query does not\n        complete within the specified time, an error will be raised.\n    \"\"\"\n    return (\n        flatten_columns(await self.to_arrow(timeout=timeout), flatten)\n    ).to_pandas()\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncQuery.to_polars","title":"to_polars  <code>async</code>","text":"<pre><code>to_polars(timeout: Optional[timedelta] = None) -&gt; 'pl.DataFrame'\n</code></pre> <p>Execute the query and collect the results into a Polars DataFrame.</p> <p>This method will collect all results into memory before returning.  If you expect a large number of results, you may want to use to_batches and convert each batch to polars separately.</p> <p>Parameters:</p> <ul> <li> <code>timeout</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum time to wait for the query to complete. If not specified, no timeout is applied. If the query does not complete within the specified time, an error will be raised.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import asyncio\n&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from lancedb import connect_async\n&gt;&gt;&gt; async def doctest_example():\n...     conn = await connect_async(\"./.lancedb\")\n...     table = await conn.create_table(\"my_table\", data=[{\"a\": 1, \"b\": 2}])\n...     async for batch in await table.query().to_batches():\n...         batch_df = pl.from_arrow(batch)\n&gt;&gt;&gt; asyncio.run(doctest_example())\n</code></pre> Source code in <code>lancedb/query.py</code> <pre><code>async def to_polars(\n    self,\n    timeout: Optional[timedelta] = None,\n) -&gt; \"pl.DataFrame\":\n    \"\"\"\n    Execute the query and collect the results into a Polars DataFrame.\n\n    This method will collect all results into memory before returning.  If you\n    expect a large number of results, you may want to use\n    [to_batches][lancedb.query.AsyncQueryBase.to_batches] and convert each batch to\n    polars separately.\n\n    Parameters\n    ----------\n    timeout: Optional[timedelta]\n        The maximum time to wait for the query to complete.\n        If not specified, no timeout is applied. If the query does not\n        complete within the specified time, an error will be raised.\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; import asyncio\n    &gt;&gt;&gt; import polars as pl\n    &gt;&gt;&gt; from lancedb import connect_async\n    &gt;&gt;&gt; async def doctest_example():\n    ...     conn = await connect_async(\"./.lancedb\")\n    ...     table = await conn.create_table(\"my_table\", data=[{\"a\": 1, \"b\": 2}])\n    ...     async for batch in await table.query().to_batches():\n    ...         batch_df = pl.from_arrow(batch)\n    &gt;&gt;&gt; asyncio.run(doctest_example())\n    \"\"\"\n    import polars as pl\n\n    return pl.from_arrow(await self.to_arrow(timeout=timeout))\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncQuery.explain_plan","title":"explain_plan  <code>async</code>","text":"<pre><code>explain_plan(verbose: Optional[bool] = False)\n</code></pre> <p>Return the execution plan for this query.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import asyncio\n&gt;&gt;&gt; from lancedb import connect_async\n&gt;&gt;&gt; async def doctest_example():\n...     conn = await connect_async(\"./.lancedb\")\n...     table = await conn.create_table(\"my_table\", [{\"vector\": [99, 99]}])\n...     query = [100, 100]\n...     plan = await table.query().nearest_to([1, 2]).explain_plan(True)\n...     print(plan)\n&gt;&gt;&gt; asyncio.run(doctest_example())\nProjectionExec: expr=[vector@0 as vector, _distance@2 as _distance]\n  GlobalLimitExec: skip=0, fetch=10\n    FilterExec: _distance@2 IS NOT NULL\n      SortExec: TopK(fetch=10), expr=[_distance@2 ASC NULLS LAST], preserve_partitioning=[false]\n        KNNVectorDistance: metric=l2\n          LanceScan: uri=..., projection=[vector], row_id=true, row_addr=false, ordered=false\n</code></pre> <p>Parameters:</p> <ul> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Use a verbose output format.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>plan</code> (              <code>str</code> )          \u2013            </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>async def explain_plan(self, verbose: Optional[bool] = False):\n    \"\"\"Return the execution plan for this query.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import asyncio\n    &gt;&gt;&gt; from lancedb import connect_async\n    &gt;&gt;&gt; async def doctest_example():\n    ...     conn = await connect_async(\"./.lancedb\")\n    ...     table = await conn.create_table(\"my_table\", [{\"vector\": [99, 99]}])\n    ...     query = [100, 100]\n    ...     plan = await table.query().nearest_to([1, 2]).explain_plan(True)\n    ...     print(plan)\n    &gt;&gt;&gt; asyncio.run(doctest_example()) # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n    ProjectionExec: expr=[vector@0 as vector, _distance@2 as _distance]\n      GlobalLimitExec: skip=0, fetch=10\n        FilterExec: _distance@2 IS NOT NULL\n          SortExec: TopK(fetch=10), expr=[_distance@2 ASC NULLS LAST], preserve_partitioning=[false]\n            KNNVectorDistance: metric=l2\n              LanceScan: uri=..., projection=[vector], row_id=true, row_addr=false, ordered=false\n\n    Parameters\n    ----------\n    verbose : bool, default False\n        Use a verbose output format.\n\n    Returns\n    -------\n    plan : str\n    \"\"\"  # noqa: E501\n    return await self._inner.explain_plan(verbose)\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncQuery.analyze_plan","title":"analyze_plan  <code>async</code>","text":"<pre><code>analyze_plan()\n</code></pre> <p>Execute the query and display with runtime metrics.</p> <p>Returns:</p> <ul> <li> <code>plan</code> (              <code>str</code> )          \u2013            </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>async def analyze_plan(self):\n    \"\"\"Execute the query and display with runtime metrics.\n\n    Returns\n    -------\n    plan : str\n    \"\"\"\n    return await self._inner.analyze_plan()\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncQuery.__init__","title":"__init__","text":"<pre><code>__init__(inner: Query)\n</code></pre> <p>Construct an AsyncQuery</p> <p>This method is not intended to be called directly.  Instead, use the AsyncTable.query method to create a query.</p> Source code in <code>lancedb/query.py</code> <pre><code>def __init__(self, inner: LanceQuery):\n    \"\"\"\n    Construct an AsyncQuery\n\n    This method is not intended to be called directly.  Instead, use the\n    [AsyncTable.query][lancedb.table.AsyncTable.query] method to create a query.\n    \"\"\"\n    super().__init__(inner)\n    self._inner = inner\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncQuery.nearest_to","title":"nearest_to","text":"<pre><code>nearest_to(query_vector: Union[VEC, Tuple, List[VEC]]) -&gt; AsyncVectorQuery\n</code></pre> <p>Find the nearest vectors to the given query vector.</p> <p>This converts the query from a plain query to a vector query.</p> <p>This method will attempt to convert the input to the query vector expected by the embedding model.  If the input cannot be converted then an error will be thrown.</p> <p>By default, there is no embedding model, and the input should be something that can be converted to a pyarrow array of floats.  This includes lists, numpy arrays, and tuples.</p> <p>If there is only one vector column (a column whose data type is a fixed size list of floats) then the column does not need to be specified. If there is more than one vector column you must use AsyncVectorQuery.column to specify which column you would like to compare with.</p> <p>If no index has been created on the vector column then a vector query will perform a distance comparison between the query vector and every vector in the database and then sort the results.  This is sometimes called a \"flat search\"</p> <p>For small databases, with tens of thousands of vectors or less, this can be reasonably fast.  In larger databases you should create a vector index on the column.  If there is a vector index then an \"approximate\" nearest neighbor search (frequently called an ANN search) will be performed.  This search is much faster, but the results will be approximate.</p> <p>The query can be further parameterized using the returned builder.  There are various ANN search parameters that will let you fine tune your recall accuracy vs search latency.</p> <p>Vector searches always have a limit.  If <code>limit</code> has not been called then a default <code>limit</code> of 10 will be used.</p> <p>Typically, a single vector is passed in as the query. However, you can also pass in multiple vectors. When multiple vectors are passed in, if the vector column is with multivector type, then the vectors will be treated as a single query. Or the vectors will be treated as multiple queries, this can be useful if you want to find the nearest vectors to multiple query vectors. This is not expected to be faster than making multiple queries concurrently; it is just a convenience method. If multiple vectors are passed in then an additional column <code>query_index</code> will be added to the results. This column will contain the index of the query vector that the result is nearest to.</p> Source code in <code>lancedb/query.py</code> <pre><code>def nearest_to(\n    self,\n    query_vector: Union[VEC, Tuple, List[VEC]],\n) -&gt; AsyncVectorQuery:\n    \"\"\"\n    Find the nearest vectors to the given query vector.\n\n    This converts the query from a plain query to a vector query.\n\n    This method will attempt to convert the input to the query vector\n    expected by the embedding model.  If the input cannot be converted\n    then an error will be thrown.\n\n    By default, there is no embedding model, and the input should be\n    something that can be converted to a pyarrow array of floats.  This\n    includes lists, numpy arrays, and tuples.\n\n    If there is only one vector column (a column whose data type is a\n    fixed size list of floats) then the column does not need to be specified.\n    If there is more than one vector column you must use\n    [AsyncVectorQuery.column][lancedb.query.AsyncVectorQuery.column] to specify\n    which column you would like to compare with.\n\n    If no index has been created on the vector column then a vector query\n    will perform a distance comparison between the query vector and every\n    vector in the database and then sort the results.  This is sometimes\n    called a \"flat search\"\n\n    For small databases, with tens of thousands of vectors or less, this can\n    be reasonably fast.  In larger databases you should create a vector index\n    on the column.  If there is a vector index then an \"approximate\" nearest\n    neighbor search (frequently called an ANN search) will be performed.  This\n    search is much faster, but the results will be approximate.\n\n    The query can be further parameterized using the returned builder.  There\n    are various ANN search parameters that will let you fine tune your recall\n    accuracy vs search latency.\n\n    Vector searches always have a [limit][].  If `limit` has not been called then\n    a default `limit` of 10 will be used.\n\n    Typically, a single vector is passed in as the query. However, you can also\n    pass in multiple vectors. When multiple vectors are passed in, if the vector\n    column is with multivector type, then the vectors will be treated as a single\n    query. Or the vectors will be treated as multiple queries, this can be useful\n    if you want to find the nearest vectors to multiple query vectors.\n    This is not expected to be faster than making multiple queries concurrently;\n    it is just a convenience method. If multiple vectors are passed in then\n    an additional column `query_index` will be added to the results. This column\n    will contain the index of the query vector that the result is nearest to.\n    \"\"\"\n    if query_vector is None:\n        raise ValueError(\"query_vector can not be None\")\n\n    if (\n        isinstance(query_vector, (list, np.ndarray, pa.Array))\n        and len(query_vector) &gt; 0\n        and isinstance(query_vector[0], (list, np.ndarray, pa.Array))\n    ):\n        # multiple have been passed\n        query_vectors = [AsyncQuery._query_vec_to_array(v) for v in query_vector]\n        new_self = self._inner.nearest_to(query_vectors[0])\n        for v in query_vectors[1:]:\n            new_self.add_query_vector(v)\n        return AsyncVectorQuery(new_self)\n    else:\n        return AsyncVectorQuery(\n            self._inner.nearest_to(AsyncQuery._query_vec_to_array(query_vector))\n        )\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncQuery.nearest_to_text","title":"nearest_to_text","text":"<pre><code>nearest_to_text(query: str | FullTextQuery, columns: Union[str, List[str], None] = None) -&gt; AsyncFTSQuery\n</code></pre> <p>Find the documents that are most relevant to the given text query.</p> <p>This method will perform a full text search on the table and return the most relevant documents.  The relevance is determined by BM25.</p> <p>The columns to search must be with native FTS index (Tantivy-based can't work with this method).</p> <p>By default, all indexed columns are searched, now only one column can be searched at a time.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str | FullTextQuery</code>)           \u2013            <p>The text query to search for.</p> </li> <li> <code>columns</code>               (<code>Union[str, List[str], None]</code>, default:                   <code>None</code> )           \u2013            <p>The columns to search in. If None, all indexed columns are searched. For now only one column can be searched at a time.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def nearest_to_text(\n    self, query: str | FullTextQuery, columns: Union[str, List[str], None] = None\n) -&gt; AsyncFTSQuery:\n    \"\"\"\n    Find the documents that are most relevant to the given text query.\n\n    This method will perform a full text search on the table and return\n    the most relevant documents.  The relevance is determined by BM25.\n\n    The columns to search must be with native FTS index\n    (Tantivy-based can't work with this method).\n\n    By default, all indexed columns are searched,\n    now only one column can be searched at a time.\n\n    Parameters\n    ----------\n    query: str\n        The text query to search for.\n    columns: str or list of str, default None\n        The columns to search in. If None, all indexed columns are searched.\n        For now only one column can be searched at a time.\n    \"\"\"\n    if isinstance(columns, str):\n        columns = [columns]\n    if columns is None:\n        columns = []\n\n    if isinstance(query, str):\n        return AsyncFTSQuery(\n            self._inner.nearest_to_text({\"query\": query, \"columns\": columns})\n        )\n    # FullTextQuery object\n    return AsyncFTSQuery(self._inner.nearest_to_text({\"query\": query.to_dict()}))\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncVectorQuery","title":"lancedb.query.AsyncVectorQuery","text":"<p>               Bases: <code>AsyncQueryBase</code>, <code>AsyncVectorQueryBase</code></p> Source code in <code>lancedb/query.py</code> <pre><code>class AsyncVectorQuery(AsyncQueryBase, AsyncVectorQueryBase):\n    def __init__(self, inner: LanceVectorQuery):\n        \"\"\"\n        Construct an AsyncVectorQuery\n\n        This method is not intended to be called directly.  Instead, create\n        a query first with [AsyncTable.query][lancedb.table.AsyncTable.query] and then\n        use [AsyncQuery.nearest_to][lancedb.query.AsyncQuery.nearest_to]] to convert to\n        a vector query.  Or you can use\n        [AsyncTable.vector_search][lancedb.table.AsyncTable.vector_search]\n        \"\"\"\n        super().__init__(inner)\n        self._inner = inner\n        self._reranker = None\n        self._query_string = None\n\n    def rerank(\n        self, reranker: Reranker = RRFReranker(), query_string: Optional[str] = None\n    ) -&gt; AsyncHybridQuery:\n        if reranker and not isinstance(reranker, Reranker):\n            raise ValueError(\"reranker must be an instance of Reranker class.\")\n\n        self._reranker = reranker\n\n        if not self._query_string and not query_string:\n            raise ValueError(\"query_string must be provided to rerank the results.\")\n\n        self._query_string = query_string\n\n        return self\n\n    def nearest_to_text(\n        self, query: str | FullTextQuery, columns: Union[str, List[str], None] = None\n    ) -&gt; AsyncHybridQuery:\n        \"\"\"\n        Find the documents that are most relevant to the given text query,\n        in addition to vector search.\n\n        This converts the vector query into a hybrid query.\n\n        This search will perform a full text search on the table and return\n        the most relevant documents, combined with the vector query results.\n        The text relevance is determined by BM25.\n\n        The columns to search must be with native FTS index\n        (Tantivy-based can't work with this method).\n\n        By default, all indexed columns are searched,\n        now only one column can be searched at a time.\n\n        Parameters\n        ----------\n        query: str\n            The text query to search for.\n        columns: str or list of str, default None\n            The columns to search in. If None, all indexed columns are searched.\n            For now only one column can be searched at a time.\n        \"\"\"\n        if isinstance(columns, str):\n            columns = [columns]\n        if columns is None:\n            columns = []\n\n        if isinstance(query, str):\n            return AsyncHybridQuery(\n                self._inner.nearest_to_text({\"query\": query, \"columns\": columns})\n            )\n        # FullTextQuery object\n        return AsyncHybridQuery(self._inner.nearest_to_text({\"query\": query.to_dict()}))\n\n    async def to_batches(\n        self,\n        *,\n        max_batch_length: Optional[int] = None,\n        timeout: Optional[timedelta] = None,\n    ) -&gt; AsyncRecordBatchReader:\n        reader = await super().to_batches(timeout=timeout)\n        results = pa.Table.from_batches(await reader.read_all(), reader.schema)\n        if self._reranker:\n            results = self._reranker.rerank_vector(self._query_string, results)\n        return AsyncRecordBatchReader(results, max_batch_length=max_batch_length)\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncVectorQuery.column","title":"column","text":"<pre><code>column(column: str) -&gt; Self\n</code></pre> <p>Set the vector column to query</p> <p>This controls which column is compared to the query vector supplied in the call to AsyncQuery.nearest_to.</p> <p>This parameter must be specified if the table has more than one column whose data type is a fixed-size-list of floats.</p> Source code in <code>lancedb/query.py</code> <pre><code>def column(self, column: str) -&gt; Self:\n    \"\"\"\n    Set the vector column to query\n\n    This controls which column is compared to the query vector supplied in\n    the call to [AsyncQuery.nearest_to][lancedb.query.AsyncQuery.nearest_to].\n\n    This parameter must be specified if the table has more than one column\n    whose data type is a fixed-size-list of floats.\n    \"\"\"\n    self._inner.column(column)\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncVectorQuery.nprobes","title":"nprobes","text":"<pre><code>nprobes(nprobes: int) -&gt; Self\n</code></pre> <p>Set the number of partitions to search (probe)</p> <p>This argument is only used when the vector column has an IVF-based index. If there is no index then this value is ignored.</p> <p>The IVF stage of IVF PQ divides the input into partitions (clusters) of related values.</p> <p>The partition whose centroids are closest to the query vector will be exhaustiely searched to find matches.  This parameter controls how many partitions should be searched.</p> <p>Increasing this value will increase the recall of your query but will also increase the latency of your query.  The default value is 20.  This default is good for many cases but the best value to use will depend on your data and the recall that you need to achieve.</p> <p>For best results we recommend tuning this parameter with a benchmark against your actual data to find the smallest possible value that will still give you the desired recall.</p> Source code in <code>lancedb/query.py</code> <pre><code>def nprobes(self, nprobes: int) -&gt; Self:\n    \"\"\"\n    Set the number of partitions to search (probe)\n\n    This argument is only used when the vector column has an IVF-based index.\n    If there is no index then this value is ignored.\n\n    The IVF stage of IVF PQ divides the input into partitions (clusters) of\n    related values.\n\n    The partition whose centroids are closest to the query vector will be\n    exhaustiely searched to find matches.  This parameter controls how many\n    partitions should be searched.\n\n    Increasing this value will increase the recall of your query but will\n    also increase the latency of your query.  The default value is 20.  This\n    default is good for many cases but the best value to use will depend on\n    your data and the recall that you need to achieve.\n\n    For best results we recommend tuning this parameter with a benchmark against\n    your actual data to find the smallest possible value that will still give\n    you the desired recall.\n    \"\"\"\n    self._inner.nprobes(nprobes)\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncVectorQuery.distance_range","title":"distance_range","text":"<pre><code>distance_range(lower_bound: Optional[float] = None, upper_bound: Optional[float] = None) -&gt; Self\n</code></pre> <p>Set the distance range to use.</p> <p>Only rows with distances within range [lower_bound, upper_bound) will be returned.</p> <p>Parameters:</p> <ul> <li> <code>lower_bound</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>The lower bound of the distance range.</p> </li> <li> <code>upper_bound</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>The upper bound of the distance range.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AsyncVectorQuery</code>           \u2013            <p>The AsyncVectorQuery object.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def distance_range(\n    self, lower_bound: Optional[float] = None, upper_bound: Optional[float] = None\n) -&gt; Self:\n    \"\"\"Set the distance range to use.\n\n    Only rows with distances within range [lower_bound, upper_bound)\n    will be returned.\n\n    Parameters\n    ----------\n    lower_bound: Optional[float]\n        The lower bound of the distance range.\n    upper_bound: Optional[float]\n        The upper bound of the distance range.\n\n    Returns\n    -------\n    AsyncVectorQuery\n        The AsyncVectorQuery object.\n    \"\"\"\n    self._inner.distance_range(lower_bound, upper_bound)\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncVectorQuery.ef","title":"ef","text":"<pre><code>ef(ef: int) -&gt; Self\n</code></pre> <p>Set the number of candidates to consider during search</p> <p>This argument is only used when the vector column has an HNSW index. If there is no index then this value is ignored.</p> <p>Increasing this value will increase the recall of your query but will also increase the latency of your query.  The default value is 1.5 * limit.  This default is good for many cases but the best value to use will depend on your data and the recall that you need to achieve.</p> Source code in <code>lancedb/query.py</code> <pre><code>def ef(self, ef: int) -&gt; Self:\n    \"\"\"\n    Set the number of candidates to consider during search\n\n    This argument is only used when the vector column has an HNSW index.\n    If there is no index then this value is ignored.\n\n    Increasing this value will increase the recall of your query but will also\n    increase the latency of your query.  The default value is 1.5 * limit.  This\n    default is good for many cases but the best value to use will depend on your\n    data and the recall that you need to achieve.\n    \"\"\"\n    self._inner.ef(ef)\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncVectorQuery.refine_factor","title":"refine_factor","text":"<pre><code>refine_factor(refine_factor: int) -&gt; Self\n</code></pre> <p>A multiplier to control how many additional rows are taken during the refine step</p> <p>This argument is only used when the vector column has an IVF PQ index. If there is no index then this value is ignored.</p> <p>An IVF PQ index stores compressed (quantized) values.  They query vector is compared against these values and, since they are compressed, the comparison is inaccurate.</p> <p>This parameter can be used to refine the results.  It can improve both improve recall and correct the ordering of the nearest results.</p> <p>To refine results LanceDb will first perform an ANN search to find the nearest <code>limit</code> * <code>refine_factor</code> results.  In other words, if <code>refine_factor</code> is 3 and <code>limit</code> is the default (10) then the first 30 results will be selected.  LanceDb then fetches the full, uncompressed, values for these 30 results.  The results are then reordered by the true distance and only the nearest 10 are kept.</p> <p>Note: there is a difference between calling this method with a value of 1 and never calling this method at all.  Calling this method with any value will have an impact on your search latency.  When you call this method with a <code>refine_factor</code> of 1 then LanceDb still needs to fetch the full, uncompressed, values so that it can potentially reorder the results.</p> <p>Note: if this method is NOT called then the distances returned in the _distance column will be approximate distances based on the comparison of the quantized query vector and the quantized result vectors.  This can be considerably different than the true distance between the query vector and the actual uncompressed vector.</p> Source code in <code>lancedb/query.py</code> <pre><code>def refine_factor(self, refine_factor: int) -&gt; Self:\n    \"\"\"\n    A multiplier to control how many additional rows are taken during the refine\n    step\n\n    This argument is only used when the vector column has an IVF PQ index.\n    If there is no index then this value is ignored.\n\n    An IVF PQ index stores compressed (quantized) values.  They query vector is\n    compared against these values and, since they are compressed, the comparison is\n    inaccurate.\n\n    This parameter can be used to refine the results.  It can improve both improve\n    recall and correct the ordering of the nearest results.\n\n    To refine results LanceDb will first perform an ANN search to find the nearest\n    `limit` * `refine_factor` results.  In other words, if `refine_factor` is 3 and\n    `limit` is the default (10) then the first 30 results will be selected.  LanceDb\n    then fetches the full, uncompressed, values for these 30 results.  The results\n    are then reordered by the true distance and only the nearest 10 are kept.\n\n    Note: there is a difference between calling this method with a value of 1 and\n    never calling this method at all.  Calling this method with any value will have\n    an impact on your search latency.  When you call this method with a\n    `refine_factor` of 1 then LanceDb still needs to fetch the full, uncompressed,\n    values so that it can potentially reorder the results.\n\n    Note: if this method is NOT called then the distances returned in the _distance\n    column will be approximate distances based on the comparison of the quantized\n    query vector and the quantized result vectors.  This can be considerably\n    different than the true distance between the query vector and the actual\n    uncompressed vector.\n    \"\"\"\n    self._inner.refine_factor(refine_factor)\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncVectorQuery.distance_type","title":"distance_type","text":"<pre><code>distance_type(distance_type: str) -&gt; Self\n</code></pre> <p>Set the distance metric to use</p> <p>When performing a vector search we try and find the \"nearest\" vectors according to some kind of distance metric.  This parameter controls which distance metric to use.  See @see {@link IvfPqOptions.distanceType} for more details on the different distance metrics available.</p> <p>Note: if there is a vector index then the distance type used MUST match the distance type used to train the vector index.  If this is not done then the results will be invalid.</p> <p>By default \"l2\" is used.</p> Source code in <code>lancedb/query.py</code> <pre><code>def distance_type(self, distance_type: str) -&gt; Self:\n    \"\"\"\n    Set the distance metric to use\n\n    When performing a vector search we try and find the \"nearest\" vectors according\n    to some kind of distance metric.  This parameter controls which distance metric\n    to use.  See @see {@link IvfPqOptions.distanceType} for more details on the\n    different distance metrics available.\n\n    Note: if there is a vector index then the distance type used MUST match the\n    distance type used to train the vector index.  If this is not done then the\n    results will be invalid.\n\n    By default \"l2\" is used.\n    \"\"\"\n    self._inner.distance_type(distance_type)\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncVectorQuery.bypass_vector_index","title":"bypass_vector_index","text":"<pre><code>bypass_vector_index() -&gt; Self\n</code></pre> <p>If this is called then any vector index is skipped</p> <p>An exhaustive (flat) search will be performed.  The query vector will be compared to every vector in the table.  At high scales this can be expensive.  However, this is often still useful.  For example, skipping the vector index can give you ground truth results which you can use to calculate your recall to select an appropriate value for nprobes.</p> Source code in <code>lancedb/query.py</code> <pre><code>def bypass_vector_index(self) -&gt; Self:\n    \"\"\"\n    If this is called then any vector index is skipped\n\n    An exhaustive (flat) search will be performed.  The query vector will\n    be compared to every vector in the table.  At high scales this can be\n    expensive.  However, this is often still useful.  For example, skipping\n    the vector index can give you ground truth results which you can use to\n    calculate your recall to select an appropriate value for nprobes.\n    \"\"\"\n    self._inner.bypass_vector_index()\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncVectorQuery.where","title":"where","text":"<pre><code>where(predicate: str) -&gt; Self\n</code></pre> <p>Only return rows matching the given predicate</p> <p>The predicate should be supplied as an SQL query string.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; predicate = \"x &gt; 10\"\n&gt;&gt;&gt; predicate = \"y &gt; 0 AND y &lt; 100\"\n&gt;&gt;&gt; predicate = \"x &gt; 5 OR y = 'test'\"\n</code></pre> <p>Filtering performance can often be improved by creating a scalar index on the filter column(s).</p> Source code in <code>lancedb/query.py</code> <pre><code>def where(self, predicate: str) -&gt; Self:\n    \"\"\"\n    Only return rows matching the given predicate\n\n    The predicate should be supplied as an SQL query string.\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; predicate = \"x &gt; 10\"\n    &gt;&gt;&gt; predicate = \"y &gt; 0 AND y &lt; 100\"\n    &gt;&gt;&gt; predicate = \"x &gt; 5 OR y = 'test'\"\n\n    Filtering performance can often be improved by creating a scalar index\n    on the filter column(s).\n    \"\"\"\n    self._inner.where(predicate)\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncVectorQuery.select","title":"select","text":"<pre><code>select(columns: Union[List[str], dict[str, str]]) -&gt; Self\n</code></pre> <p>Return only the specified columns.</p> <p>By default a query will return all columns from the table.  However, this can have a very significant impact on latency.  LanceDb stores data in a columnar fashion.  This means we can finely tune our I/O to select exactly the columns we need.</p> <p>As a best practice you should always limit queries to the columns that you need. If you pass in a list of column names then only those columns will be returned.</p> <p>You can also use this method to create new \"dynamic\" columns based on your existing columns. For example, you may not care about \"a\" or \"b\" but instead simply want \"a + b\".  This is often seen in the SELECT clause of an SQL query (e.g. <code>SELECT a+b FROM my_table</code>).</p> <p>To create dynamic columns you can pass in a dict[str, str].  A column will be returned for each entry in the map.  The key provides the name of the column. The value is an SQL string used to specify how the column is calculated.</p> <p>For example, an SQL query might state <code>SELECT a + b AS combined, c</code>.  The equivalent input to this method would be <code>{\"combined\": \"a + b\", \"c\": \"c\"}</code>.</p> <p>Columns will always be returned in the order given, even if that order is different than the order used when adding the data.</p> Source code in <code>lancedb/query.py</code> <pre><code>def select(self, columns: Union[List[str], dict[str, str]]) -&gt; Self:\n    \"\"\"\n    Return only the specified columns.\n\n    By default a query will return all columns from the table.  However, this can\n    have a very significant impact on latency.  LanceDb stores data in a columnar\n    fashion.  This\n    means we can finely tune our I/O to select exactly the columns we need.\n\n    As a best practice you should always limit queries to the columns that you need.\n    If you pass in a list of column names then only those columns will be\n    returned.\n\n    You can also use this method to create new \"dynamic\" columns based on your\n    existing columns. For example, you may not care about \"a\" or \"b\" but instead\n    simply want \"a + b\".  This is often seen in the SELECT clause of an SQL query\n    (e.g. `SELECT a+b FROM my_table`).\n\n    To create dynamic columns you can pass in a dict[str, str].  A column will be\n    returned for each entry in the map.  The key provides the name of the column.\n    The value is an SQL string used to specify how the column is calculated.\n\n    For example, an SQL query might state `SELECT a + b AS combined, c`.  The\n    equivalent input to this method would be `{\"combined\": \"a + b\", \"c\": \"c\"}`.\n\n    Columns will always be returned in the order given, even if that order is\n    different than the order used when adding the data.\n    \"\"\"\n    if isinstance(columns, list) and all(isinstance(c, str) for c in columns):\n        self._inner.select_columns(columns)\n    elif isinstance(columns, dict) and all(\n        isinstance(k, str) and isinstance(v, str) for k, v in columns.items()\n    ):\n        self._inner.select(list(columns.items()))\n    else:\n        raise TypeError(\"columns must be a list of column names or a dict\")\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncVectorQuery.limit","title":"limit","text":"<pre><code>limit(limit: int) -&gt; Self\n</code></pre> <p>Set the maximum number of results to return.</p> <p>By default, a plain search has no limit.  If this method is not called then every valid row from the table will be returned.</p> Source code in <code>lancedb/query.py</code> <pre><code>def limit(self, limit: int) -&gt; Self:\n    \"\"\"\n    Set the maximum number of results to return.\n\n    By default, a plain search has no limit.  If this method is not\n    called then every valid row from the table will be returned.\n    \"\"\"\n    self._inner.limit(limit)\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncVectorQuery.offset","title":"offset","text":"<pre><code>offset(offset: int) -&gt; Self\n</code></pre> <p>Set the offset for the results.</p> <p>Parameters:</p> <ul> <li> <code>offset</code>               (<code>int</code>)           \u2013            <p>The offset to start fetching results from.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def offset(self, offset: int) -&gt; Self:\n    \"\"\"\n    Set the offset for the results.\n\n    Parameters\n    ----------\n    offset: int\n        The offset to start fetching results from.\n    \"\"\"\n    self._inner.offset(offset)\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncVectorQuery.fast_search","title":"fast_search","text":"<pre><code>fast_search() -&gt; Self\n</code></pre> <p>Skip searching un-indexed data.</p> <p>This can make queries faster, but will miss any data that has not been indexed.</p> <p>Tip</p> <p>You can add new data into an existing index by calling AsyncTable.optimize.</p> Source code in <code>lancedb/query.py</code> <pre><code>def fast_search(self) -&gt; Self:\n    \"\"\"\n    Skip searching un-indexed data.\n\n    This can make queries faster, but will miss any data that has not been\n    indexed.\n\n    !!! tip\n        You can add new data into an existing index by calling\n        [AsyncTable.optimize][lancedb.table.AsyncTable.optimize].\n    \"\"\"\n    self._inner.fast_search()\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncVectorQuery.with_row_id","title":"with_row_id","text":"<pre><code>with_row_id() -&gt; Self\n</code></pre> <p>Include the _rowid column in the results.</p> Source code in <code>lancedb/query.py</code> <pre><code>def with_row_id(self) -&gt; Self:\n    \"\"\"\n    Include the _rowid column in the results.\n    \"\"\"\n    self._inner.with_row_id()\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncVectorQuery.postfilter","title":"postfilter","text":"<pre><code>postfilter() -&gt; Self\n</code></pre> <p>If this is called then filtering will happen after the search instead of before. By default filtering will be performed before the search.  This is how filtering is typically understood to work.  This prefilter step does add some additional latency.  Creating a scalar index on the filter column(s) can often improve this latency.  However, sometimes a filter is too complex or scalar indices cannot be applied to the column.  In these cases postfiltering can be used instead of prefiltering to improve latency. Post filtering applies the filter to the results of the search.  This means we only run the filter on a much smaller set of data.  However, it can cause the query to return fewer than <code>limit</code> results (or even no results) if none of the nearest results match the filter. Post filtering happens during the \"refine stage\" (described in more detail in @see {@link VectorQuery#refineFactor}).  This means that setting a higher refine factor can often help restore some of the results lost by post filtering.</p> Source code in <code>lancedb/query.py</code> <pre><code>def postfilter(self) -&gt; Self:\n    \"\"\"\n    If this is called then filtering will happen after the search instead of\n    before.\n    By default filtering will be performed before the search.  This is how\n    filtering is typically understood to work.  This prefilter step does add some\n    additional latency.  Creating a scalar index on the filter column(s) can\n    often improve this latency.  However, sometimes a filter is too complex or\n    scalar indices cannot be applied to the column.  In these cases postfiltering\n    can be used instead of prefiltering to improve latency.\n    Post filtering applies the filter to the results of the search.  This\n    means we only run the filter on a much smaller set of data.  However, it can\n    cause the query to return fewer than `limit` results (or even no results) if\n    none of the nearest results match the filter.\n    Post filtering happens during the \"refine stage\" (described in more detail in\n    @see {@link VectorQuery#refineFactor}).  This means that setting a higher refine\n    factor can often help restore some of the results lost by post filtering.\n    \"\"\"\n    self._inner.postfilter()\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncVectorQuery.to_arrow","title":"to_arrow  <code>async</code>","text":"<pre><code>to_arrow(timeout: Optional[timedelta] = None) -&gt; Table\n</code></pre> <p>Execute the query and collect the results into an Apache Arrow Table.</p> <p>This method will collect all results into memory before returning.  If you expect a large number of results, you may want to use to_batches</p> <p>Parameters:</p> <ul> <li> <code>timeout</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum time to wait for the query to complete. If not specified, no timeout is applied. If the query does not complete within the specified time, an error will be raised.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>async def to_arrow(self, timeout: Optional[timedelta] = None) -&gt; pa.Table:\n    \"\"\"\n    Execute the query and collect the results into an Apache Arrow Table.\n\n    This method will collect all results into memory before returning.  If\n    you expect a large number of results, you may want to use\n    [to_batches][lancedb.query.AsyncQueryBase.to_batches]\n\n    Parameters\n    ----------\n    timeout: Optional[timedelta]\n        The maximum time to wait for the query to complete.\n        If not specified, no timeout is applied. If the query does not\n        complete within the specified time, an error will be raised.\n    \"\"\"\n    batch_iter = await self.to_batches(timeout=timeout)\n    return pa.Table.from_batches(\n        await batch_iter.read_all(), schema=batch_iter.schema\n    )\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncVectorQuery.to_list","title":"to_list  <code>async</code>","text":"<pre><code>to_list(timeout: Optional[timedelta] = None) -&gt; List[dict]\n</code></pre> <p>Execute the query and return the results as a list of dictionaries.</p> <p>Each list entry is a dictionary with the selected column names as keys, or all table columns if <code>select</code> is not called. The vector and the \"_distance\" fields are returned whether or not they're explicitly selected.</p> <p>Parameters:</p> <ul> <li> <code>timeout</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum time to wait for the query to complete. If not specified, no timeout is applied. If the query does not complete within the specified time, an error will be raised.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>async def to_list(self, timeout: Optional[timedelta] = None) -&gt; List[dict]:\n    \"\"\"\n    Execute the query and return the results as a list of dictionaries.\n\n    Each list entry is a dictionary with the selected column names as keys,\n    or all table columns if `select` is not called. The vector and the \"_distance\"\n    fields are returned whether or not they're explicitly selected.\n\n    Parameters\n    ----------\n    timeout: Optional[timedelta]\n        The maximum time to wait for the query to complete.\n        If not specified, no timeout is applied. If the query does not\n        complete within the specified time, an error will be raised.\n    \"\"\"\n    return (await self.to_arrow(timeout=timeout)).to_pylist()\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncVectorQuery.to_pandas","title":"to_pandas  <code>async</code>","text":"<pre><code>to_pandas(flatten: Optional[Union[int, bool]] = None, timeout: Optional[timedelta] = None) -&gt; 'pd.DataFrame'\n</code></pre> <p>Execute the query and collect the results into a pandas DataFrame.</p> <p>This method will collect all results into memory before returning.  If you expect a large number of results, you may want to use to_batches and convert each batch to pandas separately.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import asyncio\n&gt;&gt;&gt; from lancedb import connect_async\n&gt;&gt;&gt; async def doctest_example():\n...     conn = await connect_async(\"./.lancedb\")\n...     table = await conn.create_table(\"my_table\", data=[{\"a\": 1, \"b\": 2}])\n...     async for batch in await table.query().to_batches():\n...         batch_df = batch.to_pandas()\n&gt;&gt;&gt; asyncio.run(doctest_example())\n</code></pre> <p>Parameters:</p> <ul> <li> <code>flatten</code>               (<code>Optional[Union[int, bool]]</code>, default:                   <code>None</code> )           \u2013            <p>If flatten is True, flatten all nested columns. If flatten is an integer, flatten the nested columns up to the specified depth. If unspecified, do not flatten the nested columns.</p> </li> <li> <code>timeout</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum time to wait for the query to complete. If not specified, no timeout is applied. If the query does not complete within the specified time, an error will be raised.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>async def to_pandas(\n    self,\n    flatten: Optional[Union[int, bool]] = None,\n    timeout: Optional[timedelta] = None,\n) -&gt; \"pd.DataFrame\":\n    \"\"\"\n    Execute the query and collect the results into a pandas DataFrame.\n\n    This method will collect all results into memory before returning.  If you\n    expect a large number of results, you may want to use\n    [to_batches][lancedb.query.AsyncQueryBase.to_batches] and convert each batch to\n    pandas separately.\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; import asyncio\n    &gt;&gt;&gt; from lancedb import connect_async\n    &gt;&gt;&gt; async def doctest_example():\n    ...     conn = await connect_async(\"./.lancedb\")\n    ...     table = await conn.create_table(\"my_table\", data=[{\"a\": 1, \"b\": 2}])\n    ...     async for batch in await table.query().to_batches():\n    ...         batch_df = batch.to_pandas()\n    &gt;&gt;&gt; asyncio.run(doctest_example())\n\n    Parameters\n    ----------\n    flatten: Optional[Union[int, bool]]\n        If flatten is True, flatten all nested columns.\n        If flatten is an integer, flatten the nested columns up to the\n        specified depth.\n        If unspecified, do not flatten the nested columns.\n    timeout: Optional[timedelta]\n        The maximum time to wait for the query to complete.\n        If not specified, no timeout is applied. If the query does not\n        complete within the specified time, an error will be raised.\n    \"\"\"\n    return (\n        flatten_columns(await self.to_arrow(timeout=timeout), flatten)\n    ).to_pandas()\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncVectorQuery.to_polars","title":"to_polars  <code>async</code>","text":"<pre><code>to_polars(timeout: Optional[timedelta] = None) -&gt; 'pl.DataFrame'\n</code></pre> <p>Execute the query and collect the results into a Polars DataFrame.</p> <p>This method will collect all results into memory before returning.  If you expect a large number of results, you may want to use to_batches and convert each batch to polars separately.</p> <p>Parameters:</p> <ul> <li> <code>timeout</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum time to wait for the query to complete. If not specified, no timeout is applied. If the query does not complete within the specified time, an error will be raised.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import asyncio\n&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from lancedb import connect_async\n&gt;&gt;&gt; async def doctest_example():\n...     conn = await connect_async(\"./.lancedb\")\n...     table = await conn.create_table(\"my_table\", data=[{\"a\": 1, \"b\": 2}])\n...     async for batch in await table.query().to_batches():\n...         batch_df = pl.from_arrow(batch)\n&gt;&gt;&gt; asyncio.run(doctest_example())\n</code></pre> Source code in <code>lancedb/query.py</code> <pre><code>async def to_polars(\n    self,\n    timeout: Optional[timedelta] = None,\n) -&gt; \"pl.DataFrame\":\n    \"\"\"\n    Execute the query and collect the results into a Polars DataFrame.\n\n    This method will collect all results into memory before returning.  If you\n    expect a large number of results, you may want to use\n    [to_batches][lancedb.query.AsyncQueryBase.to_batches] and convert each batch to\n    polars separately.\n\n    Parameters\n    ----------\n    timeout: Optional[timedelta]\n        The maximum time to wait for the query to complete.\n        If not specified, no timeout is applied. If the query does not\n        complete within the specified time, an error will be raised.\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; import asyncio\n    &gt;&gt;&gt; import polars as pl\n    &gt;&gt;&gt; from lancedb import connect_async\n    &gt;&gt;&gt; async def doctest_example():\n    ...     conn = await connect_async(\"./.lancedb\")\n    ...     table = await conn.create_table(\"my_table\", data=[{\"a\": 1, \"b\": 2}])\n    ...     async for batch in await table.query().to_batches():\n    ...         batch_df = pl.from_arrow(batch)\n    &gt;&gt;&gt; asyncio.run(doctest_example())\n    \"\"\"\n    import polars as pl\n\n    return pl.from_arrow(await self.to_arrow(timeout=timeout))\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncVectorQuery.explain_plan","title":"explain_plan  <code>async</code>","text":"<pre><code>explain_plan(verbose: Optional[bool] = False)\n</code></pre> <p>Return the execution plan for this query.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import asyncio\n&gt;&gt;&gt; from lancedb import connect_async\n&gt;&gt;&gt; async def doctest_example():\n...     conn = await connect_async(\"./.lancedb\")\n...     table = await conn.create_table(\"my_table\", [{\"vector\": [99, 99]}])\n...     query = [100, 100]\n...     plan = await table.query().nearest_to([1, 2]).explain_plan(True)\n...     print(plan)\n&gt;&gt;&gt; asyncio.run(doctest_example())\nProjectionExec: expr=[vector@0 as vector, _distance@2 as _distance]\n  GlobalLimitExec: skip=0, fetch=10\n    FilterExec: _distance@2 IS NOT NULL\n      SortExec: TopK(fetch=10), expr=[_distance@2 ASC NULLS LAST], preserve_partitioning=[false]\n        KNNVectorDistance: metric=l2\n          LanceScan: uri=..., projection=[vector], row_id=true, row_addr=false, ordered=false\n</code></pre> <p>Parameters:</p> <ul> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Use a verbose output format.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>plan</code> (              <code>str</code> )          \u2013            </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>async def explain_plan(self, verbose: Optional[bool] = False):\n    \"\"\"Return the execution plan for this query.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import asyncio\n    &gt;&gt;&gt; from lancedb import connect_async\n    &gt;&gt;&gt; async def doctest_example():\n    ...     conn = await connect_async(\"./.lancedb\")\n    ...     table = await conn.create_table(\"my_table\", [{\"vector\": [99, 99]}])\n    ...     query = [100, 100]\n    ...     plan = await table.query().nearest_to([1, 2]).explain_plan(True)\n    ...     print(plan)\n    &gt;&gt;&gt; asyncio.run(doctest_example()) # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n    ProjectionExec: expr=[vector@0 as vector, _distance@2 as _distance]\n      GlobalLimitExec: skip=0, fetch=10\n        FilterExec: _distance@2 IS NOT NULL\n          SortExec: TopK(fetch=10), expr=[_distance@2 ASC NULLS LAST], preserve_partitioning=[false]\n            KNNVectorDistance: metric=l2\n              LanceScan: uri=..., projection=[vector], row_id=true, row_addr=false, ordered=false\n\n    Parameters\n    ----------\n    verbose : bool, default False\n        Use a verbose output format.\n\n    Returns\n    -------\n    plan : str\n    \"\"\"  # noqa: E501\n    return await self._inner.explain_plan(verbose)\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncVectorQuery.analyze_plan","title":"analyze_plan  <code>async</code>","text":"<pre><code>analyze_plan()\n</code></pre> <p>Execute the query and display with runtime metrics.</p> <p>Returns:</p> <ul> <li> <code>plan</code> (              <code>str</code> )          \u2013            </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>async def analyze_plan(self):\n    \"\"\"Execute the query and display with runtime metrics.\n\n    Returns\n    -------\n    plan : str\n    \"\"\"\n    return await self._inner.analyze_plan()\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncVectorQuery.__init__","title":"__init__","text":"<pre><code>__init__(inner: VectorQuery)\n</code></pre> <p>Construct an AsyncVectorQuery</p> <p>This method is not intended to be called directly.  Instead, create a query first with AsyncTable.query and then use AsyncQuery.nearest_to] to convert to a vector query.  Or you can use AsyncTable.vector_search</p> Source code in <code>lancedb/query.py</code> <pre><code>def __init__(self, inner: LanceVectorQuery):\n    \"\"\"\n    Construct an AsyncVectorQuery\n\n    This method is not intended to be called directly.  Instead, create\n    a query first with [AsyncTable.query][lancedb.table.AsyncTable.query] and then\n    use [AsyncQuery.nearest_to][lancedb.query.AsyncQuery.nearest_to]] to convert to\n    a vector query.  Or you can use\n    [AsyncTable.vector_search][lancedb.table.AsyncTable.vector_search]\n    \"\"\"\n    super().__init__(inner)\n    self._inner = inner\n    self._reranker = None\n    self._query_string = None\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncVectorQuery.nearest_to_text","title":"nearest_to_text","text":"<pre><code>nearest_to_text(query: str | FullTextQuery, columns: Union[str, List[str], None] = None) -&gt; AsyncHybridQuery\n</code></pre> <p>Find the documents that are most relevant to the given text query, in addition to vector search.</p> <p>This converts the vector query into a hybrid query.</p> <p>This search will perform a full text search on the table and return the most relevant documents, combined with the vector query results. The text relevance is determined by BM25.</p> <p>The columns to search must be with native FTS index (Tantivy-based can't work with this method).</p> <p>By default, all indexed columns are searched, now only one column can be searched at a time.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str | FullTextQuery</code>)           \u2013            <p>The text query to search for.</p> </li> <li> <code>columns</code>               (<code>Union[str, List[str], None]</code>, default:                   <code>None</code> )           \u2013            <p>The columns to search in. If None, all indexed columns are searched. For now only one column can be searched at a time.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def nearest_to_text(\n    self, query: str | FullTextQuery, columns: Union[str, List[str], None] = None\n) -&gt; AsyncHybridQuery:\n    \"\"\"\n    Find the documents that are most relevant to the given text query,\n    in addition to vector search.\n\n    This converts the vector query into a hybrid query.\n\n    This search will perform a full text search on the table and return\n    the most relevant documents, combined with the vector query results.\n    The text relevance is determined by BM25.\n\n    The columns to search must be with native FTS index\n    (Tantivy-based can't work with this method).\n\n    By default, all indexed columns are searched,\n    now only one column can be searched at a time.\n\n    Parameters\n    ----------\n    query: str\n        The text query to search for.\n    columns: str or list of str, default None\n        The columns to search in. If None, all indexed columns are searched.\n        For now only one column can be searched at a time.\n    \"\"\"\n    if isinstance(columns, str):\n        columns = [columns]\n    if columns is None:\n        columns = []\n\n    if isinstance(query, str):\n        return AsyncHybridQuery(\n            self._inner.nearest_to_text({\"query\": query, \"columns\": columns})\n        )\n    # FullTextQuery object\n    return AsyncHybridQuery(self._inner.nearest_to_text({\"query\": query.to_dict()}))\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncFTSQuery","title":"lancedb.query.AsyncFTSQuery","text":"<p>               Bases: <code>AsyncQueryBase</code></p> <p>A query for full text search for LanceDB.</p> Source code in <code>lancedb/query.py</code> <pre><code>class AsyncFTSQuery(AsyncQueryBase):\n    \"\"\"A query for full text search for LanceDB.\"\"\"\n\n    def __init__(self, inner: LanceFTSQuery):\n        super().__init__(inner)\n        self._inner = inner\n        self._reranker = None\n\n    def get_query(self) -&gt; str:\n        return self._inner.get_query()\n\n    def rerank(\n        self,\n        reranker: Reranker = RRFReranker(),\n    ) -&gt; AsyncFTSQuery:\n        if reranker and not isinstance(reranker, Reranker):\n            raise ValueError(\"reranker must be an instance of Reranker class.\")\n\n        self._reranker = reranker\n\n        return self\n\n    def nearest_to(\n        self,\n        query_vector: Union[VEC, Tuple, List[VEC]],\n    ) -&gt; AsyncHybridQuery:\n        \"\"\"\n        In addition doing text search on the LanceDB Table, also\n        find the nearest vectors to the given query vector.\n\n        This converts the query from a FTS Query to a Hybrid query. Results\n        from the vector search will be combined with results from the FTS query.\n\n        This method will attempt to convert the input to the query vector\n        expected by the embedding model.  If the input cannot be converted\n        then an error will be thrown.\n\n        By default, there is no embedding model, and the input should be\n        something that can be converted to a pyarrow array of floats.  This\n        includes lists, numpy arrays, and tuples.\n\n        If there is only one vector column (a column whose data type is a\n        fixed size list of floats) then the column does not need to be specified.\n        If there is more than one vector column you must use\n        [AsyncVectorQuery.column][lancedb.query.AsyncVectorQuery.column] to specify\n        which column you would like to compare with.\n\n        If no index has been created on the vector column then a vector query\n        will perform a distance comparison between the query vector and every\n        vector in the database and then sort the results.  This is sometimes\n        called a \"flat search\"\n\n        For small databases, with tens of thousands of vectors or less, this can\n        be reasonably fast.  In larger databases you should create a vector index\n        on the column.  If there is a vector index then an \"approximate\" nearest\n        neighbor search (frequently called an ANN search) will be performed.  This\n        search is much faster, but the results will be approximate.\n\n        The query can be further parameterized using the returned builder.  There\n        are various ANN search parameters that will let you fine tune your recall\n        accuracy vs search latency.\n\n        Hybrid searches always have a [limit][].  If `limit` has not been called then\n        a default `limit` of 10 will be used.\n\n        Typically, a single vector is passed in as the query. However, you can also\n        pass in multiple vectors.  This can be useful if you want to find the nearest\n        vectors to multiple query vectors. This is not expected to be faster than\n        making multiple queries concurrently; it is just a convenience method.\n        If multiple vectors are passed in then an additional column `query_index`\n        will be added to the results.  This column will contain the index of the\n        query vector that the result is nearest to.\n        \"\"\"\n        if query_vector is None:\n            raise ValueError(\"query_vector can not be None\")\n\n        if (\n            isinstance(query_vector, list)\n            and len(query_vector) &gt; 0\n            and not isinstance(query_vector[0], (float, int))\n        ):\n            # multiple have been passed\n            query_vectors = [AsyncQuery._query_vec_to_array(v) for v in query_vector]\n            new_self = self._inner.nearest_to(query_vectors[0])\n            for v in query_vectors[1:]:\n                new_self.add_query_vector(v)\n            return AsyncHybridQuery(new_self)\n        else:\n            return AsyncHybridQuery(\n                self._inner.nearest_to(AsyncQuery._query_vec_to_array(query_vector))\n            )\n\n    async def to_batches(\n        self,\n        *,\n        max_batch_length: Optional[int] = None,\n        timeout: Optional[timedelta] = None,\n    ) -&gt; AsyncRecordBatchReader:\n        reader = await super().to_batches(timeout=timeout)\n        results = pa.Table.from_batches(await reader.read_all(), reader.schema)\n        if self._reranker:\n            results = self._reranker.rerank_fts(self.get_query(), results)\n        return AsyncRecordBatchReader(results, max_batch_length=max_batch_length)\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncFTSQuery.where","title":"where","text":"<pre><code>where(predicate: str) -&gt; Self\n</code></pre> <p>Only return rows matching the given predicate</p> <p>The predicate should be supplied as an SQL query string.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; predicate = \"x &gt; 10\"\n&gt;&gt;&gt; predicate = \"y &gt; 0 AND y &lt; 100\"\n&gt;&gt;&gt; predicate = \"x &gt; 5 OR y = 'test'\"\n</code></pre> <p>Filtering performance can often be improved by creating a scalar index on the filter column(s).</p> Source code in <code>lancedb/query.py</code> <pre><code>def where(self, predicate: str) -&gt; Self:\n    \"\"\"\n    Only return rows matching the given predicate\n\n    The predicate should be supplied as an SQL query string.\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; predicate = \"x &gt; 10\"\n    &gt;&gt;&gt; predicate = \"y &gt; 0 AND y &lt; 100\"\n    &gt;&gt;&gt; predicate = \"x &gt; 5 OR y = 'test'\"\n\n    Filtering performance can often be improved by creating a scalar index\n    on the filter column(s).\n    \"\"\"\n    self._inner.where(predicate)\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncFTSQuery.select","title":"select","text":"<pre><code>select(columns: Union[List[str], dict[str, str]]) -&gt; Self\n</code></pre> <p>Return only the specified columns.</p> <p>By default a query will return all columns from the table.  However, this can have a very significant impact on latency.  LanceDb stores data in a columnar fashion.  This means we can finely tune our I/O to select exactly the columns we need.</p> <p>As a best practice you should always limit queries to the columns that you need. If you pass in a list of column names then only those columns will be returned.</p> <p>You can also use this method to create new \"dynamic\" columns based on your existing columns. For example, you may not care about \"a\" or \"b\" but instead simply want \"a + b\".  This is often seen in the SELECT clause of an SQL query (e.g. <code>SELECT a+b FROM my_table</code>).</p> <p>To create dynamic columns you can pass in a dict[str, str].  A column will be returned for each entry in the map.  The key provides the name of the column. The value is an SQL string used to specify how the column is calculated.</p> <p>For example, an SQL query might state <code>SELECT a + b AS combined, c</code>.  The equivalent input to this method would be <code>{\"combined\": \"a + b\", \"c\": \"c\"}</code>.</p> <p>Columns will always be returned in the order given, even if that order is different than the order used when adding the data.</p> Source code in <code>lancedb/query.py</code> <pre><code>def select(self, columns: Union[List[str], dict[str, str]]) -&gt; Self:\n    \"\"\"\n    Return only the specified columns.\n\n    By default a query will return all columns from the table.  However, this can\n    have a very significant impact on latency.  LanceDb stores data in a columnar\n    fashion.  This\n    means we can finely tune our I/O to select exactly the columns we need.\n\n    As a best practice you should always limit queries to the columns that you need.\n    If you pass in a list of column names then only those columns will be\n    returned.\n\n    You can also use this method to create new \"dynamic\" columns based on your\n    existing columns. For example, you may not care about \"a\" or \"b\" but instead\n    simply want \"a + b\".  This is often seen in the SELECT clause of an SQL query\n    (e.g. `SELECT a+b FROM my_table`).\n\n    To create dynamic columns you can pass in a dict[str, str].  A column will be\n    returned for each entry in the map.  The key provides the name of the column.\n    The value is an SQL string used to specify how the column is calculated.\n\n    For example, an SQL query might state `SELECT a + b AS combined, c`.  The\n    equivalent input to this method would be `{\"combined\": \"a + b\", \"c\": \"c\"}`.\n\n    Columns will always be returned in the order given, even if that order is\n    different than the order used when adding the data.\n    \"\"\"\n    if isinstance(columns, list) and all(isinstance(c, str) for c in columns):\n        self._inner.select_columns(columns)\n    elif isinstance(columns, dict) and all(\n        isinstance(k, str) and isinstance(v, str) for k, v in columns.items()\n    ):\n        self._inner.select(list(columns.items()))\n    else:\n        raise TypeError(\"columns must be a list of column names or a dict\")\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncFTSQuery.limit","title":"limit","text":"<pre><code>limit(limit: int) -&gt; Self\n</code></pre> <p>Set the maximum number of results to return.</p> <p>By default, a plain search has no limit.  If this method is not called then every valid row from the table will be returned.</p> Source code in <code>lancedb/query.py</code> <pre><code>def limit(self, limit: int) -&gt; Self:\n    \"\"\"\n    Set the maximum number of results to return.\n\n    By default, a plain search has no limit.  If this method is not\n    called then every valid row from the table will be returned.\n    \"\"\"\n    self._inner.limit(limit)\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncFTSQuery.offset","title":"offset","text":"<pre><code>offset(offset: int) -&gt; Self\n</code></pre> <p>Set the offset for the results.</p> <p>Parameters:</p> <ul> <li> <code>offset</code>               (<code>int</code>)           \u2013            <p>The offset to start fetching results from.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def offset(self, offset: int) -&gt; Self:\n    \"\"\"\n    Set the offset for the results.\n\n    Parameters\n    ----------\n    offset: int\n        The offset to start fetching results from.\n    \"\"\"\n    self._inner.offset(offset)\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncFTSQuery.fast_search","title":"fast_search","text":"<pre><code>fast_search() -&gt; Self\n</code></pre> <p>Skip searching un-indexed data.</p> <p>This can make queries faster, but will miss any data that has not been indexed.</p> <p>Tip</p> <p>You can add new data into an existing index by calling AsyncTable.optimize.</p> Source code in <code>lancedb/query.py</code> <pre><code>def fast_search(self) -&gt; Self:\n    \"\"\"\n    Skip searching un-indexed data.\n\n    This can make queries faster, but will miss any data that has not been\n    indexed.\n\n    !!! tip\n        You can add new data into an existing index by calling\n        [AsyncTable.optimize][lancedb.table.AsyncTable.optimize].\n    \"\"\"\n    self._inner.fast_search()\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncFTSQuery.with_row_id","title":"with_row_id","text":"<pre><code>with_row_id() -&gt; Self\n</code></pre> <p>Include the _rowid column in the results.</p> Source code in <code>lancedb/query.py</code> <pre><code>def with_row_id(self) -&gt; Self:\n    \"\"\"\n    Include the _rowid column in the results.\n    \"\"\"\n    self._inner.with_row_id()\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncFTSQuery.postfilter","title":"postfilter","text":"<pre><code>postfilter() -&gt; Self\n</code></pre> <p>If this is called then filtering will happen after the search instead of before. By default filtering will be performed before the search.  This is how filtering is typically understood to work.  This prefilter step does add some additional latency.  Creating a scalar index on the filter column(s) can often improve this latency.  However, sometimes a filter is too complex or scalar indices cannot be applied to the column.  In these cases postfiltering can be used instead of prefiltering to improve latency. Post filtering applies the filter to the results of the search.  This means we only run the filter on a much smaller set of data.  However, it can cause the query to return fewer than <code>limit</code> results (or even no results) if none of the nearest results match the filter. Post filtering happens during the \"refine stage\" (described in more detail in @see {@link VectorQuery#refineFactor}).  This means that setting a higher refine factor can often help restore some of the results lost by post filtering.</p> Source code in <code>lancedb/query.py</code> <pre><code>def postfilter(self) -&gt; Self:\n    \"\"\"\n    If this is called then filtering will happen after the search instead of\n    before.\n    By default filtering will be performed before the search.  This is how\n    filtering is typically understood to work.  This prefilter step does add some\n    additional latency.  Creating a scalar index on the filter column(s) can\n    often improve this latency.  However, sometimes a filter is too complex or\n    scalar indices cannot be applied to the column.  In these cases postfiltering\n    can be used instead of prefiltering to improve latency.\n    Post filtering applies the filter to the results of the search.  This\n    means we only run the filter on a much smaller set of data.  However, it can\n    cause the query to return fewer than `limit` results (or even no results) if\n    none of the nearest results match the filter.\n    Post filtering happens during the \"refine stage\" (described in more detail in\n    @see {@link VectorQuery#refineFactor}).  This means that setting a higher refine\n    factor can often help restore some of the results lost by post filtering.\n    \"\"\"\n    self._inner.postfilter()\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncFTSQuery.to_arrow","title":"to_arrow  <code>async</code>","text":"<pre><code>to_arrow(timeout: Optional[timedelta] = None) -&gt; Table\n</code></pre> <p>Execute the query and collect the results into an Apache Arrow Table.</p> <p>This method will collect all results into memory before returning.  If you expect a large number of results, you may want to use to_batches</p> <p>Parameters:</p> <ul> <li> <code>timeout</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum time to wait for the query to complete. If not specified, no timeout is applied. If the query does not complete within the specified time, an error will be raised.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>async def to_arrow(self, timeout: Optional[timedelta] = None) -&gt; pa.Table:\n    \"\"\"\n    Execute the query and collect the results into an Apache Arrow Table.\n\n    This method will collect all results into memory before returning.  If\n    you expect a large number of results, you may want to use\n    [to_batches][lancedb.query.AsyncQueryBase.to_batches]\n\n    Parameters\n    ----------\n    timeout: Optional[timedelta]\n        The maximum time to wait for the query to complete.\n        If not specified, no timeout is applied. If the query does not\n        complete within the specified time, an error will be raised.\n    \"\"\"\n    batch_iter = await self.to_batches(timeout=timeout)\n    return pa.Table.from_batches(\n        await batch_iter.read_all(), schema=batch_iter.schema\n    )\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncFTSQuery.to_list","title":"to_list  <code>async</code>","text":"<pre><code>to_list(timeout: Optional[timedelta] = None) -&gt; List[dict]\n</code></pre> <p>Execute the query and return the results as a list of dictionaries.</p> <p>Each list entry is a dictionary with the selected column names as keys, or all table columns if <code>select</code> is not called. The vector and the \"_distance\" fields are returned whether or not they're explicitly selected.</p> <p>Parameters:</p> <ul> <li> <code>timeout</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum time to wait for the query to complete. If not specified, no timeout is applied. If the query does not complete within the specified time, an error will be raised.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>async def to_list(self, timeout: Optional[timedelta] = None) -&gt; List[dict]:\n    \"\"\"\n    Execute the query and return the results as a list of dictionaries.\n\n    Each list entry is a dictionary with the selected column names as keys,\n    or all table columns if `select` is not called. The vector and the \"_distance\"\n    fields are returned whether or not they're explicitly selected.\n\n    Parameters\n    ----------\n    timeout: Optional[timedelta]\n        The maximum time to wait for the query to complete.\n        If not specified, no timeout is applied. If the query does not\n        complete within the specified time, an error will be raised.\n    \"\"\"\n    return (await self.to_arrow(timeout=timeout)).to_pylist()\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncFTSQuery.to_pandas","title":"to_pandas  <code>async</code>","text":"<pre><code>to_pandas(flatten: Optional[Union[int, bool]] = None, timeout: Optional[timedelta] = None) -&gt; 'pd.DataFrame'\n</code></pre> <p>Execute the query and collect the results into a pandas DataFrame.</p> <p>This method will collect all results into memory before returning.  If you expect a large number of results, you may want to use to_batches and convert each batch to pandas separately.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import asyncio\n&gt;&gt;&gt; from lancedb import connect_async\n&gt;&gt;&gt; async def doctest_example():\n...     conn = await connect_async(\"./.lancedb\")\n...     table = await conn.create_table(\"my_table\", data=[{\"a\": 1, \"b\": 2}])\n...     async for batch in await table.query().to_batches():\n...         batch_df = batch.to_pandas()\n&gt;&gt;&gt; asyncio.run(doctest_example())\n</code></pre> <p>Parameters:</p> <ul> <li> <code>flatten</code>               (<code>Optional[Union[int, bool]]</code>, default:                   <code>None</code> )           \u2013            <p>If flatten is True, flatten all nested columns. If flatten is an integer, flatten the nested columns up to the specified depth. If unspecified, do not flatten the nested columns.</p> </li> <li> <code>timeout</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum time to wait for the query to complete. If not specified, no timeout is applied. If the query does not complete within the specified time, an error will be raised.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>async def to_pandas(\n    self,\n    flatten: Optional[Union[int, bool]] = None,\n    timeout: Optional[timedelta] = None,\n) -&gt; \"pd.DataFrame\":\n    \"\"\"\n    Execute the query and collect the results into a pandas DataFrame.\n\n    This method will collect all results into memory before returning.  If you\n    expect a large number of results, you may want to use\n    [to_batches][lancedb.query.AsyncQueryBase.to_batches] and convert each batch to\n    pandas separately.\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; import asyncio\n    &gt;&gt;&gt; from lancedb import connect_async\n    &gt;&gt;&gt; async def doctest_example():\n    ...     conn = await connect_async(\"./.lancedb\")\n    ...     table = await conn.create_table(\"my_table\", data=[{\"a\": 1, \"b\": 2}])\n    ...     async for batch in await table.query().to_batches():\n    ...         batch_df = batch.to_pandas()\n    &gt;&gt;&gt; asyncio.run(doctest_example())\n\n    Parameters\n    ----------\n    flatten: Optional[Union[int, bool]]\n        If flatten is True, flatten all nested columns.\n        If flatten is an integer, flatten the nested columns up to the\n        specified depth.\n        If unspecified, do not flatten the nested columns.\n    timeout: Optional[timedelta]\n        The maximum time to wait for the query to complete.\n        If not specified, no timeout is applied. If the query does not\n        complete within the specified time, an error will be raised.\n    \"\"\"\n    return (\n        flatten_columns(await self.to_arrow(timeout=timeout), flatten)\n    ).to_pandas()\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncFTSQuery.to_polars","title":"to_polars  <code>async</code>","text":"<pre><code>to_polars(timeout: Optional[timedelta] = None) -&gt; 'pl.DataFrame'\n</code></pre> <p>Execute the query and collect the results into a Polars DataFrame.</p> <p>This method will collect all results into memory before returning.  If you expect a large number of results, you may want to use to_batches and convert each batch to polars separately.</p> <p>Parameters:</p> <ul> <li> <code>timeout</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum time to wait for the query to complete. If not specified, no timeout is applied. If the query does not complete within the specified time, an error will be raised.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import asyncio\n&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from lancedb import connect_async\n&gt;&gt;&gt; async def doctest_example():\n...     conn = await connect_async(\"./.lancedb\")\n...     table = await conn.create_table(\"my_table\", data=[{\"a\": 1, \"b\": 2}])\n...     async for batch in await table.query().to_batches():\n...         batch_df = pl.from_arrow(batch)\n&gt;&gt;&gt; asyncio.run(doctest_example())\n</code></pre> Source code in <code>lancedb/query.py</code> <pre><code>async def to_polars(\n    self,\n    timeout: Optional[timedelta] = None,\n) -&gt; \"pl.DataFrame\":\n    \"\"\"\n    Execute the query and collect the results into a Polars DataFrame.\n\n    This method will collect all results into memory before returning.  If you\n    expect a large number of results, you may want to use\n    [to_batches][lancedb.query.AsyncQueryBase.to_batches] and convert each batch to\n    polars separately.\n\n    Parameters\n    ----------\n    timeout: Optional[timedelta]\n        The maximum time to wait for the query to complete.\n        If not specified, no timeout is applied. If the query does not\n        complete within the specified time, an error will be raised.\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; import asyncio\n    &gt;&gt;&gt; import polars as pl\n    &gt;&gt;&gt; from lancedb import connect_async\n    &gt;&gt;&gt; async def doctest_example():\n    ...     conn = await connect_async(\"./.lancedb\")\n    ...     table = await conn.create_table(\"my_table\", data=[{\"a\": 1, \"b\": 2}])\n    ...     async for batch in await table.query().to_batches():\n    ...         batch_df = pl.from_arrow(batch)\n    &gt;&gt;&gt; asyncio.run(doctest_example())\n    \"\"\"\n    import polars as pl\n\n    return pl.from_arrow(await self.to_arrow(timeout=timeout))\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncFTSQuery.explain_plan","title":"explain_plan  <code>async</code>","text":"<pre><code>explain_plan(verbose: Optional[bool] = False)\n</code></pre> <p>Return the execution plan for this query.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import asyncio\n&gt;&gt;&gt; from lancedb import connect_async\n&gt;&gt;&gt; async def doctest_example():\n...     conn = await connect_async(\"./.lancedb\")\n...     table = await conn.create_table(\"my_table\", [{\"vector\": [99, 99]}])\n...     query = [100, 100]\n...     plan = await table.query().nearest_to([1, 2]).explain_plan(True)\n...     print(plan)\n&gt;&gt;&gt; asyncio.run(doctest_example())\nProjectionExec: expr=[vector@0 as vector, _distance@2 as _distance]\n  GlobalLimitExec: skip=0, fetch=10\n    FilterExec: _distance@2 IS NOT NULL\n      SortExec: TopK(fetch=10), expr=[_distance@2 ASC NULLS LAST], preserve_partitioning=[false]\n        KNNVectorDistance: metric=l2\n          LanceScan: uri=..., projection=[vector], row_id=true, row_addr=false, ordered=false\n</code></pre> <p>Parameters:</p> <ul> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Use a verbose output format.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>plan</code> (              <code>str</code> )          \u2013            </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>async def explain_plan(self, verbose: Optional[bool] = False):\n    \"\"\"Return the execution plan for this query.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import asyncio\n    &gt;&gt;&gt; from lancedb import connect_async\n    &gt;&gt;&gt; async def doctest_example():\n    ...     conn = await connect_async(\"./.lancedb\")\n    ...     table = await conn.create_table(\"my_table\", [{\"vector\": [99, 99]}])\n    ...     query = [100, 100]\n    ...     plan = await table.query().nearest_to([1, 2]).explain_plan(True)\n    ...     print(plan)\n    &gt;&gt;&gt; asyncio.run(doctest_example()) # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n    ProjectionExec: expr=[vector@0 as vector, _distance@2 as _distance]\n      GlobalLimitExec: skip=0, fetch=10\n        FilterExec: _distance@2 IS NOT NULL\n          SortExec: TopK(fetch=10), expr=[_distance@2 ASC NULLS LAST], preserve_partitioning=[false]\n            KNNVectorDistance: metric=l2\n              LanceScan: uri=..., projection=[vector], row_id=true, row_addr=false, ordered=false\n\n    Parameters\n    ----------\n    verbose : bool, default False\n        Use a verbose output format.\n\n    Returns\n    -------\n    plan : str\n    \"\"\"  # noqa: E501\n    return await self._inner.explain_plan(verbose)\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncFTSQuery.analyze_plan","title":"analyze_plan  <code>async</code>","text":"<pre><code>analyze_plan()\n</code></pre> <p>Execute the query and display with runtime metrics.</p> <p>Returns:</p> <ul> <li> <code>plan</code> (              <code>str</code> )          \u2013            </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>async def analyze_plan(self):\n    \"\"\"Execute the query and display with runtime metrics.\n\n    Returns\n    -------\n    plan : str\n    \"\"\"\n    return await self._inner.analyze_plan()\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncFTSQuery.nearest_to","title":"nearest_to","text":"<pre><code>nearest_to(query_vector: Union[VEC, Tuple, List[VEC]]) -&gt; AsyncHybridQuery\n</code></pre> <p>In addition doing text search on the LanceDB Table, also find the nearest vectors to the given query vector.</p> <p>This converts the query from a FTS Query to a Hybrid query. Results from the vector search will be combined with results from the FTS query.</p> <p>This method will attempt to convert the input to the query vector expected by the embedding model.  If the input cannot be converted then an error will be thrown.</p> <p>By default, there is no embedding model, and the input should be something that can be converted to a pyarrow array of floats.  This includes lists, numpy arrays, and tuples.</p> <p>If there is only one vector column (a column whose data type is a fixed size list of floats) then the column does not need to be specified. If there is more than one vector column you must use AsyncVectorQuery.column to specify which column you would like to compare with.</p> <p>If no index has been created on the vector column then a vector query will perform a distance comparison between the query vector and every vector in the database and then sort the results.  This is sometimes called a \"flat search\"</p> <p>For small databases, with tens of thousands of vectors or less, this can be reasonably fast.  In larger databases you should create a vector index on the column.  If there is a vector index then an \"approximate\" nearest neighbor search (frequently called an ANN search) will be performed.  This search is much faster, but the results will be approximate.</p> <p>The query can be further parameterized using the returned builder.  There are various ANN search parameters that will let you fine tune your recall accuracy vs search latency.</p> <p>Hybrid searches always have a limit.  If <code>limit</code> has not been called then a default <code>limit</code> of 10 will be used.</p> <p>Typically, a single vector is passed in as the query. However, you can also pass in multiple vectors.  This can be useful if you want to find the nearest vectors to multiple query vectors. This is not expected to be faster than making multiple queries concurrently; it is just a convenience method. If multiple vectors are passed in then an additional column <code>query_index</code> will be added to the results.  This column will contain the index of the query vector that the result is nearest to.</p> Source code in <code>lancedb/query.py</code> <pre><code>def nearest_to(\n    self,\n    query_vector: Union[VEC, Tuple, List[VEC]],\n) -&gt; AsyncHybridQuery:\n    \"\"\"\n    In addition doing text search on the LanceDB Table, also\n    find the nearest vectors to the given query vector.\n\n    This converts the query from a FTS Query to a Hybrid query. Results\n    from the vector search will be combined with results from the FTS query.\n\n    This method will attempt to convert the input to the query vector\n    expected by the embedding model.  If the input cannot be converted\n    then an error will be thrown.\n\n    By default, there is no embedding model, and the input should be\n    something that can be converted to a pyarrow array of floats.  This\n    includes lists, numpy arrays, and tuples.\n\n    If there is only one vector column (a column whose data type is a\n    fixed size list of floats) then the column does not need to be specified.\n    If there is more than one vector column you must use\n    [AsyncVectorQuery.column][lancedb.query.AsyncVectorQuery.column] to specify\n    which column you would like to compare with.\n\n    If no index has been created on the vector column then a vector query\n    will perform a distance comparison between the query vector and every\n    vector in the database and then sort the results.  This is sometimes\n    called a \"flat search\"\n\n    For small databases, with tens of thousands of vectors or less, this can\n    be reasonably fast.  In larger databases you should create a vector index\n    on the column.  If there is a vector index then an \"approximate\" nearest\n    neighbor search (frequently called an ANN search) will be performed.  This\n    search is much faster, but the results will be approximate.\n\n    The query can be further parameterized using the returned builder.  There\n    are various ANN search parameters that will let you fine tune your recall\n    accuracy vs search latency.\n\n    Hybrid searches always have a [limit][].  If `limit` has not been called then\n    a default `limit` of 10 will be used.\n\n    Typically, a single vector is passed in as the query. However, you can also\n    pass in multiple vectors.  This can be useful if you want to find the nearest\n    vectors to multiple query vectors. This is not expected to be faster than\n    making multiple queries concurrently; it is just a convenience method.\n    If multiple vectors are passed in then an additional column `query_index`\n    will be added to the results.  This column will contain the index of the\n    query vector that the result is nearest to.\n    \"\"\"\n    if query_vector is None:\n        raise ValueError(\"query_vector can not be None\")\n\n    if (\n        isinstance(query_vector, list)\n        and len(query_vector) &gt; 0\n        and not isinstance(query_vector[0], (float, int))\n    ):\n        # multiple have been passed\n        query_vectors = [AsyncQuery._query_vec_to_array(v) for v in query_vector]\n        new_self = self._inner.nearest_to(query_vectors[0])\n        for v in query_vectors[1:]:\n            new_self.add_query_vector(v)\n        return AsyncHybridQuery(new_self)\n    else:\n        return AsyncHybridQuery(\n            self._inner.nearest_to(AsyncQuery._query_vec_to_array(query_vector))\n        )\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncHybridQuery","title":"lancedb.query.AsyncHybridQuery","text":"<p>               Bases: <code>AsyncQueryBase</code>, <code>AsyncVectorQueryBase</code></p> <p>A query builder that performs hybrid vector and full text search. Results are combined and reranked based on the specified reranker. By default, the results are reranked using the RRFReranker, which uses reciprocal rank fusion score for reranking.</p> <p>To make the vector and fts results comparable, the scores are normalized. Instead of normalizing scores, the <code>normalize</code> parameter can be set to \"rank\" in the <code>rerank</code> method to convert the scores to ranks and then normalize them.</p> Source code in <code>lancedb/query.py</code> <pre><code>class AsyncHybridQuery(AsyncQueryBase, AsyncVectorQueryBase):\n    \"\"\"\n    A query builder that performs hybrid vector and full text search.\n    Results are combined and reranked based on the specified reranker.\n    By default, the results are reranked using the RRFReranker, which\n    uses reciprocal rank fusion score for reranking.\n\n    To make the vector and fts results comparable, the scores are normalized.\n    Instead of normalizing scores, the `normalize` parameter can be set to \"rank\"\n    in the `rerank` method to convert the scores to ranks and then normalize them.\n    \"\"\"\n\n    def __init__(self, inner: LanceHybridQuery):\n        super().__init__(inner)\n        self._inner = inner\n        self._norm = \"score\"\n        self._reranker = RRFReranker()\n\n    def rerank(\n        self, reranker: Reranker = RRFReranker(), normalize: str = \"score\"\n    ) -&gt; AsyncHybridQuery:\n        \"\"\"\n        Rerank the hybrid search results using the specified reranker. The reranker\n        must be an instance of Reranker class.\n\n        Parameters\n        ----------\n        reranker: Reranker, default RRFReranker()\n            The reranker to use. Must be an instance of Reranker class.\n        normalize: str, default \"score\"\n            The method to normalize the scores. Can be \"rank\" or \"score\". If \"rank\",\n            the scores are converted to ranks and then normalized. If \"score\", the\n            scores are normalized directly.\n        Returns\n        -------\n        AsyncHybridQuery\n            The AsyncHybridQuery object.\n        \"\"\"\n        if normalize not in [\"rank\", \"score\"]:\n            raise ValueError(\"normalize must be 'rank' or 'score'.\")\n        if reranker and not isinstance(reranker, Reranker):\n            raise ValueError(\"reranker must be an instance of Reranker class.\")\n\n        self._norm = normalize\n        self._reranker = reranker\n\n        return self\n\n    async def to_batches(\n        self,\n        *,\n        max_batch_length: Optional[int] = None,\n        timeout: Optional[timedelta] = None,\n    ) -&gt; AsyncRecordBatchReader:\n        fts_query = AsyncFTSQuery(self._inner.to_fts_query())\n        vec_query = AsyncVectorQuery(self._inner.to_vector_query())\n\n        # save the row ID choice that was made on the query builder and force it\n        # to actually fetch the row ids because we need this for reranking\n        with_row_ids = self._inner.get_with_row_id()\n        fts_query.with_row_id()\n        vec_query.with_row_id()\n\n        fts_results, vector_results = await asyncio.gather(\n            fts_query.to_arrow(timeout=timeout),\n            vec_query.to_arrow(timeout=timeout),\n        )\n\n        result = LanceHybridQueryBuilder._combine_hybrid_results(\n            fts_results=fts_results,\n            vector_results=vector_results,\n            norm=self._norm,\n            fts_query=fts_query.get_query(),\n            reranker=self._reranker,\n            limit=self._inner.get_limit(),\n            with_row_ids=with_row_ids,\n        )\n\n        return AsyncRecordBatchReader(result, max_batch_length=max_batch_length)\n\n    async def explain_plan(self, verbose: Optional[bool] = False):\n        \"\"\"Return the execution plan for this query.\n\n        The output includes both the vector and FTS search plans.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import asyncio\n        &gt;&gt;&gt; from lancedb import connect_async\n        &gt;&gt;&gt; from lancedb.index import FTS\n        &gt;&gt;&gt; async def doctest_example():\n        ...     conn = await connect_async(\"./.lancedb\")\n        ...     table = await conn.create_table(\"my_table\", [{\"vector\": [99, 99], \"text\": \"hello world\"}])\n        ...     await table.create_index(\"text\", config=FTS(with_position=False))\n        ...     query = [100, 100]\n        ...     plan = await table.query().nearest_to([1, 2]).nearest_to_text(\"hello\").explain_plan(True)\n        ...     print(plan)\n        &gt;&gt;&gt; asyncio.run(doctest_example()) # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n        Vector Search Plan:\n        ProjectionExec: expr=[vector@0 as vector, text@3 as text, _distance@2 as _distance]\n            Take: columns=\"vector, _rowid, _distance, (text)\"\n                CoalesceBatchesExec: target_batch_size=1024\n                GlobalLimitExec: skip=0, fetch=10\n                    FilterExec: _distance@2 IS NOT NULL\n                    SortExec: TopK(fetch=10), expr=[_distance@2 ASC NULLS LAST], preserve_partitioning=[false]\n                        KNNVectorDistance: metric=l2\n                        LanceScan: uri=..., projection=[vector], row_id=true, row_addr=false, ordered=false\n        FTS Search Plan:\n        LanceScan: uri=..., projection=[vector, text], row_id=false, row_addr=false, ordered=true\n\n        Parameters\n        ----------\n        verbose : bool, default False\n            Use a verbose output format.\n\n        Returns\n        -------\n        plan : str\n        \"\"\"  # noqa: E501\n\n        results = [\"Vector Search Plan:\"]\n        results.append(await self._inner.to_vector_query().explain_plan(verbose))\n        results.append(\"FTS Search Plan:\")\n        results.append(await self._inner.to_fts_query().explain_plan(verbose))\n\n        return \"\\n\".join(results)\n\n    async def analyze_plan(self):\n        \"\"\"\n        Execute the query and return the physical execution plan with runtime metrics.\n\n        This runs both the vector and FTS (full-text search) queries and returns\n        detailed metrics for each step of execution\u2014such as rows processed,\n        elapsed time, I/O stats, and more. It\u2019s useful for debugging and\n        performance analysis.\n\n        Returns\n        -------\n        plan : str\n        \"\"\"\n        results = [\"Vector Search Query:\"]\n        results.append(await self._inner.to_vector_query().analyze_plan())\n        results.append(\"FTS Search Query:\")\n        results.append(await self._inner.to_fts_query().analyze_plan())\n\n        return \"\\n\".join(results)\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncHybridQuery.column","title":"column","text":"<pre><code>column(column: str) -&gt; Self\n</code></pre> <p>Set the vector column to query</p> <p>This controls which column is compared to the query vector supplied in the call to AsyncQuery.nearest_to.</p> <p>This parameter must be specified if the table has more than one column whose data type is a fixed-size-list of floats.</p> Source code in <code>lancedb/query.py</code> <pre><code>def column(self, column: str) -&gt; Self:\n    \"\"\"\n    Set the vector column to query\n\n    This controls which column is compared to the query vector supplied in\n    the call to [AsyncQuery.nearest_to][lancedb.query.AsyncQuery.nearest_to].\n\n    This parameter must be specified if the table has more than one column\n    whose data type is a fixed-size-list of floats.\n    \"\"\"\n    self._inner.column(column)\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncHybridQuery.nprobes","title":"nprobes","text":"<pre><code>nprobes(nprobes: int) -&gt; Self\n</code></pre> <p>Set the number of partitions to search (probe)</p> <p>This argument is only used when the vector column has an IVF-based index. If there is no index then this value is ignored.</p> <p>The IVF stage of IVF PQ divides the input into partitions (clusters) of related values.</p> <p>The partition whose centroids are closest to the query vector will be exhaustiely searched to find matches.  This parameter controls how many partitions should be searched.</p> <p>Increasing this value will increase the recall of your query but will also increase the latency of your query.  The default value is 20.  This default is good for many cases but the best value to use will depend on your data and the recall that you need to achieve.</p> <p>For best results we recommend tuning this parameter with a benchmark against your actual data to find the smallest possible value that will still give you the desired recall.</p> Source code in <code>lancedb/query.py</code> <pre><code>def nprobes(self, nprobes: int) -&gt; Self:\n    \"\"\"\n    Set the number of partitions to search (probe)\n\n    This argument is only used when the vector column has an IVF-based index.\n    If there is no index then this value is ignored.\n\n    The IVF stage of IVF PQ divides the input into partitions (clusters) of\n    related values.\n\n    The partition whose centroids are closest to the query vector will be\n    exhaustiely searched to find matches.  This parameter controls how many\n    partitions should be searched.\n\n    Increasing this value will increase the recall of your query but will\n    also increase the latency of your query.  The default value is 20.  This\n    default is good for many cases but the best value to use will depend on\n    your data and the recall that you need to achieve.\n\n    For best results we recommend tuning this parameter with a benchmark against\n    your actual data to find the smallest possible value that will still give\n    you the desired recall.\n    \"\"\"\n    self._inner.nprobes(nprobes)\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncHybridQuery.distance_range","title":"distance_range","text":"<pre><code>distance_range(lower_bound: Optional[float] = None, upper_bound: Optional[float] = None) -&gt; Self\n</code></pre> <p>Set the distance range to use.</p> <p>Only rows with distances within range [lower_bound, upper_bound) will be returned.</p> <p>Parameters:</p> <ul> <li> <code>lower_bound</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>The lower bound of the distance range.</p> </li> <li> <code>upper_bound</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>The upper bound of the distance range.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AsyncVectorQuery</code>           \u2013            <p>The AsyncVectorQuery object.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def distance_range(\n    self, lower_bound: Optional[float] = None, upper_bound: Optional[float] = None\n) -&gt; Self:\n    \"\"\"Set the distance range to use.\n\n    Only rows with distances within range [lower_bound, upper_bound)\n    will be returned.\n\n    Parameters\n    ----------\n    lower_bound: Optional[float]\n        The lower bound of the distance range.\n    upper_bound: Optional[float]\n        The upper bound of the distance range.\n\n    Returns\n    -------\n    AsyncVectorQuery\n        The AsyncVectorQuery object.\n    \"\"\"\n    self._inner.distance_range(lower_bound, upper_bound)\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncHybridQuery.ef","title":"ef","text":"<pre><code>ef(ef: int) -&gt; Self\n</code></pre> <p>Set the number of candidates to consider during search</p> <p>This argument is only used when the vector column has an HNSW index. If there is no index then this value is ignored.</p> <p>Increasing this value will increase the recall of your query but will also increase the latency of your query.  The default value is 1.5 * limit.  This default is good for many cases but the best value to use will depend on your data and the recall that you need to achieve.</p> Source code in <code>lancedb/query.py</code> <pre><code>def ef(self, ef: int) -&gt; Self:\n    \"\"\"\n    Set the number of candidates to consider during search\n\n    This argument is only used when the vector column has an HNSW index.\n    If there is no index then this value is ignored.\n\n    Increasing this value will increase the recall of your query but will also\n    increase the latency of your query.  The default value is 1.5 * limit.  This\n    default is good for many cases but the best value to use will depend on your\n    data and the recall that you need to achieve.\n    \"\"\"\n    self._inner.ef(ef)\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncHybridQuery.refine_factor","title":"refine_factor","text":"<pre><code>refine_factor(refine_factor: int) -&gt; Self\n</code></pre> <p>A multiplier to control how many additional rows are taken during the refine step</p> <p>This argument is only used when the vector column has an IVF PQ index. If there is no index then this value is ignored.</p> <p>An IVF PQ index stores compressed (quantized) values.  They query vector is compared against these values and, since they are compressed, the comparison is inaccurate.</p> <p>This parameter can be used to refine the results.  It can improve both improve recall and correct the ordering of the nearest results.</p> <p>To refine results LanceDb will first perform an ANN search to find the nearest <code>limit</code> * <code>refine_factor</code> results.  In other words, if <code>refine_factor</code> is 3 and <code>limit</code> is the default (10) then the first 30 results will be selected.  LanceDb then fetches the full, uncompressed, values for these 30 results.  The results are then reordered by the true distance and only the nearest 10 are kept.</p> <p>Note: there is a difference between calling this method with a value of 1 and never calling this method at all.  Calling this method with any value will have an impact on your search latency.  When you call this method with a <code>refine_factor</code> of 1 then LanceDb still needs to fetch the full, uncompressed, values so that it can potentially reorder the results.</p> <p>Note: if this method is NOT called then the distances returned in the _distance column will be approximate distances based on the comparison of the quantized query vector and the quantized result vectors.  This can be considerably different than the true distance between the query vector and the actual uncompressed vector.</p> Source code in <code>lancedb/query.py</code> <pre><code>def refine_factor(self, refine_factor: int) -&gt; Self:\n    \"\"\"\n    A multiplier to control how many additional rows are taken during the refine\n    step\n\n    This argument is only used when the vector column has an IVF PQ index.\n    If there is no index then this value is ignored.\n\n    An IVF PQ index stores compressed (quantized) values.  They query vector is\n    compared against these values and, since they are compressed, the comparison is\n    inaccurate.\n\n    This parameter can be used to refine the results.  It can improve both improve\n    recall and correct the ordering of the nearest results.\n\n    To refine results LanceDb will first perform an ANN search to find the nearest\n    `limit` * `refine_factor` results.  In other words, if `refine_factor` is 3 and\n    `limit` is the default (10) then the first 30 results will be selected.  LanceDb\n    then fetches the full, uncompressed, values for these 30 results.  The results\n    are then reordered by the true distance and only the nearest 10 are kept.\n\n    Note: there is a difference between calling this method with a value of 1 and\n    never calling this method at all.  Calling this method with any value will have\n    an impact on your search latency.  When you call this method with a\n    `refine_factor` of 1 then LanceDb still needs to fetch the full, uncompressed,\n    values so that it can potentially reorder the results.\n\n    Note: if this method is NOT called then the distances returned in the _distance\n    column will be approximate distances based on the comparison of the quantized\n    query vector and the quantized result vectors.  This can be considerably\n    different than the true distance between the query vector and the actual\n    uncompressed vector.\n    \"\"\"\n    self._inner.refine_factor(refine_factor)\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncHybridQuery.distance_type","title":"distance_type","text":"<pre><code>distance_type(distance_type: str) -&gt; Self\n</code></pre> <p>Set the distance metric to use</p> <p>When performing a vector search we try and find the \"nearest\" vectors according to some kind of distance metric.  This parameter controls which distance metric to use.  See @see {@link IvfPqOptions.distanceType} for more details on the different distance metrics available.</p> <p>Note: if there is a vector index then the distance type used MUST match the distance type used to train the vector index.  If this is not done then the results will be invalid.</p> <p>By default \"l2\" is used.</p> Source code in <code>lancedb/query.py</code> <pre><code>def distance_type(self, distance_type: str) -&gt; Self:\n    \"\"\"\n    Set the distance metric to use\n\n    When performing a vector search we try and find the \"nearest\" vectors according\n    to some kind of distance metric.  This parameter controls which distance metric\n    to use.  See @see {@link IvfPqOptions.distanceType} for more details on the\n    different distance metrics available.\n\n    Note: if there is a vector index then the distance type used MUST match the\n    distance type used to train the vector index.  If this is not done then the\n    results will be invalid.\n\n    By default \"l2\" is used.\n    \"\"\"\n    self._inner.distance_type(distance_type)\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncHybridQuery.bypass_vector_index","title":"bypass_vector_index","text":"<pre><code>bypass_vector_index() -&gt; Self\n</code></pre> <p>If this is called then any vector index is skipped</p> <p>An exhaustive (flat) search will be performed.  The query vector will be compared to every vector in the table.  At high scales this can be expensive.  However, this is often still useful.  For example, skipping the vector index can give you ground truth results which you can use to calculate your recall to select an appropriate value for nprobes.</p> Source code in <code>lancedb/query.py</code> <pre><code>def bypass_vector_index(self) -&gt; Self:\n    \"\"\"\n    If this is called then any vector index is skipped\n\n    An exhaustive (flat) search will be performed.  The query vector will\n    be compared to every vector in the table.  At high scales this can be\n    expensive.  However, this is often still useful.  For example, skipping\n    the vector index can give you ground truth results which you can use to\n    calculate your recall to select an appropriate value for nprobes.\n    \"\"\"\n    self._inner.bypass_vector_index()\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncHybridQuery.where","title":"where","text":"<pre><code>where(predicate: str) -&gt; Self\n</code></pre> <p>Only return rows matching the given predicate</p> <p>The predicate should be supplied as an SQL query string.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; predicate = \"x &gt; 10\"\n&gt;&gt;&gt; predicate = \"y &gt; 0 AND y &lt; 100\"\n&gt;&gt;&gt; predicate = \"x &gt; 5 OR y = 'test'\"\n</code></pre> <p>Filtering performance can often be improved by creating a scalar index on the filter column(s).</p> Source code in <code>lancedb/query.py</code> <pre><code>def where(self, predicate: str) -&gt; Self:\n    \"\"\"\n    Only return rows matching the given predicate\n\n    The predicate should be supplied as an SQL query string.\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; predicate = \"x &gt; 10\"\n    &gt;&gt;&gt; predicate = \"y &gt; 0 AND y &lt; 100\"\n    &gt;&gt;&gt; predicate = \"x &gt; 5 OR y = 'test'\"\n\n    Filtering performance can often be improved by creating a scalar index\n    on the filter column(s).\n    \"\"\"\n    self._inner.where(predicate)\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncHybridQuery.select","title":"select","text":"<pre><code>select(columns: Union[List[str], dict[str, str]]) -&gt; Self\n</code></pre> <p>Return only the specified columns.</p> <p>By default a query will return all columns from the table.  However, this can have a very significant impact on latency.  LanceDb stores data in a columnar fashion.  This means we can finely tune our I/O to select exactly the columns we need.</p> <p>As a best practice you should always limit queries to the columns that you need. If you pass in a list of column names then only those columns will be returned.</p> <p>You can also use this method to create new \"dynamic\" columns based on your existing columns. For example, you may not care about \"a\" or \"b\" but instead simply want \"a + b\".  This is often seen in the SELECT clause of an SQL query (e.g. <code>SELECT a+b FROM my_table</code>).</p> <p>To create dynamic columns you can pass in a dict[str, str].  A column will be returned for each entry in the map.  The key provides the name of the column. The value is an SQL string used to specify how the column is calculated.</p> <p>For example, an SQL query might state <code>SELECT a + b AS combined, c</code>.  The equivalent input to this method would be <code>{\"combined\": \"a + b\", \"c\": \"c\"}</code>.</p> <p>Columns will always be returned in the order given, even if that order is different than the order used when adding the data.</p> Source code in <code>lancedb/query.py</code> <pre><code>def select(self, columns: Union[List[str], dict[str, str]]) -&gt; Self:\n    \"\"\"\n    Return only the specified columns.\n\n    By default a query will return all columns from the table.  However, this can\n    have a very significant impact on latency.  LanceDb stores data in a columnar\n    fashion.  This\n    means we can finely tune our I/O to select exactly the columns we need.\n\n    As a best practice you should always limit queries to the columns that you need.\n    If you pass in a list of column names then only those columns will be\n    returned.\n\n    You can also use this method to create new \"dynamic\" columns based on your\n    existing columns. For example, you may not care about \"a\" or \"b\" but instead\n    simply want \"a + b\".  This is often seen in the SELECT clause of an SQL query\n    (e.g. `SELECT a+b FROM my_table`).\n\n    To create dynamic columns you can pass in a dict[str, str].  A column will be\n    returned for each entry in the map.  The key provides the name of the column.\n    The value is an SQL string used to specify how the column is calculated.\n\n    For example, an SQL query might state `SELECT a + b AS combined, c`.  The\n    equivalent input to this method would be `{\"combined\": \"a + b\", \"c\": \"c\"}`.\n\n    Columns will always be returned in the order given, even if that order is\n    different than the order used when adding the data.\n    \"\"\"\n    if isinstance(columns, list) and all(isinstance(c, str) for c in columns):\n        self._inner.select_columns(columns)\n    elif isinstance(columns, dict) and all(\n        isinstance(k, str) and isinstance(v, str) for k, v in columns.items()\n    ):\n        self._inner.select(list(columns.items()))\n    else:\n        raise TypeError(\"columns must be a list of column names or a dict\")\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncHybridQuery.limit","title":"limit","text":"<pre><code>limit(limit: int) -&gt; Self\n</code></pre> <p>Set the maximum number of results to return.</p> <p>By default, a plain search has no limit.  If this method is not called then every valid row from the table will be returned.</p> Source code in <code>lancedb/query.py</code> <pre><code>def limit(self, limit: int) -&gt; Self:\n    \"\"\"\n    Set the maximum number of results to return.\n\n    By default, a plain search has no limit.  If this method is not\n    called then every valid row from the table will be returned.\n    \"\"\"\n    self._inner.limit(limit)\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncHybridQuery.offset","title":"offset","text":"<pre><code>offset(offset: int) -&gt; Self\n</code></pre> <p>Set the offset for the results.</p> <p>Parameters:</p> <ul> <li> <code>offset</code>               (<code>int</code>)           \u2013            <p>The offset to start fetching results from.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def offset(self, offset: int) -&gt; Self:\n    \"\"\"\n    Set the offset for the results.\n\n    Parameters\n    ----------\n    offset: int\n        The offset to start fetching results from.\n    \"\"\"\n    self._inner.offset(offset)\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncHybridQuery.fast_search","title":"fast_search","text":"<pre><code>fast_search() -&gt; Self\n</code></pre> <p>Skip searching un-indexed data.</p> <p>This can make queries faster, but will miss any data that has not been indexed.</p> <p>Tip</p> <p>You can add new data into an existing index by calling AsyncTable.optimize.</p> Source code in <code>lancedb/query.py</code> <pre><code>def fast_search(self) -&gt; Self:\n    \"\"\"\n    Skip searching un-indexed data.\n\n    This can make queries faster, but will miss any data that has not been\n    indexed.\n\n    !!! tip\n        You can add new data into an existing index by calling\n        [AsyncTable.optimize][lancedb.table.AsyncTable.optimize].\n    \"\"\"\n    self._inner.fast_search()\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncHybridQuery.with_row_id","title":"with_row_id","text":"<pre><code>with_row_id() -&gt; Self\n</code></pre> <p>Include the _rowid column in the results.</p> Source code in <code>lancedb/query.py</code> <pre><code>def with_row_id(self) -&gt; Self:\n    \"\"\"\n    Include the _rowid column in the results.\n    \"\"\"\n    self._inner.with_row_id()\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncHybridQuery.postfilter","title":"postfilter","text":"<pre><code>postfilter() -&gt; Self\n</code></pre> <p>If this is called then filtering will happen after the search instead of before. By default filtering will be performed before the search.  This is how filtering is typically understood to work.  This prefilter step does add some additional latency.  Creating a scalar index on the filter column(s) can often improve this latency.  However, sometimes a filter is too complex or scalar indices cannot be applied to the column.  In these cases postfiltering can be used instead of prefiltering to improve latency. Post filtering applies the filter to the results of the search.  This means we only run the filter on a much smaller set of data.  However, it can cause the query to return fewer than <code>limit</code> results (or even no results) if none of the nearest results match the filter. Post filtering happens during the \"refine stage\" (described in more detail in @see {@link VectorQuery#refineFactor}).  This means that setting a higher refine factor can often help restore some of the results lost by post filtering.</p> Source code in <code>lancedb/query.py</code> <pre><code>def postfilter(self) -&gt; Self:\n    \"\"\"\n    If this is called then filtering will happen after the search instead of\n    before.\n    By default filtering will be performed before the search.  This is how\n    filtering is typically understood to work.  This prefilter step does add some\n    additional latency.  Creating a scalar index on the filter column(s) can\n    often improve this latency.  However, sometimes a filter is too complex or\n    scalar indices cannot be applied to the column.  In these cases postfiltering\n    can be used instead of prefiltering to improve latency.\n    Post filtering applies the filter to the results of the search.  This\n    means we only run the filter on a much smaller set of data.  However, it can\n    cause the query to return fewer than `limit` results (or even no results) if\n    none of the nearest results match the filter.\n    Post filtering happens during the \"refine stage\" (described in more detail in\n    @see {@link VectorQuery#refineFactor}).  This means that setting a higher refine\n    factor can often help restore some of the results lost by post filtering.\n    \"\"\"\n    self._inner.postfilter()\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncHybridQuery.to_arrow","title":"to_arrow  <code>async</code>","text":"<pre><code>to_arrow(timeout: Optional[timedelta] = None) -&gt; Table\n</code></pre> <p>Execute the query and collect the results into an Apache Arrow Table.</p> <p>This method will collect all results into memory before returning.  If you expect a large number of results, you may want to use to_batches</p> <p>Parameters:</p> <ul> <li> <code>timeout</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum time to wait for the query to complete. If not specified, no timeout is applied. If the query does not complete within the specified time, an error will be raised.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>async def to_arrow(self, timeout: Optional[timedelta] = None) -&gt; pa.Table:\n    \"\"\"\n    Execute the query and collect the results into an Apache Arrow Table.\n\n    This method will collect all results into memory before returning.  If\n    you expect a large number of results, you may want to use\n    [to_batches][lancedb.query.AsyncQueryBase.to_batches]\n\n    Parameters\n    ----------\n    timeout: Optional[timedelta]\n        The maximum time to wait for the query to complete.\n        If not specified, no timeout is applied. If the query does not\n        complete within the specified time, an error will be raised.\n    \"\"\"\n    batch_iter = await self.to_batches(timeout=timeout)\n    return pa.Table.from_batches(\n        await batch_iter.read_all(), schema=batch_iter.schema\n    )\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncHybridQuery.to_list","title":"to_list  <code>async</code>","text":"<pre><code>to_list(timeout: Optional[timedelta] = None) -&gt; List[dict]\n</code></pre> <p>Execute the query and return the results as a list of dictionaries.</p> <p>Each list entry is a dictionary with the selected column names as keys, or all table columns if <code>select</code> is not called. The vector and the \"_distance\" fields are returned whether or not they're explicitly selected.</p> <p>Parameters:</p> <ul> <li> <code>timeout</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum time to wait for the query to complete. If not specified, no timeout is applied. If the query does not complete within the specified time, an error will be raised.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>async def to_list(self, timeout: Optional[timedelta] = None) -&gt; List[dict]:\n    \"\"\"\n    Execute the query and return the results as a list of dictionaries.\n\n    Each list entry is a dictionary with the selected column names as keys,\n    or all table columns if `select` is not called. The vector and the \"_distance\"\n    fields are returned whether or not they're explicitly selected.\n\n    Parameters\n    ----------\n    timeout: Optional[timedelta]\n        The maximum time to wait for the query to complete.\n        If not specified, no timeout is applied. If the query does not\n        complete within the specified time, an error will be raised.\n    \"\"\"\n    return (await self.to_arrow(timeout=timeout)).to_pylist()\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncHybridQuery.to_pandas","title":"to_pandas  <code>async</code>","text":"<pre><code>to_pandas(flatten: Optional[Union[int, bool]] = None, timeout: Optional[timedelta] = None) -&gt; 'pd.DataFrame'\n</code></pre> <p>Execute the query and collect the results into a pandas DataFrame.</p> <p>This method will collect all results into memory before returning.  If you expect a large number of results, you may want to use to_batches and convert each batch to pandas separately.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import asyncio\n&gt;&gt;&gt; from lancedb import connect_async\n&gt;&gt;&gt; async def doctest_example():\n...     conn = await connect_async(\"./.lancedb\")\n...     table = await conn.create_table(\"my_table\", data=[{\"a\": 1, \"b\": 2}])\n...     async for batch in await table.query().to_batches():\n...         batch_df = batch.to_pandas()\n&gt;&gt;&gt; asyncio.run(doctest_example())\n</code></pre> <p>Parameters:</p> <ul> <li> <code>flatten</code>               (<code>Optional[Union[int, bool]]</code>, default:                   <code>None</code> )           \u2013            <p>If flatten is True, flatten all nested columns. If flatten is an integer, flatten the nested columns up to the specified depth. If unspecified, do not flatten the nested columns.</p> </li> <li> <code>timeout</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum time to wait for the query to complete. If not specified, no timeout is applied. If the query does not complete within the specified time, an error will be raised.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>async def to_pandas(\n    self,\n    flatten: Optional[Union[int, bool]] = None,\n    timeout: Optional[timedelta] = None,\n) -&gt; \"pd.DataFrame\":\n    \"\"\"\n    Execute the query and collect the results into a pandas DataFrame.\n\n    This method will collect all results into memory before returning.  If you\n    expect a large number of results, you may want to use\n    [to_batches][lancedb.query.AsyncQueryBase.to_batches] and convert each batch to\n    pandas separately.\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; import asyncio\n    &gt;&gt;&gt; from lancedb import connect_async\n    &gt;&gt;&gt; async def doctest_example():\n    ...     conn = await connect_async(\"./.lancedb\")\n    ...     table = await conn.create_table(\"my_table\", data=[{\"a\": 1, \"b\": 2}])\n    ...     async for batch in await table.query().to_batches():\n    ...         batch_df = batch.to_pandas()\n    &gt;&gt;&gt; asyncio.run(doctest_example())\n\n    Parameters\n    ----------\n    flatten: Optional[Union[int, bool]]\n        If flatten is True, flatten all nested columns.\n        If flatten is an integer, flatten the nested columns up to the\n        specified depth.\n        If unspecified, do not flatten the nested columns.\n    timeout: Optional[timedelta]\n        The maximum time to wait for the query to complete.\n        If not specified, no timeout is applied. If the query does not\n        complete within the specified time, an error will be raised.\n    \"\"\"\n    return (\n        flatten_columns(await self.to_arrow(timeout=timeout), flatten)\n    ).to_pandas()\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncHybridQuery.to_polars","title":"to_polars  <code>async</code>","text":"<pre><code>to_polars(timeout: Optional[timedelta] = None) -&gt; 'pl.DataFrame'\n</code></pre> <p>Execute the query and collect the results into a Polars DataFrame.</p> <p>This method will collect all results into memory before returning.  If you expect a large number of results, you may want to use to_batches and convert each batch to polars separately.</p> <p>Parameters:</p> <ul> <li> <code>timeout</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum time to wait for the query to complete. If not specified, no timeout is applied. If the query does not complete within the specified time, an error will be raised.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import asyncio\n&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from lancedb import connect_async\n&gt;&gt;&gt; async def doctest_example():\n...     conn = await connect_async(\"./.lancedb\")\n...     table = await conn.create_table(\"my_table\", data=[{\"a\": 1, \"b\": 2}])\n...     async for batch in await table.query().to_batches():\n...         batch_df = pl.from_arrow(batch)\n&gt;&gt;&gt; asyncio.run(doctest_example())\n</code></pre> Source code in <code>lancedb/query.py</code> <pre><code>async def to_polars(\n    self,\n    timeout: Optional[timedelta] = None,\n) -&gt; \"pl.DataFrame\":\n    \"\"\"\n    Execute the query and collect the results into a Polars DataFrame.\n\n    This method will collect all results into memory before returning.  If you\n    expect a large number of results, you may want to use\n    [to_batches][lancedb.query.AsyncQueryBase.to_batches] and convert each batch to\n    polars separately.\n\n    Parameters\n    ----------\n    timeout: Optional[timedelta]\n        The maximum time to wait for the query to complete.\n        If not specified, no timeout is applied. If the query does not\n        complete within the specified time, an error will be raised.\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; import asyncio\n    &gt;&gt;&gt; import polars as pl\n    &gt;&gt;&gt; from lancedb import connect_async\n    &gt;&gt;&gt; async def doctest_example():\n    ...     conn = await connect_async(\"./.lancedb\")\n    ...     table = await conn.create_table(\"my_table\", data=[{\"a\": 1, \"b\": 2}])\n    ...     async for batch in await table.query().to_batches():\n    ...         batch_df = pl.from_arrow(batch)\n    &gt;&gt;&gt; asyncio.run(doctest_example())\n    \"\"\"\n    import polars as pl\n\n    return pl.from_arrow(await self.to_arrow(timeout=timeout))\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncHybridQuery.rerank","title":"rerank","text":"<pre><code>rerank(reranker: Reranker = RRFReranker(), normalize: str = 'score') -&gt; AsyncHybridQuery\n</code></pre> <p>Rerank the hybrid search results using the specified reranker. The reranker must be an instance of Reranker class.</p> <p>Parameters:</p> <ul> <li> <code>reranker</code>               (<code>Reranker</code>, default:                   <code>RRFReranker()</code> )           \u2013            <p>The reranker to use. Must be an instance of Reranker class.</p> </li> <li> <code>normalize</code>               (<code>str</code>, default:                   <code>'score'</code> )           \u2013            <p>The method to normalize the scores. Can be \"rank\" or \"score\". If \"rank\", the scores are converted to ranks and then normalized. If \"score\", the scores are normalized directly.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AsyncHybridQuery</code>           \u2013            <p>The AsyncHybridQuery object.</p> </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>def rerank(\n    self, reranker: Reranker = RRFReranker(), normalize: str = \"score\"\n) -&gt; AsyncHybridQuery:\n    \"\"\"\n    Rerank the hybrid search results using the specified reranker. The reranker\n    must be an instance of Reranker class.\n\n    Parameters\n    ----------\n    reranker: Reranker, default RRFReranker()\n        The reranker to use. Must be an instance of Reranker class.\n    normalize: str, default \"score\"\n        The method to normalize the scores. Can be \"rank\" or \"score\". If \"rank\",\n        the scores are converted to ranks and then normalized. If \"score\", the\n        scores are normalized directly.\n    Returns\n    -------\n    AsyncHybridQuery\n        The AsyncHybridQuery object.\n    \"\"\"\n    if normalize not in [\"rank\", \"score\"]:\n        raise ValueError(\"normalize must be 'rank' or 'score'.\")\n    if reranker and not isinstance(reranker, Reranker):\n        raise ValueError(\"reranker must be an instance of Reranker class.\")\n\n    self._norm = normalize\n    self._reranker = reranker\n\n    return self\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncHybridQuery.explain_plan","title":"explain_plan  <code>async</code>","text":"<pre><code>explain_plan(verbose: Optional[bool] = False)\n</code></pre> <p>Return the execution plan for this query.</p> <p>The output includes both the vector and FTS search plans.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import asyncio\n&gt;&gt;&gt; from lancedb import connect_async\n&gt;&gt;&gt; from lancedb.index import FTS\n&gt;&gt;&gt; async def doctest_example():\n...     conn = await connect_async(\"./.lancedb\")\n...     table = await conn.create_table(\"my_table\", [{\"vector\": [99, 99], \"text\": \"hello world\"}])\n...     await table.create_index(\"text\", config=FTS(with_position=False))\n...     query = [100, 100]\n...     plan = await table.query().nearest_to([1, 2]).nearest_to_text(\"hello\").explain_plan(True)\n...     print(plan)\n&gt;&gt;&gt; asyncio.run(doctest_example())\nVector Search Plan:\nProjectionExec: expr=[vector@0 as vector, text@3 as text, _distance@2 as _distance]\n    Take: columns=\"vector, _rowid, _distance, (text)\"\n        CoalesceBatchesExec: target_batch_size=1024\n        GlobalLimitExec: skip=0, fetch=10\n            FilterExec: _distance@2 IS NOT NULL\n            SortExec: TopK(fetch=10), expr=[_distance@2 ASC NULLS LAST], preserve_partitioning=[false]\n                KNNVectorDistance: metric=l2\n                LanceScan: uri=..., projection=[vector], row_id=true, row_addr=false, ordered=false\nFTS Search Plan:\nLanceScan: uri=..., projection=[vector, text], row_id=false, row_addr=false, ordered=true\n</code></pre> <p>Parameters:</p> <ul> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Use a verbose output format.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>plan</code> (              <code>str</code> )          \u2013            </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>async def explain_plan(self, verbose: Optional[bool] = False):\n    \"\"\"Return the execution plan for this query.\n\n    The output includes both the vector and FTS search plans.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import asyncio\n    &gt;&gt;&gt; from lancedb import connect_async\n    &gt;&gt;&gt; from lancedb.index import FTS\n    &gt;&gt;&gt; async def doctest_example():\n    ...     conn = await connect_async(\"./.lancedb\")\n    ...     table = await conn.create_table(\"my_table\", [{\"vector\": [99, 99], \"text\": \"hello world\"}])\n    ...     await table.create_index(\"text\", config=FTS(with_position=False))\n    ...     query = [100, 100]\n    ...     plan = await table.query().nearest_to([1, 2]).nearest_to_text(\"hello\").explain_plan(True)\n    ...     print(plan)\n    &gt;&gt;&gt; asyncio.run(doctest_example()) # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n    Vector Search Plan:\n    ProjectionExec: expr=[vector@0 as vector, text@3 as text, _distance@2 as _distance]\n        Take: columns=\"vector, _rowid, _distance, (text)\"\n            CoalesceBatchesExec: target_batch_size=1024\n            GlobalLimitExec: skip=0, fetch=10\n                FilterExec: _distance@2 IS NOT NULL\n                SortExec: TopK(fetch=10), expr=[_distance@2 ASC NULLS LAST], preserve_partitioning=[false]\n                    KNNVectorDistance: metric=l2\n                    LanceScan: uri=..., projection=[vector], row_id=true, row_addr=false, ordered=false\n    FTS Search Plan:\n    LanceScan: uri=..., projection=[vector, text], row_id=false, row_addr=false, ordered=true\n\n    Parameters\n    ----------\n    verbose : bool, default False\n        Use a verbose output format.\n\n    Returns\n    -------\n    plan : str\n    \"\"\"  # noqa: E501\n\n    results = [\"Vector Search Plan:\"]\n    results.append(await self._inner.to_vector_query().explain_plan(verbose))\n    results.append(\"FTS Search Plan:\")\n    results.append(await self._inner.to_fts_query().explain_plan(verbose))\n\n    return \"\\n\".join(results)\n</code></pre>"},{"location":"python/python/#lancedb.query.AsyncHybridQuery.analyze_plan","title":"analyze_plan  <code>async</code>","text":"<pre><code>analyze_plan()\n</code></pre> <p>Execute the query and return the physical execution plan with runtime metrics.</p> <p>This runs both the vector and FTS (full-text search) queries and returns detailed metrics for each step of execution\u2014such as rows processed, elapsed time, I/O stats, and more. It\u2019s useful for debugging and performance analysis.</p> <p>Returns:</p> <ul> <li> <code>plan</code> (              <code>str</code> )          \u2013            </li> </ul> Source code in <code>lancedb/query.py</code> <pre><code>async def analyze_plan(self):\n    \"\"\"\n    Execute the query and return the physical execution plan with runtime metrics.\n\n    This runs both the vector and FTS (full-text search) queries and returns\n    detailed metrics for each step of execution\u2014such as rows processed,\n    elapsed time, I/O stats, and more. It\u2019s useful for debugging and\n    performance analysis.\n\n    Returns\n    -------\n    plan : str\n    \"\"\"\n    results = [\"Vector Search Query:\"]\n    results.append(await self._inner.to_vector_query().analyze_plan())\n    results.append(\"FTS Search Query:\")\n    results.append(await self._inner.to_fts_query().analyze_plan())\n\n    return \"\\n\".join(results)\n</code></pre>"},{"location":"python/python/#common-issues","title":"Common issues","text":"<p>Multiprocessing with <code>fork</code> is not supported. You should use <code>spawn</code> instead.</p>"},{"location":"python/saas-python/","title":"Python API Reference (SaaS)","text":"<p>This section contains the API reference for the LanceDB Cloud Python API.</p>"},{"location":"python/saas-python/#installation","title":"Installation","text":"<pre><code>pip install lancedb\n</code></pre>"},{"location":"python/saas-python/#connection","title":"Connection","text":""},{"location":"python/saas-python/#lancedb.connect","title":"lancedb.connect","text":"<pre><code>connect(uri: URI, *, api_key: Optional[str] = None, region: str = 'us-east-1', host_override: Optional[str] = None, read_consistency_interval: Optional[timedelta] = None, request_thread_pool: Optional[Union[int, ThreadPoolExecutor]] = None, client_config: Union[ClientConfig, Dict[str, Any], None] = None, storage_options: Optional[Dict[str, str]] = None, **kwargs: Any) -&gt; DBConnection\n</code></pre> <p>Connect to a LanceDB database.</p> <p>Parameters:</p> <ul> <li> <code>uri</code>               (<code>URI</code>)           \u2013            <p>The uri of the database.</p> </li> <li> <code>api_key</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>If presented, connect to LanceDB cloud. Otherwise, connect to a database on file system or cloud storage. Can be set via environment variable <code>LANCEDB_API_KEY</code>.</p> </li> <li> <code>region</code>               (<code>str</code>, default:                   <code>'us-east-1'</code> )           \u2013            <p>The region to use for LanceDB Cloud.</p> </li> <li> <code>host_override</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The override url for LanceDB Cloud.</p> </li> <li> <code>read_consistency_interval</code>               (<code>Optional[timedelta]</code>, default:                   <code>None</code> )           \u2013            <p>(For LanceDB OSS only) The interval at which to check for updates to the table from other processes. If None, then consistency is not checked. For performance reasons, this is the default. For strong consistency, set this to zero seconds. Then every read will check for updates from other processes. As a compromise, you can set this to a non-zero timedelta for eventual consistency. If more than that interval has passed since the last check, then the table will be checked for updates. Note: this consistency only applies to read operations. Write operations are always consistent.</p> </li> <li> <code>client_config</code>               (<code>Union[ClientConfig, Dict[str, Any], None]</code>, default:                   <code>None</code> )           \u2013            <p>Configuration options for the LanceDB Cloud HTTP client. If a dict, then the keys are the attributes of the ClientConfig class. If None, then the default configuration is used.</p> </li> <li> <code>storage_options</code>               (<code>Optional[Dict[str, str]]</code>, default:                   <code>None</code> )           \u2013            <p>Additional options for the storage backend. See available options at https://lancedb.github.io/lancedb/guides/storage/</p> </li> </ul> <p>Examples:</p> <p>For a local directory, provide a path for the database:</p> <pre><code>&gt;&gt;&gt; import lancedb\n&gt;&gt;&gt; db = lancedb.connect(\"~/.lancedb\")\n</code></pre> <p>For object storage, use a URI prefix:</p> <pre><code>&gt;&gt;&gt; db = lancedb.connect(\"s3://my-bucket/lancedb\",\n...                      storage_options={\"aws_access_key_id\": \"***\"})\n</code></pre> <p>Connect to LanceDB cloud:</p> <pre><code>&gt;&gt;&gt; db = lancedb.connect(\"db://my_database\", api_key=\"ldb_...\",\n...                      client_config={\"retry_config\": {\"retries\": 5}})\n</code></pre> <p>Returns:</p> <ul> <li> <code>conn</code> (              <code>DBConnection</code> )          \u2013            <p>A connection to a LanceDB database.</p> </li> </ul> Source code in <code>lancedb/__init__.py</code> <pre><code>def connect(\n    uri: URI,\n    *,\n    api_key: Optional[str] = None,\n    region: str = \"us-east-1\",\n    host_override: Optional[str] = None,\n    read_consistency_interval: Optional[timedelta] = None,\n    request_thread_pool: Optional[Union[int, ThreadPoolExecutor]] = None,\n    client_config: Union[ClientConfig, Dict[str, Any], None] = None,\n    storage_options: Optional[Dict[str, str]] = None,\n    **kwargs: Any,\n) -&gt; DBConnection:\n    \"\"\"Connect to a LanceDB database.\n\n    Parameters\n    ----------\n    uri: str or Path\n        The uri of the database.\n    api_key: str, optional\n        If presented, connect to LanceDB cloud.\n        Otherwise, connect to a database on file system or cloud storage.\n        Can be set via environment variable `LANCEDB_API_KEY`.\n    region: str, default \"us-east-1\"\n        The region to use for LanceDB Cloud.\n    host_override: str, optional\n        The override url for LanceDB Cloud.\n    read_consistency_interval: timedelta, default None\n        (For LanceDB OSS only)\n        The interval at which to check for updates to the table from other\n        processes. If None, then consistency is not checked. For performance\n        reasons, this is the default. For strong consistency, set this to\n        zero seconds. Then every read will check for updates from other\n        processes. As a compromise, you can set this to a non-zero timedelta\n        for eventual consistency. If more than that interval has passed since\n        the last check, then the table will be checked for updates. Note: this\n        consistency only applies to read operations. Write operations are\n        always consistent.\n    client_config: ClientConfig or dict, optional\n        Configuration options for the LanceDB Cloud HTTP client. If a dict, then\n        the keys are the attributes of the ClientConfig class. If None, then the\n        default configuration is used.\n    storage_options: dict, optional\n        Additional options for the storage backend. See available options at\n        &lt;https://lancedb.github.io/lancedb/guides/storage/&gt;\n\n    Examples\n    --------\n\n    For a local directory, provide a path for the database:\n\n    &gt;&gt;&gt; import lancedb\n    &gt;&gt;&gt; db = lancedb.connect(\"~/.lancedb\")\n\n    For object storage, use a URI prefix:\n\n    &gt;&gt;&gt; db = lancedb.connect(\"s3://my-bucket/lancedb\",\n    ...                      storage_options={\"aws_access_key_id\": \"***\"})\n\n    Connect to LanceDB cloud:\n\n    &gt;&gt;&gt; db = lancedb.connect(\"db://my_database\", api_key=\"ldb_...\",\n    ...                      client_config={\"retry_config\": {\"retries\": 5}})\n\n    Returns\n    -------\n    conn : DBConnection\n        A connection to a LanceDB database.\n    \"\"\"\n    if isinstance(uri, str) and uri.startswith(\"db://\"):\n        if api_key is None:\n            api_key = os.environ.get(\"LANCEDB_API_KEY\")\n        if api_key is None:\n            raise ValueError(f\"api_key is required to connected LanceDB cloud: {uri}\")\n        if isinstance(request_thread_pool, int):\n            request_thread_pool = ThreadPoolExecutor(request_thread_pool)\n        return RemoteDBConnection(\n            uri,\n            api_key,\n            region,\n            host_override,\n            # TODO: remove this (deprecation warning downstream)\n            request_thread_pool=request_thread_pool,\n            client_config=client_config,\n            storage_options=storage_options,\n            **kwargs,\n        )\n\n    if kwargs:\n        raise ValueError(f\"Unknown keyword arguments: {kwargs}\")\n    return LanceDBConnection(\n        uri,\n        read_consistency_interval=read_consistency_interval,\n        storage_options=storage_options,\n    )\n</code></pre>"},{"location":"python/saas-python/#lancedb.remote.db.RemoteDBConnection","title":"lancedb.remote.db.RemoteDBConnection","text":"<p>               Bases: <code>DBConnection</code></p> <p>A connection to a remote LanceDB database.</p> Source code in <code>lancedb/remote/db.py</code> <pre><code>class RemoteDBConnection(DBConnection):\n    \"\"\"A connection to a remote LanceDB database.\"\"\"\n\n    def __init__(\n        self,\n        db_url: str,\n        api_key: str,\n        region: str,\n        host_override: Optional[str] = None,\n        request_thread_pool: Optional[ThreadPoolExecutor] = None,\n        client_config: Union[ClientConfig, Dict[str, Any], None] = None,\n        connection_timeout: Optional[float] = None,\n        read_timeout: Optional[float] = None,\n        storage_options: Optional[Dict[str, str]] = None,\n    ):\n        \"\"\"Connect to a remote LanceDB database.\"\"\"\n        if isinstance(client_config, dict):\n            client_config = ClientConfig(**client_config)\n        elif client_config is None:\n            client_config = ClientConfig()\n\n        # These are legacy options from the old Python-based client. We keep them\n        # here for backwards compatibility, but will remove them in a future release.\n        if request_thread_pool is not None:\n            warnings.warn(\n                \"request_thread_pool is no longer used and will be removed in \"\n                \"a future release.\",\n                DeprecationWarning,\n            )\n\n        if connection_timeout is not None:\n            warnings.warn(\n                \"connection_timeout is deprecated and will be removed in a future \"\n                \"release. Please use client_config.timeout_config.connect_timeout \"\n                \"instead.\",\n                DeprecationWarning,\n            )\n            client_config.timeout_config.connect_timeout = timedelta(\n                seconds=connection_timeout\n            )\n\n        if read_timeout is not None:\n            warnings.warn(\n                \"read_timeout is deprecated and will be removed in a future release. \"\n                \"Please use client_config.timeout_config.read_timeout instead.\",\n                DeprecationWarning,\n            )\n            client_config.timeout_config.read_timeout = timedelta(seconds=read_timeout)\n\n        parsed = urlparse(db_url)\n        if parsed.scheme != \"db\":\n            raise ValueError(f\"Invalid scheme: {parsed.scheme}, only accepts db://\")\n        self.db_name = parsed.netloc\n\n        self.client_config = client_config\n\n        # Import connect_async here to avoid circular import\n        from lancedb import connect_async\n\n        self._conn = LOOP.run(\n            connect_async(\n                db_url,\n                api_key=api_key,\n                region=region,\n                host_override=host_override,\n                client_config=client_config,\n                storage_options=storage_options,\n            )\n        )\n\n    def __repr__(self) -&gt; str:\n        return f\"RemoteConnect(name={self.db_name})\"\n\n    @override\n    def table_names(\n        self, page_token: Optional[str] = None, limit: int = 10\n    ) -&gt; Iterable[str]:\n        \"\"\"List the names of all tables in the database.\n\n        Parameters\n        ----------\n        page_token: str\n            The last token to start the new page.\n        limit: int, default 10\n            The maximum number of tables to return for each page.\n\n        Returns\n        -------\n        An iterator of table names.\n        \"\"\"\n        return LOOP.run(self._conn.table_names(start_after=page_token, limit=limit))\n\n    @override\n    def open_table(\n        self,\n        name: str,\n        *,\n        storage_options: Optional[Dict[str, str]] = None,\n        index_cache_size: Optional[int] = None,\n    ) -&gt; Table:\n        \"\"\"Open a Lance Table in the database.\n\n        Parameters\n        ----------\n        name: str\n            The name of the table.\n\n        Returns\n        -------\n        A LanceTable object representing the table.\n        \"\"\"\n        from .table import RemoteTable\n\n        if index_cache_size is not None:\n            logging.info(\n                \"index_cache_size is ignored in LanceDb Cloud\"\n                \" (there is no local cache to configure)\"\n            )\n\n        table = LOOP.run(self._conn.open_table(name))\n        return RemoteTable(table, self.db_name)\n\n    @override\n    def create_table(\n        self,\n        name: str,\n        data: DATA = None,\n        schema: Optional[Union[pa.Schema, LanceModel]] = None,\n        on_bad_vectors: str = \"error\",\n        fill_value: float = 0.0,\n        mode: Optional[str] = None,\n        embedding_functions: Optional[List[EmbeddingFunctionConfig]] = None,\n    ) -&gt; Table:\n        \"\"\"Create a [Table][lancedb.table.Table] in the database.\n\n        Parameters\n        ----------\n        name: str\n            The name of the table.\n        data: The data to initialize the table, *optional*\n            User must provide at least one of `data` or `schema`.\n            Acceptable types are:\n\n            - dict or list-of-dict\n\n            - pandas.DataFrame\n\n            - pyarrow.Table or pyarrow.RecordBatch\n        schema: The schema of the table, *optional*\n            Acceptable types are:\n\n            - pyarrow.Schema\n\n            - [LanceModel][lancedb.pydantic.LanceModel]\n        on_bad_vectors: str, default \"error\"\n            What to do if any of the vectors are not the same size or contains NaNs.\n            One of \"error\", \"drop\", \"fill\".\n        fill_value: float\n            The value to use when filling vectors. Only used if on_bad_vectors=\"fill\".\n\n        Returns\n        -------\n        LanceTable\n            A reference to the newly created table.\n\n        !!! note\n\n            The vector index won't be created by default.\n            To create the index, call the `create_index` method on the table.\n\n        Examples\n        --------\n\n        Can create with list of tuples or dictionaries:\n\n        &gt;&gt;&gt; import lancedb\n        &gt;&gt;&gt; db = lancedb.connect(\"db://...\", api_key=\"...\", # doctest: +SKIP\n        ...                      region=\"...\")              # doctest: +SKIP\n        &gt;&gt;&gt; data = [{\"vector\": [1.1, 1.2], \"lat\": 45.5, \"long\": -122.7},\n        ...         {\"vector\": [0.2, 1.8], \"lat\": 40.1, \"long\":  -74.1}]\n        &gt;&gt;&gt; db.create_table(\"my_table\", data) # doctest: +SKIP\n        LanceTable(my_table)\n\n        You can also pass a pandas DataFrame:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; data = pd.DataFrame({\n        ...    \"vector\": [[1.1, 1.2], [0.2, 1.8]],\n        ...    \"lat\": [45.5, 40.1],\n        ...    \"long\": [-122.7, -74.1]\n        ... })\n        &gt;&gt;&gt; db.create_table(\"table2\", data) # doctest: +SKIP\n        LanceTable(table2)\n\n        &gt;&gt;&gt; custom_schema = pa.schema([\n        ...   pa.field(\"vector\", pa.list_(pa.float32(), 2)),\n        ...   pa.field(\"lat\", pa.float32()),\n        ...   pa.field(\"long\", pa.float32())\n        ... ])\n        &gt;&gt;&gt; db.create_table(\"table3\", data, schema = custom_schema) # doctest: +SKIP\n        LanceTable(table3)\n\n        It is also possible to create an table from `[Iterable[pa.RecordBatch]]`:\n\n        &gt;&gt;&gt; import pyarrow as pa\n        &gt;&gt;&gt; def make_batches():\n        ...     for i in range(5):\n        ...         yield pa.RecordBatch.from_arrays(\n        ...             [\n        ...                 pa.array([[3.1, 4.1], [5.9, 26.5]],\n        ...                     pa.list_(pa.float32(), 2)),\n        ...                 pa.array([\"foo\", \"bar\"]),\n        ...                 pa.array([10.0, 20.0]),\n        ...             ],\n        ...             [\"vector\", \"item\", \"price\"],\n        ...         )\n        &gt;&gt;&gt; schema=pa.schema([\n        ...     pa.field(\"vector\", pa.list_(pa.float32(), 2)),\n        ...     pa.field(\"item\", pa.utf8()),\n        ...     pa.field(\"price\", pa.float32()),\n        ... ])\n        &gt;&gt;&gt; db.create_table(\"table4\", make_batches(), schema=schema) # doctest: +SKIP\n        LanceTable(table4)\n\n        \"\"\"\n        validate_table_name(name)\n        if embedding_functions is not None:\n            logging.warning(\n                \"embedding_functions is not yet supported on LanceDB Cloud.\"\n                \"Please vote https://github.com/lancedb/lancedb/issues/626 \"\n                \"for this feature.\"\n            )\n\n        from .table import RemoteTable\n\n        table = LOOP.run(\n            self._conn.create_table(\n                name,\n                data,\n                mode=mode,\n                schema=schema,\n                on_bad_vectors=on_bad_vectors,\n                fill_value=fill_value,\n            )\n        )\n        return RemoteTable(table, self.db_name)\n\n    @override\n    def drop_table(self, name: str):\n        \"\"\"Drop a table from the database.\n\n        Parameters\n        ----------\n        name: str\n            The name of the table.\n        \"\"\"\n        LOOP.run(self._conn.drop_table(name))\n\n    @override\n    def rename_table(self, cur_name: str, new_name: str):\n        \"\"\"Rename a table in the database.\n\n        Parameters\n        ----------\n        cur_name: str\n            The current name of the table.\n        new_name: str\n            The new name of the table.\n        \"\"\"\n        LOOP.run(self._conn.rename_table(cur_name, new_name))\n\n    async def close(self):\n        \"\"\"Close the connection to the database.\"\"\"\n        self._client.close()\n</code></pre>"},{"location":"python/saas-python/#lancedb.remote.db.RemoteDBConnection.__init__","title":"__init__","text":"<pre><code>__init__(db_url: str, api_key: str, region: str, host_override: Optional[str] = None, request_thread_pool: Optional[ThreadPoolExecutor] = None, client_config: Union[ClientConfig, Dict[str, Any], None] = None, connection_timeout: Optional[float] = None, read_timeout: Optional[float] = None, storage_options: Optional[Dict[str, str]] = None)\n</code></pre> <p>Connect to a remote LanceDB database.</p> Source code in <code>lancedb/remote/db.py</code> <pre><code>def __init__(\n    self,\n    db_url: str,\n    api_key: str,\n    region: str,\n    host_override: Optional[str] = None,\n    request_thread_pool: Optional[ThreadPoolExecutor] = None,\n    client_config: Union[ClientConfig, Dict[str, Any], None] = None,\n    connection_timeout: Optional[float] = None,\n    read_timeout: Optional[float] = None,\n    storage_options: Optional[Dict[str, str]] = None,\n):\n    \"\"\"Connect to a remote LanceDB database.\"\"\"\n    if isinstance(client_config, dict):\n        client_config = ClientConfig(**client_config)\n    elif client_config is None:\n        client_config = ClientConfig()\n\n    # These are legacy options from the old Python-based client. We keep them\n    # here for backwards compatibility, but will remove them in a future release.\n    if request_thread_pool is not None:\n        warnings.warn(\n            \"request_thread_pool is no longer used and will be removed in \"\n            \"a future release.\",\n            DeprecationWarning,\n        )\n\n    if connection_timeout is not None:\n        warnings.warn(\n            \"connection_timeout is deprecated and will be removed in a future \"\n            \"release. Please use client_config.timeout_config.connect_timeout \"\n            \"instead.\",\n            DeprecationWarning,\n        )\n        client_config.timeout_config.connect_timeout = timedelta(\n            seconds=connection_timeout\n        )\n\n    if read_timeout is not None:\n        warnings.warn(\n            \"read_timeout is deprecated and will be removed in a future release. \"\n            \"Please use client_config.timeout_config.read_timeout instead.\",\n            DeprecationWarning,\n        )\n        client_config.timeout_config.read_timeout = timedelta(seconds=read_timeout)\n\n    parsed = urlparse(db_url)\n    if parsed.scheme != \"db\":\n        raise ValueError(f\"Invalid scheme: {parsed.scheme}, only accepts db://\")\n    self.db_name = parsed.netloc\n\n    self.client_config = client_config\n\n    # Import connect_async here to avoid circular import\n    from lancedb import connect_async\n\n    self._conn = LOOP.run(\n        connect_async(\n            db_url,\n            api_key=api_key,\n            region=region,\n            host_override=host_override,\n            client_config=client_config,\n            storage_options=storage_options,\n        )\n    )\n</code></pre>"},{"location":"python/saas-python/#lancedb.remote.db.RemoteDBConnection.table_names","title":"table_names","text":"<pre><code>table_names(page_token: Optional[str] = None, limit: int = 10) -&gt; Iterable[str]\n</code></pre> <p>List the names of all tables in the database.</p> <p>Parameters:</p> <ul> <li> <code>page_token</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The last token to start the new page.</p> </li> <li> <code>limit</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The maximum number of tables to return for each page.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>An iterator of table names.</code>           \u2013            </li> </ul> Source code in <code>lancedb/remote/db.py</code> <pre><code>@override\ndef table_names(\n    self, page_token: Optional[str] = None, limit: int = 10\n) -&gt; Iterable[str]:\n    \"\"\"List the names of all tables in the database.\n\n    Parameters\n    ----------\n    page_token: str\n        The last token to start the new page.\n    limit: int, default 10\n        The maximum number of tables to return for each page.\n\n    Returns\n    -------\n    An iterator of table names.\n    \"\"\"\n    return LOOP.run(self._conn.table_names(start_after=page_token, limit=limit))\n</code></pre>"},{"location":"python/saas-python/#lancedb.remote.db.RemoteDBConnection.open_table","title":"open_table","text":"<pre><code>open_table(name: str, *, storage_options: Optional[Dict[str, str]] = None, index_cache_size: Optional[int] = None) -&gt; Table\n</code></pre> <p>Open a Lance Table in the database.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the table.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>A LanceTable object representing the table.</code>           \u2013            </li> </ul> Source code in <code>lancedb/remote/db.py</code> <pre><code>@override\ndef open_table(\n    self,\n    name: str,\n    *,\n    storage_options: Optional[Dict[str, str]] = None,\n    index_cache_size: Optional[int] = None,\n) -&gt; Table:\n    \"\"\"Open a Lance Table in the database.\n\n    Parameters\n    ----------\n    name: str\n        The name of the table.\n\n    Returns\n    -------\n    A LanceTable object representing the table.\n    \"\"\"\n    from .table import RemoteTable\n\n    if index_cache_size is not None:\n        logging.info(\n            \"index_cache_size is ignored in LanceDb Cloud\"\n            \" (there is no local cache to configure)\"\n        )\n\n    table = LOOP.run(self._conn.open_table(name))\n    return RemoteTable(table, self.db_name)\n</code></pre>"},{"location":"python/saas-python/#lancedb.remote.db.RemoteDBConnection.create_table","title":"create_table","text":"<pre><code>create_table(name: str, data: DATA = None, schema: Optional[Union[Schema, LanceModel]] = None, on_bad_vectors: str = 'error', fill_value: float = 0.0, mode: Optional[str] = None, embedding_functions: Optional[List[EmbeddingFunctionConfig]] = None) -&gt; Table\n</code></pre> <p>Create a Table in the database.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the table.</p> </li> <li> <code>data</code>               (<code>DATA</code>, default:                   <code>None</code> )           \u2013            <p>User must provide at least one of <code>data</code> or <code>schema</code>. Acceptable types are:</p> <ul> <li> <p>dict or list-of-dict</p> </li> <li> <p>pandas.DataFrame</p> </li> <li> <p>pyarrow.Table or pyarrow.RecordBatch</p> </li> </ul> </li> <li> <code>schema</code>               (<code>Optional[Union[Schema, LanceModel]]</code>, default:                   <code>None</code> )           \u2013            <p>Acceptable types are:</p> <ul> <li> <p>pyarrow.Schema</p> </li> <li> <p>LanceModel</p> </li> </ul> </li> <li> <code>on_bad_vectors</code>               (<code>str</code>, default:                   <code>'error'</code> )           \u2013            <p>What to do if any of the vectors are not the same size or contains NaNs. One of \"error\", \"drop\", \"fill\".</p> </li> <li> <code>fill_value</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>The value to use when filling vectors. Only used if on_bad_vectors=\"fill\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LanceTable</code>           \u2013            <p>A reference to the newly created table.</p> </li> <li> <code>!!! note</code>           \u2013            <p>The vector index won't be created by default. To create the index, call the <code>create_index</code> method on the table.</p> </li> </ul> <p>Examples:</p> <p>Can create with list of tuples or dictionaries:</p> <pre><code>&gt;&gt;&gt; import lancedb\n&gt;&gt;&gt; db = lancedb.connect(\"db://...\", api_key=\"...\",\n...                      region=\"...\")\n&gt;&gt;&gt; data = [{\"vector\": [1.1, 1.2], \"lat\": 45.5, \"long\": -122.7},\n...         {\"vector\": [0.2, 1.8], \"lat\": 40.1, \"long\":  -74.1}]\n&gt;&gt;&gt; db.create_table(\"my_table\", data)\nLanceTable(my_table)\n</code></pre> <p>You can also pass a pandas DataFrame:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = pd.DataFrame({\n...    \"vector\": [[1.1, 1.2], [0.2, 1.8]],\n...    \"lat\": [45.5, 40.1],\n...    \"long\": [-122.7, -74.1]\n... })\n&gt;&gt;&gt; db.create_table(\"table2\", data)\nLanceTable(table2)\n</code></pre> <pre><code>&gt;&gt;&gt; custom_schema = pa.schema([\n...   pa.field(\"vector\", pa.list_(pa.float32(), 2)),\n...   pa.field(\"lat\", pa.float32()),\n...   pa.field(\"long\", pa.float32())\n... ])\n&gt;&gt;&gt; db.create_table(\"table3\", data, schema = custom_schema)\nLanceTable(table3)\n</code></pre> <p>It is also possible to create an table from <code>[Iterable[pa.RecordBatch]]</code>:</p> <pre><code>&gt;&gt;&gt; import pyarrow as pa\n&gt;&gt;&gt; def make_batches():\n...     for i in range(5):\n...         yield pa.RecordBatch.from_arrays(\n...             [\n...                 pa.array([[3.1, 4.1], [5.9, 26.5]],\n...                     pa.list_(pa.float32(), 2)),\n...                 pa.array([\"foo\", \"bar\"]),\n...                 pa.array([10.0, 20.0]),\n...             ],\n...             [\"vector\", \"item\", \"price\"],\n...         )\n&gt;&gt;&gt; schema=pa.schema([\n...     pa.field(\"vector\", pa.list_(pa.float32(), 2)),\n...     pa.field(\"item\", pa.utf8()),\n...     pa.field(\"price\", pa.float32()),\n... ])\n&gt;&gt;&gt; db.create_table(\"table4\", make_batches(), schema=schema)\nLanceTable(table4)\n</code></pre> Source code in <code>lancedb/remote/db.py</code> <pre><code>@override\ndef create_table(\n    self,\n    name: str,\n    data: DATA = None,\n    schema: Optional[Union[pa.Schema, LanceModel]] = None,\n    on_bad_vectors: str = \"error\",\n    fill_value: float = 0.0,\n    mode: Optional[str] = None,\n    embedding_functions: Optional[List[EmbeddingFunctionConfig]] = None,\n) -&gt; Table:\n    \"\"\"Create a [Table][lancedb.table.Table] in the database.\n\n    Parameters\n    ----------\n    name: str\n        The name of the table.\n    data: The data to initialize the table, *optional*\n        User must provide at least one of `data` or `schema`.\n        Acceptable types are:\n\n        - dict or list-of-dict\n\n        - pandas.DataFrame\n\n        - pyarrow.Table or pyarrow.RecordBatch\n    schema: The schema of the table, *optional*\n        Acceptable types are:\n\n        - pyarrow.Schema\n\n        - [LanceModel][lancedb.pydantic.LanceModel]\n    on_bad_vectors: str, default \"error\"\n        What to do if any of the vectors are not the same size or contains NaNs.\n        One of \"error\", \"drop\", \"fill\".\n    fill_value: float\n        The value to use when filling vectors. Only used if on_bad_vectors=\"fill\".\n\n    Returns\n    -------\n    LanceTable\n        A reference to the newly created table.\n\n    !!! note\n\n        The vector index won't be created by default.\n        To create the index, call the `create_index` method on the table.\n\n    Examples\n    --------\n\n    Can create with list of tuples or dictionaries:\n\n    &gt;&gt;&gt; import lancedb\n    &gt;&gt;&gt; db = lancedb.connect(\"db://...\", api_key=\"...\", # doctest: +SKIP\n    ...                      region=\"...\")              # doctest: +SKIP\n    &gt;&gt;&gt; data = [{\"vector\": [1.1, 1.2], \"lat\": 45.5, \"long\": -122.7},\n    ...         {\"vector\": [0.2, 1.8], \"lat\": 40.1, \"long\":  -74.1}]\n    &gt;&gt;&gt; db.create_table(\"my_table\", data) # doctest: +SKIP\n    LanceTable(my_table)\n\n    You can also pass a pandas DataFrame:\n\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; data = pd.DataFrame({\n    ...    \"vector\": [[1.1, 1.2], [0.2, 1.8]],\n    ...    \"lat\": [45.5, 40.1],\n    ...    \"long\": [-122.7, -74.1]\n    ... })\n    &gt;&gt;&gt; db.create_table(\"table2\", data) # doctest: +SKIP\n    LanceTable(table2)\n\n    &gt;&gt;&gt; custom_schema = pa.schema([\n    ...   pa.field(\"vector\", pa.list_(pa.float32(), 2)),\n    ...   pa.field(\"lat\", pa.float32()),\n    ...   pa.field(\"long\", pa.float32())\n    ... ])\n    &gt;&gt;&gt; db.create_table(\"table3\", data, schema = custom_schema) # doctest: +SKIP\n    LanceTable(table3)\n\n    It is also possible to create an table from `[Iterable[pa.RecordBatch]]`:\n\n    &gt;&gt;&gt; import pyarrow as pa\n    &gt;&gt;&gt; def make_batches():\n    ...     for i in range(5):\n    ...         yield pa.RecordBatch.from_arrays(\n    ...             [\n    ...                 pa.array([[3.1, 4.1], [5.9, 26.5]],\n    ...                     pa.list_(pa.float32(), 2)),\n    ...                 pa.array([\"foo\", \"bar\"]),\n    ...                 pa.array([10.0, 20.0]),\n    ...             ],\n    ...             [\"vector\", \"item\", \"price\"],\n    ...         )\n    &gt;&gt;&gt; schema=pa.schema([\n    ...     pa.field(\"vector\", pa.list_(pa.float32(), 2)),\n    ...     pa.field(\"item\", pa.utf8()),\n    ...     pa.field(\"price\", pa.float32()),\n    ... ])\n    &gt;&gt;&gt; db.create_table(\"table4\", make_batches(), schema=schema) # doctest: +SKIP\n    LanceTable(table4)\n\n    \"\"\"\n    validate_table_name(name)\n    if embedding_functions is not None:\n        logging.warning(\n            \"embedding_functions is not yet supported on LanceDB Cloud.\"\n            \"Please vote https://github.com/lancedb/lancedb/issues/626 \"\n            \"for this feature.\"\n        )\n\n    from .table import RemoteTable\n\n    table = LOOP.run(\n        self._conn.create_table(\n            name,\n            data,\n            mode=mode,\n            schema=schema,\n            on_bad_vectors=on_bad_vectors,\n            fill_value=fill_value,\n        )\n    )\n    return RemoteTable(table, self.db_name)\n</code></pre>"},{"location":"python/saas-python/#lancedb.remote.db.RemoteDBConnection.drop_table","title":"drop_table","text":"<pre><code>drop_table(name: str)\n</code></pre> <p>Drop a table from the database.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the table.</p> </li> </ul> Source code in <code>lancedb/remote/db.py</code> <pre><code>@override\ndef drop_table(self, name: str):\n    \"\"\"Drop a table from the database.\n\n    Parameters\n    ----------\n    name: str\n        The name of the table.\n    \"\"\"\n    LOOP.run(self._conn.drop_table(name))\n</code></pre>"},{"location":"python/saas-python/#lancedb.remote.db.RemoteDBConnection.rename_table","title":"rename_table","text":"<pre><code>rename_table(cur_name: str, new_name: str)\n</code></pre> <p>Rename a table in the database.</p> <p>Parameters:</p> <ul> <li> <code>cur_name</code>               (<code>str</code>)           \u2013            <p>The current name of the table.</p> </li> <li> <code>new_name</code>               (<code>str</code>)           \u2013            <p>The new name of the table.</p> </li> </ul> Source code in <code>lancedb/remote/db.py</code> <pre><code>@override\ndef rename_table(self, cur_name: str, new_name: str):\n    \"\"\"Rename a table in the database.\n\n    Parameters\n    ----------\n    cur_name: str\n        The current name of the table.\n    new_name: str\n        The new name of the table.\n    \"\"\"\n    LOOP.run(self._conn.rename_table(cur_name, new_name))\n</code></pre>"},{"location":"python/saas-python/#lancedb.remote.db.RemoteDBConnection.close","title":"close  <code>async</code>","text":"<pre><code>close()\n</code></pre> <p>Close the connection to the database.</p> Source code in <code>lancedb/remote/db.py</code> <pre><code>async def close(self):\n    \"\"\"Close the connection to the database.\"\"\"\n    self._client.close()\n</code></pre>"},{"location":"python/saas-python/#table","title":"Table","text":""},{"location":"python/saas-python/#lancedb.remote.table.RemoteTable","title":"lancedb.remote.table.RemoteTable","text":"<p>               Bases: <code>Table</code></p> Source code in <code>lancedb/remote/table.py</code> <pre><code>class RemoteTable(Table):\n    def __init__(\n        self,\n        table: AsyncTable,\n        db_name: str,\n    ):\n        self._table = table\n        self.db_name = db_name\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"The name of the table\"\"\"\n        return self._table.name\n\n    def __repr__(self) -&gt; str:\n        return f\"RemoteTable({self.db_name}.{self.name})\"\n\n    def __len__(self) -&gt; int:\n        self.count_rows(None)\n\n    @property\n    def schema(self) -&gt; pa.Schema:\n        \"\"\"The [Arrow Schema](https://arrow.apache.org/docs/python/api/datatypes.html#)\n        of this Table\n\n        \"\"\"\n        return LOOP.run(self._table.schema())\n\n    @property\n    def version(self) -&gt; int:\n        \"\"\"Get the current version of the table\"\"\"\n        return LOOP.run(self._table.version())\n\n    @cached_property\n    def embedding_functions(self) -&gt; Dict[str, EmbeddingFunctionConfig]:\n        \"\"\"\n        Get the embedding functions for the table\n\n        Returns\n        -------\n        funcs: dict\n            A mapping of the vector column to the embedding function\n            or empty dict if not configured.\n        \"\"\"\n        return EmbeddingFunctionRegistry.get_instance().parse_functions(\n            self.schema.metadata\n        )\n\n    def list_versions(self):\n        \"\"\"List all versions of the table\"\"\"\n        return LOOP.run(self._table.list_versions())\n\n    def to_arrow(self) -&gt; pa.Table:\n        \"\"\"to_arrow() is not yet supported on LanceDB cloud.\"\"\"\n        raise NotImplementedError(\"to_arrow() is not yet supported on LanceDB cloud.\")\n\n    def to_pandas(self):\n        \"\"\"to_pandas() is not yet supported on LanceDB cloud.\"\"\"\n        return NotImplementedError(\"to_pandas() is not yet supported on LanceDB cloud.\")\n\n    def checkout(self, version: int):\n        return LOOP.run(self._table.checkout(version))\n\n    def checkout_latest(self):\n        return LOOP.run(self._table.checkout_latest())\n\n    def restore(self, version: Optional[int] = None):\n        return LOOP.run(self._table.restore(version))\n\n    def list_indices(self) -&gt; Iterable[IndexConfig]:\n        \"\"\"List all the indices on the table\"\"\"\n        return LOOP.run(self._table.list_indices())\n\n    def index_stats(self, index_uuid: str) -&gt; Optional[IndexStatistics]:\n        \"\"\"List all the stats of a specified index\"\"\"\n        return LOOP.run(self._table.index_stats(index_uuid))\n\n    def create_scalar_index(\n        self,\n        column: str,\n        index_type: Literal[\"BTREE\", \"BITMAP\", \"LABEL_LIST\", \"scalar\"] = \"scalar\",\n        *,\n        replace: bool = False,\n        wait_timeout: timedelta = None,\n    ):\n        \"\"\"Creates a scalar index\n        Parameters\n        ----------\n        column : str\n            The column to be indexed.  Must be a boolean, integer, float,\n            or string column.\n        index_type : str\n            The index type of the scalar index. Must be \"scalar\" (BTREE),\n            \"BTREE\", \"BITMAP\", or \"LABEL_LIST\",\n        replace : bool\n            If True, replace the existing index with the new one.\n        \"\"\"\n        if index_type == \"scalar\" or index_type == \"BTREE\":\n            config = BTree()\n        elif index_type == \"BITMAP\":\n            config = Bitmap()\n        elif index_type == \"LABEL_LIST\":\n            config = LabelList()\n        else:\n            raise ValueError(f\"Unknown index type: {index_type}\")\n\n        LOOP.run(\n            self._table.create_index(\n                column, config=config, replace=replace, wait_timeout=wait_timeout\n            )\n        )\n\n    def create_fts_index(\n        self,\n        column: str,\n        *,\n        replace: bool = False,\n        wait_timeout: timedelta = None,\n        with_position: bool = True,\n        # tokenizer configs:\n        base_tokenizer: str = \"simple\",\n        language: str = \"English\",\n        max_token_length: Optional[int] = 40,\n        lower_case: bool = True,\n        stem: bool = False,\n        remove_stop_words: bool = False,\n        ascii_folding: bool = False,\n    ):\n        config = FTS(\n            with_position=with_position,\n            base_tokenizer=base_tokenizer,\n            language=language,\n            max_token_length=max_token_length,\n            lower_case=lower_case,\n            stem=stem,\n            remove_stop_words=remove_stop_words,\n            ascii_folding=ascii_folding,\n        )\n        LOOP.run(\n            self._table.create_index(\n                column, config=config, replace=replace, wait_timeout=wait_timeout\n            )\n        )\n\n    def create_index(\n        self,\n        metric=\"l2\",\n        vector_column_name: str = VECTOR_COLUMN_NAME,\n        index_cache_size: Optional[int] = None,\n        num_partitions: Optional[int] = None,\n        num_sub_vectors: Optional[int] = None,\n        replace: Optional[bool] = None,\n        accelerator: Optional[str] = None,\n        index_type=\"vector\",\n        wait_timeout: Optional[timedelta] = None,\n    ):\n        \"\"\"Create an index on the table.\n        Currently, the only parameters that matter are\n        the metric and the vector column name.\n\n        Parameters\n        ----------\n        metric : str\n            The metric to use for the index. Default is \"l2\".\n        vector_column_name : str\n            The name of the vector column. Default is \"vector\".\n\n        Examples\n        --------\n        &gt;&gt;&gt; import lancedb\n        &gt;&gt;&gt; import uuid\n        &gt;&gt;&gt; from lancedb.schema import vector\n        &gt;&gt;&gt; db = lancedb.connect(\"db://...\", api_key=\"...\", # doctest: +SKIP\n        ...                      region=\"...\") # doctest: +SKIP\n        &gt;&gt;&gt; table_name = uuid.uuid4().hex\n        &gt;&gt;&gt; schema = pa.schema(\n        ...     [\n        ...             pa.field(\"id\", pa.uint32(), False),\n        ...            pa.field(\"vector\", vector(128), False),\n        ...             pa.field(\"s\", pa.string(), False),\n        ...     ]\n        ... )\n        &gt;&gt;&gt; table = db.create_table( # doctest: +SKIP\n        ...     table_name, # doctest: +SKIP\n        ...     schema=schema, # doctest: +SKIP\n        ... )\n        &gt;&gt;&gt; table.create_index(\"l2\", \"vector\") # doctest: +SKIP\n        \"\"\"\n\n        if num_partitions is not None:\n            logging.warning(\n                \"num_partitions is not supported on LanceDB cloud.\"\n                \"This parameter will be tuned automatically.\"\n            )\n        if num_sub_vectors is not None:\n            logging.warning(\n                \"num_sub_vectors is not supported on LanceDB cloud.\"\n                \"This parameter will be tuned automatically.\"\n            )\n        if accelerator is not None:\n            logging.warning(\n                \"GPU accelerator is not yet supported on LanceDB cloud.\"\n                \"If you have 100M+ vectors to index,\"\n                \"please contact us at contact@lancedb.com\"\n            )\n        if replace is not None:\n            logging.warning(\n                \"replace is not supported on LanceDB cloud.\"\n                \"Existing indexes will always be replaced.\"\n            )\n\n        index_type = index_type.upper()\n        if index_type == \"VECTOR\" or index_type == \"IVF_PQ\":\n            config = IvfPq(distance_type=metric)\n        elif index_type == \"IVF_HNSW_PQ\":\n            config = HnswPq(distance_type=metric)\n        elif index_type == \"IVF_HNSW_SQ\":\n            config = HnswSq(distance_type=metric)\n        elif index_type == \"IVF_FLAT\":\n            config = IvfFlat(distance_type=metric)\n        else:\n            raise ValueError(\n                f\"Unknown vector index type: {index_type}. Valid options are\"\n                \" 'IVF_FLAT', 'IVF_PQ', 'IVF_HNSW_PQ', 'IVF_HNSW_SQ'\"\n            )\n\n        LOOP.run(\n            self._table.create_index(\n                vector_column_name, config=config, wait_timeout=wait_timeout\n            )\n        )\n\n    def add(\n        self,\n        data: DATA,\n        mode: str = \"append\",\n        on_bad_vectors: str = \"error\",\n        fill_value: float = 0.0,\n    ) -&gt; int:\n        \"\"\"Add more data to the [Table](Table). It has the same API signature as\n        the OSS version.\n\n        Parameters\n        ----------\n        data: DATA\n            The data to insert into the table. Acceptable types are:\n\n            - dict or list-of-dict\n\n            - pandas.DataFrame\n\n            - pyarrow.Table or pyarrow.RecordBatch\n        mode: str\n            The mode to use when writing the data. Valid values are\n            \"append\" and \"overwrite\".\n        on_bad_vectors: str, default \"error\"\n            What to do if any of the vectors are not the same size or contains NaNs.\n            One of \"error\", \"drop\", \"fill\".\n        fill_value: float, default 0.\n            The value to use when filling vectors. Only used if on_bad_vectors=\"fill\".\n\n        \"\"\"\n        LOOP.run(\n            self._table.add(\n                data, mode=mode, on_bad_vectors=on_bad_vectors, fill_value=fill_value\n            )\n        )\n\n    def search(\n        self,\n        query: Union[VEC, str] = None,\n        vector_column_name: Optional[str] = None,\n        query_type=\"auto\",\n        fts_columns: Optional[Union[str, List[str]]] = None,\n        fast_search: bool = False,\n    ) -&gt; LanceVectorQueryBuilder:\n        \"\"\"Create a search query to find the nearest neighbors\n        of the given query vector. We currently support [vector search][search]\n\n        All query options are defined in\n        [LanceVectorQueryBuilder][lancedb.query.LanceVectorQueryBuilder].\n\n        Examples\n        --------\n        &gt;&gt;&gt; import lancedb\n        &gt;&gt;&gt; db = lancedb.connect(\"db://...\", api_key=\"...\", # doctest: +SKIP\n        ...                      region=\"...\") # doctest: +SKIP\n        &gt;&gt;&gt; data = [\n        ...    {\"original_width\": 100, \"caption\": \"bar\", \"vector\": [0.1, 2.3, 4.5]},\n        ...    {\"original_width\": 2000, \"caption\": \"foo\",  \"vector\": [0.5, 3.4, 1.3]},\n        ...    {\"original_width\": 3000, \"caption\": \"test\", \"vector\": [0.3, 6.2, 2.6]}\n        ... ]\n        &gt;&gt;&gt; table = db.create_table(\"my_table\", data) # doctest: +SKIP\n        &gt;&gt;&gt; query = [0.4, 1.4, 2.4]\n        &gt;&gt;&gt; (table.search(query) # doctest: +SKIP\n        ...     .where(\"original_width &gt; 1000\", prefilter=True) # doctest: +SKIP\n        ...     .select([\"caption\", \"original_width\"]) # doctest: +SKIP\n        ...     .limit(2) # doctest: +SKIP\n        ...     .to_pandas()) # doctest: +SKIP\n          caption  original_width           vector  _distance # doctest: +SKIP\n        0     foo            2000  [0.5, 3.4, 1.3]   5.220000 # doctest: +SKIP\n        1    test            3000  [0.3, 6.2, 2.6]  23.089996 # doctest: +SKIP\n\n        Parameters\n        ----------\n        query: list/np.ndarray/str/PIL.Image.Image, default None\n            The targetted vector to search for.\n\n            - *default None*.\n            Acceptable types are: list, np.ndarray, PIL.Image.Image\n\n        vector_column_name: str, optional\n            The name of the vector column to search.\n\n            - If not specified then the vector column is inferred from\n            the table schema\n\n            - If the table has multiple vector columns then the *vector_column_name*\n            needs to be specified. Otherwise, an error is raised.\n\n        fast_search: bool, optional\n            Skip a flat search of unindexed data. This may improve\n            search performance but search results will not include unindexed data.\n\n            - *default False*.\n\n        Returns\n        -------\n        LanceQueryBuilder\n            A query builder object representing the query.\n            Once executed, the query returns\n\n            - selected columns\n\n            - the vector\n\n            - and also the \"_distance\" column which is the distance between the query\n            vector and the returned vector.\n        \"\"\"\n        return LanceQueryBuilder.create(\n            self,\n            query,\n            query_type,\n            vector_column_name=vector_column_name,\n            fts_columns=fts_columns,\n            fast_search=fast_search,\n        )\n\n    def _execute_query(\n        self,\n        query: Query,\n        *,\n        batch_size: Optional[int] = None,\n        timeout: Optional[timedelta] = None,\n    ) -&gt; pa.RecordBatchReader:\n        async_iter = LOOP.run(\n            self._table._execute_query(query, batch_size=batch_size, timeout=timeout)\n        )\n\n        def iter_sync():\n            try:\n                while True:\n                    yield LOOP.run(async_iter.__anext__())\n            except StopAsyncIteration:\n                return\n\n        return pa.RecordBatchReader.from_batches(async_iter.schema, iter_sync())\n\n    def _explain_plan(self, query: Query, verbose: Optional[bool] = False) -&gt; str:\n        return LOOP.run(self._table._explain_plan(query, verbose))\n\n    def _analyze_plan(self, query: Query) -&gt; str:\n        return LOOP.run(self._table._analyze_plan(query))\n\n    def merge_insert(self, on: Union[str, Iterable[str]]) -&gt; LanceMergeInsertBuilder:\n        \"\"\"Returns a [`LanceMergeInsertBuilder`][lancedb.merge.LanceMergeInsertBuilder]\n        that can be used to create a \"merge insert\" operation.\n\n        See [`Table.merge_insert`][lancedb.table.Table.merge_insert] for more details.\n        \"\"\"\n        return super().merge_insert(on)\n\n    def _do_merge(\n        self,\n        merge: LanceMergeInsertBuilder,\n        new_data: DATA,\n        on_bad_vectors: str,\n        fill_value: float,\n    ):\n        LOOP.run(self._table._do_merge(merge, new_data, on_bad_vectors, fill_value))\n\n    def delete(self, predicate: str):\n        \"\"\"Delete rows from the table.\n\n        This can be used to delete a single row, many rows, all rows, or\n        sometimes no rows (if your predicate matches nothing).\n\n        Parameters\n        ----------\n        predicate: str\n            The SQL where clause to use when deleting rows.\n\n            - For example, 'x = 2' or 'x IN (1, 2, 3)'.\n\n            The filter must not be empty, or it will error.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import lancedb\n        &gt;&gt;&gt; data = [\n        ...    {\"x\": 1, \"vector\": [1, 2]},\n        ...    {\"x\": 2, \"vector\": [3, 4]},\n        ...    {\"x\": 3, \"vector\": [5, 6]}\n        ... ]\n        &gt;&gt;&gt; db = lancedb.connect(\"db://...\", api_key=\"...\", # doctest: +SKIP\n        ...                      region=\"...\") # doctest: +SKIP\n        &gt;&gt;&gt; table = db.create_table(\"my_table\", data) # doctest: +SKIP\n        &gt;&gt;&gt; table.search([10,10]).to_pandas() # doctest: +SKIP\n           x      vector  _distance # doctest: +SKIP\n        0  3  [5.0, 6.0]       41.0 # doctest: +SKIP\n        1  2  [3.0, 4.0]       85.0 # doctest: +SKIP\n        2  1  [1.0, 2.0]      145.0 # doctest: +SKIP\n        &gt;&gt;&gt; table.delete(\"x = 2\") # doctest: +SKIP\n        &gt;&gt;&gt; table.search([10,10]).to_pandas() # doctest: +SKIP\n           x      vector  _distance # doctest: +SKIP\n        0  3  [5.0, 6.0]       41.0 # doctest: +SKIP\n        1  1  [1.0, 2.0]      145.0 # doctest: +SKIP\n\n        If you have a list of values to delete, you can combine them into a\n        stringified list and use the `IN` operator:\n\n        &gt;&gt;&gt; to_remove = [1, 3] # doctest: +SKIP\n        &gt;&gt;&gt; to_remove = \", \".join([str(v) for v in to_remove]) # doctest: +SKIP\n        &gt;&gt;&gt; table.delete(f\"x IN ({to_remove})\") # doctest: +SKIP\n        &gt;&gt;&gt; table.search([10,10]).to_pandas() # doctest: +SKIP\n           x      vector  _distance # doctest: +SKIP\n        0  2  [3.0, 4.0]       85.0 # doctest: +SKIP\n        \"\"\"\n        LOOP.run(self._table.delete(predicate))\n\n    def update(\n        self,\n        where: Optional[str] = None,\n        values: Optional[dict] = None,\n        *,\n        values_sql: Optional[Dict[str, str]] = None,\n    ):\n        \"\"\"\n        This can be used to update zero to all rows depending on how many\n        rows match the where clause.\n\n        Parameters\n        ----------\n        where: str, optional\n            The SQL where clause to use when updating rows. For example, 'x = 2'\n            or 'x IN (1, 2, 3)'. The filter must not be empty, or it will error.\n        values: dict, optional\n            The values to update. The keys are the column names and the values\n            are the values to set.\n        values_sql: dict, optional\n            The values to update, expressed as SQL expression strings. These can\n            reference existing columns. For example, {\"x\": \"x + 1\"} will increment\n            the x column by 1.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import lancedb\n        &gt;&gt;&gt; data = [\n        ...    {\"x\": 1, \"vector\": [1, 2]},\n        ...    {\"x\": 2, \"vector\": [3, 4]},\n        ...    {\"x\": 3, \"vector\": [5, 6]}\n        ... ]\n        &gt;&gt;&gt; db = lancedb.connect(\"db://...\", api_key=\"...\", # doctest: +SKIP\n        ...                      region=\"...\") # doctest: +SKIP\n        &gt;&gt;&gt; table = db.create_table(\"my_table\", data) # doctest: +SKIP\n        &gt;&gt;&gt; table.to_pandas() # doctest: +SKIP\n           x      vector # doctest: +SKIP\n        0  1  [1.0, 2.0] # doctest: +SKIP\n        1  2  [3.0, 4.0] # doctest: +SKIP\n        2  3  [5.0, 6.0] # doctest: +SKIP\n        &gt;&gt;&gt; table.update(where=\"x = 2\", values={\"vector\": [10, 10]}) # doctest: +SKIP\n        &gt;&gt;&gt; table.to_pandas() # doctest: +SKIP\n           x        vector # doctest: +SKIP\n        0  1    [1.0, 2.0] # doctest: +SKIP\n        1  3    [5.0, 6.0] # doctest: +SKIP\n        2  2  [10.0, 10.0] # doctest: +SKIP\n\n        \"\"\"\n        LOOP.run(\n            self._table.update(where=where, updates=values, updates_sql=values_sql)\n        )\n\n    def cleanup_old_versions(self, *_):\n        \"\"\"\n        cleanup_old_versions() is a no-op on LanceDB Cloud.\n\n        Tables are automatically cleaned up and optimized.\n        \"\"\"\n        warnings.warn(\n            \"cleanup_old_versions() is a no-op on LanceDB Cloud. \"\n            \"Tables are automatically cleaned up and optimized.\"\n        )\n        pass\n\n    def compact_files(self, *_):\n        \"\"\"\n        compact_files() is a no-op on LanceDB Cloud.\n\n        Tables are automatically compacted and optimized.\n        \"\"\"\n        warnings.warn(\n            \"compact_files() is a no-op on LanceDB Cloud. \"\n            \"Tables are automatically compacted and optimized.\"\n        )\n        pass\n\n    def optimize(\n        self,\n        *,\n        cleanup_older_than: Optional[timedelta] = None,\n        delete_unverified: bool = False,\n    ):\n        \"\"\"\n        optimize() is a no-op on LanceDB Cloud.\n\n        Indices are optimized automatically.\n        \"\"\"\n        warnings.warn(\n            \"optimize() is a no-op on LanceDB Cloud. \"\n            \"Indices are optimized automatically.\"\n        )\n        pass\n\n    def count_rows(self, filter: Optional[str] = None) -&gt; int:\n        return LOOP.run(self._table.count_rows(filter))\n\n    def add_columns(self, transforms: Dict[str, str]):\n        return LOOP.run(self._table.add_columns(transforms))\n\n    def alter_columns(self, *alterations: Iterable[Dict[str, str]]):\n        return LOOP.run(self._table.alter_columns(*alterations))\n\n    def drop_columns(self, columns: Iterable[str]):\n        return LOOP.run(self._table.drop_columns(columns))\n\n    def drop_index(self, index_name: str):\n        return LOOP.run(self._table.drop_index(index_name))\n\n    def wait_for_index(\n        self, index_names: Iterable[str], timeout: timedelta = timedelta(seconds=300)\n    ):\n        return LOOP.run(self._table.wait_for_index(index_names, timeout))\n\n    def uses_v2_manifest_paths(self) -&gt; bool:\n        raise NotImplementedError(\n            \"uses_v2_manifest_paths() is not supported on the LanceDB Cloud\"\n        )\n\n    def migrate_v2_manifest_paths(self):\n        raise NotImplementedError(\n            \"migrate_v2_manifest_paths() is not supported on the LanceDB Cloud\"\n        )\n</code></pre>"},{"location":"python/saas-python/#lancedb.remote.table.RemoteTable.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the table</p>"},{"location":"python/saas-python/#lancedb.remote.table.RemoteTable.schema","title":"schema  <code>property</code>","text":"<pre><code>schema: Schema\n</code></pre> <p>The Arrow Schema of this Table</p>"},{"location":"python/saas-python/#lancedb.remote.table.RemoteTable.version","title":"version  <code>property</code>","text":"<pre><code>version: int\n</code></pre> <p>Get the current version of the table</p>"},{"location":"python/saas-python/#lancedb.remote.table.RemoteTable.embedding_functions","title":"embedding_functions  <code>cached</code> <code>property</code>","text":"<pre><code>embedding_functions: Dict[str, EmbeddingFunctionConfig]\n</code></pre> <p>Get the embedding functions for the table</p> <p>Returns:</p> <ul> <li> <code>funcs</code> (              <code>dict</code> )          \u2013            <p>A mapping of the vector column to the embedding function or empty dict if not configured.</p> </li> </ul>"},{"location":"python/saas-python/#lancedb.remote.table.RemoteTable.list_versions","title":"list_versions","text":"<pre><code>list_versions()\n</code></pre> <p>List all versions of the table</p> Source code in <code>lancedb/remote/table.py</code> <pre><code>def list_versions(self):\n    \"\"\"List all versions of the table\"\"\"\n    return LOOP.run(self._table.list_versions())\n</code></pre>"},{"location":"python/saas-python/#lancedb.remote.table.RemoteTable.to_arrow","title":"to_arrow","text":"<pre><code>to_arrow() -&gt; Table\n</code></pre> <p>to_arrow() is not yet supported on LanceDB cloud.</p> Source code in <code>lancedb/remote/table.py</code> <pre><code>def to_arrow(self) -&gt; pa.Table:\n    \"\"\"to_arrow() is not yet supported on LanceDB cloud.\"\"\"\n    raise NotImplementedError(\"to_arrow() is not yet supported on LanceDB cloud.\")\n</code></pre>"},{"location":"python/saas-python/#lancedb.remote.table.RemoteTable.to_pandas","title":"to_pandas","text":"<pre><code>to_pandas()\n</code></pre> <p>to_pandas() is not yet supported on LanceDB cloud.</p> Source code in <code>lancedb/remote/table.py</code> <pre><code>def to_pandas(self):\n    \"\"\"to_pandas() is not yet supported on LanceDB cloud.\"\"\"\n    return NotImplementedError(\"to_pandas() is not yet supported on LanceDB cloud.\")\n</code></pre>"},{"location":"python/saas-python/#lancedb.remote.table.RemoteTable.list_indices","title":"list_indices","text":"<pre><code>list_indices() -&gt; Iterable[IndexConfig]\n</code></pre> <p>List all the indices on the table</p> Source code in <code>lancedb/remote/table.py</code> <pre><code>def list_indices(self) -&gt; Iterable[IndexConfig]:\n    \"\"\"List all the indices on the table\"\"\"\n    return LOOP.run(self._table.list_indices())\n</code></pre>"},{"location":"python/saas-python/#lancedb.remote.table.RemoteTable.index_stats","title":"index_stats","text":"<pre><code>index_stats(index_uuid: str) -&gt; Optional[IndexStatistics]\n</code></pre> <p>List all the stats of a specified index</p> Source code in <code>lancedb/remote/table.py</code> <pre><code>def index_stats(self, index_uuid: str) -&gt; Optional[IndexStatistics]:\n    \"\"\"List all the stats of a specified index\"\"\"\n    return LOOP.run(self._table.index_stats(index_uuid))\n</code></pre>"},{"location":"python/saas-python/#lancedb.remote.table.RemoteTable.create_scalar_index","title":"create_scalar_index","text":"<pre><code>create_scalar_index(column: str, index_type: Literal['BTREE', 'BITMAP', 'LABEL_LIST', 'scalar'] = 'scalar', *, replace: bool = False, wait_timeout: timedelta = None)\n</code></pre> <p>Creates a scalar index</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>str</code>)           \u2013            <p>The column to be indexed.  Must be a boolean, integer, float, or string column.</p> </li> <li> <code>index_type</code>               (<code>str</code>, default:                   <code>'scalar'</code> )           \u2013            <p>The index type of the scalar index. Must be \"scalar\" (BTREE), \"BTREE\", \"BITMAP\", or \"LABEL_LIST\",</p> </li> <li> <code>replace</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, replace the existing index with the new one.</p> </li> </ul> Source code in <code>lancedb/remote/table.py</code> <pre><code>def create_scalar_index(\n    self,\n    column: str,\n    index_type: Literal[\"BTREE\", \"BITMAP\", \"LABEL_LIST\", \"scalar\"] = \"scalar\",\n    *,\n    replace: bool = False,\n    wait_timeout: timedelta = None,\n):\n    \"\"\"Creates a scalar index\n    Parameters\n    ----------\n    column : str\n        The column to be indexed.  Must be a boolean, integer, float,\n        or string column.\n    index_type : str\n        The index type of the scalar index. Must be \"scalar\" (BTREE),\n        \"BTREE\", \"BITMAP\", or \"LABEL_LIST\",\n    replace : bool\n        If True, replace the existing index with the new one.\n    \"\"\"\n    if index_type == \"scalar\" or index_type == \"BTREE\":\n        config = BTree()\n    elif index_type == \"BITMAP\":\n        config = Bitmap()\n    elif index_type == \"LABEL_LIST\":\n        config = LabelList()\n    else:\n        raise ValueError(f\"Unknown index type: {index_type}\")\n\n    LOOP.run(\n        self._table.create_index(\n            column, config=config, replace=replace, wait_timeout=wait_timeout\n        )\n    )\n</code></pre>"},{"location":"python/saas-python/#lancedb.remote.table.RemoteTable.create_index","title":"create_index","text":"<pre><code>create_index(metric='l2', vector_column_name: str = VECTOR_COLUMN_NAME, index_cache_size: Optional[int] = None, num_partitions: Optional[int] = None, num_sub_vectors: Optional[int] = None, replace: Optional[bool] = None, accelerator: Optional[str] = None, index_type='vector', wait_timeout: Optional[timedelta] = None)\n</code></pre> <p>Create an index on the table. Currently, the only parameters that matter are the metric and the vector column name.</p> <p>Parameters:</p> <ul> <li> <code>metric</code>               (<code>str</code>, default:                   <code>'l2'</code> )           \u2013            <p>The metric to use for the index. Default is \"l2\".</p> </li> <li> <code>vector_column_name</code>               (<code>str</code>, default:                   <code>VECTOR_COLUMN_NAME</code> )           \u2013            <p>The name of the vector column. Default is \"vector\".</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import lancedb\n&gt;&gt;&gt; import uuid\n&gt;&gt;&gt; from lancedb.schema import vector\n&gt;&gt;&gt; db = lancedb.connect(\"db://...\", api_key=\"...\",\n...                      region=\"...\")\n&gt;&gt;&gt; table_name = uuid.uuid4().hex\n&gt;&gt;&gt; schema = pa.schema(\n...     [\n...             pa.field(\"id\", pa.uint32(), False),\n...            pa.field(\"vector\", vector(128), False),\n...             pa.field(\"s\", pa.string(), False),\n...     ]\n... )\n&gt;&gt;&gt; table = db.create_table(\n...     table_name,\n...     schema=schema,\n... )\n&gt;&gt;&gt; table.create_index(\"l2\", \"vector\")\n</code></pre> Source code in <code>lancedb/remote/table.py</code> <pre><code>def create_index(\n    self,\n    metric=\"l2\",\n    vector_column_name: str = VECTOR_COLUMN_NAME,\n    index_cache_size: Optional[int] = None,\n    num_partitions: Optional[int] = None,\n    num_sub_vectors: Optional[int] = None,\n    replace: Optional[bool] = None,\n    accelerator: Optional[str] = None,\n    index_type=\"vector\",\n    wait_timeout: Optional[timedelta] = None,\n):\n    \"\"\"Create an index on the table.\n    Currently, the only parameters that matter are\n    the metric and the vector column name.\n\n    Parameters\n    ----------\n    metric : str\n        The metric to use for the index. Default is \"l2\".\n    vector_column_name : str\n        The name of the vector column. Default is \"vector\".\n\n    Examples\n    --------\n    &gt;&gt;&gt; import lancedb\n    &gt;&gt;&gt; import uuid\n    &gt;&gt;&gt; from lancedb.schema import vector\n    &gt;&gt;&gt; db = lancedb.connect(\"db://...\", api_key=\"...\", # doctest: +SKIP\n    ...                      region=\"...\") # doctest: +SKIP\n    &gt;&gt;&gt; table_name = uuid.uuid4().hex\n    &gt;&gt;&gt; schema = pa.schema(\n    ...     [\n    ...             pa.field(\"id\", pa.uint32(), False),\n    ...            pa.field(\"vector\", vector(128), False),\n    ...             pa.field(\"s\", pa.string(), False),\n    ...     ]\n    ... )\n    &gt;&gt;&gt; table = db.create_table( # doctest: +SKIP\n    ...     table_name, # doctest: +SKIP\n    ...     schema=schema, # doctest: +SKIP\n    ... )\n    &gt;&gt;&gt; table.create_index(\"l2\", \"vector\") # doctest: +SKIP\n    \"\"\"\n\n    if num_partitions is not None:\n        logging.warning(\n            \"num_partitions is not supported on LanceDB cloud.\"\n            \"This parameter will be tuned automatically.\"\n        )\n    if num_sub_vectors is not None:\n        logging.warning(\n            \"num_sub_vectors is not supported on LanceDB cloud.\"\n            \"This parameter will be tuned automatically.\"\n        )\n    if accelerator is not None:\n        logging.warning(\n            \"GPU accelerator is not yet supported on LanceDB cloud.\"\n            \"If you have 100M+ vectors to index,\"\n            \"please contact us at contact@lancedb.com\"\n        )\n    if replace is not None:\n        logging.warning(\n            \"replace is not supported on LanceDB cloud.\"\n            \"Existing indexes will always be replaced.\"\n        )\n\n    index_type = index_type.upper()\n    if index_type == \"VECTOR\" or index_type == \"IVF_PQ\":\n        config = IvfPq(distance_type=metric)\n    elif index_type == \"IVF_HNSW_PQ\":\n        config = HnswPq(distance_type=metric)\n    elif index_type == \"IVF_HNSW_SQ\":\n        config = HnswSq(distance_type=metric)\n    elif index_type == \"IVF_FLAT\":\n        config = IvfFlat(distance_type=metric)\n    else:\n        raise ValueError(\n            f\"Unknown vector index type: {index_type}. Valid options are\"\n            \" 'IVF_FLAT', 'IVF_PQ', 'IVF_HNSW_PQ', 'IVF_HNSW_SQ'\"\n        )\n\n    LOOP.run(\n        self._table.create_index(\n            vector_column_name, config=config, wait_timeout=wait_timeout\n        )\n    )\n</code></pre>"},{"location":"python/saas-python/#lancedb.remote.table.RemoteTable.add","title":"add","text":"<pre><code>add(data: DATA, mode: str = 'append', on_bad_vectors: str = 'error', fill_value: float = 0.0) -&gt; int\n</code></pre> <p>Add more data to the Table. It has the same API signature as the OSS version.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>DATA</code>)           \u2013            <p>The data to insert into the table. Acceptable types are:</p> <ul> <li> <p>dict or list-of-dict</p> </li> <li> <p>pandas.DataFrame</p> </li> <li> <p>pyarrow.Table or pyarrow.RecordBatch</p> </li> </ul> </li> <li> <code>mode</code>               (<code>str</code>, default:                   <code>'append'</code> )           \u2013            <p>The mode to use when writing the data. Valid values are \"append\" and \"overwrite\".</p> </li> <li> <code>on_bad_vectors</code>               (<code>str</code>, default:                   <code>'error'</code> )           \u2013            <p>What to do if any of the vectors are not the same size or contains NaNs. One of \"error\", \"drop\", \"fill\".</p> </li> <li> <code>fill_value</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>The value to use when filling vectors. Only used if on_bad_vectors=\"fill\".</p> </li> </ul> Source code in <code>lancedb/remote/table.py</code> <pre><code>def add(\n    self,\n    data: DATA,\n    mode: str = \"append\",\n    on_bad_vectors: str = \"error\",\n    fill_value: float = 0.0,\n) -&gt; int:\n    \"\"\"Add more data to the [Table](Table). It has the same API signature as\n    the OSS version.\n\n    Parameters\n    ----------\n    data: DATA\n        The data to insert into the table. Acceptable types are:\n\n        - dict or list-of-dict\n\n        - pandas.DataFrame\n\n        - pyarrow.Table or pyarrow.RecordBatch\n    mode: str\n        The mode to use when writing the data. Valid values are\n        \"append\" and \"overwrite\".\n    on_bad_vectors: str, default \"error\"\n        What to do if any of the vectors are not the same size or contains NaNs.\n        One of \"error\", \"drop\", \"fill\".\n    fill_value: float, default 0.\n        The value to use when filling vectors. Only used if on_bad_vectors=\"fill\".\n\n    \"\"\"\n    LOOP.run(\n        self._table.add(\n            data, mode=mode, on_bad_vectors=on_bad_vectors, fill_value=fill_value\n        )\n    )\n</code></pre>"},{"location":"python/saas-python/#lancedb.remote.table.RemoteTable.search","title":"search","text":"<pre><code>search(query: Union[VEC, str] = None, vector_column_name: Optional[str] = None, query_type='auto', fts_columns: Optional[Union[str, List[str]]] = None, fast_search: bool = False) -&gt; LanceVectorQueryBuilder\n</code></pre> <p>Create a search query to find the nearest neighbors of the given query vector. We currently support vector search</p> <p>All query options are defined in LanceVectorQueryBuilder.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import lancedb\n&gt;&gt;&gt; db = lancedb.connect(\"db://...\", api_key=\"...\",\n...                      region=\"...\")\n&gt;&gt;&gt; data = [\n...    {\"original_width\": 100, \"caption\": \"bar\", \"vector\": [0.1, 2.3, 4.5]},\n...    {\"original_width\": 2000, \"caption\": \"foo\",  \"vector\": [0.5, 3.4, 1.3]},\n...    {\"original_width\": 3000, \"caption\": \"test\", \"vector\": [0.3, 6.2, 2.6]}\n... ]\n&gt;&gt;&gt; table = db.create_table(\"my_table\", data)\n&gt;&gt;&gt; query = [0.4, 1.4, 2.4]\n&gt;&gt;&gt; (table.search(query)\n...     .where(\"original_width &gt; 1000\", prefilter=True)\n...     .select([\"caption\", \"original_width\"])\n...     .limit(2)\n...     .to_pandas())\n  caption  original_width           vector  _distance\n0     foo            2000  [0.5, 3.4, 1.3]   5.220000\n1    test            3000  [0.3, 6.2, 2.6]  23.089996\n</code></pre> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>Union[VEC, str]</code>, default:                   <code>None</code> )           \u2013            <p>The targetted vector to search for.</p> <ul> <li>default None. Acceptable types are: list, np.ndarray, PIL.Image.Image</li> </ul> </li> <li> <code>vector_column_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The name of the vector column to search.</p> <ul> <li> <p>If not specified then the vector column is inferred from the table schema</p> </li> <li> <p>If the table has multiple vector columns then the vector_column_name needs to be specified. Otherwise, an error is raised.</p> </li> </ul> </li> <li> <code>fast_search</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Skip a flat search of unindexed data. This may improve search performance but search results will not include unindexed data.</p> <ul> <li>default False.</li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> <code>LanceQueryBuilder</code>           \u2013            <p>A query builder object representing the query. Once executed, the query returns</p> <ul> <li> <p>selected columns</p> </li> <li> <p>the vector</p> </li> <li> <p>and also the \"_distance\" column which is the distance between the query vector and the returned vector.</p> </li> </ul> </li> </ul> Source code in <code>lancedb/remote/table.py</code> <pre><code>def search(\n    self,\n    query: Union[VEC, str] = None,\n    vector_column_name: Optional[str] = None,\n    query_type=\"auto\",\n    fts_columns: Optional[Union[str, List[str]]] = None,\n    fast_search: bool = False,\n) -&gt; LanceVectorQueryBuilder:\n    \"\"\"Create a search query to find the nearest neighbors\n    of the given query vector. We currently support [vector search][search]\n\n    All query options are defined in\n    [LanceVectorQueryBuilder][lancedb.query.LanceVectorQueryBuilder].\n\n    Examples\n    --------\n    &gt;&gt;&gt; import lancedb\n    &gt;&gt;&gt; db = lancedb.connect(\"db://...\", api_key=\"...\", # doctest: +SKIP\n    ...                      region=\"...\") # doctest: +SKIP\n    &gt;&gt;&gt; data = [\n    ...    {\"original_width\": 100, \"caption\": \"bar\", \"vector\": [0.1, 2.3, 4.5]},\n    ...    {\"original_width\": 2000, \"caption\": \"foo\",  \"vector\": [0.5, 3.4, 1.3]},\n    ...    {\"original_width\": 3000, \"caption\": \"test\", \"vector\": [0.3, 6.2, 2.6]}\n    ... ]\n    &gt;&gt;&gt; table = db.create_table(\"my_table\", data) # doctest: +SKIP\n    &gt;&gt;&gt; query = [0.4, 1.4, 2.4]\n    &gt;&gt;&gt; (table.search(query) # doctest: +SKIP\n    ...     .where(\"original_width &gt; 1000\", prefilter=True) # doctest: +SKIP\n    ...     .select([\"caption\", \"original_width\"]) # doctest: +SKIP\n    ...     .limit(2) # doctest: +SKIP\n    ...     .to_pandas()) # doctest: +SKIP\n      caption  original_width           vector  _distance # doctest: +SKIP\n    0     foo            2000  [0.5, 3.4, 1.3]   5.220000 # doctest: +SKIP\n    1    test            3000  [0.3, 6.2, 2.6]  23.089996 # doctest: +SKIP\n\n    Parameters\n    ----------\n    query: list/np.ndarray/str/PIL.Image.Image, default None\n        The targetted vector to search for.\n\n        - *default None*.\n        Acceptable types are: list, np.ndarray, PIL.Image.Image\n\n    vector_column_name: str, optional\n        The name of the vector column to search.\n\n        - If not specified then the vector column is inferred from\n        the table schema\n\n        - If the table has multiple vector columns then the *vector_column_name*\n        needs to be specified. Otherwise, an error is raised.\n\n    fast_search: bool, optional\n        Skip a flat search of unindexed data. This may improve\n        search performance but search results will not include unindexed data.\n\n        - *default False*.\n\n    Returns\n    -------\n    LanceQueryBuilder\n        A query builder object representing the query.\n        Once executed, the query returns\n\n        - selected columns\n\n        - the vector\n\n        - and also the \"_distance\" column which is the distance between the query\n        vector and the returned vector.\n    \"\"\"\n    return LanceQueryBuilder.create(\n        self,\n        query,\n        query_type,\n        vector_column_name=vector_column_name,\n        fts_columns=fts_columns,\n        fast_search=fast_search,\n    )\n</code></pre>"},{"location":"python/saas-python/#lancedb.remote.table.RemoteTable.merge_insert","title":"merge_insert","text":"<pre><code>merge_insert(on: Union[str, Iterable[str]]) -&gt; LanceMergeInsertBuilder\n</code></pre> <p>Returns a <code>LanceMergeInsertBuilder</code> that can be used to create a \"merge insert\" operation.</p> <p>See <code>Table.merge_insert</code> for more details.</p> Source code in <code>lancedb/remote/table.py</code> <pre><code>def merge_insert(self, on: Union[str, Iterable[str]]) -&gt; LanceMergeInsertBuilder:\n    \"\"\"Returns a [`LanceMergeInsertBuilder`][lancedb.merge.LanceMergeInsertBuilder]\n    that can be used to create a \"merge insert\" operation.\n\n    See [`Table.merge_insert`][lancedb.table.Table.merge_insert] for more details.\n    \"\"\"\n    return super().merge_insert(on)\n</code></pre>"},{"location":"python/saas-python/#lancedb.remote.table.RemoteTable.delete","title":"delete","text":"<pre><code>delete(predicate: str)\n</code></pre> <p>Delete rows from the table.</p> <p>This can be used to delete a single row, many rows, all rows, or sometimes no rows (if your predicate matches nothing).</p> <p>Parameters:</p> <ul> <li> <code>predicate</code>               (<code>str</code>)           \u2013            <p>The SQL where clause to use when deleting rows.</p> <ul> <li>For example, 'x = 2' or 'x IN (1, 2, 3)'.</li> </ul> <p>The filter must not be empty, or it will error.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import lancedb\n&gt;&gt;&gt; data = [\n...    {\"x\": 1, \"vector\": [1, 2]},\n...    {\"x\": 2, \"vector\": [3, 4]},\n...    {\"x\": 3, \"vector\": [5, 6]}\n... ]\n&gt;&gt;&gt; db = lancedb.connect(\"db://...\", api_key=\"...\",\n...                      region=\"...\")\n&gt;&gt;&gt; table = db.create_table(\"my_table\", data)\n&gt;&gt;&gt; table.search([10,10]).to_pandas()\n   x      vector  _distance\n0  3  [5.0, 6.0]       41.0\n1  2  [3.0, 4.0]       85.0\n2  1  [1.0, 2.0]      145.0\n&gt;&gt;&gt; table.delete(\"x = 2\")\n&gt;&gt;&gt; table.search([10,10]).to_pandas()\n   x      vector  _distance\n0  3  [5.0, 6.0]       41.0\n1  1  [1.0, 2.0]      145.0\n</code></pre> <p>If you have a list of values to delete, you can combine them into a stringified list and use the <code>IN</code> operator:</p> <pre><code>&gt;&gt;&gt; to_remove = [1, 3]\n&gt;&gt;&gt; to_remove = \", \".join([str(v) for v in to_remove])\n&gt;&gt;&gt; table.delete(f\"x IN ({to_remove})\")\n&gt;&gt;&gt; table.search([10,10]).to_pandas()\n   x      vector  _distance\n0  2  [3.0, 4.0]       85.0\n</code></pre> Source code in <code>lancedb/remote/table.py</code> <pre><code>def delete(self, predicate: str):\n    \"\"\"Delete rows from the table.\n\n    This can be used to delete a single row, many rows, all rows, or\n    sometimes no rows (if your predicate matches nothing).\n\n    Parameters\n    ----------\n    predicate: str\n        The SQL where clause to use when deleting rows.\n\n        - For example, 'x = 2' or 'x IN (1, 2, 3)'.\n\n        The filter must not be empty, or it will error.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import lancedb\n    &gt;&gt;&gt; data = [\n    ...    {\"x\": 1, \"vector\": [1, 2]},\n    ...    {\"x\": 2, \"vector\": [3, 4]},\n    ...    {\"x\": 3, \"vector\": [5, 6]}\n    ... ]\n    &gt;&gt;&gt; db = lancedb.connect(\"db://...\", api_key=\"...\", # doctest: +SKIP\n    ...                      region=\"...\") # doctest: +SKIP\n    &gt;&gt;&gt; table = db.create_table(\"my_table\", data) # doctest: +SKIP\n    &gt;&gt;&gt; table.search([10,10]).to_pandas() # doctest: +SKIP\n       x      vector  _distance # doctest: +SKIP\n    0  3  [5.0, 6.0]       41.0 # doctest: +SKIP\n    1  2  [3.0, 4.0]       85.0 # doctest: +SKIP\n    2  1  [1.0, 2.0]      145.0 # doctest: +SKIP\n    &gt;&gt;&gt; table.delete(\"x = 2\") # doctest: +SKIP\n    &gt;&gt;&gt; table.search([10,10]).to_pandas() # doctest: +SKIP\n       x      vector  _distance # doctest: +SKIP\n    0  3  [5.0, 6.0]       41.0 # doctest: +SKIP\n    1  1  [1.0, 2.0]      145.0 # doctest: +SKIP\n\n    If you have a list of values to delete, you can combine them into a\n    stringified list and use the `IN` operator:\n\n    &gt;&gt;&gt; to_remove = [1, 3] # doctest: +SKIP\n    &gt;&gt;&gt; to_remove = \", \".join([str(v) for v in to_remove]) # doctest: +SKIP\n    &gt;&gt;&gt; table.delete(f\"x IN ({to_remove})\") # doctest: +SKIP\n    &gt;&gt;&gt; table.search([10,10]).to_pandas() # doctest: +SKIP\n       x      vector  _distance # doctest: +SKIP\n    0  2  [3.0, 4.0]       85.0 # doctest: +SKIP\n    \"\"\"\n    LOOP.run(self._table.delete(predicate))\n</code></pre>"},{"location":"python/saas-python/#lancedb.remote.table.RemoteTable.update","title":"update","text":"<pre><code>update(where: Optional[str] = None, values: Optional[dict] = None, *, values_sql: Optional[Dict[str, str]] = None)\n</code></pre> <p>This can be used to update zero to all rows depending on how many rows match the where clause.</p> <p>Parameters:</p> <ul> <li> <code>where</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The SQL where clause to use when updating rows. For example, 'x = 2' or 'x IN (1, 2, 3)'. The filter must not be empty, or it will error.</p> </li> <li> <code>values</code>               (<code>Optional[dict]</code>, default:                   <code>None</code> )           \u2013            <p>The values to update. The keys are the column names and the values are the values to set.</p> </li> <li> <code>values_sql</code>               (<code>Optional[Dict[str, str]]</code>, default:                   <code>None</code> )           \u2013            <p>The values to update, expressed as SQL expression strings. These can reference existing columns. For example, {\"x\": \"x + 1\"} will increment the x column by 1.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import lancedb\n&gt;&gt;&gt; data = [\n...    {\"x\": 1, \"vector\": [1, 2]},\n...    {\"x\": 2, \"vector\": [3, 4]},\n...    {\"x\": 3, \"vector\": [5, 6]}\n... ]\n&gt;&gt;&gt; db = lancedb.connect(\"db://...\", api_key=\"...\",\n...                      region=\"...\")\n&gt;&gt;&gt; table = db.create_table(\"my_table\", data)\n&gt;&gt;&gt; table.to_pandas()\n   x      vector\n0  1  [1.0, 2.0]\n1  2  [3.0, 4.0]\n2  3  [5.0, 6.0]\n&gt;&gt;&gt; table.update(where=\"x = 2\", values={\"vector\": [10, 10]})\n&gt;&gt;&gt; table.to_pandas()\n   x        vector\n0  1    [1.0, 2.0]\n1  3    [5.0, 6.0]\n2  2  [10.0, 10.0]\n</code></pre> Source code in <code>lancedb/remote/table.py</code> <pre><code>def update(\n    self,\n    where: Optional[str] = None,\n    values: Optional[dict] = None,\n    *,\n    values_sql: Optional[Dict[str, str]] = None,\n):\n    \"\"\"\n    This can be used to update zero to all rows depending on how many\n    rows match the where clause.\n\n    Parameters\n    ----------\n    where: str, optional\n        The SQL where clause to use when updating rows. For example, 'x = 2'\n        or 'x IN (1, 2, 3)'. The filter must not be empty, or it will error.\n    values: dict, optional\n        The values to update. The keys are the column names and the values\n        are the values to set.\n    values_sql: dict, optional\n        The values to update, expressed as SQL expression strings. These can\n        reference existing columns. For example, {\"x\": \"x + 1\"} will increment\n        the x column by 1.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import lancedb\n    &gt;&gt;&gt; data = [\n    ...    {\"x\": 1, \"vector\": [1, 2]},\n    ...    {\"x\": 2, \"vector\": [3, 4]},\n    ...    {\"x\": 3, \"vector\": [5, 6]}\n    ... ]\n    &gt;&gt;&gt; db = lancedb.connect(\"db://...\", api_key=\"...\", # doctest: +SKIP\n    ...                      region=\"...\") # doctest: +SKIP\n    &gt;&gt;&gt; table = db.create_table(\"my_table\", data) # doctest: +SKIP\n    &gt;&gt;&gt; table.to_pandas() # doctest: +SKIP\n       x      vector # doctest: +SKIP\n    0  1  [1.0, 2.0] # doctest: +SKIP\n    1  2  [3.0, 4.0] # doctest: +SKIP\n    2  3  [5.0, 6.0] # doctest: +SKIP\n    &gt;&gt;&gt; table.update(where=\"x = 2\", values={\"vector\": [10, 10]}) # doctest: +SKIP\n    &gt;&gt;&gt; table.to_pandas() # doctest: +SKIP\n       x        vector # doctest: +SKIP\n    0  1    [1.0, 2.0] # doctest: +SKIP\n    1  3    [5.0, 6.0] # doctest: +SKIP\n    2  2  [10.0, 10.0] # doctest: +SKIP\n\n    \"\"\"\n    LOOP.run(\n        self._table.update(where=where, updates=values, updates_sql=values_sql)\n    )\n</code></pre>"},{"location":"rag/adaptive_rag/","title":"Adaptive RAG \ud83e\udd39\u200d\u2642\ufe0f","text":"<p>Adaptive RAG introduces a RAG technique that combines query analysis with self-corrective RAG. </p> <p>For Query Analysis, it uses a small classifier(LLM), to decide the query\u2019s complexity. Query Analysis guides adjustment between different retrieval strategies: No retrieval, Single-shot RAG or Iterative RAG.</p> <p>Official Paper</p> Adaptive-RAG: Source <p>Official Implementation</p> <p>Here\u2019s a code snippet for query analysis:</p> <pre><code>from langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_openai import ChatOpenAI\n\nclass RouteQuery(BaseModel):\n    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n\n    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n        ...,\n        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n    )\n\n\n# LLM with function call\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\nstructured_llm_router = llm.with_structured_output(RouteQuery)\n</code></pre> <p>The following example defines and queries a retriever:</p> <pre><code># add documents in LanceDB\nvectorstore = LanceDB.from_documents(\n    documents=doc_splits,\n    embedding=OpenAIEmbeddings(),\n)\nretriever = vectorstore.as_retriever()\n\n# query using defined retriever\nquestion = \"How adaptive RAG works\"\ndocs = retriever.get_relevant_documents(question)\n</code></pre>"},{"location":"rag/agentic_rag/","title":"Agentic RAG \ud83e\udd16","text":"<p>Agentic RAG introduces an advanced framework for answering questions by using intelligent agents instead of just relying on large language models. These agents act like expert researchers, handling complex tasks such as detailed planning, multi-step reasoning, and using external tools. They navigate multiple documents, compare information, and generate accurate answers. This system is easily scalable, with each new document set managed by a sub-agent, making it a powerful tool for tackling a wide range of information needs.</p> Agent-based RAG <p></p> <p>Here\u2019s a code snippet for defining retriever using Langchain:</p> <pre><code>from langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import LanceDB\nfrom langchain_openai import OpenAIEmbeddings\n\nurls = [\n    \"https://content.dgft.gov.in/Website/CIEP.pdf\",\n    \"https://content.dgft.gov.in/Website/GAE.pdf\",\n    \"https://content.dgft.gov.in/Website/HTE.pdf\",\n]\n\n\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=100, chunk_overlap=50\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# add documents in LanceDB\nvectorstore = LanceDB.from_documents(\n    documents=doc_splits,\n    embedding=OpenAIEmbeddings(),\n)\nretriever = vectorstore.as_retriever()\n</code></pre> <p>Here is an agent that formulates an improved query for better retrieval results and then grades the retrieved documents:</p> <pre><code>def grade_documents(state) -&gt; Literal[\"generate\", \"rewrite\"]:\n    class grade(BaseModel):\n        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n\n    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\n    llm_with_tool = model.with_structured_output(grade)\n    prompt = PromptTemplate(\n        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n        Here is the retrieved document: \\n\\n {context} \\n\\n\n        Here is the user question: {question} \\n\n        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n        input_variables=[\"context\", \"question\"],\n    )\n    chain = prompt | llm_with_tool\n\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    question = messages[0].content\n    docs = last_message.content\n\n    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n    score = scored_result.binary_score\n\n    return \"generate\" if score == \"yes\" else \"rewrite\"\n\n\ndef agent(state):\n    messages = state[\"messages\"]\n    model = ChatOpenAI(temperature=0, streaming=True, model=\"gpt-4-turbo\")\n    model = model.bind_tools(tools)\n    response = model.invoke(messages)\n    return {\"messages\": [response]}\n\n\ndef rewrite(state):\n    messages = state[\"messages\"]\n    question = messages[0].content\n    msg = [\n        HumanMessage(\n            content=f\"\"\" \\n\n            Look at the input and try to reason about the underlying semantic intent / meaning. \\n\n            Here is the initial question:\n            \\n ------- \\n\n            {question}\n            \\n ------- \\n\n            Formulate an improved question: \"\"\",\n        )\n    ]\n    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\n    response = model.invoke(msg)\n    return {\"messages\": [response]}\n</code></pre> <p></p>"},{"location":"rag/corrective_rag/","title":"Corrective RAG \u2705","text":"<p>Corrective-RAG (CRAG) is a strategy for Retrieval-Augmented Generation (RAG) that includes self-reflection and self-grading of retrieved documents. Here\u2019s a simplified breakdown of the steps involved:</p> <ol> <li>Relevance Check: If at least one document meets the relevance threshold, the process moves forward to the generation phase.</li> <li>Knowledge Refinement: Before generating an answer, the process refines the knowledge by dividing the document into smaller segments called \"knowledge strips\".</li> <li>Grading and Filtering: Each \"knowledge strip\" is graded, and irrelevant ones are filtered out.</li> <li>Additional Data Source: If all documents are below the relevance threshold, or if the system is unsure about their relevance, it will seek additional information by performing a web search to supplement the retrieved data.</li> </ol> <p>Above steps are mentioned in  Official Paper</p> Corrective RAG: Source <p>Corrective Retrieval-Augmented Generation (CRAG) is a method that works like a built-in fact-checker.</p> <p>Official Implementation</p> <p></p> <p>Here\u2019s a code snippet for defining a table with the Embedding API, and retrieves the relevant documents:</p> <pre><code>import pandas as pd\nimport lancedb\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.embeddings import get_registry\n\ndb = lancedb.connect(\"/tmp/db\")\nmodel = get_registry().get(\"sentence-transformers\").create(name=\"BAAI/bge-small-en-v1.5\", device=\"cpu\")\n\nclass Docs(LanceModel):\n    text: str = model.SourceField()\n    vector: Vector(model.ndims()) = model.VectorField()\n\ntable = db.create_table(\"docs\", schema=Docs)\n\n# considering chunks are in list format\ndf = pd.DataFrame({'text':chunks})\ntable.add(data=df)\n\n# as per document feeded\nquery = \"How Transformers work?\" \nactual = table.search(query).limit(1).to_list()[0]\nprint(actual.text)\n</code></pre> <p>Code snippet for grading retrieved documents, filtering out irrelevant ones, and performing a web search if necessary:</p> <pre><code>def grade_documents(state):\n    \"\"\"\n        Determines whether the retrieved documents are relevant to the question\n\n        Args:\n            state (dict): The current graph state\n\n        Returns:\n            state (dict): Updates documents key with relevant documents\n        \"\"\"\n\n    state_dict = state[\"keys\"]\n    question = state_dict[\"question\"]\n    documents = state_dict[\"documents\"]\n\n    class grade(BaseModel):\n        \"\"\"\n            Binary score for relevance check\n        \"\"\"\n\n        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n\n    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\n    # grading using openai\n    grade_tool_oai = convert_to_openai_tool(grade)\n    llm_with_tool = model.bind(\n        tools=[convert_to_openai_tool(grade_tool_oai)],\n        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"grade\"}},\n    )\n\n    parser_tool = PydanticToolsParser(tools=[grade])\n    prompt = PromptTemplate(\n        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n        Here is the retrieved document: \\n\\n {context} \\n\\n\n        Here is the user question: {question} \\n\n        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n        input_variables=[\"context\", \"question\"],\n    )\n\n    chain = prompt | llm_with_tool | parser_tool\n\n    filtered_docs = []\n    search = \"No\" \n    for d in documents:\n        score = chain.invoke({\"question\": question, \"context\": d.page_content})\n        grade = score[0].binary_score\n        if grade == \"yes\":\n            filtered_docs.append(d)\n        else:\n            search = \"Yes\" \n            continue\n\n    return {\n        \"keys\": {\n            \"documents\": filtered_docs,\n            \"question\": question,\n            \"run_web_search\": search,\n        }\n    }\n</code></pre> <p>Check Colab for the Implementation of CRAG with Langgraph:</p> <p></p>"},{"location":"rag/graph_rag/","title":"Graph RAG \ud83d\udcca","text":"<p>Graph RAG uses knowledge graphs together with large language models (LLMs) to improve how information is retrieved and generated. It overcomes the limits of traditional search methods by using knowledge graphs, which organize data as connected entities and relationships.</p> <p>One of the main benefits of Graph RAG is its ability to capture and represent complex relationships between entities, something that traditional text-based retrieval systems struggle with. By using this structured knowledge, LLMs can better grasp the context and details of a query, resulting in more accurate and insightful answers.</p> <p>Official Paper</p> <p>Official Implementation</p> <p>Microsoft Research Blog</p> <p>Default VectorDB</p> <p>Graph RAG uses LanceDB as the default vector database for performing vector search to retrieve relevant entities.</p> <p>Working with Graph RAG is quite straightforward</p> <ul> <li>Installation and API KEY as env variable</li> </ul> <p>Set <code>OPENAI_API_KEY</code> as <code>GRAPHRAG_API_KEY</code></p> <pre><code>pip install graphrag\nexport GRAPHRAG_API_KEY=\"sk-...\"\n</code></pre> <ul> <li>Initial structure for indexing dataset</li> </ul> <pre><code>python3 -m graphrag.index --init --root dataset-dir\n</code></pre> <ul> <li>Index Dataset</li> </ul> <pre><code>python3 -m graphrag.index --root dataset-dir\n</code></pre> <ul> <li>Execute Query</li> </ul> <p>Global Query Execution gives a broad overview of dataset:</p> <pre><code>python3 -m graphrag.query --root dataset-dir --method global \"query-question\"\n</code></pre> <p>Local Query Execution gives a detailed and specific answers based on the context of the entities:</p> <pre><code>python3 -m graphrag.query --root  dataset-dir --method local \"query-question\"\n</code></pre> <p></p>"},{"location":"rag/multi_head_rag/","title":"Multi-Head RAG \ud83d\udcc3","text":"<p>Multi-head RAG (MRAG) is designed to handle queries that need multiple documents with diverse content. These queries are tough because the documents\u2019 embeddings can be far apart, making retrieval difficult. MRAG simplifies this by using the activations from a Transformer's multi-head attention layer, rather than the decoder layer, to fetch these varied documents. Different attention heads capture different aspects of the data, so using these activations helps create embeddings that better represent various data facets and improves retrieval accuracy for complex queries.</p> <p>Official Paper</p> Multi-Head RAG: Source <p>MRAG is cost-effective and energy-efficient because it avoids extra LLM queries, multiple model instances, increased storage, and additional inference passes.</p> <p>Official Implementation</p> <p>Here\u2019s a code snippet for defining different embedding spaces with the Embedding API:</p> <pre><code>import lancedb\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.embeddings import get_registry\n\n# model definition using LanceDB Embedding API\nmodel1 = get_registry().get(\"openai\").create()\nmodel2 = get_registry().get(\"ollama\").create(name=\"llama3\")\nmodel3 = get_registry().get(\"ollama\").create(name=\"mistral\")\n\n\n# define schema for creating embedding spaces with Embedding API\nclass Space1(LanceModel):\n    text: str = model1.SourceField()\n    vector: Vector(model1.ndims()) = model1.VectorField()\n\n\nclass Space2(LanceModel):\n    text: str = model2.SourceField()\n    vector: Vector(model2.ndims()) = model2.VectorField()\n\n\nclass Space3(LanceModel):\n    text: str = model3.SourceField()\n    vector: Vector(model3.ndims()) = model3.VectorField()\n</code></pre> <p>Create different tables using defined embedding spaces, then make queries to each embedding space. Use the resulting closest documents from each embedding space to generate answers.</p>"},{"location":"rag/self_rag/","title":"Self RAG \ud83e\udd33","text":"<p>Self-RAG is a strategy for Retrieval-Augmented Generation (RAG) to get better retrieved information, generated text, and validation, without loss of flexibility. Unlike the traditional Retrieval-Augmented Generation (RAG) method, Self-RAG retrieves information as needed, can skip retrieval if not needed, and evaluates its own output while generating text. It also uses a process to pick the best output based on different preferences.</p> <p>Official Paper</p> Self RAG: Source <p>Official Implementation</p> <p>Self-RAG starts by generating a response without retrieving extra info if it's not needed. For questions that need more details, it retrieves to get the necessary information.</p> <p>Here\u2019s a code snippet for defining retriever using Langchain:</p> <pre><code>from langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import LanceDB\nfrom langchain_openai import OpenAIEmbeddings\n\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n]\n\n\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=100, chunk_overlap=50\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# add documents in LanceDB\nvectorstore = LanceDB.from_documents(\n    documents=doc_splits,\n    embedding=OpenAIEmbeddings(),\n)\nretriever = vectorstore.as_retriever()\n</code></pre> <p>The following functions grade the retrieved documents and formulate an improved query for better retrieval results, if required:</p> <pre><code>def grade_documents(state) -&gt; Literal[\"generate\", \"rewrite\"]:\n    class grade(BaseModel):\n        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n\n    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\n    llm_with_tool = model.with_structured_output(grade)\n    prompt = PromptTemplate(\n        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n        Here is the retrieved document: \\n\\n {context} \\n\\n\n        Here is the user question: {question} \\n\n        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n        input_variables=[\"context\", \"question\"],\n    )\n    chain = prompt | llm_with_tool\n\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    question = messages[0].content\n    docs = last_message.content\n\n    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n    score = scored_result.binary_score\n\n    return \"generate\" if score == \"yes\" else \"rewrite\"\n\n\ndef rewrite(state):\n    messages = state[\"messages\"]\n    question = messages[0].content\n    msg = [\n        HumanMessage(\n            content=f\"\"\" \\n\n            Look at the input and try to reason about the underlying semantic intent / meaning. \\n\n            Here is the initial question:\n            \\n ------- \\n\n            {question}\n            \\n ------- \\n\n            Formulate an improved question: \"\"\",\n        )\n    ]\n    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\n    response = model.invoke(msg)\n    return {\"messages\": [response]}\n</code></pre>"},{"location":"rag/sfr_rag/","title":"SFR RAG \ud83d\udcd1","text":"<p>Salesforce AI Research introduced SFR-RAG, a 9-billion-parameter language model trained with a significant emphasis on reliable, precise, and faithful contextual generation abilities specific to real-world RAG use cases and relevant agentic tasks. It targets precise factual knowledge extraction, distinction between relevant and distracting contexts, citation of appropriate sources along with answers, production of complex and multi-hop reasoning over multiple contexts, consistent format following, as well as minimization of hallucination over unanswerable queries.</p> <p>Official Implementation</p> Average Scores in ContextualBench: Source <p>To reliably evaluate LLMs in contextual question-answering for RAG, Saleforce introduced ContextualBench, featuring 7 benchmarks like HotpotQA and 2WikiHopQA with consistent setups. </p> <p>SFR-RAG outperforms GPT-4o, achieving state-of-the-art results in 3 out of 7 benchmarks, and significantly surpasses Command-R+ while using 10 times fewer parameters. It also excels at handling context, even when facts are altered or conflicting.</p> <p>Saleforce AI Research Blog</p>"},{"location":"rag/vanilla_rag/","title":"Vanilla RAG \ud83c\udf31","text":"<p>RAG(Retrieval-Augmented Generation) works by finding documents related to the user's question, combining them with a prompt for a large language model (LLM), and then using the LLM to create more accurate and relevant answers.</p> <p>Here\u2019s a simple guide to building a RAG pipeline from scratch:</p> <ol> <li> <p>Data Loading: Gather and load the documents you want to use for answering questions.</p> </li> <li> <p>Chunking and Embedding: Split the documents into smaller chunks and convert them into numerical vectors (embeddings) that capture their meaning.</p> </li> <li> <p>Vector Store: Create a LanceDB table to store and manage these vectors for quick access during retrieval.</p> </li> <li> <p>Retrieval &amp; Prompt Preparation: When a question is asked, find the most relevant document chunks from the table and prepare a prompt combining these chunks with the question.</p> </li> <li> <p>Answer Generation: Send the prepared prompt to a LLM to generate a detailed and accurate answer.</p> </li> </ol> Vanilla RAG    <p></p> <p>Here\u2019s a code snippet for defining a table with the Embedding API, which simplifies the process by handling embedding extraction and querying in one step.</p> <pre><code>import pandas as pd\nimport lancedb\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.embeddings import get_registry\n\ndb = lancedb.connect(\"/tmp/db\")\nmodel = get_registry().get(\"sentence-transformers\").create(name=\"BAAI/bge-small-en-v1.5\", device=\"cpu\")\n\nclass Docs(LanceModel):\n    text: str = model.SourceField()\n    vector: Vector(model.ndims()) = model.VectorField()\n\ntable = db.create_table(\"docs\", schema=Docs)\n\n# considering chunks are in list format\ndf = pd.DataFrame({'text':chunks})\ntable.add(data=df)\n\nquery = \"What is issue date of lease?\"\nactual = table.search(query).limit(1).to_list()[0]\nprint(actual.text)\n</code></pre> <p>Check Colab for the complete code</p> <p></p>"},{"location":"rag/advanced_techniques/flare/","title":"FLARE \ud83d\udca5","text":"<p>FLARE, stands for Forward-Looking Active REtrieval augmented generation is a generic retrieval-augmented generation method that actively decides when and what to retrieve using a prediction of the upcoming sentence to anticipate future content and utilize it as the query to retrieve relevant documents if it contains low-confidence tokens.</p> <p>Official Paper</p> FLARE: Source <p></p> <p>Here\u2019s a code snippet for using FLARE with Langchain:</p> <pre><code>from langchain.vectorstores import LanceDB\nfrom langchain.document_loaders import ArxivLoader\nfrom langchain.chains import FlareChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\nfrom langchain.llms import OpenAI\n\nllm = OpenAI()\n\n# load dataset\n\n# LanceDB retriever\nvector_store = LanceDB.from_documents(doc_chunks, embeddings, connection=table)\nretriever = vector_store.as_retriever()\n\n# define flare chain\nflare = FlareChain.from_llm(llm=llm,retriever=vector_store_retriever,max_generation_len=300,min_prob=0.45)\n\nresult = flare.run(input_text)\n</code></pre> <p></p>"},{"location":"rag/advanced_techniques/hyde/","title":"HyDE: Hypothetical Document Embeddings \ud83e\udd39\u200d\u2642\ufe0f","text":"<p>HyDE, stands for Hypothetical Document Embeddings is an approach used for precise zero-shot dense retrieval without relevance labels. It focuses on augmenting and improving similarity searches, often intertwined with vector stores in information retrieval. The method generates a hypothetical document for an incoming query, which is then embedded and used to look up real documents that are similar to the hypothetical document.</p> <p>Official Paper</p> HyDE: Source <p></p> <p>Here\u2019s a code snippet for using HyDE with Langchain:</p> <pre><code>from langchain.llms import OpenAI\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain, HypotheticalDocumentEmbedder\nfrom langchain.vectorstores import LanceDB\n\n# set OPENAI_API_KEY as env variable before this step\n# initialize LLM and embedding function\nllm = OpenAI()\nemebeddings = OpenAIEmbeddings()\n\n# HyDE embedding\nembeddings = HypotheticalDocumentEmbedder(llm_chain=llm_chain,base_embeddings=embeddings)\n\n# load dataset\n\n# LanceDB retriever\nretriever = LanceDB.from_documents(documents, embeddings, connection=table)\n\n# prompt template\nprompt_template = \"\"\"\nAs a knowledgeable and helpful research assistant, your task is to provide informative answers based on the given context. Use your extensive knowledge base to offer clear, concise, and accurate responses to the user's inquiries.\nif quetion is not related to documents simply say you dont know\nQuestion: {question}\n\nAnswer:\n\"\"\"\n\nprompt = PromptTemplate(input_variables=[\"question\"], template=prompt_template)\n\n# LLM Chain\nllm_chain = LLMChain(llm=llm, prompt=prompt)\n\n# vector search\nretriever.similarity_search(query)\nllm_chain.run(query)\n</code></pre> <p></p>"},{"location":"reranking/","title":"Reranking in LanceDB","text":"<p>Reranking is the process of reordering a list of items based on some criteria. In the context of search, reranking is used to reorder the search results returned by a search engine based on some criteria. This can be useful when the initial ranking of the search results is not satisfactory or when the user has provided additional information that can be used to improve the ranking of the search results.</p> <p>LanceDB comes with some built-in rerankers. Some of the rerankers that are available in LanceDB are:</p> Reranker Description Supported Query Types <code>LinearCombinationReranker</code> Reranks search results based on a linear combination of FTS and vector search scores Hybrid <code>CohereReranker</code> Uses cohere Reranker API to rerank results Vector, FTS, Hybrid <code>CrossEncoderReranker</code> Uses a cross-encoder model to rerank search results Vector, FTS, Hybrid <code>ColbertReranker</code> Uses a colbert model to rerank search results Vector, FTS, Hybrid <code>OpenaiReranker</code>(Experimental) Uses OpenAI's chat model to rerank search results Vector, FTS, Hybrid <code>VoyageAIReranker</code> Uses voyageai Reranker API to rerank results Vector, FTS, Hybrid"},{"location":"reranking/#using-a-reranker","title":"Using a Reranker","text":"<p>Using rerankers is optional for vector and FTS. However, for hybrid search, rerankers are required. To use a reranker, you need to create an instance of the reranker and pass it to the <code>rerank</code> method of the query builder:</p> <pre><code>import lancedb\nfrom lancedb.embeddings import get_registry\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.rerankers import CohereReranker\n\nembedder = get_registry().get(\"sentence-transformers\").create()\ndb = lancedb.connect(\"~/.lancedb\")\n\nclass Schema(LanceModel):\n    text: str = embedder.SourceField()\n    vector: Vector(embedder.ndims()) = embedder.VectorField()\n\ndata = [\n    {\"text\": \"hello world\"},\n    {\"text\": \"goodbye world\"}\n    ]\ntbl = db.create_table(\"test\", data)\nreranker = CohereReranker(api_key=\"your_api_key\")\n\n# Run vector search with a reranker\nresult = tbl.search(\"hello\").rerank(reranker).to_list() \n\n# Run FTS search with a reranker\nresult = tbl.search(\"hello\", query_type=\"fts\").rerank(reranker).to_list()\n\n# Run hybrid search with a reranker\ntbl.create_fts_index(\"text\")\nresult = tbl.search(\"hello\", query_type=\"hybrid\").rerank(reranker).to_list()\n</code></pre>"},{"location":"reranking/#multi-vector-reranking","title":"Multi-vector reranking","text":"<p>Most rerankers support reranking based on multiple vectors. To rerank based on multiple vectors, you can pass a list of vectors to the <code>rerank</code> method. Here's an example of how to rerank based on multiple vector columns using the <code>CrossEncoderReranker</code>:</p> <pre><code>from lancedb.rerankers import CrossEncoderReranker\n\nreranker = CrossEncoderReranker()\n\nquery = \"hello\"\n\nres1 = table.search(query, vector_column_name=\"vector\").limit(3)\nres2 = table.search(query, vector_column_name=\"text_vector\").limit(3)\nres3 = table.search(query, vector_column_name=\"meta_vector\").limit(3)\n\nreranked = reranker.rerank_multivector([res1, res2, res3],  deduplicate=True)\n</code></pre>"},{"location":"reranking/#available-rerankers","title":"Available Rerankers","text":"<p>LanceDB comes with the following built-in rerankers:</p> <ul> <li>Cohere Reranker</li> <li>Cross Encoder Reranker</li> <li>ColBERT Reranker</li> <li>OpenAI Reranker</li> <li>Linear Combination Reranker</li> <li>Jina Reranker</li> <li>AnswerDotAI Rerankers</li> <li>Reciprocal Rank Fusion Reranker</li> <li>VoyageAI Reranker</li> </ul>"},{"location":"reranking/#creating-custom-rerankers","title":"Creating Custom Rerankers","text":"<p>LanceDB also you to create custom rerankers by extending the base <code>Reranker</code> class. The custom reranker should implement the <code>rerank</code> method that takes a list of search results and returns a reranked list of search results. This is covered in more detail in the Creating Custom Rerankers section.</p>"},{"location":"reranking/answerdotai/","title":"AnswersDotAI Rerankers in LanceDB","text":"<p>This integration uses AnswersDotAI's rerankers to rerank the search results, providing a lightweight, low-dependency, unified API to use all common reranking and cross-encoder models.</p> <p>Note</p> <p>Supported Query Types: Hybrid, Vector, FTS</p> <pre><code>import numpy\nimport lancedb\nfrom lancedb.embeddings import get_registry\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.rerankers import AnswerdotaiRerankers\n\nembedder = get_registry().get(\"sentence-transformers\").create()\ndb = lancedb.connect(\"~/.lancedb\")\n\nclass Schema(LanceModel):\n    text: str = embedder.SourceField()\n    vector: Vector(embedder.ndims()) = embedder.VectorField()\n\ndata = [\n    {\"text\": \"hello world\"},\n    {\"text\": \"goodbye world\"}\n    ]\ntbl = db.create_table(\"test\", schema=Schema, mode=\"overwrite\")\ntbl.add(data)\nreranker = AnswerdotaiRerankers()\n\n# Run vector search with a reranker\nresult = tbl.search(\"hello\").rerank(reranker=reranker).to_list() \n\n# Run FTS search with a reranker\nresult = tbl.search(\"hello\", query_type=\"fts\").rerank(reranker=reranker).to_list()\n\n# Run hybrid search with a reranker\ntbl.create_fts_index(\"text\", replace=True)\nresult = tbl.search(\"hello\", query_type=\"hybrid\").rerank(reranker=reranker).to_list()\n</code></pre>"},{"location":"reranking/answerdotai/#accepted-arguments","title":"Accepted Arguments","text":"Argument Type Default Description <code>model_type</code> <code>str</code> <code>\"colbert\"</code> The type of model to use. Supported model types can be found here: https://github.com/AnswerDotAI/rerankers. <code>model_name</code> <code>str</code> <code>\"answerdotai/answerai-colbert-small-v1\"</code> The name of the reranker model to use. <code>column</code> <code>str</code> <code>\"text\"</code> The name of the column to use as input to the cross encoder model. <code>return_score</code> str <code>\"relevance\"</code> Options are \"relevance\" or \"all\". The type of score to return. If \"relevance\", will return only the `_relevance_score. If \"all\" is supported, will return relevance score along with the vector and/or fts scores depending on query type."},{"location":"reranking/answerdotai/#supported-scores-for-each-query-type","title":"Supported Scores for each query type","text":"<p>You can specify the type of scores you want the reranker to return. The following are the supported scores for each query type:</p>"},{"location":"reranking/answerdotai/#hybrid-search","title":"Hybrid Search","text":"<code>return_score</code> Status Description <code>relevance</code> \u2705 Supported Results only have the <code>_relevance_score</code> column. <code>all</code> \u274c Not Supported Results have vector(<code>_distance</code>) and FTS(<code>score</code>) along with Hybrid Search score(<code>_relevance_score</code>)."},{"location":"reranking/answerdotai/#vector-search","title":"Vector Search","text":"<code>return_score</code> Status Description <code>relevance</code> \u2705 Supported Results only have the <code>_relevance_score</code> column. <code>all</code> \u2705 Supported Results have vector(<code>_distance</code>) along with Hybrid Search score(<code>_relevance_score</code>)."},{"location":"reranking/answerdotai/#fts-search","title":"FTS Search","text":"<code>return_score</code> Status Description <code>relevance</code> \u2705 Supported Results only have the <code>_relevance_score</code> column. <code>all</code> \u2705 Supported Results have FTS(<code>score</code>) along with Hybrid Search score(<code>_relevance_score</code>)."},{"location":"reranking/cohere/","title":"Cohere Reranker in LanceDB","text":"<p>This reranker uses the Cohere API to rerank the search results. You can use this reranker by passing <code>CohereReranker()</code> to the <code>rerank()</code> method. Note that you'll either need to set the <code>COHERE_API_KEY</code> environment variable or pass the <code>api_key</code> argument to use this reranker.</p> <p>Note</p> <p>Supported Query Types: Hybrid, Vector, FTS</p> <pre><code>pip install cohere\n</code></pre> <pre><code>import numpy\nimport lancedb\nfrom lancedb.embeddings import get_registry\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.rerankers import CohereReranker\n\nembedder = get_registry().get(\"sentence-transformers\").create()\ndb = lancedb.connect(\"~/.lancedb\")\n\nclass Schema(LanceModel):\n    text: str = embedder.SourceField()\n    vector: Vector(embedder.ndims()) = embedder.VectorField()\n\ndata = [\n    {\"text\": \"hello world\"},\n    {\"text\": \"goodbye world\"}\n    ]\ntbl = db.create_table(\"test\", schema=Schema, mode=\"overwrite\")\ntbl.add(data)\nreranker = CohereReranker(api_key=\"key\")\n\n# Run vector search with a reranker\nresult = tbl.search(\"hello\").rerank(reranker=reranker).to_list() \n\n# Run FTS search with a reranker\nresult = tbl.search(\"hello\", query_type=\"fts\").rerank(reranker=reranker).to_list()\n\n# Run hybrid search with a reranker\ntbl.create_fts_index(\"text\", replace=True)\nresult = tbl.search(\"hello\", query_type=\"hybrid\").rerank(reranker=reranker).to_list()\n</code></pre>"},{"location":"reranking/cohere/#accepted-arguments","title":"Accepted Arguments","text":"Argument Type Default Description <code>model_name</code> <code>str</code> <code>\"rerank-english-v2.0\"</code> The name of the reranker model to use. Available cohere models are: rerank-english-v2.0, rerank-multilingual-v2.0 <code>column</code> <code>str</code> <code>\"text\"</code> The name of the column to use as input to the cross encoder model. <code>top_n</code> <code>str</code> <code>None</code> The number of results to return. If None, will return all results. <code>api_key</code> <code>str</code> <code>None</code> The API key for the Cohere API. If not provided, the <code>COHERE_API_KEY</code> environment variable is used. <code>return_score</code> str <code>\"relevance\"</code> Options are \"relevance\" or \"all\". The type of score to return. If \"relevance\", will return only the `_relevance_score. If \"all\" is supported, will return relevance score along with the vector and/or fts scores depending on query type"},{"location":"reranking/cohere/#supported-scores-for-each-query-type","title":"Supported Scores for each query type","text":"<p>You can specify the type of scores you want the reranker to return. The following are the supported scores for each query type:</p>"},{"location":"reranking/cohere/#hybrid-search","title":"Hybrid Search","text":"<code>return_score</code> Status Description <code>relevance</code> \u2705 Supported Results only have the <code>_relevance_score</code> column <code>all</code> \u274c Not Supported Results have vector(<code>_distance</code>) and FTS(<code>score</code>) along with Hybrid Search score(<code>_relevance_score</code>)"},{"location":"reranking/cohere/#vector-search","title":"Vector Search","text":"<code>return_score</code> Status Description <code>relevance</code> \u2705 Supported Results only have the <code>_relevance_score</code> column <code>all</code> \u2705 Supported Results have vector(<code>_distance</code>) along with Hybrid Search score(<code>_relevance_score</code>)"},{"location":"reranking/cohere/#fts-search","title":"FTS Search","text":"<code>return_score</code> Status Description <code>relevance</code> \u2705 Supported Results only have the <code>_relevance_score</code> column <code>all</code> \u2705 Supported Results have FTS(<code>score</code>) along with Hybrid Search score(<code>_relevance_score</code>)"},{"location":"reranking/colbert/","title":"ColBERT Reranker in LanceDB","text":"<p>This reranker uses ColBERT model to rerank the search results. You can use this reranker by passing <code>ColbertReranker()</code> to the <code>rerank()</code> method. </p> <p>Note</p> <p>Supported Query Types: Hybrid, Vector, FTS</p> <pre><code>import numpy\nimport lancedb\nfrom lancedb.embeddings import get_registry\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.rerankers import ColbertReranker\n\nembedder = get_registry().get(\"sentence-transformers\").create()\ndb = lancedb.connect(\"~/.lancedb\")\n\nclass Schema(LanceModel):\n    text: str = embedder.SourceField()\n    vector: Vector(embedder.ndims()) = embedder.VectorField()\n\ndata = [\n    {\"text\": \"hello world\"},\n    {\"text\": \"goodbye world\"}\n    ]\ntbl = db.create_table(\"test\", schema=Schema, mode=\"overwrite\")\ntbl.add(data)\nreranker = ColbertReranker()\n\n# Run vector search with a reranker\nresult = tbl.search(\"hello\").rerank(reranker=reranker).to_list() \n\n# Run FTS search with a reranker\nresult = tbl.search(\"hello\", query_type=\"fts\").rerank(reranker=reranker).to_list()\n\n# Run hybrid search with a reranker\ntbl.create_fts_index(\"text\", replace=True)\nresult = tbl.search(\"hello\", query_type=\"hybrid\").rerank(reranker=reranker).to_list()\n</code></pre>"},{"location":"reranking/colbert/#accepted-arguments","title":"Accepted Arguments","text":"Argument Type Default Description <code>model_name</code> <code>str</code> <code>\"colbert-ir/colbertv2.0\"</code> The name of the reranker model to use. <code>column</code> <code>str</code> <code>\"text\"</code> The name of the column to use as input to the cross encoder model. <code>device</code> <code>str</code> <code>None</code> The device to use for the cross encoder model. If None, will use \"cuda\" if available, otherwise \"cpu\". <code>return_score</code> str <code>\"relevance\"</code> Options are \"relevance\" or \"all\". The type of score to return. If \"relevance\", will return only the `_relevance_score. If \"all\" is supported, will return relevance score along with the vector and/or fts scores depending on query type."},{"location":"reranking/colbert/#supported-scores-for-each-query-type","title":"Supported Scores for each query type","text":"<p>You can specify the type of scores you want the reranker to return. The following are the supported scores for each query type:</p>"},{"location":"reranking/colbert/#hybrid-search","title":"Hybrid Search","text":"<code>return_score</code> Status Description <code>relevance</code> \u2705 Supported Results only have the <code>_relevance_score</code> column. <code>all</code> \u274c Not Supported Results have vector(<code>_distance</code>) and FTS(<code>score</code>) along with Hybrid Search score(<code>_relevance_score</code>)."},{"location":"reranking/colbert/#vector-search","title":"Vector Search","text":"<code>return_score</code> Status Description <code>relevance</code> \u2705 Supported Results only have the <code>_relevance_score</code> column. <code>all</code> \u2705 Supported Results have vector(<code>_distance</code>) along with Hybrid Search score(<code>_relevance_score</code>)."},{"location":"reranking/colbert/#fts-search","title":"FTS Search","text":"<code>return_score</code> Status Description <code>relevance</code> \u2705 Supported Results only have the <code>_relevance_score</code> column. <code>all</code> \u2705 Supported Results have FTS(<code>score</code>) along with Hybrid Search score(<code>_relevance_score</code>)."},{"location":"reranking/cross_encoder/","title":"Cross Encoder Reranker in LanceDB","text":"<p>This reranker uses Cross Encoder models from sentence-transformers to rerank the search results. You can use this reranker by passing <code>CrossEncoderReranker()</code> to the <code>rerank()</code> method. </p> <p>Note</p> <p>Supported Query Types: Hybrid, Vector, FTS</p> <pre><code>import numpy\nimport lancedb\nfrom lancedb.embeddings import get_registry\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.rerankers import CrossEncoderReranker\n\nembedder = get_registry().get(\"sentence-transformers\").create()\ndb = lancedb.connect(\"~/.lancedb\")\n\nclass Schema(LanceModel):\n    text: str = embedder.SourceField()\n    vector: Vector(embedder.ndims()) = embedder.VectorField()\n\ndata = [\n    {\"text\": \"hello world\"},\n    {\"text\": \"goodbye world\"}\n    ]\ntbl = db.create_table(\"test\", schema=Schema, mode=\"overwrite\")\ntbl.add(data)\nreranker = CrossEncoderReranker()\n\n# Run vector search with a reranker\nresult = tbl.search(\"hello\").rerank(reranker=reranker).to_list() \n\n# Run FTS search with a reranker\nresult = tbl.search(\"hello\", query_type=\"fts\").rerank(reranker=reranker).to_list()\n\n# Run hybrid search with a reranker\ntbl.create_fts_index(\"text\", replace=True)\nresult = tbl.search(\"hello\", query_type=\"hybrid\").rerank(reranker=reranker).to_list()\n</code></pre>"},{"location":"reranking/cross_encoder/#accepted-arguments","title":"Accepted Arguments","text":"Argument Type Default Description <code>model_name</code> <code>str</code> <code>\"\"cross-encoder/ms-marco-TinyBERT-L-6\"</code> The name of the reranker model to use. <code>column</code> <code>str</code> <code>\"text\"</code> The name of the column to use as input to the cross encoder model. <code>device</code> <code>str</code> <code>None</code> The device to use for the cross encoder model. If None, will use \"cuda\" if available, otherwise \"cpu\". <code>return_score</code> str <code>\"relevance\"</code> Options are \"relevance\" or \"all\". The type of score to return. If \"relevance\", will return only the `_relevance_score. If \"all\" is supported, will return relevance score along with the vector and/or fts scores depending on query type."},{"location":"reranking/cross_encoder/#supported-scores-for-each-query-type","title":"Supported Scores for each query type","text":"<p>You can specify the type of scores you want the reranker to return. The following are the supported scores for each query type:</p>"},{"location":"reranking/cross_encoder/#hybrid-search","title":"Hybrid Search","text":"<code>return_score</code> Status Description <code>relevance</code> \u2705 Supported Results only have the <code>_relevance_score</code> column. <code>all</code> \u274c Not Supported Results have vector(<code>_distance</code>) and FTS(<code>score</code>) along with Hybrid Search score(<code>_relevance_score</code>)."},{"location":"reranking/cross_encoder/#vector-search","title":"Vector Search","text":"<code>return_score</code> Status Description <code>relevance</code> \u2705 Supported Results only have the <code>_relevance_score</code> column. <code>all</code> \u2705 Supported Results have vector(<code>_distance</code>) along with Hybrid Search score(<code>_relevance_score</code>)."},{"location":"reranking/cross_encoder/#fts-search","title":"FTS Search","text":"<code>return_score</code> Status Description <code>relevance</code> \u2705 Supported Results only have the <code>_relevance_score</code> column. <code>all</code> \u2705 Supported Results have FTS(<code>score</code>) along with Hybrid Search score(<code>_relevance_score</code>)."},{"location":"reranking/custom_reranker/","title":"Building Custom Rerankers in LanceDB","text":"<p>You can build your own custom reranker by subclassing the <code>Reranker</code> class and implementing the <code>rerank_hybrid()</code> method. Optionally, you can also implement the <code>rerank_vector()</code> and <code>rerank_fts()</code> methods if you want to support reranking for vector and FTS search separately.</p> <p>The <code>Reranker</code> base interface comes with a <code>merge_results()</code> method that can be used to combine the results of semantic and full-text search. This is a vanilla merging algorithm that simply concatenates the results and removes the duplicates without taking the scores into consideration. It only keeps the first copy of the row encountered. This works well in cases that don't require the scores of semantic and full-text search to combine the results. If you want to use the scores or want to support <code>return_score=\"all\"</code>, you'll need to implement your own merging algorithm.</p> <p>Here's an example of a custom reranker that combines the results of semantic and full-text search using a linear combination of the scores:</p> <pre><code>from lancedb.rerankers import Reranker\nimport pyarrow as pa\n\nclass MyReranker(Reranker):\n    def __init__(self, param1, param2, ..., return_score=\"relevance\"):\n        super().__init__(return_score)\n        self.param1 = param1\n        self.param2 = param2\n\n    def rerank_hybrid(self, query: str, vector_results: pa.Table, fts_results: pa.Table):\n        # Use the built-in merging function\n        combined_result = self.merge_results(vector_results, fts_results)\n\n        # Do something with the combined results\n        # ...\n\n        # Return the combined results\n        return combined_result\n\n    def rerank_vector(self, query: str, vector_results: pa.Table):\n        # Do something with the vector results\n        # ...\n\n        # Return the vector results\n        return vector_results\n\n    def rerank_fts(self, query: str, fts_results: pa.Table):\n        # Do something with the FTS results\n        # ...\n\n        # Return the FTS results\n        return fts_results\n</code></pre>"},{"location":"reranking/custom_reranker/#example-of-a-custom-reranker","title":"Example of a Custom Reranker","text":"<p>For the sake of simplicity let's build custom reranker that enhances the Cohere Reranker by accepting a filter query, and accepts other CohereReranker params as kwargs.</p> <pre><code>from typing import List, Union\nimport pandas as pd\nfrom lancedb.rerankers import CohereReranker\n\nclass ModifiedCohereReranker(CohereReranker):\n    def __init__(self, filters: Union[str, List[str]], **kwargs):\n        super().__init__(**kwargs)\n        filters = filters if isinstance(filters, list) else [filters]\n        self.filters = filters\n\n    def rerank_hybrid(self, query: str, vector_results: pa.Table, fts_results: pa.Table)-&gt; pa.Table:\n        combined_result = super().rerank_hybrid(query, vector_results, fts_results)\n        df = combined_result.to_pandas()\n        for filter in self.filters:\n            df = df.query(\"not text.str.contains(@filter)\")\n\n        return pa.Table.from_pandas(df)\n\n    def rerank_vector(self, query: str, vector_results: pa.Table)-&gt; pa.Table:\n        vector_results = super().rerank_vector(query, vector_results)\n        df = vector_results.to_pandas()\n        for filter in self.filters:\n            df = df.query(\"not text.str.contains(@filter)\")\n\n        return pa.Table.from_pandas(df)\n\n    def rerank_fts(self, query: str, fts_results: pa.Table)-&gt; pa.Table:\n        fts_results = super().rerank_fts(query, fts_results)\n        df = fts_results.to_pandas()\n        for filter in self.filters:\n            df = df.query(\"not text.str.contains(@filter)\")\n\n        return pa.Table.from_pandas(df)\n</code></pre> <p>Tip</p> <p>The <code>vector_results</code> and <code>fts_results</code> are pyarrow tables. Lean more about pyarrow tables here. It can be converted to other data types like pandas dataframe, pydict, pylist etc.</p> <p>For example, You can convert them to pandas dataframes using <code>to_pandas()</code> method and perform any operations you want. After you are done, you can convert the dataframe back to pyarrow table using <code>pa.Table.from_pandas()</code> method and return it.</p>"},{"location":"reranking/eval/","title":"Hybrid Search","text":"<p>Hybrid Search is a broad (often misused) term. It can mean anything from combining multiple methods for searching, to applying ranking methods to better sort the results. In this blog, we use the definition of \"hybrid search\" to mean using a combination of keyword-based and vector search.</p>"},{"location":"reranking/eval/#the-challenge-of-reranking-search-results","title":"The challenge of (re)ranking search results","text":"<p>Once you have a group of the most relevant search results from multiple search sources, you'd likely standardize the score and rank them accordingly. This process can also be seen as another independent step:\u200areranking. There are two approaches for reranking search results from multiple sources.</p> <ul> <li> <p>Score-based: Calculate final relevance scores based on a weighted linear combination of individual search algorithm scores. Example:\u200aWeighted linear combination of semantic search &amp; keyword-based search results.</p> </li> <li> <p>Relevance-based: Discards the existing scores and calculates the relevance of each search result-query pair. Example:\u200aCross Encoder models</p> </li> </ul> <p>Even though there are many strategies for reranking search results, none works for all cases. Moreover, evaluating them itself is a challenge. Also, reranking can be dataset or application specific so it's hard to generalize.</p>"},{"location":"reranking/eval/#example-evaluation-of-hybrid-search-with-reranking","title":"Example evaluation of hybrid search with Reranking","text":"<p>Here's some evaluation numbers from an experiment comparing these rerankers on about 800 queries. It is modified version of an evaluation script from llama-index that measures hit-rate at top-k.</p> <p> With OpenAI ada2 embedding </p> <p>Vector Search baseline: <code>0.64</code></p> Reranker Top-3 Top-5 Top-10 Linear Combination <code>0.73</code> <code>0.74</code> <code>0.85</code> Cross Encoder <code>0.71</code> <code>0.70</code> <code>0.77</code> Cohere <code>0.81</code> <code>0.81</code> <code>0.85</code> ColBERT <code>0.68</code> <code>0.68</code> <code>0.73</code> <p> </p> <p> With OpenAI embedding-v3-small </p> <p>Vector Search baseline: <code>0.59</code></p> Reranker Top-3 Top-5 Top-10 Linear Combination <code>0.68</code> <code>0.70</code> <code>0.84</code> Cross Encoder <code>0.72</code> <code>0.72</code> <code>0.79</code> Cohere <code>0.79</code> <code>0.79</code> <code>0.84</code> ColBERT <code>0.70</code> <code>0.70</code> <code>0.76</code> <p> </p>"},{"location":"reranking/eval/#conclusion","title":"Conclusion","text":"<p>The results show that the reranking methods are able to improve the search results. However, the improvement is not consistent across all rerankers. The choice of reranker depends on the dataset and the application. It is also important to note that the reranking methods are not a replacement for the search methods. They are complementary and should be used together to get the best results. The speed to recall tradeoff is also an important factor to consider when choosing the reranker.</p>"},{"location":"reranking/jina/","title":"Jina Reranker in LanceDB","text":"<p>This reranker uses the Jina API to rerank the search results. You can use this reranker by passing <code>JinaReranker()</code> to the <code>rerank()</code> method. Note that you'll either need to set the <code>JINA_API_KEY</code> environment variable or pass the <code>api_key</code> argument to use this reranker.</p> <p>Note</p> <p>Supported Query Types: Hybrid, Vector, FTS</p> <pre><code>import os\nimport lancedb\nfrom lancedb.embeddings import get_registry\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.rerankers import JinaReranker\n\nos.environ['JINA_API_KEY'] = \"jina_*\"\n\n\nembedder = get_registry().get(\"jina\").create()\ndb = lancedb.connect(\"~/.lancedb\")\n\nclass Schema(LanceModel):\n    text: str = embedder.SourceField()\n    vector: Vector(embedder.ndims()) = embedder.VectorField()\n\ndata = [\n    {\"text\": \"hello world\"},\n    {\"text\": \"goodbye world\"}\n    ]\ntbl = db.create_table(\"test\", schema=Schema, mode=\"overwrite\")\ntbl.add(data)\nreranker = JinaReranker(api_key=\"key\")\n\n# Run vector search with a reranker\nresult = tbl.search(\"hello\").rerank(reranker=reranker).to_list() \n\n# Run FTS search with a reranker\nresult = tbl.search(\"hello\", query_type=\"fts\").rerank(reranker=reranker).to_list()\n\n# Run hybrid search with a reranker\ntbl.create_fts_index(\"text\", replace=True)\nresult = tbl.search(\"hello\", query_type=\"hybrid\").rerank(reranker=reranker).to_list()\n</code></pre>"},{"location":"reranking/jina/#accepted-arguments","title":"Accepted Arguments","text":"Argument Type Default Description <code>model_name</code> <code>str</code> <code>\"jina-reranker-v2-base-multilingual\"</code> The name of the reranker model to use. You can find the list of available models in https://jina.ai/reranker. <code>column</code> <code>str</code> <code>\"text\"</code> The name of the column to use as input to the cross encoder model. <code>top_n</code> <code>str</code> <code>None</code> The number of results to return. If None, will return all results. <code>api_key</code> <code>str</code> <code>None</code> The API key for the Jina API. If not provided, the <code>JINA_API_KEY</code> environment variable is used. <code>return_score</code> str <code>\"relevance\"</code> Options are \"relevance\" or \"all\". The type of score to return. If \"relevance\", will return only the `_relevance_score. If \"all\" is supported, will return relevance score along with the vector and/or fts scores depending on query type."},{"location":"reranking/jina/#supported-scores-for-each-query-type","title":"Supported Scores for each query type","text":"<p>You can specify the type of scores you want the reranker to return. The following are the supported scores for each query type:</p>"},{"location":"reranking/jina/#hybrid-search","title":"Hybrid Search","text":"<code>return_score</code> Status Description <code>relevance</code> \u2705 Supported Results only have the <code>_relevance_score</code> column. <code>all</code> \u274c Not Supported Results have vector(<code>_distance</code>) and FTS(<code>score</code>) along with Hybrid Search score(<code>_relevance_score</code>)."},{"location":"reranking/jina/#vector-search","title":"Vector Search","text":"<code>return_score</code> Status Description <code>relevance</code> \u2705 Supported Results only have the <code>_relevance_score</code> column. <code>all</code> \u2705 Supported Results have vector(<code>_distance</code>) along with Hybrid Search score(<code>_relevance_score</code>)."},{"location":"reranking/jina/#fts-search","title":"FTS Search","text":"<code>return_score</code> Status Description <code>relevance</code> \u2705 Supported Results only have the <code>_relevance_score</code> column. <code>all</code> \u2705 Supported Results have FTS(<code>score</code>) along with Hybrid Search score(<code>_relevance_score</code>)."},{"location":"reranking/linear_combination/","title":"Linear Combination Reranker","text":"<p>title: Linear Combination Reranker in LanceDB | Score-Based Search Optimization description: Learn about LanceDB's deprecated Linear Combination Reranker for combining semantic and full-text search scores. Note: RRFReranker is recommended for score-based reranking.</p>"},{"location":"reranking/linear_combination/#linear-combination-reranker-in-lancedb","title":"Linear Combination Reranker in LanceDB","text":"<p>Note</p> <p>This is deprecated. It is recommended to use the <code>RRFReranker</code> instead, if you want to use a score-based reranker.</p> <p>The Linear Combination Reranker combines the results of semantic and full-text search using a linear combination of the scores. The weights for the linear combination can be specified, and defaults to 0.7, i.e, 70% weight for semantic search and 30% weight for full-text search.</p> <p>Note</p> <p>Supported Query Types: Hybrid</p> <pre><code>import numpy\nimport lancedb\nfrom lancedb.embeddings import get_registry\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.rerankers import LinearCombinationReranker\n\nembedder = get_registry().get(\"sentence-transformers\").create()\ndb = lancedb.connect(\"~/.lancedb\")\n\nclass Schema(LanceModel):\n    text: str = embedder.SourceField()\n    vector: Vector(embedder.ndims()) = embedder.VectorField()\n\ndata = [\n    {\"text\": \"hello world\"},\n    {\"text\": \"goodbye world\"}\n    ]\ntbl = db.create_table(\"test\", schema=Schema, mode=\"overwrite\")\ntbl.add(data)\nreranker = LinearCombinationReranker()\n\n# Run hybrid search with a reranker\ntbl.create_fts_index(\"text\", replace=True)\nresult = tbl.search(\"hello\", query_type=\"hybrid\").rerank(reranker=reranker).to_list()\n</code></pre>"},{"location":"reranking/linear_combination/#accepted-arguments","title":"Accepted Arguments","text":"Argument Type Default Description <code>weight</code> <code>float</code> <code>0.7</code> The weight to use for the semantic search score. The weight for the full-text search score is <code>1 - weights</code>. <code>return_score</code> str <code>\"relevance\"</code> Options are \"relevance\" or \"all\". The type of score to return. If \"relevance\", will return only the `_relevance_score. If \"all\", will return all scores from the vector and FTS search along with the relevance score."},{"location":"reranking/linear_combination/#supported-scores-for-each-query-type","title":"Supported Scores for each query type","text":"<p>You can specify the type of scores you want the reranker to return. The following are the supported scores for each query type:</p>"},{"location":"reranking/linear_combination/#hybrid-search","title":"Hybrid Search","text":"<code>return_score</code> Status Description <code>relevance</code> \u2705 Supported Results only have the <code>_relevance_score</code> column <code>all</code> \u2705 Supported Results have vector(<code>_distance</code>) and FTS(<code>score</code>) along with Hybrid Search score(<code>_distance</code>)"},{"location":"reranking/openai/","title":"OpenAI Reranker (Experimental) in LanceDB","text":"<p>This reranker uses OpenAI chat model to rerank the search results. You can use this reranker by passing <code>OpenAI()</code> to the <code>rerank()</code> method. </p> <p>Note</p> <p>Supported Query Types: Hybrid, Vector, FTS</p> <p>Warning</p> <p>This reranker is experimental. OpenAI doesn't have a dedicated reranking model, so we are using the chat model for reranking. </p> <pre><code>import numpy\nimport lancedb\nfrom lancedb.embeddings import get_registry\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.rerankers import OpenaiReranker\n\nembedder = get_registry().get(\"sentence-transformers\").create()\ndb = lancedb.connect(\"~/.lancedb\")\n\nclass Schema(LanceModel):\n    text: str = embedder.SourceField()\n    vector: Vector(embedder.ndims()) = embedder.VectorField()\n\ndata = [\n    {\"text\": \"hello world\"},\n    {\"text\": \"goodbye world\"}\n    ]\ntbl = db.create_table(\"test\", schema=Schema, mode=\"overwrite\")\ntbl.add(data)\nreranker = OpenaiReranker()\n\n# Run vector search with a reranker\nresult = tbl.search(\"hello\").rerank(reranker=reranker).to_list() \n\n# Run FTS search with a reranker\nresult = tbl.search(\"hello\", query_type=\"fts\").rerank(reranker=reranker).to_list()\n\n# Run hybrid search with a reranker\ntbl.create_fts_index(\"text\", replace=True)\nresult = tbl.search(\"hello\", query_type=\"hybrid\").rerank(reranker=reranker).to_list()\n</code></pre>"},{"location":"reranking/openai/#accepted-arguments","title":"Accepted Arguments","text":"Argument Type Default Description <code>model_name</code> <code>str</code> <code>\"gpt-4-turbo-preview\"</code> The name of the reranker model to use. <code>column</code> <code>str</code> <code>\"text\"</code> The name of the column to use as input to the cross encoder model. <code>return_score</code> str <code>\"relevance\"</code> Options are \"relevance\" or \"all\". The type of score to return. If \"relevance\", will return only the `_relevance_score. If \"all\" is supported, will return relevance score along with the vector and/or fts scores depending on query type. <code>api_key</code> str <code>None</code> The API key to use. If None, will use the OPENAI_API_KEY environment variable."},{"location":"reranking/openai/#supported-scores-for-each-query-type","title":"Supported Scores for each query type","text":"<p>You can specify the type of scores you want the reranker to return. The following are the supported scores for each query type:</p>"},{"location":"reranking/openai/#hybrid-search","title":"Hybrid Search","text":"<code>return_score</code> Status Description <code>relevance</code> \u2705 Supported Results only have the <code>_relevance_score</code> column. <code>all</code> \u274c Not Supported Results have vector(<code>_distance</code>) and FTS(<code>score</code>) along with Hybrid Search score(<code>_relevance_score</code>)."},{"location":"reranking/openai/#vector-search","title":"Vector Search","text":"<code>return_score</code> Status Description <code>relevance</code> \u2705 Supported Results only have the <code>_relevance_score</code> column. <code>all</code> \u2705 Supported Results have vector(<code>_distance</code>) along with Hybrid Search score(<code>_relevance_score</code>)."},{"location":"reranking/openai/#fts-search","title":"FTS Search","text":"<code>return_score</code> Status Description <code>relevance</code> \u2705 Supported Results only have the <code>_relevance_score</code> column. <code>all</code> \u2705 Supported Results have FTS(<code>score</code>) along with Hybrid Search score(<code>_relevance_score</code>)."},{"location":"reranking/rrf/","title":"Reciprocal Rank Fusion Reranker in LanceDB","text":"<p>This is the default reranker used by LanceDB hybrid search. Reciprocal Rank Fusion (RRF) is an algorithm that evaluates the search scores by leveraging the positions/rank of the documents. The implementation follows this paper.</p> <p>Note</p> <p>Supported Query Types: Hybrid</p> <pre><code>import numpy\nimport lancedb\nfrom lancedb.embeddings import get_registry\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.rerankers import RRFReranker\n\nembedder = get_registry().get(\"sentence-transformers\").create()\ndb = lancedb.connect(\"~/.lancedb\")\n\nclass Schema(LanceModel):\n    text: str = embedder.SourceField()\n    vector: Vector(embedder.ndims()) = embedder.VectorField()\n\ndata = [\n    {\"text\": \"hello world\"},\n    {\"text\": \"goodbye world\"}\n    ]\ntbl = db.create_table(\"test\", schema=Schema, mode=\"overwrite\")\ntbl.add(data)\nreranker = RRFReranker()\n\n# Run hybrid search with a reranker\ntbl.create_fts_index(\"text\", replace=True)\nresult = tbl.search(\"hello\", query_type=\"hybrid\").rerank(reranker=reranker).to_list()\n</code></pre>"},{"location":"reranking/rrf/#accepted-arguments","title":"Accepted Arguments","text":"Argument Type Default Description <code>K</code> <code>int</code> <code>60</code> A constant used in the RRF formula (default is 60). Experiments indicate that k = 60 was near-optimal, but that the choice is not critical. <code>return_score</code> str <code>\"relevance\"</code> Options are \"relevance\" or \"all\". The type of score to return. If \"relevance\", will return only the <code>_relevance_score</code>. If \"all\", will return all scores from the vector and FTS search along with the relevance score."},{"location":"reranking/rrf/#supported-scores-for-each-query-type","title":"Supported Scores for each query type","text":"<p>You can specify the type of scores you want the reranker to return. The following are the supported scores for each query type:</p>"},{"location":"reranking/rrf/#hybrid-search","title":"Hybrid Search","text":"<code>return_score</code> Status Description <code>relevance</code> \u2705 Supported Returned rows only have the <code>_relevance_score</code> column. <code>all</code> \u2705 Supported Returned rows have vector(<code>_distance</code>) and FTS(<code>score</code>) along with Hybrid Search score(<code>_relevance_score</code>)."},{"location":"reranking/voyageai/","title":"Voyage AI Reranker in LanceDB","text":"<p>Voyage AI provides cutting-edge embedding and rerankers.</p> <p>This reranker uses the VoyageAI API to rerank the search results. You can use this reranker by passing <code>VoyageAIReranker()</code> to the <code>rerank()</code> method. Note that you'll either need to set the <code>VOYAGE_API_KEY</code> environment variable or pass the <code>api_key</code> argument to use this reranker.</p> <p>Note</p> <p>Supported Query Types: Hybrid, Vector, FTS</p> <pre><code>import numpy\nimport lancedb\nfrom lancedb.embeddings import get_registry\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.rerankers import VoyageAIReranker\n\nembedder = get_registry().get(\"sentence-transformers\").create()\ndb = lancedb.connect(\"~/.lancedb\")\n\nclass Schema(LanceModel):\n    text: str = embedder.SourceField()\n    vector: Vector(embedder.ndims()) = embedder.VectorField()\n\ndata = [\n    {\"text\": \"hello world\"},\n    {\"text\": \"goodbye world\"}\n    ]\ntbl = db.create_table(\"test\", schema=Schema, mode=\"overwrite\")\ntbl.add(data)\nreranker = VoyageAIReranker(model_name=\"rerank-2\")\n\n# Run vector search with a reranker\nresult = tbl.search(\"hello\").rerank(reranker=reranker).to_list() \n\n# Run FTS search with a reranker\nresult = tbl.search(\"hello\", query_type=\"fts\").rerank(reranker=reranker).to_list()\n\n# Run hybrid search with a reranker\ntbl.create_fts_index(\"text\", replace=True)\nresult = tbl.search(\"hello\", query_type=\"hybrid\").rerank(reranker=reranker).to_list()\n</code></pre>"},{"location":"reranking/voyageai/#accepted-arguments","title":"Accepted Arguments","text":"Argument Type Default Description <code>model_name</code> <code>str</code> <code>None</code> The name of the reranker model to use. Available models are: rerank-2, rerank-2-lite <code>column</code> <code>str</code> <code>\"text\"</code> The name of the column to use as input to the cross encoder model. <code>top_n</code> <code>str</code> <code>None</code> The number of results to return. If None, will return all results. <code>api_key</code> <code>str</code> <code>None</code> The API key for the Voyage AI API. If not provided, the <code>VOYAGE_API_KEY</code> environment variable is used. <code>return_score</code> str <code>\"relevance\"</code> Options are \"relevance\" or \"all\". The type of score to return. If \"relevance\", will return only the `_relevance_score. If \"all\" is supported, will return relevance score along with the vector and/or fts scores depending on query type <code>truncation</code> <code>bool</code> <code>None</code> Whether to truncate the input to satisfy the \"context length limit\" on the query and the documents."},{"location":"reranking/voyageai/#supported-scores-for-each-query-type","title":"Supported Scores for each query type","text":"<p>You can specify the type of scores you want the reranker to return. The following are the supported scores for each query type:</p>"},{"location":"reranking/voyageai/#hybrid-search","title":"Hybrid Search","text":"<code>return_score</code> Status Description <code>relevance</code> \u2705 Supported Returns only have the <code>_relevance_score</code> column <code>all</code> \u274c Not Supported Returns have vector(<code>_distance</code>) and FTS(<code>score</code>) along with Hybrid Search score(<code>_relevance_score</code>)"},{"location":"reranking/voyageai/#vector-search","title":"Vector Search","text":"<code>return_score</code> Status Description <code>relevance</code> \u2705 Supported Returns only have the <code>_relevance_score</code> column <code>all</code> \u2705 Supported Returns have vector(<code>_distance</code>) along with Hybrid Search score(<code>_relevance_score</code>)"},{"location":"reranking/voyageai/#fts-search","title":"FTS Search","text":"<code>return_score</code> Status Description <code>relevance</code> \u2705 Supported Returns only have the <code>_relevance_score</code> column <code>all</code> \u2705 Supported Returns have FTS(<code>score</code>) along with Hybrid Search score(<code>_relevance_score</code>)"},{"location":"studies/overview/","title":"LanceDB Performance Studies","text":"<p>This is a list of benchmarks and reports we've worked on at LanceDB. Some of these are continuously updated, while others are one-off reports.</p> <ul> <li>Improve retrievers with hybrid search and reranking</li> </ul>"}]}